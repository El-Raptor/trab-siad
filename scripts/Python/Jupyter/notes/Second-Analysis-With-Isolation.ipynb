{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "## Celso Antonio Uliana Junior\n",
    "## July 2 2020\n",
    "####\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#####\n",
    "## Consuming and shaping the data to analysis\n",
    "## Covid-19 numbers in Brazil by date\n",
    "## Isolation percentage in Brazil by date\n",
    "#####\n",
    "\n",
    "data_raw_covid = pd.read_csv(\"C:/Users/PCDOMILHAO/Documents/GitHub/trab-siad/scripts/Python/Jupyter/dados/covidBrasil.csv\", sep = \";\", decimal = \",\")\n",
    "data_raw_isolation = pd.read_csv(\"C:/Users/PCDOMILHAO/Documents/GitHub/trab-siad/scripts/Python/Jupyter/dados/isolamento.csv\", sep = \";\", decimal = \",\")\n",
    "data_covid = data_raw_covid['Data'].values.copy()\n",
    "data_covid = data_raw_covid.dropna().set_index(\"Data\")\n",
    "data_isolation = data_raw_isolation['Data'].values.copy()\n",
    "data_isolation = data_raw_isolation.dropna().set_index(\"Data\")\n",
    "\n",
    "####\n",
    "## Shaping a central pandas dataFrame for all our ML needs\n",
    "####\n",
    "\n",
    "data2 = data_covid\n",
    "data2['Taxa'] = data_isolation['Taxa'].values.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Casos</th>\n",
       "      <th>Taxa</th>\n",
       "      <th>CasosNormalizados</th>\n",
       "      <th>TaxaNormalizadas</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26/2/20</th>\n",
       "      <td>1</td>\n",
       "      <td>24.7</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>27.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>31.4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/3/20</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.461333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>27.7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>30.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/3/20</th>\n",
       "      <td>1</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6/3/20</th>\n",
       "      <td>5</td>\n",
       "      <td>28.5</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.101333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7/3/20</th>\n",
       "      <td>5</td>\n",
       "      <td>31.8</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.189333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.434667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9/3/20</th>\n",
       "      <td>12</td>\n",
       "      <td>29.2</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/3/20</th>\n",
       "      <td>9</td>\n",
       "      <td>28.3</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.096000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/3/20</th>\n",
       "      <td>18</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13/3/20</th>\n",
       "      <td>25</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14/3/20</th>\n",
       "      <td>21</td>\n",
       "      <td>35.7</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.293333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15/3/20</th>\n",
       "      <td>23</td>\n",
       "      <td>42.6</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.477333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16/3/20</th>\n",
       "      <td>79</td>\n",
       "      <td>32.1</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.197333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Casos  Taxa  CasosNormalizados  TaxaNormalizadas\n",
       "Data                                                     \n",
       "26/2/20      1  24.7           0.000018          0.000000\n",
       "27/2/20      0  27.5           0.000000          0.074667\n",
       "28/2/20      0  26.6           0.000000          0.050667\n",
       "29/2/20      0  31.4           0.000000          0.178667\n",
       "1/3/20       1    42           0.000018          0.461333\n",
       "2/3/20       0  27.7           0.000000          0.080000\n",
       "3/3/20       0    29           0.000000          0.114667\n",
       "4/3/20       0  30.2           0.000000          0.146667\n",
       "5/3/20       1  29.7           0.000018          0.133333\n",
       "6/3/20       5  28.5           0.000091          0.101333\n",
       "7/3/20       5  31.8           0.000091          0.189333\n",
       "8/3/20       0    41           0.000000          0.434667\n",
       "9/3/20      12  29.2           0.000219          0.120000\n",
       "10/3/20      0  29.7           0.000000          0.133333\n",
       "11/3/20      9  28.3           0.000164          0.096000\n",
       "12/3/20     18  30.1           0.000329          0.144000\n",
       "13/3/20     25  30.1           0.000456          0.144000\n",
       "14/3/20     21  35.7           0.000383          0.293333\n",
       "15/3/20     23  42.6           0.000420          0.477333\n",
       "16/3/20     79  32.1           0.001442          0.197333"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "####\n",
    "## normalizing values for both covid and isolation percentage \n",
    "## between range [0,1] using sklearn MinMaxScaler\n",
    "####\n",
    "\n",
    "covid_norm = data_covid[\"Casos\"].values.copy()\n",
    "covid_norm.shape = (len(covid_norm), 1)\n",
    "\n",
    "isolation_norm = data_isolation[\"Taxa\"].values.copy()\n",
    "isolation_norm.shape = (len(isolation_norm), 1)\n",
    "\n",
    "####\n",
    "## Shaping the central dataFrame with normalized values\n",
    "####\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "covid_norm = min_max_scaler.fit_transform(covid_norm)\n",
    "isolation_norm = min_max_scaler.fit_transform(isolation_norm)\n",
    "\n",
    "data2[\"CasosNormalizados\"] = covid_norm\n",
    "data2[\"TaxaNormalizadas\"] = isolation_norm\n",
    "data = data2.copy()\n",
    "#data = data.iloc[20:]\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               E0        E1        E2        E3        E4        E5        E6  \\\n",
      "Data                                                                            \n",
      "26/2/20  0.000018  0.000000  0.000000  0.000000  0.000018  0.000000  0.000000   \n",
      "27/2/20  0.000000  0.000000  0.000000  0.000018  0.000000  0.000000  0.000000   \n",
      "28/2/20  0.000000  0.000000  0.000018  0.000000  0.000000  0.000000  0.000018   \n",
      "29/2/20  0.000000  0.000018  0.000000  0.000000  0.000000  0.000018  0.000091   \n",
      "1/3/20   0.000018  0.000000  0.000000  0.000000  0.000018  0.000091  0.000091   \n",
      "...           ...       ...       ...       ...       ...       ...       ...   \n",
      "10/6/20  0.585912  0.600920  0.555257  0.474375  0.396268  0.312392  0.376970   \n",
      "11/6/20  0.600920  0.555257  0.474375  0.396268  0.312392  0.376970  0.637527   \n",
      "12/6/20  0.555257  0.474375  0.396268  0.312392  0.376970  0.637527  0.587683   \n",
      "13/6/20  0.474375  0.396268  0.312392  0.376970  0.637527  0.587683  0.415640   \n",
      "14/6/20  0.396268  0.312392  0.376970  0.637527  0.587683  0.415640  1.000000   \n",
      "\n",
      "               E7        E8        E9       E10       E11       E12       E13  \\\n",
      "Data                                                                            \n",
      "26/2/20  0.000000  0.074667  0.050667  0.178667  0.461333  0.080000  0.114667   \n",
      "27/2/20  0.074667  0.050667  0.178667  0.461333  0.080000  0.114667  0.146667   \n",
      "28/2/20  0.050667  0.178667  0.461333  0.080000  0.114667  0.146667  0.133333   \n",
      "29/2/20  0.178667  0.461333  0.080000  0.114667  0.146667  0.133333  0.101333   \n",
      "1/3/20   0.461333  0.080000  0.114667  0.146667  0.133333  0.101333  0.189333   \n",
      "...           ...       ...       ...       ...       ...       ...       ...   \n",
      "10/6/20  0.389333  0.506667  0.322667  0.410667  0.645333  0.381333  0.384000   \n",
      "11/6/20  0.506667  0.322667  0.410667  0.645333  0.381333  0.384000  0.336000   \n",
      "12/6/20  0.322667  0.410667  0.645333  0.381333  0.384000  0.336000  0.368000   \n",
      "13/6/20  0.410667  0.645333  0.381333  0.384000  0.336000  0.368000  0.266667   \n",
      "14/6/20  0.645333  0.381333  0.384000  0.336000  0.368000  0.266667  0.384000   \n",
      "\n",
      "              E15  \n",
      "Data               \n",
      "26/2/20  0.000000  \n",
      "27/2/20  0.000018  \n",
      "28/2/20  0.000091  \n",
      "29/2/20  0.000091  \n",
      "1/3/20   0.000000  \n",
      "...           ...  \n",
      "10/6/20  0.637527  \n",
      "11/6/20  0.587683  \n",
      "12/6/20  0.415640  \n",
      "13/6/20  1.000000  \n",
      "14/6/20  0.632926  \n",
      "\n",
      "[110 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Sliding window\n",
    "####\n",
    "\n",
    "df = pd.DataFrame()\n",
    "window_size = 7\n",
    "for i in range(0, window_size):\n",
    "    df['E{}'.format(i)] = data['CasosNormalizados'].shift(-i)\n",
    "    if(i == window_size - 1):\n",
    "        for j in range(0, window_size):\n",
    "             df['E{}'.format(j + i + 1)] = data['TaxaNormalizadas'].shift(-j)\n",
    "        df['E{}'.format(window_size * 2 + 1)] = data['CasosNormalizados'].shift(-window_size)\n",
    "df = df.iloc[:-window_size]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.82578372e-05 0.00000000e+00 0.00000000e+00 ... 4.61333333e-01\n",
      "  8.00000000e-02 1.14666667e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 8.00000000e-02\n",
      "  1.14666667e-01 1.46666667e-01]\n",
      " [0.00000000e+00 0.00000000e+00 1.82578372e-05 ... 1.14666667e-01\n",
      "  1.46666667e-01 1.33333333e-01]\n",
      " ...\n",
      " [5.55257344e-01 4.74375126e-01 3.96268098e-01 ... 3.84000000e-01\n",
      "  3.36000000e-01 3.68000000e-01]\n",
      " [4.74375126e-01 3.96268098e-01 3.12391594e-01 ... 3.36000000e-01\n",
      "  3.68000000e-01 2.66666667e-01]\n",
      " [3.96268098e-01 3.12391594e-01 3.76969564e-01 ... 3.68000000e-01\n",
      "  2.66666667e-01 3.84000000e-01]]\n",
      "[0.00000000e+00 1.82578372e-05 9.12891859e-05 9.12891859e-05\n",
      " 0.00000000e+00 2.19094046e-04 0.00000000e+00 1.64320535e-04\n",
      " 3.28641069e-04 4.56445929e-04 3.83414581e-04 4.19930255e-04\n",
      " 1.44236914e-03 6.20766464e-04 1.04069672e-03 2.50132369e-03\n",
      " 3.52376258e-03 5.16696792e-03 4.08975553e-03 7.63177594e-03\n",
      " 6.29895383e-03 5.65992952e-03 4.23581822e-03 8.80027752e-03\n",
      " 9.16543426e-03 8.89156671e-03 6.42675869e-03 5.89728141e-03\n",
      " 2.07774187e-02 2.04305198e-02 1.96089171e-02 2.09234814e-02\n",
      " 2.23110770e-02 1.55556773e-02 1.69067572e-02 3.03262676e-02\n",
      " 4.03498202e-02 3.52376258e-02 3.25172080e-02 1.98827847e-02\n",
      " 2.63278012e-02 2.30231327e-02 3.34483577e-02 5.58324661e-02\n",
      " 3.84327473e-02 5.94657757e-02 5.32581110e-02 3.75198554e-02\n",
      " 3.51828522e-02 4.56080773e-02 4.88944880e-02 6.81930219e-02\n",
      " 6.39572036e-02 1.00673714e-01 6.16932318e-02 8.42234029e-02\n",
      " 9.83184532e-02 1.14586186e-01 1.31785069e-01 1.13362911e-01\n",
      " 9.07414508e-02 8.37669570e-02 1.21104234e-01 1.26618101e-01\n",
      " 1.91762064e-01 1.80533494e-01 1.86631612e-01 1.93733910e-01\n",
      " 1.23422979e-01 1.02828139e-01 1.69031057e-01 2.07865476e-01\n",
      " 2.54587282e-01 2.79436198e-01 2.72388673e-01 1.44930712e-01\n",
      " 2.39907981e-01 3.17832430e-01 3.64262110e-01 3.37916050e-01\n",
      " 3.79817787e-01 3.01400376e-01 2.88711179e-01 2.13379343e-01\n",
      " 2.98040934e-01 3.76093188e-01 4.82317285e-01 4.91647039e-01\n",
      " 6.07511274e-01 2.99592850e-01 2.11754396e-01 5.28308777e-01\n",
      " 5.22776652e-01 5.64459294e-01 5.62889120e-01 4.94330942e-01\n",
      " 3.45456537e-01 2.85808183e-01 5.85912253e-01 6.00920195e-01\n",
      " 5.55257344e-01 4.74375126e-01 3.96268098e-01 3.12391594e-01\n",
      " 3.76969564e-01 6.37527159e-01 5.87683263e-01 4.15639663e-01\n",
      " 1.00000000e+00 6.32926184e-01]\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Manipulating the data to split into X(a window size of values)\n",
    "## and target, or Y, the value X \"produces\"\n",
    "####\n",
    "\n",
    "arr = df.values\n",
    "#print(arr)\n",
    "\n",
    "\n",
    "X = arr[:, : -1]\n",
    "target = arr[:, -1]\n",
    "print(X)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.04895867\n",
      "Iteration 2, loss = 0.08487725\n",
      "Iteration 3, loss = 0.00535692\n",
      "Iteration 4, loss = 0.03632832\n",
      "Iteration 5, loss = 0.02033726\n",
      "Iteration 6, loss = 0.00439384\n",
      "Iteration 7, loss = 0.00921927\n",
      "Iteration 8, loss = 0.01474418\n",
      "Iteration 9, loss = 0.01190464\n",
      "Iteration 10, loss = 0.00707400\n",
      "Iteration 11, loss = 0.00387416\n",
      "Iteration 12, loss = 0.00397519\n",
      "Iteration 13, loss = 0.00582081\n",
      "Iteration 14, loss = 0.00706859\n",
      "Iteration 15, loss = 0.00664157\n",
      "Iteration 16, loss = 0.00510470\n",
      "Iteration 17, loss = 0.00367125\n",
      "Iteration 18, loss = 0.00320628\n",
      "Iteration 19, loss = 0.00376336\n",
      "Iteration 20, loss = 0.00456340\n",
      "Iteration 21, loss = 0.00473930\n",
      "Iteration 22, loss = 0.00417600\n",
      "Iteration 23, loss = 0.00339164\n",
      "Iteration 24, loss = 0.00300625\n",
      "Iteration 25, loss = 0.00331796\n",
      "Iteration 26, loss = 0.00363594\n",
      "Iteration 27, loss = 0.00359684\n",
      "Iteration 28, loss = 0.00331761\n",
      "Iteration 29, loss = 0.00303780\n",
      "Iteration 30, loss = 0.00292291\n",
      "Iteration 31, loss = 0.00299008\n",
      "Iteration 32, loss = 0.00308305\n",
      "Iteration 33, loss = 0.00307386\n",
      "Iteration 34, loss = 0.00297580\n",
      "Iteration 35, loss = 0.00286139\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04889057\n",
      "Iteration 2, loss = 0.08482223\n",
      "Iteration 3, loss = 0.00541790\n",
      "Iteration 4, loss = 0.03655407\n",
      "Iteration 5, loss = 0.02031413\n",
      "Iteration 6, loss = 0.00437651\n",
      "Iteration 7, loss = 0.00920519\n",
      "Iteration 8, loss = 0.01479197\n",
      "Iteration 9, loss = 0.01197618\n",
      "Iteration 10, loss = 0.00710491\n",
      "Iteration 11, loss = 0.00385372\n",
      "Iteration 12, loss = 0.00398231\n",
      "Iteration 13, loss = 0.00591454\n",
      "Iteration 14, loss = 0.00725536\n",
      "Iteration 15, loss = 0.00688158\n",
      "Iteration 16, loss = 0.00530634\n",
      "Iteration 17, loss = 0.00377459\n",
      "Iteration 18, loss = 0.00319660\n",
      "Iteration 19, loss = 0.00370017\n",
      "Iteration 20, loss = 0.00454072\n",
      "Iteration 21, loss = 0.00482853\n",
      "Iteration 22, loss = 0.00435082\n",
      "Iteration 23, loss = 0.00355112\n",
      "Iteration 24, loss = 0.00304133\n",
      "Iteration 25, loss = 0.00325068\n",
      "Iteration 26, loss = 0.00362721\n",
      "Iteration 27, loss = 0.00365830\n",
      "Iteration 28, loss = 0.00340006\n",
      "Iteration 29, loss = 0.00309136\n",
      "Iteration 30, loss = 0.00292652\n",
      "Iteration 31, loss = 0.00298141\n",
      "Iteration 32, loss = 0.00305325\n",
      "Iteration 33, loss = 0.00304842\n",
      "Iteration 34, loss = 0.00298266\n",
      "Iteration 35, loss = 0.00288797\n",
      "Iteration 36, loss = 0.00281494\n",
      "Iteration 37, loss = 0.00279476\n",
      "Iteration 38, loss = 0.00282082\n",
      "Iteration 39, loss = 0.00285097\n",
      "Iteration 40, loss = 0.00284391\n",
      "Iteration 41, loss = 0.00279568\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04888537\n",
      "Iteration 2, loss = 0.08470953\n",
      "Iteration 3, loss = 0.00528185\n",
      "Iteration 4, loss = 0.03465198\n",
      "Iteration 5, loss = 0.01935912\n",
      "Iteration 6, loss = 0.00376454\n",
      "Iteration 7, loss = 0.01256463\n",
      "Iteration 8, loss = 0.01562818\n",
      "Iteration 9, loss = 0.00993618\n",
      "Iteration 10, loss = 0.00470451\n",
      "Iteration 11, loss = 0.00369940\n",
      "Iteration 12, loss = 0.00573034\n",
      "Iteration 13, loss = 0.00765744\n",
      "Iteration 14, loss = 0.00775292\n",
      "Iteration 15, loss = 0.00624798\n",
      "Iteration 16, loss = 0.00439121\n",
      "Iteration 17, loss = 0.00333134\n",
      "Iteration 18, loss = 0.00345687\n",
      "Iteration 19, loss = 0.00429592\n",
      "Iteration 20, loss = 0.00496469\n",
      "Iteration 21, loss = 0.00487970\n",
      "Iteration 22, loss = 0.00415321\n",
      "Iteration 23, loss = 0.00335749\n",
      "Iteration 24, loss = 0.00299593\n",
      "Iteration 25, loss = 0.00322523\n",
      "Iteration 26, loss = 0.00360047\n",
      "Iteration 27, loss = 0.00360342\n",
      "Iteration 28, loss = 0.00327927\n",
      "Iteration 29, loss = 0.00296084\n",
      "Iteration 30, loss = 0.00297394\n",
      "Iteration 31, loss = 0.00314695\n",
      "Iteration 32, loss = 0.00310791\n",
      "Iteration 33, loss = 0.00292043\n",
      "Iteration 34, loss = 0.00280688\n",
      "Iteration 35, loss = 0.00288684\n",
      "Iteration 36, loss = 0.00292834\n",
      "Iteration 37, loss = 0.00283350\n",
      "Iteration 38, loss = 0.00272634\n",
      "Iteration 39, loss = 0.00271380\n",
      "Iteration 40, loss = 0.00275759\n",
      "Iteration 41, loss = 0.00275139\n",
      "Iteration 42, loss = 0.00268947\n",
      "Iteration 43, loss = 0.00263518\n",
      "Iteration 44, loss = 0.00262414\n",
      "Iteration 45, loss = 0.00263611\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04899451\n",
      "Iteration 2, loss = 0.08493653\n",
      "Iteration 3, loss = 0.00524281\n",
      "Iteration 4, loss = 0.03498343\n",
      "Iteration 5, loss = 0.02125644\n",
      "Iteration 6, loss = 0.00462704\n",
      "Iteration 7, loss = 0.00917761\n",
      "Iteration 8, loss = 0.01514987\n",
      "Iteration 9, loss = 0.01219371\n",
      "Iteration 10, loss = 0.00722779\n",
      "Iteration 11, loss = 0.00392127\n",
      "Iteration 12, loss = 0.00400602\n",
      "Iteration 13, loss = 0.00591719\n",
      "Iteration 14, loss = 0.00737471\n",
      "Iteration 15, loss = 0.00723251\n",
      "Iteration 16, loss = 0.00580745\n",
      "Iteration 17, loss = 0.00415290\n",
      "Iteration 18, loss = 0.00321270\n",
      "Iteration 19, loss = 0.00333707\n",
      "Iteration 20, loss = 0.00412729\n",
      "Iteration 21, loss = 0.00467732\n",
      "Iteration 22, loss = 0.00459468\n",
      "Iteration 23, loss = 0.00401730\n",
      "Iteration 24, loss = 0.00334783\n",
      "Iteration 25, loss = 0.00299662\n",
      "Iteration 26, loss = 0.00314889\n",
      "Iteration 27, loss = 0.00350156\n",
      "Iteration 28, loss = 0.00362203\n",
      "Iteration 29, loss = 0.00343419\n",
      "Iteration 30, loss = 0.00311261\n",
      "Iteration 31, loss = 0.00289500\n",
      "Iteration 32, loss = 0.00294533\n",
      "Iteration 33, loss = 0.00304315\n",
      "Iteration 34, loss = 0.00305032\n",
      "Iteration 35, loss = 0.00297471\n",
      "Iteration 36, loss = 0.00287564\n",
      "Iteration 37, loss = 0.00280260\n",
      "Iteration 38, loss = 0.00276429\n",
      "Iteration 39, loss = 0.00276215\n",
      "Iteration 40, loss = 0.00277575\n",
      "Iteration 41, loss = 0.00276496\n",
      "Iteration 42, loss = 0.00272043\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04891112\n",
      "Iteration 2, loss = 0.08447714\n",
      "Iteration 3, loss = 0.00550831\n",
      "Iteration 4, loss = 0.03588168\n",
      "Iteration 5, loss = 0.01882289\n",
      "Iteration 6, loss = 0.00371560\n",
      "Iteration 7, loss = 0.01256750\n",
      "Iteration 8, loss = 0.01533459\n",
      "Iteration 9, loss = 0.00967368\n",
      "Iteration 10, loss = 0.00462147\n",
      "Iteration 11, loss = 0.00367075\n",
      "Iteration 12, loss = 0.00564861\n",
      "Iteration 13, loss = 0.00749883\n",
      "Iteration 14, loss = 0.00757360\n",
      "Iteration 15, loss = 0.00613281\n",
      "Iteration 16, loss = 0.00436986\n",
      "Iteration 17, loss = 0.00337928\n",
      "Iteration 18, loss = 0.00349409\n",
      "Iteration 19, loss = 0.00429376\n",
      "Iteration 20, loss = 0.00492620\n",
      "Iteration 21, loss = 0.00482756\n",
      "Iteration 22, loss = 0.00411806\n",
      "Iteration 23, loss = 0.00337072\n",
      "Iteration 24, loss = 0.00304376\n",
      "Iteration 25, loss = 0.00328697\n",
      "Iteration 26, loss = 0.00367528\n",
      "Iteration 27, loss = 0.00376780\n",
      "Iteration 28, loss = 0.00353286\n",
      "Iteration 29, loss = 0.00317871\n",
      "Iteration 30, loss = 0.00294056\n",
      "Iteration 31, loss = 0.00293398\n",
      "Iteration 32, loss = 0.00305838\n",
      "Iteration 33, loss = 0.00310824\n",
      "Iteration 34, loss = 0.00302814\n",
      "Iteration 35, loss = 0.00288821\n",
      "Iteration 36, loss = 0.00282479\n",
      "Iteration 37, loss = 0.00290223\n",
      "Iteration 38, loss = 0.00287073\n",
      "Iteration 39, loss = 0.00278322\n",
      "Iteration 40, loss = 0.00274958\n",
      "Iteration 41, loss = 0.00276120\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04883378\n",
      "Iteration 2, loss = 0.08456789\n",
      "Iteration 3, loss = 0.00536712\n",
      "Iteration 4, loss = 0.03459481\n",
      "Iteration 5, loss = 0.01942795\n",
      "Iteration 6, loss = 0.00379048\n",
      "Iteration 7, loss = 0.01270824\n",
      "Iteration 8, loss = 0.01570126\n",
      "Iteration 9, loss = 0.00989037\n",
      "Iteration 10, loss = 0.00467841\n",
      "Iteration 11, loss = 0.00369124\n",
      "Iteration 12, loss = 0.00570871\n",
      "Iteration 13, loss = 0.00761657\n",
      "Iteration 14, loss = 0.00771239\n",
      "Iteration 15, loss = 0.00622913\n",
      "Iteration 16, loss = 0.00439478\n",
      "Iteration 17, loss = 0.00333507\n",
      "Iteration 18, loss = 0.00344963\n",
      "Iteration 19, loss = 0.00427309\n",
      "Iteration 20, loss = 0.00491837\n",
      "Iteration 21, loss = 0.00483160\n",
      "Iteration 22, loss = 0.00414937\n",
      "Iteration 23, loss = 0.00338376\n",
      "Iteration 24, loss = 0.00301886\n",
      "Iteration 25, loss = 0.00323576\n",
      "Iteration 26, loss = 0.00364781\n",
      "Iteration 27, loss = 0.00377185\n",
      "Iteration 28, loss = 0.00356257\n",
      "Iteration 29, loss = 0.00321456\n",
      "Iteration 30, loss = 0.00295747\n",
      "Iteration 31, loss = 0.00292545\n",
      "Iteration 32, loss = 0.00304814\n",
      "Iteration 33, loss = 0.00312733\n",
      "Iteration 34, loss = 0.00310032\n",
      "Iteration 35, loss = 0.00300244\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04887686\n",
      "Iteration 2, loss = 0.08457537\n",
      "Iteration 3, loss = 0.00546436\n",
      "Iteration 4, loss = 0.03633828\n",
      "Iteration 5, loss = 0.01944134\n",
      "Iteration 6, loss = 0.00384097\n",
      "Iteration 7, loss = 0.01181010\n",
      "Iteration 8, loss = 0.01510986\n",
      "Iteration 9, loss = 0.00991356\n",
      "Iteration 10, loss = 0.00493673\n",
      "Iteration 11, loss = 0.00357259\n",
      "Iteration 12, loss = 0.00500347\n",
      "Iteration 13, loss = 0.00658791\n",
      "Iteration 14, loss = 0.00674049\n",
      "Iteration 15, loss = 0.00551425\n",
      "Iteration 16, loss = 0.00401718\n",
      "Iteration 17, loss = 0.00327236\n",
      "Iteration 18, loss = 0.00358553\n",
      "Iteration 19, loss = 0.00440377\n",
      "Iteration 20, loss = 0.00481922\n",
      "Iteration 21, loss = 0.00443596\n",
      "Iteration 22, loss = 0.00363827\n",
      "Iteration 23, loss = 0.00307964\n",
      "Iteration 24, loss = 0.00315837\n",
      "Iteration 25, loss = 0.00355747\n",
      "Iteration 26, loss = 0.00366870\n",
      "Iteration 27, loss = 0.00346477\n",
      "Iteration 28, loss = 0.00310415\n",
      "Iteration 29, loss = 0.00290312\n",
      "Iteration 30, loss = 0.00309861\n",
      "Iteration 31, loss = 0.00314151\n",
      "Iteration 32, loss = 0.00301031\n",
      "Iteration 33, loss = 0.00287649\n",
      "Iteration 34, loss = 0.00283221\n",
      "Iteration 35, loss = 0.00283411\n",
      "Iteration 36, loss = 0.00284869\n",
      "Iteration 37, loss = 0.00284438\n",
      "Iteration 38, loss = 0.00281356\n",
      "Iteration 39, loss = 0.00276848\n",
      "Iteration 40, loss = 0.00273082\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04893243\n",
      "Iteration 2, loss = 0.08492648\n",
      "Iteration 3, loss = 0.00537068\n",
      "Iteration 4, loss = 0.03649163\n",
      "Iteration 5, loss = 0.02029692\n",
      "Iteration 6, loss = 0.00438366\n",
      "Iteration 7, loss = 0.00910189\n",
      "Iteration 8, loss = 0.01462633\n",
      "Iteration 9, loss = 0.01191949\n",
      "Iteration 10, loss = 0.00714778\n",
      "Iteration 11, loss = 0.00390311\n",
      "Iteration 12, loss = 0.00396107\n",
      "Iteration 13, loss = 0.00583924\n",
      "Iteration 14, loss = 0.00718109\n",
      "Iteration 15, loss = 0.00684715\n",
      "Iteration 16, loss = 0.00530945\n",
      "Iteration 17, loss = 0.00378642\n",
      "Iteration 18, loss = 0.00318869\n",
      "Iteration 19, loss = 0.00367496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, loss = 0.00454736\n",
      "Iteration 21, loss = 0.00483245\n",
      "Iteration 22, loss = 0.00432883\n",
      "Iteration 23, loss = 0.00354723\n",
      "Iteration 24, loss = 0.00304601\n",
      "Iteration 25, loss = 0.00320190\n",
      "Iteration 26, loss = 0.00357656\n",
      "Iteration 27, loss = 0.00370048\n",
      "Iteration 28, loss = 0.00351692\n",
      "Iteration 29, loss = 0.00319052\n",
      "Iteration 30, loss = 0.00295987\n",
      "Iteration 31, loss = 0.00295598\n",
      "Iteration 32, loss = 0.00306673\n",
      "Iteration 33, loss = 0.00311981\n",
      "Iteration 34, loss = 0.00307694\n",
      "Iteration 35, loss = 0.00296462\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04889451\n",
      "Iteration 2, loss = 0.08452060\n",
      "Iteration 3, loss = 0.00535089\n",
      "Iteration 4, loss = 0.03613080\n",
      "Iteration 5, loss = 0.01977677\n",
      "Iteration 6, loss = 0.00424588\n",
      "Iteration 7, loss = 0.00916987\n",
      "Iteration 8, loss = 0.01442482\n",
      "Iteration 9, loss = 0.01170546\n",
      "Iteration 10, loss = 0.00688039\n",
      "Iteration 11, loss = 0.00379534\n",
      "Iteration 12, loss = 0.00407322\n",
      "Iteration 13, loss = 0.00603931\n",
      "Iteration 14, loss = 0.00729541\n",
      "Iteration 15, loss = 0.00679503\n",
      "Iteration 16, loss = 0.00515439\n",
      "Iteration 17, loss = 0.00365977\n",
      "Iteration 18, loss = 0.00320535\n",
      "Iteration 19, loss = 0.00383102\n",
      "Iteration 20, loss = 0.00466530\n",
      "Iteration 21, loss = 0.00483265\n",
      "Iteration 22, loss = 0.00425094\n",
      "Iteration 23, loss = 0.00344241\n",
      "Iteration 24, loss = 0.00303121\n",
      "Iteration 25, loss = 0.00330858\n",
      "Iteration 26, loss = 0.00369127\n",
      "Iteration 27, loss = 0.00368721\n",
      "Iteration 28, loss = 0.00339297\n",
      "Iteration 29, loss = 0.00308142\n",
      "Iteration 30, loss = 0.00293220\n",
      "Iteration 31, loss = 0.00298508\n",
      "Iteration 32, loss = 0.00307103\n",
      "Iteration 33, loss = 0.00308165\n",
      "Iteration 34, loss = 0.00301444\n",
      "Iteration 35, loss = 0.00289960\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04888525\n",
      "Iteration 2, loss = 0.08448802\n",
      "Iteration 3, loss = 0.00551435\n",
      "Iteration 4, loss = 0.03604802\n",
      "Iteration 5, loss = 0.01894524\n",
      "Iteration 6, loss = 0.00375403\n",
      "Iteration 7, loss = 0.01224343\n",
      "Iteration 8, loss = 0.01509823\n",
      "Iteration 9, loss = 0.00963506\n",
      "Iteration 10, loss = 0.00472408\n",
      "Iteration 11, loss = 0.00365455\n",
      "Iteration 12, loss = 0.00538006\n",
      "Iteration 13, loss = 0.00702622\n",
      "Iteration 14, loss = 0.00706299\n",
      "Iteration 15, loss = 0.00563989\n",
      "Iteration 16, loss = 0.00400466\n",
      "Iteration 17, loss = 0.00328395\n",
      "Iteration 18, loss = 0.00367804\n",
      "Iteration 19, loss = 0.00456892\n",
      "Iteration 20, loss = 0.00500215\n",
      "Iteration 21, loss = 0.00456740\n",
      "Iteration 22, loss = 0.00370407\n",
      "Iteration 23, loss = 0.00310377\n",
      "Iteration 24, loss = 0.00316456\n",
      "Iteration 25, loss = 0.00360780\n",
      "Iteration 26, loss = 0.00378374\n",
      "Iteration 27, loss = 0.00359813\n",
      "Iteration 28, loss = 0.00322080\n",
      "Iteration 29, loss = 0.00294187\n",
      "Iteration 30, loss = 0.00301711\n",
      "Iteration 31, loss = 0.00312863\n",
      "Iteration 32, loss = 0.00306791\n",
      "Iteration 33, loss = 0.00295076\n",
      "Iteration 34, loss = 0.00285979\n",
      "Iteration 35, loss = 0.00282029\n",
      "Iteration 36, loss = 0.00281727\n",
      "Iteration 37, loss = 0.00282738\n",
      "Iteration 38, loss = 0.00282300\n",
      "Iteration 39, loss = 0.00279382\n",
      "Iteration 40, loss = 0.00274802\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04897483\n",
      "Iteration 2, loss = 0.08461284\n",
      "Iteration 3, loss = 0.00530344\n",
      "Iteration 4, loss = 0.03439506\n",
      "Iteration 5, loss = 0.01935030\n",
      "Iteration 6, loss = 0.00377290\n",
      "Iteration 7, loss = 0.01266419\n",
      "Iteration 8, loss = 0.01561369\n",
      "Iteration 9, loss = 0.00985359\n",
      "Iteration 10, loss = 0.00463402\n",
      "Iteration 11, loss = 0.00370064\n",
      "Iteration 12, loss = 0.00575468\n",
      "Iteration 13, loss = 0.00765396\n",
      "Iteration 14, loss = 0.00770363\n",
      "Iteration 15, loss = 0.00617750\n",
      "Iteration 16, loss = 0.00433792\n",
      "Iteration 17, loss = 0.00331352\n",
      "Iteration 18, loss = 0.00345986\n",
      "Iteration 19, loss = 0.00429467\n",
      "Iteration 20, loss = 0.00497724\n",
      "Iteration 21, loss = 0.00484867\n",
      "Iteration 22, loss = 0.00411050\n",
      "Iteration 23, loss = 0.00333334\n",
      "Iteration 24, loss = 0.00299874\n",
      "Iteration 25, loss = 0.00326853\n",
      "Iteration 26, loss = 0.00367637\n",
      "Iteration 27, loss = 0.00376112\n",
      "Iteration 28, loss = 0.00349364\n",
      "Iteration 29, loss = 0.00310949\n",
      "Iteration 30, loss = 0.00290932\n",
      "Iteration 31, loss = 0.00304901\n",
      "Iteration 32, loss = 0.00317133\n",
      "Iteration 33, loss = 0.00310459\n",
      "Iteration 34, loss = 0.00295826\n",
      "Iteration 35, loss = 0.00283908\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04889257\n",
      "Iteration 2, loss = 0.08447233\n",
      "Iteration 3, loss = 0.00550547\n",
      "Iteration 4, loss = 0.03587892\n",
      "Iteration 5, loss = 0.01879610\n",
      "Iteration 6, loss = 0.00370787\n",
      "Iteration 7, loss = 0.01257822\n",
      "Iteration 8, loss = 0.01532532\n",
      "Iteration 9, loss = 0.00965029\n",
      "Iteration 10, loss = 0.00459134\n",
      "Iteration 11, loss = 0.00367267\n",
      "Iteration 12, loss = 0.00567883\n",
      "Iteration 13, loss = 0.00752882\n",
      "Iteration 14, loss = 0.00758141\n",
      "Iteration 15, loss = 0.00611865\n",
      "Iteration 16, loss = 0.00434801\n",
      "Iteration 17, loss = 0.00336716\n",
      "Iteration 18, loss = 0.00350352\n",
      "Iteration 19, loss = 0.00429724\n",
      "Iteration 20, loss = 0.00492380\n",
      "Iteration 21, loss = 0.00483703\n",
      "Iteration 22, loss = 0.00412234\n",
      "Iteration 23, loss = 0.00336393\n",
      "Iteration 24, loss = 0.00303890\n",
      "Iteration 25, loss = 0.00328751\n",
      "Iteration 26, loss = 0.00368057\n",
      "Iteration 27, loss = 0.00377010\n",
      "Iteration 28, loss = 0.00352912\n",
      "Iteration 29, loss = 0.00317224\n",
      "Iteration 30, loss = 0.00293373\n",
      "Iteration 31, loss = 0.00293532\n",
      "Iteration 32, loss = 0.00306289\n",
      "Iteration 33, loss = 0.00310329\n",
      "Iteration 34, loss = 0.00301412\n",
      "Iteration 35, loss = 0.00287338\n",
      "Iteration 36, loss = 0.00282714\n",
      "Iteration 37, loss = 0.00290195\n",
      "Iteration 38, loss = 0.00286320\n",
      "Iteration 39, loss = 0.00277779\n",
      "Iteration 40, loss = 0.00274954\n",
      "Iteration 41, loss = 0.00276320\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04880577\n",
      "Iteration 2, loss = 0.08469096\n",
      "Iteration 3, loss = 0.00525680\n",
      "Iteration 4, loss = 0.03445950\n",
      "Iteration 5, loss = 0.01950426\n",
      "Iteration 6, loss = 0.00378782\n",
      "Iteration 7, loss = 0.01249459\n",
      "Iteration 8, loss = 0.01544075\n",
      "Iteration 9, loss = 0.00976394\n",
      "Iteration 10, loss = 0.00468166\n",
      "Iteration 11, loss = 0.00374846\n",
      "Iteration 12, loss = 0.00574553\n",
      "Iteration 13, loss = 0.00760183\n",
      "Iteration 14, loss = 0.00768747\n",
      "Iteration 15, loss = 0.00624172\n",
      "Iteration 16, loss = 0.00444568\n",
      "Iteration 17, loss = 0.00336714\n",
      "Iteration 18, loss = 0.00342191\n",
      "Iteration 19, loss = 0.00418483\n",
      "Iteration 20, loss = 0.00483108\n",
      "Iteration 21, loss = 0.00479798\n",
      "Iteration 22, loss = 0.00417981\n",
      "Iteration 23, loss = 0.00343750\n",
      "Iteration 24, loss = 0.00302533\n",
      "Iteration 25, loss = 0.00317463\n",
      "Iteration 26, loss = 0.00354921\n",
      "Iteration 27, loss = 0.00359731\n",
      "Iteration 28, loss = 0.00327862\n",
      "Iteration 29, loss = 0.00294881\n",
      "Iteration 30, loss = 0.00300830\n",
      "Iteration 31, loss = 0.00318417\n",
      "Iteration 32, loss = 0.00307634\n",
      "Iteration 33, loss = 0.00288021\n",
      "Iteration 34, loss = 0.00279807\n",
      "Iteration 35, loss = 0.00283168\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04882414\n",
      "Iteration 2, loss = 0.08481752\n",
      "Iteration 3, loss = 0.00543290\n",
      "Iteration 4, loss = 0.03638464\n",
      "Iteration 5, loss = 0.01998366\n",
      "Iteration 6, loss = 0.00429880\n",
      "Iteration 7, loss = 0.00918288\n",
      "Iteration 8, loss = 0.01452311\n",
      "Iteration 9, loss = 0.01174630\n",
      "Iteration 10, loss = 0.00698300\n",
      "Iteration 11, loss = 0.00383909\n",
      "Iteration 12, loss = 0.00401128\n",
      "Iteration 13, loss = 0.00592059\n",
      "Iteration 14, loss = 0.00721979\n",
      "Iteration 15, loss = 0.00682073\n",
      "Iteration 16, loss = 0.00523101\n",
      "Iteration 17, loss = 0.00370730\n",
      "Iteration 18, loss = 0.00316110\n",
      "Iteration 19, loss = 0.00377781\n",
      "Iteration 20, loss = 0.00453289\n",
      "Iteration 21, loss = 0.00466265\n",
      "Iteration 22, loss = 0.00415545\n",
      "Iteration 23, loss = 0.00341799\n",
      "Iteration 24, loss = 0.00302823\n",
      "Iteration 25, loss = 0.00327775\n",
      "Iteration 26, loss = 0.00365060\n",
      "Iteration 27, loss = 0.00365025\n",
      "Iteration 28, loss = 0.00337491\n",
      "Iteration 29, loss = 0.00308763\n",
      "Iteration 30, loss = 0.00296595\n",
      "Iteration 31, loss = 0.00300932\n",
      "Iteration 32, loss = 0.00307953\n",
      "Iteration 33, loss = 0.00309364\n",
      "Iteration 34, loss = 0.00303193\n",
      "Iteration 35, loss = 0.00292437\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04886368\n",
      "Iteration 2, loss = 0.08444754\n",
      "Iteration 3, loss = 0.00553704\n",
      "Iteration 4, loss = 0.03602742\n",
      "Iteration 5, loss = 0.01874314\n",
      "Iteration 6, loss = 0.00374621\n",
      "Iteration 7, loss = 0.01208697\n",
      "Iteration 8, loss = 0.01479683\n",
      "Iteration 9, loss = 0.00947380\n",
      "Iteration 10, loss = 0.00473572\n",
      "Iteration 11, loss = 0.00367323\n",
      "Iteration 12, loss = 0.00526454\n",
      "Iteration 13, loss = 0.00684216\n",
      "Iteration 14, loss = 0.00686449\n",
      "Iteration 15, loss = 0.00549055\n",
      "Iteration 16, loss = 0.00394058\n",
      "Iteration 17, loss = 0.00328254\n",
      "Iteration 18, loss = 0.00369833\n",
      "Iteration 19, loss = 0.00454936\n",
      "Iteration 20, loss = 0.00492047\n",
      "Iteration 21, loss = 0.00447024\n",
      "Iteration 22, loss = 0.00362553\n",
      "Iteration 23, loss = 0.00307211\n",
      "Iteration 24, loss = 0.00321192\n",
      "Iteration 25, loss = 0.00362362\n",
      "Iteration 26, loss = 0.00370302\n",
      "Iteration 27, loss = 0.00347069\n",
      "Iteration 28, loss = 0.00310605\n",
      "Iteration 29, loss = 0.00288864\n",
      "Iteration 30, loss = 0.00314209\n",
      "Iteration 31, loss = 0.00306634\n",
      "Iteration 32, loss = 0.00288739\n",
      "Iteration 33, loss = 0.00281914\n",
      "Iteration 34, loss = 0.00283694\n",
      "Iteration 35, loss = 0.00284994\n",
      "Iteration 36, loss = 0.00280918\n",
      "Iteration 37, loss = 0.00275815\n",
      "Iteration 38, loss = 0.00273735\n",
      "Iteration 39, loss = 0.00273936\n",
      "Iteration 40, loss = 0.00273492\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04878892\n",
      "Iteration 2, loss = 0.08445649\n",
      "Iteration 3, loss = 0.00554213\n",
      "Iteration 4, loss = 0.03604308\n",
      "Iteration 5, loss = 0.01872501\n",
      "Iteration 6, loss = 0.00374621\n",
      "Iteration 7, loss = 0.01209944\n",
      "Iteration 8, loss = 0.01479370\n",
      "Iteration 9, loss = 0.00945904\n",
      "Iteration 10, loss = 0.00471173\n",
      "Iteration 11, loss = 0.00367258\n",
      "Iteration 12, loss = 0.00528536\n",
      "Iteration 13, loss = 0.00686208\n",
      "Iteration 14, loss = 0.00687688\n",
      "Iteration 15, loss = 0.00549404\n",
      "Iteration 16, loss = 0.00394155\n",
      "Iteration 17, loss = 0.00328563\n",
      "Iteration 18, loss = 0.00370724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 0.00456008\n",
      "Iteration 20, loss = 0.00492546\n",
      "Iteration 21, loss = 0.00446861\n",
      "Iteration 22, loss = 0.00361616\n",
      "Iteration 23, loss = 0.00307340\n",
      "Iteration 24, loss = 0.00322167\n",
      "Iteration 25, loss = 0.00363082\n",
      "Iteration 26, loss = 0.00370401\n",
      "Iteration 27, loss = 0.00346909\n",
      "Iteration 28, loss = 0.00310309\n",
      "Iteration 29, loss = 0.00289804\n",
      "Iteration 30, loss = 0.00314985\n",
      "Iteration 31, loss = 0.00307813\n",
      "Iteration 32, loss = 0.00289221\n",
      "Iteration 33, loss = 0.00282039\n",
      "Iteration 34, loss = 0.00284728\n",
      "Iteration 35, loss = 0.00286077\n",
      "Iteration 36, loss = 0.00281615\n",
      "Iteration 37, loss = 0.00276213\n",
      "Iteration 38, loss = 0.00274147\n",
      "Iteration 39, loss = 0.00274458\n",
      "Iteration 40, loss = 0.00274065\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04874132\n",
      "Iteration 2, loss = 0.08400745\n",
      "Iteration 3, loss = 0.00549429\n",
      "Iteration 4, loss = 0.03479488\n",
      "Iteration 5, loss = 0.01967713\n",
      "Iteration 6, loss = 0.00382330\n",
      "Iteration 7, loss = 0.01291847\n",
      "Iteration 8, loss = 0.01596644\n",
      "Iteration 9, loss = 0.00990142\n",
      "Iteration 10, loss = 0.00462471\n",
      "Iteration 11, loss = 0.00366649\n",
      "Iteration 12, loss = 0.00569730\n",
      "Iteration 13, loss = 0.00758619\n",
      "Iteration 14, loss = 0.00766898\n",
      "Iteration 15, loss = 0.00619375\n",
      "Iteration 16, loss = 0.00438565\n",
      "Iteration 17, loss = 0.00334924\n",
      "Iteration 18, loss = 0.00346678\n",
      "Iteration 19, loss = 0.00427103\n",
      "Iteration 20, loss = 0.00492653\n",
      "Iteration 21, loss = 0.00488482\n",
      "Iteration 22, loss = 0.00421317\n",
      "Iteration 23, loss = 0.00342211\n",
      "Iteration 24, loss = 0.00300207\n",
      "Iteration 25, loss = 0.00316818\n",
      "Iteration 26, loss = 0.00359216\n",
      "Iteration 27, loss = 0.00375170\n",
      "Iteration 28, loss = 0.00355560\n",
      "Iteration 29, loss = 0.00319459\n",
      "Iteration 30, loss = 0.00291948\n",
      "Iteration 31, loss = 0.00287928\n",
      "Iteration 32, loss = 0.00301164\n",
      "Iteration 33, loss = 0.00309070\n",
      "Iteration 34, loss = 0.00304982\n",
      "Iteration 35, loss = 0.00293685\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04882966\n",
      "Iteration 2, loss = 0.08495821\n",
      "Iteration 3, loss = 0.00523148\n",
      "Iteration 4, loss = 0.03533663\n",
      "Iteration 5, loss = 0.02124731\n",
      "Iteration 6, loss = 0.00461226\n",
      "Iteration 7, loss = 0.00911779\n",
      "Iteration 8, loss = 0.01511014\n",
      "Iteration 9, loss = 0.01224786\n",
      "Iteration 10, loss = 0.00732304\n",
      "Iteration 11, loss = 0.00397946\n",
      "Iteration 12, loss = 0.00401378\n",
      "Iteration 13, loss = 0.00591367\n",
      "Iteration 14, loss = 0.00739513\n",
      "Iteration 15, loss = 0.00728705\n",
      "Iteration 16, loss = 0.00587945\n",
      "Iteration 17, loss = 0.00422188\n",
      "Iteration 18, loss = 0.00324069\n",
      "Iteration 19, loss = 0.00327712\n",
      "Iteration 20, loss = 0.00404024\n",
      "Iteration 21, loss = 0.00468578\n",
      "Iteration 22, loss = 0.00469058\n",
      "Iteration 23, loss = 0.00414374\n",
      "Iteration 24, loss = 0.00341908\n",
      "Iteration 25, loss = 0.00298749\n",
      "Iteration 26, loss = 0.00314840\n",
      "Iteration 27, loss = 0.00346091\n",
      "Iteration 28, loss = 0.00346044\n",
      "Iteration 29, loss = 0.00318583\n",
      "Iteration 30, loss = 0.00293637\n",
      "Iteration 31, loss = 0.00299456\n",
      "Iteration 32, loss = 0.00311045\n",
      "Iteration 33, loss = 0.00299269\n",
      "Iteration 34, loss = 0.00283527\n",
      "Iteration 35, loss = 0.00278592\n",
      "Iteration 36, loss = 0.00282060\n",
      "Iteration 37, loss = 0.00284624\n",
      "Iteration 38, loss = 0.00281384\n",
      "Iteration 39, loss = 0.00274281\n",
      "Iteration 40, loss = 0.00268690\n",
      "Iteration 41, loss = 0.00267395\n",
      "Iteration 42, loss = 0.00268576\n",
      "Iteration 43, loss = 0.00268125\n",
      "Iteration 44, loss = 0.00264636\n",
      "Iteration 45, loss = 0.00259391\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04856098\n",
      "Iteration 2, loss = 0.08449281\n",
      "Iteration 3, loss = 0.00541031\n",
      "Iteration 4, loss = 0.03547544\n",
      "Iteration 5, loss = 0.02145898\n",
      "Iteration 6, loss = 0.00465768\n",
      "Iteration 7, loss = 0.00939771\n",
      "Iteration 8, loss = 0.01541046\n",
      "Iteration 9, loss = 0.01211179\n",
      "Iteration 10, loss = 0.00709158\n",
      "Iteration 11, loss = 0.00386728\n",
      "Iteration 12, loss = 0.00399801\n",
      "Iteration 13, loss = 0.00591061\n",
      "Iteration 14, loss = 0.00736091\n",
      "Iteration 15, loss = 0.00723950\n",
      "Iteration 16, loss = 0.00585963\n",
      "Iteration 17, loss = 0.00423842\n",
      "Iteration 18, loss = 0.00327055\n",
      "Iteration 19, loss = 0.00330257\n",
      "Iteration 20, loss = 0.00401844\n",
      "Iteration 21, loss = 0.00463772\n",
      "Iteration 22, loss = 0.00466466\n",
      "Iteration 23, loss = 0.00412461\n",
      "Iteration 24, loss = 0.00339998\n",
      "Iteration 25, loss = 0.00296766\n",
      "Iteration 26, loss = 0.00308710\n",
      "Iteration 27, loss = 0.00347991\n",
      "Iteration 28, loss = 0.00364455\n",
      "Iteration 29, loss = 0.00344197\n",
      "Iteration 30, loss = 0.00308209\n",
      "Iteration 31, loss = 0.00283957\n",
      "Iteration 32, loss = 0.00287202\n",
      "Iteration 33, loss = 0.00302422\n",
      "Iteration 34, loss = 0.00303910\n",
      "Iteration 35, loss = 0.00291310\n",
      "Iteration 36, loss = 0.00277482\n",
      "Iteration 37, loss = 0.00273760\n",
      "Iteration 38, loss = 0.00279454\n",
      "Iteration 39, loss = 0.00278493\n",
      "Iteration 40, loss = 0.00271439\n",
      "Iteration 41, loss = 0.00265547\n",
      "Iteration 42, loss = 0.00263833\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04836282\n",
      "Iteration 2, loss = 0.08387236\n",
      "Iteration 3, loss = 0.00559700\n",
      "Iteration 4, loss = 0.03561686\n",
      "Iteration 5, loss = 0.02146493\n",
      "Iteration 6, loss = 0.00465771\n",
      "Iteration 7, loss = 0.00960182\n",
      "Iteration 8, loss = 0.01567819\n",
      "Iteration 9, loss = 0.01211575\n",
      "Iteration 10, loss = 0.00696139\n",
      "Iteration 11, loss = 0.00377403\n",
      "Iteration 12, loss = 0.00402617\n",
      "Iteration 13, loss = 0.00601769\n",
      "Iteration 14, loss = 0.00739815\n",
      "Iteration 15, loss = 0.00714565\n",
      "Iteration 16, loss = 0.00570465\n",
      "Iteration 17, loss = 0.00412962\n",
      "Iteration 18, loss = 0.00326354\n",
      "Iteration 19, loss = 0.00337077\n",
      "Iteration 20, loss = 0.00413021\n",
      "Iteration 21, loss = 0.00470695\n",
      "Iteration 22, loss = 0.00465221\n",
      "Iteration 23, loss = 0.00406827\n",
      "Iteration 24, loss = 0.00335718\n",
      "Iteration 25, loss = 0.00297229\n",
      "Iteration 26, loss = 0.00314091\n",
      "Iteration 27, loss = 0.00352761\n",
      "Iteration 28, loss = 0.00366052\n",
      "Iteration 29, loss = 0.00344386\n",
      "Iteration 30, loss = 0.00309221\n",
      "Iteration 31, loss = 0.00286325\n",
      "Iteration 32, loss = 0.00289256\n",
      "Iteration 33, loss = 0.00303062\n",
      "Iteration 34, loss = 0.00307018\n",
      "Iteration 35, loss = 0.00299167\n",
      "Iteration 36, loss = 0.00287020\n",
      "Iteration 37, loss = 0.00277489\n",
      "Iteration 38, loss = 0.00273197\n",
      "Iteration 39, loss = 0.00273843\n",
      "Iteration 40, loss = 0.00276837\n",
      "Iteration 41, loss = 0.00278246\n",
      "Iteration 42, loss = 0.00275853\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04839190\n",
      "Iteration 2, loss = 0.08378621\n",
      "Iteration 3, loss = 0.00500641\n",
      "Iteration 4, loss = 0.03377257\n",
      "Iteration 5, loss = 0.01974238\n",
      "Iteration 6, loss = 0.00434780\n",
      "Iteration 7, loss = 0.00847846\n",
      "Iteration 8, loss = 0.01234325\n",
      "Iteration 9, loss = 0.00862218\n",
      "Iteration 10, loss = 0.00466761\n",
      "Iteration 11, loss = 0.00388592\n",
      "Iteration 12, loss = 0.00593261\n",
      "Iteration 13, loss = 0.00710546\n",
      "Iteration 14, loss = 0.00590016\n",
      "Iteration 15, loss = 0.00393413\n",
      "Iteration 16, loss = 0.00318964\n",
      "Iteration 17, loss = 0.00395226\n",
      "Iteration 18, loss = 0.00492015\n",
      "Iteration 19, loss = 0.00488307\n",
      "Iteration 20, loss = 0.00397548\n",
      "Iteration 21, loss = 0.00313353\n",
      "Iteration 22, loss = 0.00306742\n",
      "Iteration 23, loss = 0.00355183\n",
      "Iteration 24, loss = 0.00382687\n",
      "Iteration 25, loss = 0.00359217\n",
      "Iteration 26, loss = 0.00312504\n",
      "Iteration 27, loss = 0.00280760\n",
      "Iteration 28, loss = 0.00292628\n",
      "Iteration 29, loss = 0.00313412\n",
      "Iteration 30, loss = 0.00310267\n",
      "Iteration 31, loss = 0.00292482\n",
      "Iteration 32, loss = 0.00272160\n",
      "Iteration 33, loss = 0.00268680\n",
      "Iteration 34, loss = 0.00277763\n",
      "Iteration 35, loss = 0.00276524\n",
      "Iteration 36, loss = 0.00264100\n",
      "Iteration 37, loss = 0.00254081\n",
      "Iteration 38, loss = 0.00256793\n",
      "Iteration 39, loss = 0.00257661\n",
      "Iteration 40, loss = 0.00250856\n",
      "Iteration 41, loss = 0.00244606\n",
      "Iteration 42, loss = 0.00242749\n",
      "Iteration 43, loss = 0.00242200\n",
      "Iteration 44, loss = 0.00238579\n",
      "Iteration 45, loss = 0.00232214\n",
      "Iteration 46, loss = 0.00226866\n",
      "Iteration 47, loss = 0.00226435\n",
      "Iteration 48, loss = 0.00224466\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04819725\n",
      "Iteration 2, loss = 0.08417380\n",
      "Iteration 3, loss = 0.00523938\n",
      "Iteration 4, loss = 0.03501245\n",
      "Iteration 5, loss = 0.02106723\n",
      "Iteration 6, loss = 0.00458723\n",
      "Iteration 7, loss = 0.00903978\n",
      "Iteration 8, loss = 0.01493898\n",
      "Iteration 9, loss = 0.01214173\n",
      "Iteration 10, loss = 0.00732126\n",
      "Iteration 11, loss = 0.00401177\n",
      "Iteration 12, loss = 0.00400289\n",
      "Iteration 13, loss = 0.00584059\n",
      "Iteration 14, loss = 0.00727572\n",
      "Iteration 15, loss = 0.00718907\n",
      "Iteration 16, loss = 0.00581531\n",
      "Iteration 17, loss = 0.00417646\n",
      "Iteration 18, loss = 0.00320325\n",
      "Iteration 19, loss = 0.00326352\n",
      "Iteration 20, loss = 0.00404069\n",
      "Iteration 21, loss = 0.00463834\n",
      "Iteration 22, loss = 0.00460262\n",
      "Iteration 23, loss = 0.00402379\n",
      "Iteration 24, loss = 0.00333155\n",
      "Iteration 25, loss = 0.00297833\n",
      "Iteration 26, loss = 0.00324292\n",
      "Iteration 27, loss = 0.00357881\n",
      "Iteration 28, loss = 0.00355453\n",
      "Iteration 29, loss = 0.00326689\n",
      "Iteration 30, loss = 0.00297370\n",
      "Iteration 31, loss = 0.00287764\n",
      "Iteration 32, loss = 0.00302535\n",
      "Iteration 33, loss = 0.00304888\n",
      "Iteration 34, loss = 0.00296847\n",
      "Iteration 35, loss = 0.00287298\n",
      "Iteration 36, loss = 0.00281144\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04808250\n",
      "Iteration 2, loss = 0.08050301\n",
      "Iteration 3, loss = 0.00570738\n",
      "Iteration 4, loss = 0.03426251\n",
      "Iteration 5, loss = 0.02014273\n",
      "Iteration 6, loss = 0.00432100\n",
      "Iteration 7, loss = 0.01008770\n",
      "Iteration 8, loss = 0.01554246\n",
      "Iteration 9, loss = 0.01158702\n",
      "Iteration 10, loss = 0.00637118\n",
      "Iteration 11, loss = 0.00357011\n",
      "Iteration 12, loss = 0.00432622\n",
      "Iteration 13, loss = 0.00644809\n",
      "Iteration 14, loss = 0.00754791\n",
      "Iteration 15, loss = 0.00690486\n",
      "Iteration 16, loss = 0.00525956\n",
      "Iteration 17, loss = 0.00377419\n",
      "Iteration 18, loss = 0.00319362\n",
      "Iteration 19, loss = 0.00353758\n",
      "Iteration 20, loss = 0.00427058\n",
      "Iteration 21, loss = 0.00466018\n",
      "Iteration 22, loss = 0.00436702\n",
      "Iteration 23, loss = 0.00361105\n",
      "Iteration 24, loss = 0.00300663\n",
      "Iteration 25, loss = 0.00306068\n",
      "Iteration 26, loss = 0.00354272\n",
      "Iteration 27, loss = 0.00363087\n",
      "Iteration 28, loss = 0.00330697\n",
      "Iteration 29, loss = 0.00294224\n",
      "Iteration 30, loss = 0.00281875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31, loss = 0.00293622\n",
      "Iteration 32, loss = 0.00305418\n",
      "Iteration 33, loss = 0.00305219\n",
      "Iteration 34, loss = 0.00295283\n",
      "Iteration 35, loss = 0.00281588\n",
      "Iteration 36, loss = 0.00273023\n",
      "Iteration 37, loss = 0.00272942\n",
      "Iteration 38, loss = 0.00278132\n",
      "Iteration 39, loss = 0.00282102\n",
      "Iteration 40, loss = 0.00280214\n",
      "Iteration 41, loss = 0.00272883\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04813867\n",
      "Iteration 2, loss = 0.08048951\n",
      "Iteration 3, loss = 0.00582904\n",
      "Iteration 4, loss = 0.03422184\n",
      "Iteration 5, loss = 0.02016352\n",
      "Iteration 6, loss = 0.00436291\n",
      "Iteration 7, loss = 0.00991332\n",
      "Iteration 8, loss = 0.01544178\n",
      "Iteration 9, loss = 0.01163587\n",
      "Iteration 10, loss = 0.00631214\n",
      "Iteration 11, loss = 0.00353424\n",
      "Iteration 12, loss = 0.00434414\n",
      "Iteration 13, loss = 0.00652045\n",
      "Iteration 14, loss = 0.00756476\n",
      "Iteration 15, loss = 0.00682054\n",
      "Iteration 16, loss = 0.00512349\n",
      "Iteration 17, loss = 0.00369105\n",
      "Iteration 18, loss = 0.00319864\n",
      "Iteration 19, loss = 0.00361057\n",
      "Iteration 20, loss = 0.00436792\n",
      "Iteration 21, loss = 0.00474852\n",
      "Iteration 22, loss = 0.00444798\n",
      "Iteration 23, loss = 0.00371668\n",
      "Iteration 24, loss = 0.00308383\n",
      "Iteration 25, loss = 0.00294197\n",
      "Iteration 26, loss = 0.00333937\n",
      "Iteration 27, loss = 0.00363377\n",
      "Iteration 28, loss = 0.00355780\n",
      "Iteration 29, loss = 0.00321407\n",
      "Iteration 30, loss = 0.00288813\n",
      "Iteration 31, loss = 0.00279927\n",
      "Iteration 32, loss = 0.00293616\n",
      "Iteration 33, loss = 0.00304130\n",
      "Iteration 34, loss = 0.00301515\n",
      "Iteration 35, loss = 0.00290094\n",
      "Iteration 36, loss = 0.00278211\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04822341\n",
      "Iteration 2, loss = 0.08143252\n",
      "Iteration 3, loss = 0.00532587\n",
      "Iteration 4, loss = 0.03440030\n",
      "Iteration 5, loss = 0.02006101\n",
      "Iteration 6, loss = 0.00437729\n",
      "Iteration 7, loss = 0.00908785\n",
      "Iteration 8, loss = 0.01459824\n",
      "Iteration 9, loss = 0.01182902\n",
      "Iteration 10, loss = 0.00697500\n",
      "Iteration 11, loss = 0.00380744\n",
      "Iteration 12, loss = 0.00401381\n",
      "Iteration 13, loss = 0.00600831\n",
      "Iteration 14, loss = 0.00728027\n",
      "Iteration 15, loss = 0.00680642\n",
      "Iteration 16, loss = 0.00520693\n",
      "Iteration 17, loss = 0.00372226\n",
      "Iteration 18, loss = 0.00320725\n",
      "Iteration 19, loss = 0.00372654\n",
      "Iteration 20, loss = 0.00454649\n",
      "Iteration 21, loss = 0.00483461\n",
      "Iteration 22, loss = 0.00431512\n",
      "Iteration 23, loss = 0.00352801\n",
      "Iteration 24, loss = 0.00305987\n",
      "Iteration 25, loss = 0.00329199\n",
      "Iteration 26, loss = 0.00365043\n",
      "Iteration 27, loss = 0.00358144\n",
      "Iteration 28, loss = 0.00322297\n",
      "Iteration 29, loss = 0.00298115\n",
      "Iteration 30, loss = 0.00309649\n",
      "Iteration 31, loss = 0.00317716\n",
      "Iteration 32, loss = 0.00304405\n",
      "Iteration 33, loss = 0.00287332\n",
      "Iteration 34, loss = 0.00284880\n",
      "Iteration 35, loss = 0.00292557\n",
      "Iteration 36, loss = 0.00294020\n",
      "Iteration 37, loss = 0.00287431\n",
      "Iteration 38, loss = 0.00280209\n",
      "Iteration 39, loss = 0.00277693\n",
      "Iteration 40, loss = 0.00277886\n",
      "Iteration 41, loss = 0.00276802\n",
      "Iteration 42, loss = 0.00272759\n",
      "Iteration 43, loss = 0.00267873\n",
      "Iteration 44, loss = 0.00264251\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04797015\n",
      "Iteration 2, loss = 0.07999794\n",
      "Iteration 3, loss = 0.00587530\n",
      "Iteration 4, loss = 0.03486082\n",
      "Iteration 5, loss = 0.01963345\n",
      "Iteration 6, loss = 0.00417488\n",
      "Iteration 7, loss = 0.01027162\n",
      "Iteration 8, loss = 0.01549087\n",
      "Iteration 9, loss = 0.01155618\n",
      "Iteration 10, loss = 0.00620066\n",
      "Iteration 11, loss = 0.00348715\n",
      "Iteration 12, loss = 0.00447032\n",
      "Iteration 13, loss = 0.00668610\n",
      "Iteration 14, loss = 0.00768742\n",
      "Iteration 15, loss = 0.00681896\n",
      "Iteration 16, loss = 0.00502732\n",
      "Iteration 17, loss = 0.00356317\n",
      "Iteration 18, loss = 0.00315747\n",
      "Iteration 19, loss = 0.00373539\n",
      "Iteration 20, loss = 0.00455580\n",
      "Iteration 21, loss = 0.00478697\n",
      "Iteration 22, loss = 0.00422756\n",
      "Iteration 23, loss = 0.00338487\n",
      "Iteration 24, loss = 0.00293647\n",
      "Iteration 25, loss = 0.00325104\n",
      "Iteration 26, loss = 0.00360250\n",
      "Iteration 27, loss = 0.00345742\n",
      "Iteration 28, loss = 0.00301223\n",
      "Iteration 29, loss = 0.00282057\n",
      "Iteration 30, loss = 0.00305409\n",
      "Iteration 31, loss = 0.00308121\n",
      "Iteration 32, loss = 0.00287853\n",
      "Iteration 33, loss = 0.00273340\n",
      "Iteration 34, loss = 0.00274532\n",
      "Iteration 35, loss = 0.00281023\n",
      "Iteration 36, loss = 0.00282412\n",
      "Iteration 37, loss = 0.00277126\n",
      "Iteration 38, loss = 0.00269320\n",
      "Iteration 39, loss = 0.00264316\n",
      "Iteration 40, loss = 0.00263757\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04794787\n",
      "Iteration 2, loss = 0.07949256\n",
      "Iteration 3, loss = 0.00594344\n",
      "Iteration 4, loss = 0.03450083\n",
      "Iteration 5, loss = 0.01996507\n",
      "Iteration 6, loss = 0.00425501\n",
      "Iteration 7, loss = 0.01017239\n",
      "Iteration 8, loss = 0.01572890\n",
      "Iteration 9, loss = 0.01142686\n",
      "Iteration 10, loss = 0.00600898\n",
      "Iteration 11, loss = 0.00345914\n",
      "Iteration 12, loss = 0.00468514\n",
      "Iteration 13, loss = 0.00693596\n",
      "Iteration 14, loss = 0.00769282\n",
      "Iteration 15, loss = 0.00653341\n",
      "Iteration 16, loss = 0.00461724\n",
      "Iteration 17, loss = 0.00331940\n",
      "Iteration 18, loss = 0.00325270\n",
      "Iteration 19, loss = 0.00400696\n",
      "Iteration 20, loss = 0.00471192\n",
      "Iteration 21, loss = 0.00474801\n",
      "Iteration 22, loss = 0.00411070\n",
      "Iteration 23, loss = 0.00330901\n",
      "Iteration 24, loss = 0.00293644\n",
      "Iteration 25, loss = 0.00327042\n",
      "Iteration 26, loss = 0.00363177\n",
      "Iteration 27, loss = 0.00356108\n",
      "Iteration 28, loss = 0.00321350\n",
      "Iteration 29, loss = 0.00290641\n",
      "Iteration 30, loss = 0.00282278\n",
      "Iteration 31, loss = 0.00290597\n",
      "Iteration 32, loss = 0.00296926\n",
      "Iteration 33, loss = 0.00295777\n",
      "Iteration 34, loss = 0.00289271\n",
      "Iteration 35, loss = 0.00280057\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04808851\n",
      "Iteration 2, loss = 0.08080889\n",
      "Iteration 3, loss = 0.00512566\n",
      "Iteration 4, loss = 0.03326081\n",
      "Iteration 5, loss = 0.01875075\n",
      "Iteration 6, loss = 0.00413425\n",
      "Iteration 7, loss = 0.00863017\n",
      "Iteration 8, loss = 0.01210438\n",
      "Iteration 9, loss = 0.00830975\n",
      "Iteration 10, loss = 0.00447864\n",
      "Iteration 11, loss = 0.00382683\n",
      "Iteration 12, loss = 0.00582690\n",
      "Iteration 13, loss = 0.00677784\n",
      "Iteration 14, loss = 0.00540377\n",
      "Iteration 15, loss = 0.00361138\n",
      "Iteration 16, loss = 0.00333246\n",
      "Iteration 17, loss = 0.00440505\n",
      "Iteration 18, loss = 0.00504809\n",
      "Iteration 19, loss = 0.00446960\n",
      "Iteration 20, loss = 0.00342590\n",
      "Iteration 21, loss = 0.00299815\n",
      "Iteration 22, loss = 0.00339855\n",
      "Iteration 23, loss = 0.00383051\n",
      "Iteration 24, loss = 0.00377887\n",
      "Iteration 25, loss = 0.00331338\n",
      "Iteration 26, loss = 0.00291240\n",
      "Iteration 27, loss = 0.00289822\n",
      "Iteration 28, loss = 0.00317877\n",
      "Iteration 29, loss = 0.00331273\n",
      "Iteration 30, loss = 0.00310189\n",
      "Iteration 31, loss = 0.00279968\n",
      "Iteration 32, loss = 0.00270230\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04818814\n",
      "Iteration 2, loss = 0.08091325\n",
      "Iteration 3, loss = 0.00513265\n",
      "Iteration 4, loss = 0.03324725\n",
      "Iteration 5, loss = 0.01854000\n",
      "Iteration 6, loss = 0.00398567\n",
      "Iteration 7, loss = 0.00907922\n",
      "Iteration 8, loss = 0.01239654\n",
      "Iteration 9, loss = 0.00848331\n",
      "Iteration 10, loss = 0.00435537\n",
      "Iteration 11, loss = 0.00395620\n",
      "Iteration 12, loss = 0.00630111\n",
      "Iteration 13, loss = 0.00730312\n",
      "Iteration 14, loss = 0.00578365\n",
      "Iteration 15, loss = 0.00373856\n",
      "Iteration 16, loss = 0.00320891\n",
      "Iteration 17, loss = 0.00425307\n",
      "Iteration 18, loss = 0.00514309\n",
      "Iteration 19, loss = 0.00477950\n",
      "Iteration 20, loss = 0.00372874\n",
      "Iteration 21, loss = 0.00301161\n",
      "Iteration 22, loss = 0.00313716\n",
      "Iteration 23, loss = 0.00364395\n",
      "Iteration 24, loss = 0.00382259\n",
      "Iteration 25, loss = 0.00352822\n",
      "Iteration 26, loss = 0.00305282\n",
      "Iteration 27, loss = 0.00277485\n",
      "Iteration 28, loss = 0.00287009\n",
      "Iteration 29, loss = 0.00307049\n",
      "Iteration 30, loss = 0.00304448\n",
      "Iteration 31, loss = 0.00284501\n",
      "Iteration 32, loss = 0.00263775\n",
      "Iteration 33, loss = 0.00262894\n",
      "Iteration 34, loss = 0.00275760\n",
      "Iteration 35, loss = 0.00274428\n",
      "Iteration 36, loss = 0.00260538\n",
      "Iteration 37, loss = 0.00250442\n",
      "Iteration 38, loss = 0.00251371\n",
      "Iteration 39, loss = 0.00256159\n",
      "Iteration 40, loss = 0.00255103\n",
      "Iteration 41, loss = 0.00247550\n",
      "Iteration 42, loss = 0.00240635\n",
      "Iteration 43, loss = 0.00239422\n",
      "Iteration 44, loss = 0.00241801\n",
      "Iteration 45, loss = 0.00241692\n",
      "Iteration 46, loss = 0.00236988\n",
      "Iteration 47, loss = 0.00231846\n",
      "Iteration 48, loss = 0.00229930\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04830311\n",
      "Iteration 2, loss = 0.08166056\n",
      "Iteration 3, loss = 0.00520663\n",
      "Iteration 4, loss = 0.03410865\n",
      "Iteration 5, loss = 0.01966323\n",
      "Iteration 6, loss = 0.00429134\n",
      "Iteration 7, loss = 0.00884835\n",
      "Iteration 8, loss = 0.01373975\n",
      "Iteration 9, loss = 0.01025549\n",
      "Iteration 10, loss = 0.00530385\n",
      "Iteration 11, loss = 0.00358443\n",
      "Iteration 12, loss = 0.00554554\n",
      "Iteration 13, loss = 0.00729695\n",
      "Iteration 14, loss = 0.00648629\n",
      "Iteration 15, loss = 0.00441296\n",
      "Iteration 16, loss = 0.00324833\n",
      "Iteration 17, loss = 0.00377814\n",
      "Iteration 18, loss = 0.00489396\n",
      "Iteration 19, loss = 0.00515885\n",
      "Iteration 20, loss = 0.00432922\n",
      "Iteration 21, loss = 0.00333524\n",
      "Iteration 22, loss = 0.00302629\n",
      "Iteration 23, loss = 0.00345864\n",
      "Iteration 24, loss = 0.00384596\n",
      "Iteration 25, loss = 0.00375102\n",
      "Iteration 26, loss = 0.00327542\n",
      "Iteration 27, loss = 0.00292189\n",
      "Iteration 28, loss = 0.00303627\n",
      "Iteration 29, loss = 0.00315475\n",
      "Iteration 30, loss = 0.00296742\n",
      "Iteration 31, loss = 0.00276619\n",
      "Iteration 32, loss = 0.00272444\n",
      "Iteration 33, loss = 0.00281308\n",
      "Iteration 34, loss = 0.00279373\n",
      "Iteration 35, loss = 0.00268161\n",
      "Iteration 36, loss = 0.00262519\n",
      "Iteration 37, loss = 0.00265508\n",
      "Iteration 38, loss = 0.00267133\n",
      "Iteration 39, loss = 0.00260797\n",
      "Iteration 40, loss = 0.00253819\n",
      "Iteration 41, loss = 0.00254152\n",
      "Iteration 42, loss = 0.00254812\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04825819\n",
      "Iteration 2, loss = 0.08139631\n",
      "Iteration 3, loss = 0.00559576\n",
      "Iteration 4, loss = 0.03484085\n",
      "Iteration 5, loss = 0.02077660\n",
      "Iteration 6, loss = 0.00448695\n",
      "Iteration 7, loss = 0.00971682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.01552341\n",
      "Iteration 9, loss = 0.01193933\n",
      "Iteration 10, loss = 0.00675608\n",
      "Iteration 11, loss = 0.00368470\n",
      "Iteration 12, loss = 0.00413337\n",
      "Iteration 13, loss = 0.00618768\n",
      "Iteration 14, loss = 0.00747185\n",
      "Iteration 15, loss = 0.00701846\n",
      "Iteration 16, loss = 0.00540825\n",
      "Iteration 17, loss = 0.00380447\n",
      "Iteration 18, loss = 0.00315348\n",
      "Iteration 19, loss = 0.00361652\n",
      "Iteration 20, loss = 0.00449482\n",
      "Iteration 21, loss = 0.00491454\n",
      "Iteration 22, loss = 0.00447799\n",
      "Iteration 23, loss = 0.00366551\n",
      "Iteration 24, loss = 0.00309023\n",
      "Iteration 25, loss = 0.00315838\n",
      "Iteration 26, loss = 0.00361303\n",
      "Iteration 27, loss = 0.00373025\n",
      "Iteration 28, loss = 0.00347674\n",
      "Iteration 29, loss = 0.00312208\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04842141\n",
      "Iteration 2, loss = 0.08122213\n",
      "Iteration 3, loss = 0.00543269\n",
      "Iteration 4, loss = 0.03537941\n",
      "Iteration 5, loss = 0.02011799\n",
      "Iteration 6, loss = 0.00434992\n",
      "Iteration 7, loss = 0.00912186\n",
      "Iteration 8, loss = 0.01477050\n",
      "Iteration 9, loss = 0.01187188\n",
      "Iteration 10, loss = 0.00701266\n",
      "Iteration 11, loss = 0.00382990\n",
      "Iteration 12, loss = 0.00399526\n",
      "Iteration 13, loss = 0.00595081\n",
      "Iteration 14, loss = 0.00731041\n",
      "Iteration 15, loss = 0.00699538\n",
      "Iteration 16, loss = 0.00543634\n",
      "Iteration 17, loss = 0.00384815\n",
      "Iteration 18, loss = 0.00318298\n",
      "Iteration 19, loss = 0.00358405\n",
      "Iteration 20, loss = 0.00444081\n",
      "Iteration 21, loss = 0.00484922\n",
      "Iteration 22, loss = 0.00446447\n",
      "Iteration 23, loss = 0.00368035\n",
      "Iteration 24, loss = 0.00310201\n",
      "Iteration 25, loss = 0.00318652\n",
      "Iteration 26, loss = 0.00363004\n",
      "Iteration 27, loss = 0.00370719\n",
      "Iteration 28, loss = 0.00345654\n",
      "Iteration 29, loss = 0.00312343\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04828900\n",
      "Iteration 2, loss = 0.08023786\n",
      "Iteration 3, loss = 0.00582848\n",
      "Iteration 4, loss = 0.03463503\n",
      "Iteration 5, loss = 0.02008483\n",
      "Iteration 6, loss = 0.00428858\n",
      "Iteration 7, loss = 0.01013272\n",
      "Iteration 8, loss = 0.01563041\n",
      "Iteration 9, loss = 0.01165090\n",
      "Iteration 10, loss = 0.00633527\n",
      "Iteration 11, loss = 0.00352114\n",
      "Iteration 12, loss = 0.00434043\n",
      "Iteration 13, loss = 0.00651679\n",
      "Iteration 14, loss = 0.00761086\n",
      "Iteration 15, loss = 0.00690025\n",
      "Iteration 16, loss = 0.00520032\n",
      "Iteration 17, loss = 0.00371117\n",
      "Iteration 18, loss = 0.00317919\n",
      "Iteration 19, loss = 0.00359951\n",
      "Iteration 20, loss = 0.00438827\n",
      "Iteration 21, loss = 0.00476558\n",
      "Iteration 22, loss = 0.00440548\n",
      "Iteration 23, loss = 0.00362328\n",
      "Iteration 24, loss = 0.00300366\n",
      "Iteration 25, loss = 0.00306023\n",
      "Iteration 26, loss = 0.00353672\n",
      "Iteration 27, loss = 0.00363151\n",
      "Iteration 28, loss = 0.00331162\n",
      "Iteration 29, loss = 0.00294789\n",
      "Iteration 30, loss = 0.00281930\n",
      "Iteration 31, loss = 0.00292246\n",
      "Iteration 32, loss = 0.00299916\n",
      "Iteration 33, loss = 0.00294039\n",
      "Iteration 34, loss = 0.00280670\n",
      "Iteration 35, loss = 0.00274279\n",
      "Iteration 36, loss = 0.00283539\n",
      "Iteration 37, loss = 0.00279627\n",
      "Iteration 38, loss = 0.00269803\n",
      "Iteration 39, loss = 0.00266751\n",
      "Iteration 40, loss = 0.00268897\n",
      "Iteration 41, loss = 0.00269080\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04816224\n",
      "Iteration 2, loss = 0.08083274\n",
      "Iteration 3, loss = 0.00566840\n",
      "Iteration 4, loss = 0.03484143\n",
      "Iteration 5, loss = 0.02035831\n",
      "Iteration 6, loss = 0.00439263\n",
      "Iteration 7, loss = 0.00986946\n",
      "Iteration 8, loss = 0.01558748\n",
      "Iteration 9, loss = 0.01181778\n",
      "Iteration 10, loss = 0.00663769\n",
      "Iteration 11, loss = 0.00366809\n",
      "Iteration 12, loss = 0.00422244\n",
      "Iteration 13, loss = 0.00630724\n",
      "Iteration 14, loss = 0.00753594\n",
      "Iteration 15, loss = 0.00703754\n",
      "Iteration 16, loss = 0.00543560\n",
      "Iteration 17, loss = 0.00388308\n",
      "Iteration 18, loss = 0.00319546\n",
      "Iteration 19, loss = 0.00350190\n",
      "Iteration 20, loss = 0.00430744\n",
      "Iteration 21, loss = 0.00483051\n",
      "Iteration 22, loss = 0.00461218\n",
      "Iteration 23, loss = 0.00393029\n",
      "Iteration 24, loss = 0.00327358\n",
      "Iteration 25, loss = 0.00302691\n",
      "Iteration 26, loss = 0.00330252\n",
      "Iteration 27, loss = 0.00364345\n",
      "Iteration 28, loss = 0.00368451\n",
      "Iteration 29, loss = 0.00341950\n",
      "Iteration 30, loss = 0.00307928\n",
      "Iteration 31, loss = 0.00290136\n",
      "Iteration 32, loss = 0.00299358\n",
      "Iteration 33, loss = 0.00312996\n",
      "Iteration 34, loss = 0.00313003\n",
      "Iteration 35, loss = 0.00302066\n",
      "Iteration 36, loss = 0.00289725\n",
      "Iteration 37, loss = 0.00282078\n",
      "Iteration 38, loss = 0.00280318\n",
      "Iteration 39, loss = 0.00282851\n",
      "Iteration 40, loss = 0.00285685\n",
      "Iteration 41, loss = 0.00285175\n",
      "Iteration 42, loss = 0.00280845\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04823747\n",
      "Iteration 2, loss = 0.08155052\n",
      "Iteration 3, loss = 0.00517863\n",
      "Iteration 4, loss = 0.03453986\n",
      "Iteration 5, loss = 0.01924950\n",
      "Iteration 6, loss = 0.00415650\n",
      "Iteration 7, loss = 0.00919496\n",
      "Iteration 8, loss = 0.01386259\n",
      "Iteration 9, loss = 0.01022109\n",
      "Iteration 10, loss = 0.00516644\n",
      "Iteration 11, loss = 0.00363875\n",
      "Iteration 12, loss = 0.00582582\n",
      "Iteration 13, loss = 0.00757718\n",
      "Iteration 14, loss = 0.00661996\n",
      "Iteration 15, loss = 0.00442025\n",
      "Iteration 16, loss = 0.00324676\n",
      "Iteration 17, loss = 0.00384363\n",
      "Iteration 18, loss = 0.00501223\n",
      "Iteration 19, loss = 0.00523383\n",
      "Iteration 20, loss = 0.00436929\n",
      "Iteration 21, loss = 0.00333376\n",
      "Iteration 22, loss = 0.00302306\n",
      "Iteration 23, loss = 0.00347871\n",
      "Iteration 24, loss = 0.00389676\n",
      "Iteration 25, loss = 0.00384265\n",
      "Iteration 26, loss = 0.00338417\n",
      "Iteration 27, loss = 0.00296231\n",
      "Iteration 28, loss = 0.00283551\n",
      "Iteration 29, loss = 0.00302941\n",
      "Iteration 30, loss = 0.00317664\n",
      "Iteration 31, loss = 0.00309674\n",
      "Iteration 32, loss = 0.00286367\n",
      "Iteration 33, loss = 0.00268969\n",
      "Iteration 34, loss = 0.00272624\n",
      "Iteration 35, loss = 0.00284285\n",
      "Iteration 36, loss = 0.00281844\n",
      "Iteration 37, loss = 0.00269444\n",
      "Iteration 38, loss = 0.00260098\n",
      "Iteration 39, loss = 0.00259904\n",
      "Iteration 40, loss = 0.00264216\n",
      "Iteration 41, loss = 0.00264296\n",
      "Iteration 42, loss = 0.00258047\n",
      "Iteration 43, loss = 0.00250585\n",
      "Iteration 44, loss = 0.00247464\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04837300\n",
      "Iteration 2, loss = 0.08358310\n",
      "Iteration 3, loss = 0.00527127\n",
      "Iteration 4, loss = 0.03441523\n",
      "Iteration 5, loss = 0.02068170\n",
      "Iteration 6, loss = 0.00449450\n",
      "Iteration 7, loss = 0.00942023\n",
      "Iteration 8, loss = 0.01506657\n",
      "Iteration 9, loss = 0.01182437\n",
      "Iteration 10, loss = 0.00690575\n",
      "Iteration 11, loss = 0.00385938\n",
      "Iteration 12, loss = 0.00413785\n",
      "Iteration 13, loss = 0.00604753\n",
      "Iteration 14, loss = 0.00735018\n",
      "Iteration 15, loss = 0.00703180\n",
      "Iteration 16, loss = 0.00553211\n",
      "Iteration 17, loss = 0.00394733\n",
      "Iteration 18, loss = 0.00316127\n",
      "Iteration 19, loss = 0.00345162\n",
      "Iteration 20, loss = 0.00427027\n",
      "Iteration 21, loss = 0.00471652\n",
      "Iteration 22, loss = 0.00452210\n",
      "Iteration 23, loss = 0.00389172\n",
      "Iteration 24, loss = 0.00325112\n",
      "Iteration 25, loss = 0.00298448\n",
      "Iteration 26, loss = 0.00322256\n",
      "Iteration 27, loss = 0.00355208\n",
      "Iteration 28, loss = 0.00361149\n",
      "Iteration 29, loss = 0.00338364\n",
      "Iteration 30, loss = 0.00306680\n",
      "Iteration 31, loss = 0.00287237\n",
      "Iteration 32, loss = 0.00291404\n",
      "Iteration 33, loss = 0.00302519\n",
      "Iteration 34, loss = 0.00305000\n",
      "Iteration 35, loss = 0.00297577\n",
      "Iteration 36, loss = 0.00286287\n",
      "Iteration 37, loss = 0.00276789\n",
      "Iteration 38, loss = 0.00271616\n",
      "Iteration 39, loss = 0.00271997\n",
      "Iteration 40, loss = 0.00275066\n",
      "Iteration 41, loss = 0.00276193\n",
      "Iteration 42, loss = 0.00270546\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04843076\n",
      "Iteration 2, loss = 0.08216060\n",
      "Iteration 3, loss = 0.00518021\n",
      "Iteration 4, loss = 0.03442589\n",
      "Iteration 5, loss = 0.01998588\n",
      "Iteration 6, loss = 0.00437899\n",
      "Iteration 7, loss = 0.00876916\n",
      "Iteration 8, loss = 0.01381810\n",
      "Iteration 9, loss = 0.01044651\n",
      "Iteration 10, loss = 0.00547861\n",
      "Iteration 11, loss = 0.00359149\n",
      "Iteration 12, loss = 0.00542515\n",
      "Iteration 13, loss = 0.00725135\n",
      "Iteration 14, loss = 0.00659263\n",
      "Iteration 15, loss = 0.00456404\n",
      "Iteration 16, loss = 0.00329985\n",
      "Iteration 17, loss = 0.00370633\n",
      "Iteration 18, loss = 0.00482073\n",
      "Iteration 19, loss = 0.00517197\n",
      "Iteration 20, loss = 0.00441992\n",
      "Iteration 21, loss = 0.00341726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 0.00303238\n",
      "Iteration 23, loss = 0.00341989\n",
      "Iteration 24, loss = 0.00383275\n",
      "Iteration 25, loss = 0.00379254\n",
      "Iteration 26, loss = 0.00334587\n",
      "Iteration 27, loss = 0.00296065\n",
      "Iteration 28, loss = 0.00301143\n",
      "Iteration 29, loss = 0.00315111\n",
      "Iteration 30, loss = 0.00298690\n",
      "Iteration 31, loss = 0.00278442\n",
      "Iteration 32, loss = 0.00272343\n",
      "Iteration 33, loss = 0.00280125\n",
      "Iteration 34, loss = 0.00278770\n",
      "Iteration 35, loss = 0.00268386\n",
      "Iteration 36, loss = 0.00261973\n",
      "Iteration 37, loss = 0.00264335\n",
      "Iteration 38, loss = 0.00266646\n",
      "Iteration 39, loss = 0.00262028\n",
      "Iteration 40, loss = 0.00255310\n",
      "Iteration 41, loss = 0.00252806\n",
      "Iteration 42, loss = 0.00254219\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04842102\n",
      "Iteration 2, loss = 0.08134053\n",
      "Iteration 3, loss = 0.00562197\n",
      "Iteration 4, loss = 0.03495351\n",
      "Iteration 5, loss = 0.02064616\n",
      "Iteration 6, loss = 0.00443676\n",
      "Iteration 7, loss = 0.00981585\n",
      "Iteration 8, loss = 0.01549436\n",
      "Iteration 9, loss = 0.01190721\n",
      "Iteration 10, loss = 0.00671547\n",
      "Iteration 11, loss = 0.00365643\n",
      "Iteration 12, loss = 0.00411911\n",
      "Iteration 13, loss = 0.00614479\n",
      "Iteration 14, loss = 0.00734722\n",
      "Iteration 15, loss = 0.00678863\n",
      "Iteration 16, loss = 0.00511915\n",
      "Iteration 17, loss = 0.00363044\n",
      "Iteration 18, loss = 0.00319821\n",
      "Iteration 19, loss = 0.00378495\n",
      "Iteration 20, loss = 0.00461848\n",
      "Iteration 21, loss = 0.00486556\n",
      "Iteration 22, loss = 0.00436273\n",
      "Iteration 23, loss = 0.00357170\n",
      "Iteration 24, loss = 0.00309761\n",
      "Iteration 25, loss = 0.00328218\n",
      "Iteration 26, loss = 0.00370287\n",
      "Iteration 27, loss = 0.00374083\n",
      "Iteration 28, loss = 0.00346913\n",
      "Iteration 29, loss = 0.00313893\n",
      "Iteration 30, loss = 0.00295117\n",
      "Iteration 31, loss = 0.00297538\n",
      "Iteration 32, loss = 0.00304810\n",
      "Iteration 33, loss = 0.00303431\n",
      "Iteration 34, loss = 0.00294621\n",
      "Iteration 35, loss = 0.00285238\n",
      "Iteration 36, loss = 0.00284595\n",
      "Iteration 37, loss = 0.00289757\n",
      "Iteration 38, loss = 0.00287063\n",
      "Iteration 39, loss = 0.00279443\n",
      "Iteration 40, loss = 0.00274710\n",
      "Iteration 41, loss = 0.00274740\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04840164\n",
      "Iteration 2, loss = 0.08137556\n",
      "Iteration 3, loss = 0.00566468\n",
      "Iteration 4, loss = 0.03462324\n",
      "Iteration 5, loss = 0.02058454\n",
      "Iteration 6, loss = 0.00441349\n",
      "Iteration 7, loss = 0.00991297\n",
      "Iteration 8, loss = 0.01566246\n",
      "Iteration 9, loss = 0.01186852\n",
      "Iteration 10, loss = 0.00662260\n",
      "Iteration 11, loss = 0.00362849\n",
      "Iteration 12, loss = 0.00420611\n",
      "Iteration 13, loss = 0.00631194\n",
      "Iteration 14, loss = 0.00757409\n",
      "Iteration 15, loss = 0.00711438\n",
      "Iteration 16, loss = 0.00551372\n",
      "Iteration 17, loss = 0.00391951\n",
      "Iteration 18, loss = 0.00317170\n",
      "Iteration 19, loss = 0.00342743\n",
      "Iteration 20, loss = 0.00420474\n",
      "Iteration 21, loss = 0.00465830\n",
      "Iteration 22, loss = 0.00441484\n",
      "Iteration 23, loss = 0.00369356\n",
      "Iteration 24, loss = 0.00304955\n",
      "Iteration 25, loss = 0.00301793\n",
      "Iteration 26, loss = 0.00350394\n",
      "Iteration 27, loss = 0.00363574\n",
      "Iteration 28, loss = 0.00333043\n",
      "Iteration 29, loss = 0.00295815\n",
      "Iteration 30, loss = 0.00284875\n",
      "Iteration 31, loss = 0.00295454\n",
      "Iteration 32, loss = 0.00302379\n",
      "Iteration 33, loss = 0.00300967\n",
      "Iteration 34, loss = 0.00293196\n",
      "Iteration 35, loss = 0.00282687\n",
      "Iteration 36, loss = 0.00275239\n",
      "Iteration 37, loss = 0.00274254\n",
      "Iteration 38, loss = 0.00277892\n",
      "Iteration 39, loss = 0.00281025\n",
      "Iteration 40, loss = 0.00279705\n",
      "Iteration 41, loss = 0.00274016\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04840392\n",
      "Iteration 2, loss = 0.08232781\n",
      "Iteration 3, loss = 0.00525432\n",
      "Iteration 4, loss = 0.03460289\n",
      "Iteration 5, loss = 0.02014563\n",
      "Iteration 6, loss = 0.00436644\n",
      "Iteration 7, loss = 0.00909607\n",
      "Iteration 8, loss = 0.01455260\n",
      "Iteration 9, loss = 0.01185075\n",
      "Iteration 10, loss = 0.00701108\n",
      "Iteration 11, loss = 0.00384137\n",
      "Iteration 12, loss = 0.00397835\n",
      "Iteration 13, loss = 0.00586544\n",
      "Iteration 14, loss = 0.00708839\n",
      "Iteration 15, loss = 0.00666047\n",
      "Iteration 16, loss = 0.00513465\n",
      "Iteration 17, loss = 0.00368276\n",
      "Iteration 18, loss = 0.00321201\n",
      "Iteration 19, loss = 0.00377724\n",
      "Iteration 20, loss = 0.00458722\n",
      "Iteration 21, loss = 0.00475432\n",
      "Iteration 22, loss = 0.00418185\n",
      "Iteration 23, loss = 0.00342542\n",
      "Iteration 24, loss = 0.00303906\n",
      "Iteration 25, loss = 0.00325846\n",
      "Iteration 26, loss = 0.00362174\n",
      "Iteration 27, loss = 0.00367069\n",
      "Iteration 28, loss = 0.00334913\n",
      "Iteration 29, loss = 0.00301123\n",
      "Iteration 30, loss = 0.00303335\n",
      "Iteration 31, loss = 0.00315943\n",
      "Iteration 32, loss = 0.00305774\n",
      "Iteration 33, loss = 0.00286238\n",
      "Iteration 34, loss = 0.00281056\n",
      "Iteration 35, loss = 0.00290511\n",
      "Iteration 36, loss = 0.00290447\n",
      "Iteration 37, loss = 0.00280481\n",
      "Iteration 38, loss = 0.00275289\n",
      "Iteration 39, loss = 0.00278126\n",
      "Iteration 40, loss = 0.00275425\n",
      "Iteration 41, loss = 0.00269917\n",
      "Iteration 42, loss = 0.00265284\n",
      "Iteration 43, loss = 0.00263133\n",
      "Iteration 44, loss = 0.00262264\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04827348\n",
      "Iteration 2, loss = 0.08328648\n",
      "Iteration 3, loss = 0.00512781\n",
      "Iteration 4, loss = 0.03453188\n",
      "Iteration 5, loss = 0.01943809\n",
      "Iteration 6, loss = 0.00419514\n",
      "Iteration 7, loss = 0.00929443\n",
      "Iteration 8, loss = 0.01404492\n",
      "Iteration 9, loss = 0.01025031\n",
      "Iteration 10, loss = 0.00518550\n",
      "Iteration 11, loss = 0.00370311\n",
      "Iteration 12, loss = 0.00593400\n",
      "Iteration 13, loss = 0.00768087\n",
      "Iteration 14, loss = 0.00679885\n",
      "Iteration 15, loss = 0.00463156\n",
      "Iteration 16, loss = 0.00327347\n",
      "Iteration 17, loss = 0.00357723\n",
      "Iteration 18, loss = 0.00474208\n",
      "Iteration 19, loss = 0.00521817\n",
      "Iteration 20, loss = 0.00462678\n",
      "Iteration 21, loss = 0.00361358\n",
      "Iteration 22, loss = 0.00298096\n",
      "Iteration 23, loss = 0.00313491\n",
      "Iteration 24, loss = 0.00361934\n",
      "Iteration 25, loss = 0.00379292\n",
      "Iteration 26, loss = 0.00353736\n",
      "Iteration 27, loss = 0.00311064\n",
      "Iteration 28, loss = 0.00282897\n",
      "Iteration 29, loss = 0.00281015\n",
      "Iteration 30, loss = 0.00296355\n",
      "Iteration 31, loss = 0.00309805\n",
      "Iteration 32, loss = 0.00307103\n",
      "Iteration 33, loss = 0.00289723\n",
      "Iteration 34, loss = 0.00271045\n",
      "Iteration 35, loss = 0.00263555\n",
      "Iteration 36, loss = 0.00268688\n",
      "Iteration 37, loss = 0.00277399\n",
      "Iteration 38, loss = 0.00279259\n",
      "Iteration 39, loss = 0.00271542\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04833429\n",
      "Iteration 2, loss = 0.08130052\n",
      "Iteration 3, loss = 0.00524276\n",
      "Iteration 4, loss = 0.03477560\n",
      "Iteration 5, loss = 0.01938300\n",
      "Iteration 6, loss = 0.00418046\n",
      "Iteration 7, loss = 0.00916489\n",
      "Iteration 8, loss = 0.01414595\n",
      "Iteration 9, loss = 0.01133574\n",
      "Iteration 10, loss = 0.00665259\n",
      "Iteration 11, loss = 0.00374850\n",
      "Iteration 12, loss = 0.00410909\n",
      "Iteration 13, loss = 0.00605692\n",
      "Iteration 14, loss = 0.00723334\n",
      "Iteration 15, loss = 0.00670600\n",
      "Iteration 16, loss = 0.00507521\n",
      "Iteration 17, loss = 0.00361241\n",
      "Iteration 18, loss = 0.00320179\n",
      "Iteration 19, loss = 0.00385717\n",
      "Iteration 20, loss = 0.00464739\n",
      "Iteration 21, loss = 0.00473989\n",
      "Iteration 22, loss = 0.00414848\n",
      "Iteration 23, loss = 0.00340623\n",
      "Iteration 24, loss = 0.00303976\n",
      "Iteration 25, loss = 0.00328438\n",
      "Iteration 26, loss = 0.00361859\n",
      "Iteration 27, loss = 0.00368980\n",
      "Iteration 28, loss = 0.00346335\n",
      "Iteration 29, loss = 0.00314609\n",
      "Iteration 30, loss = 0.00295728\n",
      "Iteration 31, loss = 0.00298844\n",
      "Iteration 32, loss = 0.00309962\n",
      "Iteration 33, loss = 0.00314225\n",
      "Iteration 34, loss = 0.00309092\n",
      "Iteration 35, loss = 0.00297100\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04835195\n",
      "Iteration 2, loss = 0.08378997\n",
      "Iteration 3, loss = 0.00523932\n",
      "Iteration 4, loss = 0.03444779\n",
      "Iteration 5, loss = 0.02071546\n",
      "Iteration 6, loss = 0.00449407\n",
      "Iteration 7, loss = 0.00941165\n",
      "Iteration 8, loss = 0.01507496\n",
      "Iteration 9, loss = 0.01185571\n",
      "Iteration 10, loss = 0.00690897\n",
      "Iteration 11, loss = 0.00385508\n",
      "Iteration 12, loss = 0.00415584\n",
      "Iteration 13, loss = 0.00608947\n",
      "Iteration 14, loss = 0.00739833\n",
      "Iteration 15, loss = 0.00705629\n",
      "Iteration 16, loss = 0.00552611\n",
      "Iteration 17, loss = 0.00392939\n",
      "Iteration 18, loss = 0.00315181\n",
      "Iteration 19, loss = 0.00346545\n",
      "Iteration 20, loss = 0.00429107\n",
      "Iteration 21, loss = 0.00473869\n",
      "Iteration 22, loss = 0.00453210\n",
      "Iteration 23, loss = 0.00388767\n",
      "Iteration 24, loss = 0.00324247\n",
      "Iteration 25, loss = 0.00298135\n",
      "Iteration 26, loss = 0.00322659\n",
      "Iteration 27, loss = 0.00355930\n",
      "Iteration 28, loss = 0.00361528\n",
      "Iteration 29, loss = 0.00338533\n",
      "Iteration 30, loss = 0.00306249\n",
      "Iteration 31, loss = 0.00287031\n",
      "Iteration 32, loss = 0.00291033\n",
      "Iteration 33, loss = 0.00302374\n",
      "Iteration 34, loss = 0.00304709\n",
      "Iteration 35, loss = 0.00297349\n",
      "Iteration 36, loss = 0.00286355\n",
      "Iteration 37, loss = 0.00277159\n",
      "Iteration 38, loss = 0.00272124\n",
      "Iteration 39, loss = 0.00272181\n",
      "Iteration 40, loss = 0.00275197\n",
      "Iteration 41, loss = 0.00276833\n",
      "Iteration 42, loss = 0.00274666\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04854978\n",
      "Iteration 2, loss = 0.08174996\n",
      "Iteration 3, loss = 0.00528249\n",
      "Iteration 4, loss = 0.03456145\n",
      "Iteration 5, loss = 0.02020707\n",
      "Iteration 6, loss = 0.00438928\n",
      "Iteration 7, loss = 0.00900531\n",
      "Iteration 8, loss = 0.01453622\n",
      "Iteration 9, loss = 0.01190484\n",
      "Iteration 10, loss = 0.00702150\n",
      "Iteration 11, loss = 0.00381525\n",
      "Iteration 12, loss = 0.00401057\n",
      "Iteration 13, loss = 0.00601894\n",
      "Iteration 14, loss = 0.00734091\n",
      "Iteration 15, loss = 0.00692661\n",
      "Iteration 16, loss = 0.00532314\n",
      "Iteration 17, loss = 0.00377386\n",
      "Iteration 18, loss = 0.00319350\n",
      "Iteration 19, loss = 0.00368276\n",
      "Iteration 20, loss = 0.00453034\n",
      "Iteration 21, loss = 0.00487428\n",
      "Iteration 22, loss = 0.00438900\n",
      "Iteration 23, loss = 0.00357260\n",
      "Iteration 24, loss = 0.00306330\n",
      "Iteration 25, loss = 0.00327351\n",
      "Iteration 26, loss = 0.00367294\n",
      "Iteration 27, loss = 0.00366546\n",
      "Iteration 28, loss = 0.00330427\n",
      "Iteration 29, loss = 0.00298332\n",
      "Iteration 30, loss = 0.00304711\n",
      "Iteration 31, loss = 0.00316593\n",
      "Iteration 32, loss = 0.00307542\n",
      "Iteration 33, loss = 0.00291162\n",
      "Iteration 34, loss = 0.00283607\n",
      "Iteration 35, loss = 0.00287018\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.04844161\n",
      "Iteration 2, loss = 0.08156044\n",
      "Iteration 3, loss = 0.00555759\n",
      "Iteration 4, loss = 0.03486983\n",
      "Iteration 5, loss = 0.02080512\n",
      "Iteration 6, loss = 0.00448222\n",
      "Iteration 7, loss = 0.00975837\n",
      "Iteration 8, loss = 0.01555411\n",
      "Iteration 9, loss = 0.01195761\n",
      "Iteration 10, loss = 0.00676509\n",
      "Iteration 11, loss = 0.00369299\n",
      "Iteration 12, loss = 0.00414688\n",
      "Iteration 13, loss = 0.00621172\n",
      "Iteration 14, loss = 0.00750742\n",
      "Iteration 15, loss = 0.00711402\n",
      "Iteration 16, loss = 0.00556682\n",
      "Iteration 17, loss = 0.00397004\n",
      "Iteration 18, loss = 0.00319851\n",
      "Iteration 19, loss = 0.00344942\n",
      "Iteration 20, loss = 0.00426889\n",
      "Iteration 21, loss = 0.00480933\n",
      "Iteration 22, loss = 0.00465623\n",
      "Iteration 23, loss = 0.00399582\n",
      "Iteration 24, loss = 0.00331446\n",
      "Iteration 25, loss = 0.00301153\n",
      "Iteration 26, loss = 0.00323646\n",
      "Iteration 27, loss = 0.00358712\n",
      "Iteration 28, loss = 0.00365076\n",
      "Iteration 29, loss = 0.00339983\n",
      "Iteration 30, loss = 0.00304580\n",
      "Iteration 31, loss = 0.00290180\n",
      "Iteration 32, loss = 0.00299715\n",
      "Iteration 33, loss = 0.00300317\n",
      "Iteration 34, loss = 0.00293789\n",
      "Iteration 35, loss = 0.00285727\n",
      "Iteration 36, loss = 0.00278953\n",
      "Iteration 37, loss = 0.00275008\n",
      "Iteration 38, loss = 0.00273906\n",
      "Iteration 39, loss = 0.00274708\n",
      "Iteration 40, loss = 0.00275248\n",
      "Iteration 41, loss = 0.00273899\n",
      "Iteration 42, loss = 0.00270596\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04863350\n",
      "Iteration 2, loss = 0.08272782\n",
      "Iteration 3, loss = 0.00524879\n",
      "Iteration 4, loss = 0.03518861\n",
      "Iteration 5, loss = 0.02052045\n",
      "Iteration 6, loss = 0.00445331\n",
      "Iteration 7, loss = 0.00904058\n",
      "Iteration 8, loss = 0.01463688\n",
      "Iteration 9, loss = 0.01195768\n",
      "Iteration 10, loss = 0.00717207\n",
      "Iteration 11, loss = 0.00391531\n",
      "Iteration 12, loss = 0.00394462\n",
      "Iteration 13, loss = 0.00577984\n",
      "Iteration 14, loss = 0.00707883\n",
      "Iteration 15, loss = 0.00677044\n",
      "Iteration 16, loss = 0.00529115\n",
      "Iteration 17, loss = 0.00379023\n",
      "Iteration 18, loss = 0.00319019\n",
      "Iteration 19, loss = 0.00365305\n",
      "Iteration 20, loss = 0.00447825\n",
      "Iteration 21, loss = 0.00478003\n",
      "Iteration 22, loss = 0.00430670\n",
      "Iteration 23, loss = 0.00352203\n",
      "Iteration 24, loss = 0.00303792\n",
      "Iteration 25, loss = 0.00324574\n",
      "Iteration 26, loss = 0.00361611\n",
      "Iteration 27, loss = 0.00358567\n",
      "Iteration 28, loss = 0.00323061\n",
      "Iteration 29, loss = 0.00294997\n",
      "Iteration 30, loss = 0.00304267\n",
      "Iteration 31, loss = 0.00313431\n",
      "Iteration 32, loss = 0.00300680\n",
      "Iteration 33, loss = 0.00283976\n",
      "Iteration 34, loss = 0.00280490\n",
      "Iteration 35, loss = 0.00287152\n",
      "Iteration 36, loss = 0.00287114\n",
      "Iteration 37, loss = 0.00280536\n",
      "Iteration 38, loss = 0.00274444\n",
      "Iteration 39, loss = 0.00272656\n",
      "Iteration 40, loss = 0.00273456\n",
      "Iteration 41, loss = 0.00273253\n",
      "Iteration 42, loss = 0.00270465\n",
      "Iteration 43, loss = 0.00266466\n",
      "Iteration 44, loss = 0.00263334\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04853273\n",
      "Iteration 2, loss = 0.08188493\n",
      "Iteration 3, loss = 0.00555261\n",
      "Iteration 4, loss = 0.03487485\n",
      "Iteration 5, loss = 0.02073679\n",
      "Iteration 6, loss = 0.00445818\n",
      "Iteration 7, loss = 0.00979609\n",
      "Iteration 8, loss = 0.01558500\n",
      "Iteration 9, loss = 0.01197284\n",
      "Iteration 10, loss = 0.00676600\n",
      "Iteration 11, loss = 0.00369994\n",
      "Iteration 12, loss = 0.00418557\n",
      "Iteration 13, loss = 0.00626229\n",
      "Iteration 14, loss = 0.00756179\n",
      "Iteration 15, loss = 0.00717161\n",
      "Iteration 16, loss = 0.00560835\n",
      "Iteration 17, loss = 0.00400018\n",
      "Iteration 18, loss = 0.00320451\n",
      "Iteration 19, loss = 0.00343757\n",
      "Iteration 20, loss = 0.00426243\n",
      "Iteration 21, loss = 0.00477951\n",
      "Iteration 22, loss = 0.00461824\n",
      "Iteration 23, loss = 0.00397381\n",
      "Iteration 24, loss = 0.00327939\n",
      "Iteration 25, loss = 0.00296440\n",
      "Iteration 26, loss = 0.00320226\n",
      "Iteration 27, loss = 0.00357370\n",
      "Iteration 28, loss = 0.00364274\n",
      "Iteration 29, loss = 0.00338590\n",
      "Iteration 30, loss = 0.00303453\n",
      "Iteration 31, loss = 0.00283476\n",
      "Iteration 32, loss = 0.00289826\n",
      "Iteration 33, loss = 0.00303331\n",
      "Iteration 34, loss = 0.00305607\n",
      "Iteration 35, loss = 0.00296662\n",
      "Iteration 36, loss = 0.00284434\n",
      "Iteration 37, loss = 0.00275262\n",
      "Iteration 38, loss = 0.00271441\n",
      "Iteration 39, loss = 0.00273078\n",
      "Iteration 40, loss = 0.00276446\n",
      "Iteration 41, loss = 0.00277062\n",
      "Iteration 42, loss = 0.00272962\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04836630\n",
      "Iteration 2, loss = 0.08329219\n",
      "Iteration 3, loss = 0.00555907\n",
      "Iteration 4, loss = 0.03521647\n",
      "Iteration 5, loss = 0.02122113\n",
      "Iteration 6, loss = 0.00457548\n",
      "Iteration 7, loss = 0.00982267\n",
      "Iteration 8, loss = 0.01591022\n",
      "Iteration 9, loss = 0.01211214\n",
      "Iteration 10, loss = 0.00678794\n",
      "Iteration 11, loss = 0.00370573\n",
      "Iteration 12, loss = 0.00418673\n",
      "Iteration 13, loss = 0.00627783\n",
      "Iteration 14, loss = 0.00761019\n",
      "Iteration 15, loss = 0.00724214\n",
      "Iteration 16, loss = 0.00568675\n",
      "Iteration 17, loss = 0.00405788\n",
      "Iteration 18, loss = 0.00322046\n",
      "Iteration 19, loss = 0.00338941\n",
      "Iteration 20, loss = 0.00415805\n",
      "Iteration 21, loss = 0.00473297\n",
      "Iteration 22, loss = 0.00465989\n",
      "Iteration 23, loss = 0.00402607\n",
      "Iteration 24, loss = 0.00329711\n",
      "Iteration 25, loss = 0.00294508\n",
      "Iteration 26, loss = 0.00318490\n",
      "Iteration 27, loss = 0.00358890\n",
      "Iteration 28, loss = 0.00366584\n",
      "Iteration 29, loss = 0.00338294\n",
      "Iteration 30, loss = 0.00301099\n",
      "Iteration 31, loss = 0.00282794\n",
      "Iteration 32, loss = 0.00292247\n",
      "Iteration 33, loss = 0.00306288\n",
      "Iteration 34, loss = 0.00307617\n",
      "Iteration 35, loss = 0.00297125\n",
      "Iteration 36, loss = 0.00283861\n",
      "Iteration 37, loss = 0.00274641\n",
      "Iteration 38, loss = 0.00271270\n",
      "Iteration 39, loss = 0.00273352\n",
      "Iteration 40, loss = 0.00277054\n",
      "Iteration 41, loss = 0.00277531\n",
      "Iteration 42, loss = 0.00272809\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04852668\n",
      "Iteration 2, loss = 0.08219340\n",
      "Iteration 3, loss = 0.00528217\n",
      "Iteration 4, loss = 0.03470457\n",
      "Iteration 5, loss = 0.02030524\n",
      "Iteration 6, loss = 0.00440591\n",
      "Iteration 7, loss = 0.00929516\n",
      "Iteration 8, loss = 0.01484290\n",
      "Iteration 9, loss = 0.01190552\n",
      "Iteration 10, loss = 0.00703576\n",
      "Iteration 11, loss = 0.00388694\n",
      "Iteration 12, loss = 0.00408278\n",
      "Iteration 13, loss = 0.00602584\n",
      "Iteration 14, loss = 0.00739031\n",
      "Iteration 15, loss = 0.00713912\n",
      "Iteration 16, loss = 0.00567855\n",
      "Iteration 17, loss = 0.00403768\n",
      "Iteration 18, loss = 0.00315472\n",
      "Iteration 19, loss = 0.00338049\n",
      "Iteration 20, loss = 0.00422430\n",
      "Iteration 21, loss = 0.00470952\n",
      "Iteration 22, loss = 0.00448454\n",
      "Iteration 23, loss = 0.00382523\n",
      "Iteration 24, loss = 0.00320215\n",
      "Iteration 25, loss = 0.00300277\n",
      "Iteration 26, loss = 0.00336886\n",
      "Iteration 27, loss = 0.00360200\n",
      "Iteration 28, loss = 0.00347615\n",
      "Iteration 29, loss = 0.00316659\n",
      "Iteration 30, loss = 0.00291093\n",
      "Iteration 31, loss = 0.00293437\n",
      "Iteration 32, loss = 0.00304468\n",
      "Iteration 33, loss = 0.00301666\n",
      "Iteration 34, loss = 0.00291325\n",
      "Iteration 35, loss = 0.00283861\n",
      "Iteration 36, loss = 0.00279112\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04839261\n",
      "Iteration 2, loss = 0.08345740\n",
      "Iteration 3, loss = 0.00518078\n",
      "Iteration 4, loss = 0.03412593\n",
      "Iteration 5, loss = 0.01993201\n",
      "Iteration 6, loss = 0.00427465\n",
      "Iteration 7, loss = 0.00946883\n",
      "Iteration 8, loss = 0.01470626\n",
      "Iteration 9, loss = 0.01162687\n",
      "Iteration 10, loss = 0.00668459\n",
      "Iteration 11, loss = 0.00376592\n",
      "Iteration 12, loss = 0.00423453\n",
      "Iteration 13, loss = 0.00622791\n",
      "Iteration 14, loss = 0.00743334\n",
      "Iteration 15, loss = 0.00691204\n",
      "Iteration 16, loss = 0.00527774\n",
      "Iteration 17, loss = 0.00373660\n",
      "Iteration 18, loss = 0.00312945\n",
      "Iteration 19, loss = 0.00357808\n",
      "Iteration 20, loss = 0.00442776\n",
      "Iteration 21, loss = 0.00482191\n",
      "Iteration 22, loss = 0.00451217\n",
      "Iteration 23, loss = 0.00377458\n",
      "Iteration 24, loss = 0.00314346\n",
      "Iteration 25, loss = 0.00299539\n",
      "Iteration 26, loss = 0.00332410\n",
      "Iteration 27, loss = 0.00365118\n",
      "Iteration 28, loss = 0.00362769\n",
      "Iteration 29, loss = 0.00332443\n",
      "Iteration 30, loss = 0.00298437\n",
      "Iteration 31, loss = 0.00282843\n",
      "Iteration 32, loss = 0.00290418\n",
      "Iteration 33, loss = 0.00301493\n",
      "Iteration 34, loss = 0.00303019\n",
      "Iteration 35, loss = 0.00293641\n",
      "Iteration 36, loss = 0.00280105\n",
      "Iteration 37, loss = 0.00268207\n",
      "Iteration 38, loss = 0.00262742\n",
      "Iteration 39, loss = 0.00264822\n",
      "Iteration 40, loss = 0.00266513\n",
      "Iteration 41, loss = 0.00265392\n",
      "Iteration 42, loss = 0.00260490\n",
      "Iteration 43, loss = 0.00254278\n",
      "Iteration 44, loss = 0.00250192\n",
      "Iteration 45, loss = 0.00249104\n",
      "Iteration 46, loss = 0.00250110\n",
      "Iteration 47, loss = 0.00250280\n",
      "Iteration 48, loss = 0.00247725\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04851684\n",
      "Iteration 2, loss = 0.08231278\n",
      "Iteration 3, loss = 0.00526122\n",
      "Iteration 4, loss = 0.03506894\n",
      "Iteration 5, loss = 0.02032129\n",
      "Iteration 6, loss = 0.00440572\n",
      "Iteration 7, loss = 0.00904688\n",
      "Iteration 8, loss = 0.01462483\n",
      "Iteration 9, loss = 0.01193143\n",
      "Iteration 10, loss = 0.00710982\n",
      "Iteration 11, loss = 0.00387650\n",
      "Iteration 12, loss = 0.00395063\n",
      "Iteration 13, loss = 0.00583725\n",
      "Iteration 14, loss = 0.00710473\n",
      "Iteration 15, loss = 0.00672651\n",
      "Iteration 16, loss = 0.00520677\n",
      "Iteration 17, loss = 0.00372240\n",
      "Iteration 18, loss = 0.00322180\n",
      "Iteration 19, loss = 0.00379979\n",
      "Iteration 20, loss = 0.00457317\n",
      "Iteration 21, loss = 0.00471725\n",
      "Iteration 22, loss = 0.00417514\n",
      "Iteration 23, loss = 0.00344412\n",
      "Iteration 24, loss = 0.00304626\n",
      "Iteration 25, loss = 0.00323205\n",
      "Iteration 26, loss = 0.00359010\n",
      "Iteration 27, loss = 0.00368943\n",
      "Iteration 28, loss = 0.00347422\n",
      "Iteration 29, loss = 0.00315410\n",
      "Iteration 30, loss = 0.00296047\n",
      "Iteration 31, loss = 0.00298991\n",
      "Iteration 32, loss = 0.00310056\n",
      "Iteration 33, loss = 0.00313918\n",
      "Iteration 34, loss = 0.00307293\n",
      "Iteration 35, loss = 0.00293709\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04856500\n",
      "Iteration 2, loss = 0.08437309\n",
      "Iteration 3, loss = 0.00522225\n",
      "Iteration 4, loss = 0.03468599\n",
      "Iteration 5, loss = 0.02095929\n",
      "Iteration 6, loss = 0.00455093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.00932262\n",
      "Iteration 8, loss = 0.01507535\n",
      "Iteration 9, loss = 0.01193521\n",
      "Iteration 10, loss = 0.00699589\n",
      "Iteration 11, loss = 0.00388028\n",
      "Iteration 12, loss = 0.00410210\n",
      "Iteration 13, loss = 0.00600321\n",
      "Iteration 14, loss = 0.00735856\n",
      "Iteration 15, loss = 0.00709655\n",
      "Iteration 16, loss = 0.00561123\n",
      "Iteration 17, loss = 0.00399453\n",
      "Iteration 18, loss = 0.00316143\n",
      "Iteration 19, loss = 0.00340301\n",
      "Iteration 20, loss = 0.00420774\n",
      "Iteration 21, loss = 0.00470317\n",
      "Iteration 22, loss = 0.00455573\n",
      "Iteration 23, loss = 0.00392682\n",
      "Iteration 24, loss = 0.00327252\n",
      "Iteration 25, loss = 0.00297520\n",
      "Iteration 26, loss = 0.00317758\n",
      "Iteration 27, loss = 0.00352072\n",
      "Iteration 28, loss = 0.00360904\n",
      "Iteration 29, loss = 0.00339802\n",
      "Iteration 30, loss = 0.00307959\n",
      "Iteration 31, loss = 0.00286389\n",
      "Iteration 32, loss = 0.00288753\n",
      "Iteration 33, loss = 0.00301091\n",
      "Iteration 34, loss = 0.00304571\n",
      "Iteration 35, loss = 0.00297893\n",
      "Iteration 36, loss = 0.00286492\n",
      "Iteration 37, loss = 0.00276825\n",
      "Iteration 38, loss = 0.00271298\n",
      "Iteration 39, loss = 0.00270944\n",
      "Iteration 40, loss = 0.00273948\n",
      "Iteration 41, loss = 0.00275888\n",
      "Iteration 42, loss = 0.00273924\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04864970\n",
      "Iteration 2, loss = 0.08187193\n",
      "Iteration 3, loss = 0.00517336\n",
      "Iteration 4, loss = 0.03461481\n",
      "Iteration 5, loss = 0.01968047\n",
      "Iteration 6, loss = 0.00424328\n",
      "Iteration 7, loss = 0.00906516\n",
      "Iteration 8, loss = 0.01426360\n",
      "Iteration 9, loss = 0.01167945\n",
      "Iteration 10, loss = 0.00688538\n",
      "Iteration 11, loss = 0.00380996\n",
      "Iteration 12, loss = 0.00404816\n",
      "Iteration 13, loss = 0.00593485\n",
      "Iteration 14, loss = 0.00709321\n",
      "Iteration 15, loss = 0.00657862\n",
      "Iteration 16, loss = 0.00499708\n",
      "Iteration 17, loss = 0.00358020\n",
      "Iteration 18, loss = 0.00321904\n",
      "Iteration 19, loss = 0.00390687\n",
      "Iteration 20, loss = 0.00467921\n",
      "Iteration 21, loss = 0.00473062\n",
      "Iteration 22, loss = 0.00409216\n",
      "Iteration 23, loss = 0.00331611\n",
      "Iteration 24, loss = 0.00301524\n",
      "Iteration 25, loss = 0.00337225\n",
      "Iteration 26, loss = 0.00369436\n",
      "Iteration 27, loss = 0.00362376\n",
      "Iteration 28, loss = 0.00330716\n",
      "Iteration 29, loss = 0.00301467\n",
      "Iteration 30, loss = 0.00291419\n",
      "Iteration 31, loss = 0.00299655\n",
      "Iteration 32, loss = 0.00306819\n",
      "Iteration 33, loss = 0.00306665\n",
      "Iteration 34, loss = 0.00297710\n",
      "Iteration 35, loss = 0.00286172\n",
      "Iteration 36, loss = 0.00278992\n",
      "Iteration 37, loss = 0.00278998\n",
      "Iteration 38, loss = 0.00283167\n",
      "Iteration 39, loss = 0.00285787\n",
      "Iteration 40, loss = 0.00283500\n",
      "Iteration 41, loss = 0.00276805\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04864364\n",
      "Iteration 2, loss = 0.08187478\n",
      "Iteration 3, loss = 0.00553829\n",
      "Iteration 4, loss = 0.03485129\n",
      "Iteration 5, loss = 0.02078602\n",
      "Iteration 6, loss = 0.00448228\n",
      "Iteration 7, loss = 0.00973263\n",
      "Iteration 8, loss = 0.01552024\n",
      "Iteration 9, loss = 0.01197589\n",
      "Iteration 10, loss = 0.00681310\n",
      "Iteration 11, loss = 0.00371692\n",
      "Iteration 12, loss = 0.00414326\n",
      "Iteration 13, loss = 0.00618497\n",
      "Iteration 14, loss = 0.00749587\n",
      "Iteration 15, loss = 0.00713825\n",
      "Iteration 16, loss = 0.00560378\n",
      "Iteration 17, loss = 0.00400546\n",
      "Iteration 18, loss = 0.00319845\n",
      "Iteration 19, loss = 0.00342375\n",
      "Iteration 20, loss = 0.00424400\n",
      "Iteration 21, loss = 0.00477133\n",
      "Iteration 22, loss = 0.00461835\n",
      "Iteration 23, loss = 0.00396462\n",
      "Iteration 24, loss = 0.00327585\n",
      "Iteration 25, loss = 0.00296282\n",
      "Iteration 26, loss = 0.00319437\n",
      "Iteration 27, loss = 0.00356198\n",
      "Iteration 28, loss = 0.00363064\n",
      "Iteration 29, loss = 0.00337313\n",
      "Iteration 30, loss = 0.00301820\n",
      "Iteration 31, loss = 0.00283479\n",
      "Iteration 32, loss = 0.00293763\n",
      "Iteration 33, loss = 0.00300485\n",
      "Iteration 34, loss = 0.00297603\n",
      "Iteration 35, loss = 0.00289115\n",
      "Iteration 36, loss = 0.00280068\n",
      "Iteration 37, loss = 0.00274158\n",
      "Iteration 38, loss = 0.00272068\n",
      "Iteration 39, loss = 0.00273355\n",
      "Iteration 40, loss = 0.00275237\n",
      "Iteration 41, loss = 0.00274880\n",
      "Iteration 42, loss = 0.00271429\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04858564\n",
      "Iteration 2, loss = 0.08164160\n",
      "Iteration 3, loss = 0.00558915\n",
      "Iteration 4, loss = 0.03479933\n",
      "Iteration 5, loss = 0.02074151\n",
      "Iteration 6, loss = 0.00444783\n",
      "Iteration 7, loss = 0.00983286\n",
      "Iteration 8, loss = 0.01562726\n",
      "Iteration 9, loss = 0.01197377\n",
      "Iteration 10, loss = 0.00671741\n",
      "Iteration 11, loss = 0.00366339\n",
      "Iteration 12, loss = 0.00418203\n",
      "Iteration 13, loss = 0.00628465\n",
      "Iteration 14, loss = 0.00757101\n",
      "Iteration 15, loss = 0.00713770\n",
      "Iteration 16, loss = 0.00555394\n",
      "Iteration 17, loss = 0.00393928\n",
      "Iteration 18, loss = 0.00317887\n",
      "Iteration 19, loss = 0.00342391\n",
      "Iteration 20, loss = 0.00420292\n",
      "Iteration 21, loss = 0.00468462\n",
      "Iteration 22, loss = 0.00444834\n",
      "Iteration 23, loss = 0.00371226\n",
      "Iteration 24, loss = 0.00305903\n",
      "Iteration 25, loss = 0.00301930\n",
      "Iteration 26, loss = 0.00350493\n",
      "Iteration 27, loss = 0.00364944\n",
      "Iteration 28, loss = 0.00334930\n",
      "Iteration 29, loss = 0.00297869\n",
      "Iteration 30, loss = 0.00286369\n",
      "Iteration 31, loss = 0.00295098\n",
      "Iteration 32, loss = 0.00301584\n",
      "Iteration 33, loss = 0.00300890\n",
      "Iteration 34, loss = 0.00293877\n",
      "Iteration 35, loss = 0.00283568\n",
      "Iteration 36, loss = 0.00276233\n",
      "Iteration 37, loss = 0.00275176\n",
      "Iteration 38, loss = 0.00278459\n",
      "Iteration 39, loss = 0.00281404\n",
      "Iteration 40, loss = 0.00280193\n",
      "Iteration 41, loss = 0.00274744\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04868449\n",
      "Iteration 2, loss = 0.08230888\n",
      "Iteration 3, loss = 0.00525865\n",
      "Iteration 4, loss = 0.03472515\n",
      "Iteration 5, loss = 0.02036479\n",
      "Iteration 6, loss = 0.00441945\n",
      "Iteration 7, loss = 0.00926954\n",
      "Iteration 8, loss = 0.01483929\n",
      "Iteration 9, loss = 0.01193494\n",
      "Iteration 10, loss = 0.00706419\n",
      "Iteration 11, loss = 0.00389611\n",
      "Iteration 12, loss = 0.00407464\n",
      "Iteration 13, loss = 0.00601451\n",
      "Iteration 14, loss = 0.00738346\n",
      "Iteration 15, loss = 0.00713306\n",
      "Iteration 16, loss = 0.00566366\n",
      "Iteration 17, loss = 0.00401783\n",
      "Iteration 18, loss = 0.00314671\n",
      "Iteration 19, loss = 0.00341310\n",
      "Iteration 20, loss = 0.00427671\n",
      "Iteration 21, loss = 0.00475027\n",
      "Iteration 22, loss = 0.00448505\n",
      "Iteration 23, loss = 0.00376817\n",
      "Iteration 24, loss = 0.00315381\n",
      "Iteration 25, loss = 0.00302974\n",
      "Iteration 26, loss = 0.00340337\n",
      "Iteration 27, loss = 0.00359249\n",
      "Iteration 28, loss = 0.00342659\n",
      "Iteration 29, loss = 0.00309892\n",
      "Iteration 30, loss = 0.00291239\n",
      "Iteration 31, loss = 0.00299655\n",
      "Iteration 32, loss = 0.00306108\n",
      "Iteration 33, loss = 0.00298708\n",
      "Iteration 34, loss = 0.00286842\n",
      "Iteration 35, loss = 0.00278476\n",
      "Iteration 36, loss = 0.00277068\n",
      "Iteration 37, loss = 0.00279164\n",
      "Iteration 38, loss = 0.00279620\n",
      "Iteration 39, loss = 0.00276728\n",
      "Iteration 40, loss = 0.00272355\n",
      "Iteration 41, loss = 0.00268186\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04875422\n",
      "Iteration 2, loss = 0.08436417\n",
      "Iteration 3, loss = 0.00528270\n",
      "Iteration 4, loss = 0.03490640\n",
      "Iteration 5, loss = 0.02122449\n",
      "Iteration 6, loss = 0.00464313\n",
      "Iteration 7, loss = 0.00913270\n",
      "Iteration 8, loss = 0.01512437\n",
      "Iteration 9, loss = 0.01214871\n",
      "Iteration 10, loss = 0.00722358\n",
      "Iteration 11, loss = 0.00393979\n",
      "Iteration 12, loss = 0.00398770\n",
      "Iteration 13, loss = 0.00585178\n",
      "Iteration 14, loss = 0.00731092\n",
      "Iteration 15, loss = 0.00720307\n",
      "Iteration 16, loss = 0.00581337\n",
      "Iteration 17, loss = 0.00417307\n",
      "Iteration 18, loss = 0.00321823\n",
      "Iteration 19, loss = 0.00331166\n",
      "Iteration 20, loss = 0.00408071\n",
      "Iteration 21, loss = 0.00464245\n",
      "Iteration 22, loss = 0.00458611\n",
      "Iteration 23, loss = 0.00403287\n",
      "Iteration 24, loss = 0.00337259\n",
      "Iteration 25, loss = 0.00300492\n",
      "Iteration 26, loss = 0.00313426\n",
      "Iteration 27, loss = 0.00349134\n",
      "Iteration 28, loss = 0.00362915\n",
      "Iteration 29, loss = 0.00345666\n",
      "Iteration 30, loss = 0.00314075\n",
      "Iteration 31, loss = 0.00291206\n",
      "Iteration 32, loss = 0.00291699\n",
      "Iteration 33, loss = 0.00303298\n",
      "Iteration 34, loss = 0.00307551\n",
      "Iteration 35, loss = 0.00301440\n",
      "Iteration 36, loss = 0.00290474\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04876629\n",
      "Iteration 2, loss = 0.08340669\n",
      "Iteration 3, loss = 0.00544241\n",
      "Iteration 4, loss = 0.03509234\n",
      "Iteration 5, loss = 0.02130166\n",
      "Iteration 6, loss = 0.00463289\n",
      "Iteration 7, loss = 0.00955348\n",
      "Iteration 8, loss = 0.01555827\n",
      "Iteration 9, loss = 0.01209650\n",
      "Iteration 10, loss = 0.00694503\n",
      "Iteration 11, loss = 0.00375993\n",
      "Iteration 12, loss = 0.00406020\n",
      "Iteration 13, loss = 0.00606540\n",
      "Iteration 14, loss = 0.00745889\n",
      "Iteration 15, loss = 0.00721210\n",
      "Iteration 16, loss = 0.00574158\n",
      "Iteration 17, loss = 0.00412883\n",
      "Iteration 18, loss = 0.00324673\n",
      "Iteration 19, loss = 0.00335316\n",
      "Iteration 20, loss = 0.00408817\n",
      "Iteration 21, loss = 0.00470223\n",
      "Iteration 22, loss = 0.00470808\n",
      "Iteration 23, loss = 0.00413912\n",
      "Iteration 24, loss = 0.00340470\n",
      "Iteration 25, loss = 0.00297184\n",
      "Iteration 26, loss = 0.00310582\n",
      "Iteration 27, loss = 0.00350744\n",
      "Iteration 28, loss = 0.00366889\n",
      "Iteration 29, loss = 0.00346091\n",
      "Iteration 30, loss = 0.00309626\n",
      "Iteration 31, loss = 0.00284790\n",
      "Iteration 32, loss = 0.00287463\n",
      "Iteration 33, loss = 0.00303655\n",
      "Iteration 34, loss = 0.00307097\n",
      "Iteration 35, loss = 0.00297036\n",
      "Iteration 36, loss = 0.00283771\n",
      "Iteration 37, loss = 0.00274728\n",
      "Iteration 38, loss = 0.00271560\n",
      "Iteration 39, loss = 0.00272950\n",
      "Iteration 40, loss = 0.00275858\n",
      "Iteration 41, loss = 0.00276389\n",
      "Iteration 42, loss = 0.00273022\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04880645\n",
      "Iteration 2, loss = 0.08311230\n",
      "Iteration 3, loss = 0.00558341\n",
      "Iteration 4, loss = 0.03500193\n",
      "Iteration 5, loss = 0.02133724\n",
      "Iteration 6, loss = 0.00465373\n",
      "Iteration 7, loss = 0.00944575\n",
      "Iteration 8, loss = 0.01545906\n",
      "Iteration 9, loss = 0.01204726\n",
      "Iteration 10, loss = 0.00688040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.00369674\n",
      "Iteration 12, loss = 0.00396373\n",
      "Iteration 13, loss = 0.00597748\n",
      "Iteration 14, loss = 0.00735147\n",
      "Iteration 15, loss = 0.00709125\n",
      "Iteration 16, loss = 0.00564864\n",
      "Iteration 17, loss = 0.00408229\n",
      "Iteration 18, loss = 0.00321976\n",
      "Iteration 19, loss = 0.00332752\n",
      "Iteration 20, loss = 0.00407000\n",
      "Iteration 21, loss = 0.00462670\n",
      "Iteration 22, loss = 0.00454239\n",
      "Iteration 23, loss = 0.00392557\n",
      "Iteration 24, loss = 0.00323206\n",
      "Iteration 25, loss = 0.00292466\n",
      "Iteration 26, loss = 0.00318094\n",
      "Iteration 27, loss = 0.00356178\n",
      "Iteration 28, loss = 0.00361119\n",
      "Iteration 29, loss = 0.00332486\n",
      "Iteration 30, loss = 0.00297521\n",
      "Iteration 31, loss = 0.00281369\n",
      "Iteration 32, loss = 0.00291461\n",
      "Iteration 33, loss = 0.00303462\n",
      "Iteration 34, loss = 0.00301826\n",
      "Iteration 35, loss = 0.00289431\n",
      "Iteration 36, loss = 0.00276778\n",
      "Iteration 37, loss = 0.00273710\n",
      "Iteration 38, loss = 0.00279626\n",
      "Iteration 39, loss = 0.00278685\n",
      "Iteration 40, loss = 0.00271343\n",
      "Iteration 41, loss = 0.00265531\n",
      "Iteration 42, loss = 0.00263844\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04875422\n",
      "Iteration 2, loss = 0.08347828\n",
      "Iteration 3, loss = 0.00544873\n",
      "Iteration 4, loss = 0.03499067\n",
      "Iteration 5, loss = 0.02123875\n",
      "Iteration 6, loss = 0.00461593\n",
      "Iteration 7, loss = 0.00952801\n",
      "Iteration 8, loss = 0.01550720\n",
      "Iteration 9, loss = 0.01208841\n",
      "Iteration 10, loss = 0.00694219\n",
      "Iteration 11, loss = 0.00375185\n",
      "Iteration 12, loss = 0.00404152\n",
      "Iteration 13, loss = 0.00604390\n",
      "Iteration 14, loss = 0.00743886\n",
      "Iteration 15, loss = 0.00719489\n",
      "Iteration 16, loss = 0.00572441\n",
      "Iteration 17, loss = 0.00410412\n",
      "Iteration 18, loss = 0.00321308\n",
      "Iteration 19, loss = 0.00331246\n",
      "Iteration 20, loss = 0.00404350\n",
      "Iteration 21, loss = 0.00460948\n",
      "Iteration 22, loss = 0.00456170\n",
      "Iteration 23, loss = 0.00397524\n",
      "Iteration 24, loss = 0.00327996\n",
      "Iteration 25, loss = 0.00293096\n",
      "Iteration 26, loss = 0.00315558\n",
      "Iteration 27, loss = 0.00353764\n",
      "Iteration 28, loss = 0.00360729\n",
      "Iteration 29, loss = 0.00333709\n",
      "Iteration 30, loss = 0.00298288\n",
      "Iteration 31, loss = 0.00280830\n",
      "Iteration 32, loss = 0.00290650\n",
      "Iteration 33, loss = 0.00303946\n",
      "Iteration 34, loss = 0.00304577\n",
      "Iteration 35, loss = 0.00294043\n",
      "Iteration 36, loss = 0.00281350\n",
      "Iteration 37, loss = 0.00272853\n",
      "Iteration 38, loss = 0.00270288\n",
      "Iteration 39, loss = 0.00272972\n",
      "Iteration 40, loss = 0.00276487\n",
      "Iteration 41, loss = 0.00276216\n",
      "Iteration 42, loss = 0.00271024\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04879350\n",
      "Iteration 2, loss = 0.08195496\n",
      "Iteration 3, loss = 0.00546838\n",
      "Iteration 4, loss = 0.03478391\n",
      "Iteration 5, loss = 0.02083663\n",
      "Iteration 6, loss = 0.00449537\n",
      "Iteration 7, loss = 0.00962196\n",
      "Iteration 8, loss = 0.01547160\n",
      "Iteration 9, loss = 0.01199560\n",
      "Iteration 10, loss = 0.00684455\n",
      "Iteration 11, loss = 0.00372343\n",
      "Iteration 12, loss = 0.00411432\n",
      "Iteration 13, loss = 0.00616085\n",
      "Iteration 14, loss = 0.00750002\n",
      "Iteration 15, loss = 0.00716612\n",
      "Iteration 16, loss = 0.00563477\n",
      "Iteration 17, loss = 0.00402235\n",
      "Iteration 18, loss = 0.00320128\n",
      "Iteration 19, loss = 0.00341135\n",
      "Iteration 20, loss = 0.00423420\n",
      "Iteration 21, loss = 0.00479625\n",
      "Iteration 22, loss = 0.00468025\n",
      "Iteration 23, loss = 0.00403838\n",
      "Iteration 24, loss = 0.00333657\n",
      "Iteration 25, loss = 0.00300171\n",
      "Iteration 26, loss = 0.00320627\n",
      "Iteration 27, loss = 0.00356970\n",
      "Iteration 28, loss = 0.00366109\n",
      "Iteration 29, loss = 0.00342205\n",
      "Iteration 30, loss = 0.00306639\n",
      "Iteration 31, loss = 0.00285497\n",
      "Iteration 32, loss = 0.00299521\n",
      "Iteration 33, loss = 0.00304706\n",
      "Iteration 34, loss = 0.00298278\n",
      "Iteration 35, loss = 0.00287949\n",
      "Iteration 36, loss = 0.00279096\n",
      "Iteration 37, loss = 0.00274334\n",
      "Iteration 38, loss = 0.00273550\n",
      "Iteration 39, loss = 0.00275214\n",
      "Iteration 40, loss = 0.00276352\n",
      "Iteration 41, loss = 0.00274947\n",
      "Iteration 42, loss = 0.00271075\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04866151\n",
      "Iteration 2, loss = 0.08293413\n",
      "Iteration 3, loss = 0.00563629\n",
      "Iteration 4, loss = 0.03482224\n",
      "Iteration 5, loss = 0.02129158\n",
      "Iteration 6, loss = 0.00469391\n",
      "Iteration 7, loss = 0.00943240\n",
      "Iteration 8, loss = 0.01546081\n",
      "Iteration 9, loss = 0.01193444\n",
      "Iteration 10, loss = 0.00682377\n",
      "Iteration 11, loss = 0.00369021\n",
      "Iteration 12, loss = 0.00394577\n",
      "Iteration 13, loss = 0.00592525\n",
      "Iteration 14, loss = 0.00728087\n",
      "Iteration 15, loss = 0.00703859\n",
      "Iteration 16, loss = 0.00562863\n",
      "Iteration 17, loss = 0.00408850\n",
      "Iteration 18, loss = 0.00323523\n",
      "Iteration 19, loss = 0.00333234\n",
      "Iteration 20, loss = 0.00403790\n",
      "Iteration 21, loss = 0.00461680\n",
      "Iteration 22, loss = 0.00457261\n",
      "Iteration 23, loss = 0.00396956\n",
      "Iteration 24, loss = 0.00326449\n",
      "Iteration 25, loss = 0.00292519\n",
      "Iteration 26, loss = 0.00314899\n",
      "Iteration 27, loss = 0.00353216\n",
      "Iteration 28, loss = 0.00361026\n",
      "Iteration 29, loss = 0.00334946\n",
      "Iteration 30, loss = 0.00299580\n",
      "Iteration 31, loss = 0.00281274\n",
      "Iteration 32, loss = 0.00289442\n",
      "Iteration 33, loss = 0.00303048\n",
      "Iteration 34, loss = 0.00304581\n",
      "Iteration 35, loss = 0.00294255\n",
      "Iteration 36, loss = 0.00280602\n",
      "Iteration 37, loss = 0.00272161\n",
      "Iteration 38, loss = 0.00273411\n",
      "Iteration 39, loss = 0.00276793\n",
      "Iteration 40, loss = 0.00274599\n",
      "Iteration 41, loss = 0.00269193\n",
      "Iteration 42, loss = 0.00264498\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04880500\n",
      "Iteration 2, loss = 0.08223049\n",
      "Iteration 3, loss = 0.00522753\n",
      "Iteration 4, loss = 0.03462143\n",
      "Iteration 5, loss = 0.02027085\n",
      "Iteration 6, loss = 0.00440285\n",
      "Iteration 7, loss = 0.00922690\n",
      "Iteration 8, loss = 0.01473240\n",
      "Iteration 9, loss = 0.01185575\n",
      "Iteration 10, loss = 0.00701041\n",
      "Iteration 11, loss = 0.00387618\n",
      "Iteration 12, loss = 0.00408188\n",
      "Iteration 13, loss = 0.00601390\n",
      "Iteration 14, loss = 0.00734517\n",
      "Iteration 15, loss = 0.00703263\n",
      "Iteration 16, loss = 0.00550236\n",
      "Iteration 17, loss = 0.00385973\n",
      "Iteration 18, loss = 0.00312409\n",
      "Iteration 19, loss = 0.00354933\n",
      "Iteration 20, loss = 0.00444504\n",
      "Iteration 21, loss = 0.00479923\n",
      "Iteration 22, loss = 0.00439745\n",
      "Iteration 23, loss = 0.00364073\n",
      "Iteration 24, loss = 0.00308849\n",
      "Iteration 25, loss = 0.00308444\n",
      "Iteration 26, loss = 0.00346130\n",
      "Iteration 27, loss = 0.00367095\n",
      "Iteration 28, loss = 0.00351522\n",
      "Iteration 29, loss = 0.00314081\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04881114\n",
      "Iteration 2, loss = 0.08448197\n",
      "Iteration 3, loss = 0.00523292\n",
      "Iteration 4, loss = 0.03487718\n",
      "Iteration 5, loss = 0.02121620\n",
      "Iteration 6, loss = 0.00465229\n",
      "Iteration 7, loss = 0.00907102\n",
      "Iteration 8, loss = 0.01501302\n",
      "Iteration 9, loss = 0.01208937\n",
      "Iteration 10, loss = 0.00721486\n",
      "Iteration 11, loss = 0.00394981\n",
      "Iteration 12, loss = 0.00399199\n",
      "Iteration 13, loss = 0.00583981\n",
      "Iteration 14, loss = 0.00728834\n",
      "Iteration 15, loss = 0.00718511\n",
      "Iteration 16, loss = 0.00580687\n",
      "Iteration 17, loss = 0.00417503\n",
      "Iteration 18, loss = 0.00322288\n",
      "Iteration 19, loss = 0.00330738\n",
      "Iteration 20, loss = 0.00406639\n",
      "Iteration 21, loss = 0.00464012\n",
      "Iteration 22, loss = 0.00460295\n",
      "Iteration 23, loss = 0.00405653\n",
      "Iteration 24, loss = 0.00338787\n",
      "Iteration 25, loss = 0.00300611\n",
      "Iteration 26, loss = 0.00312526\n",
      "Iteration 27, loss = 0.00348324\n",
      "Iteration 28, loss = 0.00363490\n",
      "Iteration 29, loss = 0.00346047\n",
      "Iteration 30, loss = 0.00314490\n",
      "Iteration 31, loss = 0.00291053\n",
      "Iteration 32, loss = 0.00290913\n",
      "Iteration 33, loss = 0.00303014\n",
      "Iteration 34, loss = 0.00307502\n",
      "Iteration 35, loss = 0.00301150\n",
      "Iteration 36, loss = 0.00289574\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04890540\n",
      "Iteration 2, loss = 0.08332918\n",
      "Iteration 3, loss = 0.00543108\n",
      "Iteration 4, loss = 0.03504816\n",
      "Iteration 5, loss = 0.02130831\n",
      "Iteration 6, loss = 0.00463209\n",
      "Iteration 7, loss = 0.00931537\n",
      "Iteration 8, loss = 0.01529235\n",
      "Iteration 9, loss = 0.01205489\n",
      "Iteration 10, loss = 0.00702972\n",
      "Iteration 11, loss = 0.00379999\n",
      "Iteration 12, loss = 0.00395162\n",
      "Iteration 13, loss = 0.00588913\n",
      "Iteration 14, loss = 0.00732476\n",
      "Iteration 15, loss = 0.00718935\n",
      "Iteration 16, loss = 0.00580855\n",
      "Iteration 17, loss = 0.00419945\n",
      "Iteration 18, loss = 0.00325174\n",
      "Iteration 19, loss = 0.00328597\n",
      "Iteration 20, loss = 0.00401102\n",
      "Iteration 21, loss = 0.00463584\n",
      "Iteration 22, loss = 0.00467918\n",
      "Iteration 23, loss = 0.00414916\n",
      "Iteration 24, loss = 0.00344648\n",
      "Iteration 25, loss = 0.00300905\n",
      "Iteration 26, loss = 0.00307864\n",
      "Iteration 27, loss = 0.00344973\n",
      "Iteration 28, loss = 0.00363716\n",
      "Iteration 29, loss = 0.00347835\n",
      "Iteration 30, loss = 0.00313727\n",
      "Iteration 31, loss = 0.00287550\n",
      "Iteration 32, loss = 0.00286722\n",
      "Iteration 33, loss = 0.00299953\n",
      "Iteration 34, loss = 0.00300787\n",
      "Iteration 35, loss = 0.00288123\n",
      "Iteration 36, loss = 0.00276098\n",
      "Iteration 37, loss = 0.00275330\n",
      "Iteration 38, loss = 0.00279654\n",
      "Iteration 39, loss = 0.00276342\n",
      "Iteration 40, loss = 0.00268808\n",
      "Iteration 41, loss = 0.00264591\n",
      "Iteration 42, loss = 0.00264640\n",
      "Iteration 43, loss = 0.00265077\n",
      "Iteration 44, loss = 0.00263090\n",
      "Iteration 45, loss = 0.00258888\n",
      "Iteration 46, loss = 0.00254429\n",
      "Iteration 47, loss = 0.00251850\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04889297\n",
      "Iteration 2, loss = 0.08141026\n",
      "Iteration 3, loss = 0.00555672\n",
      "Iteration 4, loss = 0.03426163\n",
      "Iteration 5, loss = 0.01907378\n",
      "Iteration 6, loss = 0.00374111\n",
      "Iteration 7, loss = 0.01315424\n",
      "Iteration 8, loss = 0.01579446\n",
      "Iteration 9, loss = 0.00957966\n",
      "Iteration 10, loss = 0.00443150\n",
      "Iteration 11, loss = 0.00372669\n",
      "Iteration 12, loss = 0.00592515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.00768807\n",
      "Iteration 14, loss = 0.00745903\n",
      "Iteration 15, loss = 0.00583718\n",
      "Iteration 16, loss = 0.00416798\n",
      "Iteration 17, loss = 0.00341301\n",
      "Iteration 18, loss = 0.00369863\n",
      "Iteration 19, loss = 0.00451658\n",
      "Iteration 20, loss = 0.00502540\n",
      "Iteration 21, loss = 0.00479997\n",
      "Iteration 22, loss = 0.00406079\n",
      "Iteration 23, loss = 0.00330207\n",
      "Iteration 24, loss = 0.00302705\n",
      "Iteration 25, loss = 0.00330029\n",
      "Iteration 26, loss = 0.00366067\n",
      "Iteration 27, loss = 0.00371981\n",
      "Iteration 28, loss = 0.00347251\n",
      "Iteration 29, loss = 0.00312307\n",
      "Iteration 30, loss = 0.00293677\n",
      "Iteration 31, loss = 0.00305070\n",
      "Iteration 32, loss = 0.00309850\n",
      "Iteration 33, loss = 0.00298498\n",
      "Iteration 34, loss = 0.00286179\n",
      "Iteration 35, loss = 0.00282174\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04899734\n",
      "Iteration 2, loss = 0.08411966\n",
      "Iteration 3, loss = 0.00513747\n",
      "Iteration 4, loss = 0.03459137\n",
      "Iteration 5, loss = 0.02085147\n",
      "Iteration 6, loss = 0.00451331\n",
      "Iteration 7, loss = 0.00919090\n",
      "Iteration 8, loss = 0.01495370\n",
      "Iteration 9, loss = 0.01202724\n",
      "Iteration 10, loss = 0.00712328\n",
      "Iteration 11, loss = 0.00391659\n",
      "Iteration 12, loss = 0.00405423\n",
      "Iteration 13, loss = 0.00594117\n",
      "Iteration 14, loss = 0.00732996\n",
      "Iteration 15, loss = 0.00711745\n",
      "Iteration 16, loss = 0.00566758\n",
      "Iteration 17, loss = 0.00404703\n",
      "Iteration 18, loss = 0.00317591\n",
      "Iteration 19, loss = 0.00334199\n",
      "Iteration 20, loss = 0.00412182\n",
      "Iteration 21, loss = 0.00468122\n",
      "Iteration 22, loss = 0.00461010\n",
      "Iteration 23, loss = 0.00401648\n",
      "Iteration 24, loss = 0.00332492\n",
      "Iteration 25, loss = 0.00297396\n",
      "Iteration 26, loss = 0.00315613\n",
      "Iteration 27, loss = 0.00352143\n",
      "Iteration 28, loss = 0.00364014\n",
      "Iteration 29, loss = 0.00344615\n",
      "Iteration 30, loss = 0.00311881\n",
      "Iteration 31, loss = 0.00288646\n",
      "Iteration 32, loss = 0.00289591\n",
      "Iteration 33, loss = 0.00301834\n",
      "Iteration 34, loss = 0.00305989\n",
      "Iteration 35, loss = 0.00299864\n",
      "Iteration 36, loss = 0.00288389\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04897615\n",
      "Iteration 2, loss = 0.08404121\n",
      "Iteration 3, loss = 0.00548570\n",
      "Iteration 4, loss = 0.03598877\n",
      "Iteration 5, loss = 0.01994307\n",
      "Iteration 6, loss = 0.00431340\n",
      "Iteration 7, loss = 0.00942549\n",
      "Iteration 8, loss = 0.01480266\n",
      "Iteration 9, loss = 0.01181155\n",
      "Iteration 10, loss = 0.00690698\n",
      "Iteration 11, loss = 0.00379219\n",
      "Iteration 12, loss = 0.00407933\n",
      "Iteration 13, loss = 0.00606921\n",
      "Iteration 14, loss = 0.00742793\n",
      "Iteration 15, loss = 0.00711904\n",
      "Iteration 16, loss = 0.00555618\n",
      "Iteration 17, loss = 0.00389944\n",
      "Iteration 18, loss = 0.00311749\n",
      "Iteration 19, loss = 0.00347222\n",
      "Iteration 20, loss = 0.00438873\n",
      "Iteration 21, loss = 0.00485787\n",
      "Iteration 22, loss = 0.00451693\n",
      "Iteration 23, loss = 0.00372024\n",
      "Iteration 24, loss = 0.00310627\n",
      "Iteration 25, loss = 0.00308150\n",
      "Iteration 26, loss = 0.00351482\n",
      "Iteration 27, loss = 0.00368330\n",
      "Iteration 28, loss = 0.00350445\n",
      "Iteration 29, loss = 0.00317689\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04878208\n",
      "Iteration 2, loss = 0.08343608\n",
      "Iteration 3, loss = 0.00539110\n",
      "Iteration 4, loss = 0.03484745\n",
      "Iteration 5, loss = 0.02120153\n",
      "Iteration 6, loss = 0.00459470\n",
      "Iteration 7, loss = 0.00959582\n",
      "Iteration 8, loss = 0.01557169\n",
      "Iteration 9, loss = 0.01200758\n",
      "Iteration 10, loss = 0.00682925\n",
      "Iteration 11, loss = 0.00371863\n",
      "Iteration 12, loss = 0.00406139\n",
      "Iteration 13, loss = 0.00607012\n",
      "Iteration 14, loss = 0.00740492\n",
      "Iteration 15, loss = 0.00710521\n",
      "Iteration 16, loss = 0.00562066\n",
      "Iteration 17, loss = 0.00402923\n",
      "Iteration 18, loss = 0.00318980\n",
      "Iteration 19, loss = 0.00333957\n",
      "Iteration 20, loss = 0.00407906\n",
      "Iteration 21, loss = 0.00463494\n",
      "Iteration 22, loss = 0.00456116\n",
      "Iteration 23, loss = 0.00394116\n",
      "Iteration 24, loss = 0.00323637\n",
      "Iteration 25, loss = 0.00292143\n",
      "Iteration 26, loss = 0.00315358\n",
      "Iteration 27, loss = 0.00347748\n",
      "Iteration 28, loss = 0.00346132\n",
      "Iteration 29, loss = 0.00312809\n",
      "Iteration 30, loss = 0.00283036\n",
      "Iteration 31, loss = 0.00288207\n",
      "Iteration 32, loss = 0.00306268\n",
      "Iteration 33, loss = 0.00300692\n",
      "Iteration 34, loss = 0.00282630\n",
      "Iteration 35, loss = 0.00270961\n",
      "Iteration 36, loss = 0.00271087\n",
      "Iteration 37, loss = 0.00277345\n",
      "Iteration 38, loss = 0.00279116\n",
      "Iteration 39, loss = 0.00272408\n",
      "Iteration 40, loss = 0.00263166\n",
      "Iteration 41, loss = 0.00258859\n",
      "Iteration 42, loss = 0.00260363\n",
      "Iteration 43, loss = 0.00259789\n",
      "Iteration 44, loss = 0.00255127\n",
      "Iteration 45, loss = 0.00249307\n",
      "Iteration 46, loss = 0.00245759\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04884100\n",
      "Iteration 2, loss = 0.08356489\n",
      "Iteration 3, loss = 0.00534896\n",
      "Iteration 4, loss = 0.03487105\n",
      "Iteration 5, loss = 0.02117251\n",
      "Iteration 6, loss = 0.00459789\n",
      "Iteration 7, loss = 0.00946101\n",
      "Iteration 8, loss = 0.01541968\n",
      "Iteration 9, loss = 0.01200518\n",
      "Iteration 10, loss = 0.00695132\n",
      "Iteration 11, loss = 0.00378718\n",
      "Iteration 12, loss = 0.00403225\n",
      "Iteration 13, loss = 0.00600032\n",
      "Iteration 14, loss = 0.00740226\n",
      "Iteration 15, loss = 0.00717443\n",
      "Iteration 16, loss = 0.00572131\n",
      "Iteration 17, loss = 0.00410246\n",
      "Iteration 18, loss = 0.00321186\n",
      "Iteration 19, loss = 0.00332982\n",
      "Iteration 20, loss = 0.00409842\n",
      "Iteration 21, loss = 0.00468196\n",
      "Iteration 22, loss = 0.00465905\n",
      "Iteration 23, loss = 0.00409379\n",
      "Iteration 24, loss = 0.00339250\n",
      "Iteration 25, loss = 0.00300143\n",
      "Iteration 26, loss = 0.00313509\n",
      "Iteration 27, loss = 0.00351127\n",
      "Iteration 28, loss = 0.00366242\n",
      "Iteration 29, loss = 0.00347558\n",
      "Iteration 30, loss = 0.00313745\n",
      "Iteration 31, loss = 0.00289638\n",
      "Iteration 32, loss = 0.00291440\n",
      "Iteration 33, loss = 0.00306443\n",
      "Iteration 34, loss = 0.00310680\n",
      "Iteration 35, loss = 0.00301997\n",
      "Iteration 36, loss = 0.00289725\n",
      "Iteration 37, loss = 0.00280826\n",
      "Iteration 38, loss = 0.00277057\n",
      "Iteration 39, loss = 0.00277474\n",
      "Iteration 40, loss = 0.00279961\n",
      "Iteration 41, loss = 0.00281120\n",
      "Iteration 42, loss = 0.00278965\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04894168\n",
      "Iteration 2, loss = 0.08432170\n",
      "Iteration 3, loss = 0.00541043\n",
      "Iteration 4, loss = 0.03619551\n",
      "Iteration 5, loss = 0.02007152\n",
      "Iteration 6, loss = 0.00436086\n",
      "Iteration 7, loss = 0.00897508\n",
      "Iteration 8, loss = 0.01438557\n",
      "Iteration 9, loss = 0.01172841\n",
      "Iteration 10, loss = 0.00704453\n",
      "Iteration 11, loss = 0.00387759\n",
      "Iteration 12, loss = 0.00396017\n",
      "Iteration 13, loss = 0.00584164\n",
      "Iteration 14, loss = 0.00717753\n",
      "Iteration 15, loss = 0.00685287\n",
      "Iteration 16, loss = 0.00531184\n",
      "Iteration 17, loss = 0.00378272\n",
      "Iteration 18, loss = 0.00318429\n",
      "Iteration 19, loss = 0.00365518\n",
      "Iteration 20, loss = 0.00451103\n",
      "Iteration 21, loss = 0.00479515\n",
      "Iteration 22, loss = 0.00432580\n",
      "Iteration 23, loss = 0.00356399\n",
      "Iteration 24, loss = 0.00306803\n",
      "Iteration 25, loss = 0.00318316\n",
      "Iteration 26, loss = 0.00355334\n",
      "Iteration 27, loss = 0.00370246\n",
      "Iteration 28, loss = 0.00353104\n",
      "Iteration 29, loss = 0.00320666\n",
      "Iteration 30, loss = 0.00296933\n",
      "Iteration 31, loss = 0.00295504\n",
      "Iteration 32, loss = 0.00306420\n",
      "Iteration 33, loss = 0.00312051\n",
      "Iteration 34, loss = 0.00308127\n",
      "Iteration 35, loss = 0.00297574\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04899556\n",
      "Iteration 2, loss = 0.08401575\n",
      "Iteration 3, loss = 0.00551699\n",
      "Iteration 4, loss = 0.03603200\n",
      "Iteration 5, loss = 0.02005477\n",
      "Iteration 6, loss = 0.00433493\n",
      "Iteration 7, loss = 0.00937347\n",
      "Iteration 8, loss = 0.01479054\n",
      "Iteration 9, loss = 0.01186869\n",
      "Iteration 10, loss = 0.00698078\n",
      "Iteration 11, loss = 0.00382143\n",
      "Iteration 12, loss = 0.00404793\n",
      "Iteration 13, loss = 0.00599979\n",
      "Iteration 14, loss = 0.00737784\n",
      "Iteration 15, loss = 0.00712229\n",
      "Iteration 16, loss = 0.00561117\n",
      "Iteration 17, loss = 0.00397506\n",
      "Iteration 18, loss = 0.00314485\n",
      "Iteration 19, loss = 0.00341130\n",
      "Iteration 20, loss = 0.00428088\n",
      "Iteration 21, loss = 0.00481229\n",
      "Iteration 22, loss = 0.00456421\n",
      "Iteration 23, loss = 0.00380941\n",
      "Iteration 24, loss = 0.00316392\n",
      "Iteration 25, loss = 0.00304029\n",
      "Iteration 26, loss = 0.00344084\n",
      "Iteration 27, loss = 0.00364331\n",
      "Iteration 28, loss = 0.00350568\n",
      "Iteration 29, loss = 0.00318733\n",
      "Iteration 30, loss = 0.00295120\n",
      "Iteration 31, loss = 0.00291094\n",
      "Iteration 32, loss = 0.00300220\n",
      "Iteration 33, loss = 0.00305724\n",
      "Iteration 34, loss = 0.00302512\n",
      "Iteration 35, loss = 0.00293976\n",
      "Iteration 36, loss = 0.00284720\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04901090\n",
      "Iteration 2, loss = 0.08425261\n",
      "Iteration 3, loss = 0.00521984\n",
      "Iteration 4, loss = 0.03467540\n",
      "Iteration 5, loss = 0.02116128\n",
      "Iteration 6, loss = 0.00464127\n",
      "Iteration 7, loss = 0.00903306\n",
      "Iteration 8, loss = 0.01493295\n",
      "Iteration 9, loss = 0.01204593\n",
      "Iteration 10, loss = 0.00717888\n",
      "Iteration 11, loss = 0.00392087\n",
      "Iteration 12, loss = 0.00396032\n",
      "Iteration 13, loss = 0.00581625\n",
      "Iteration 14, loss = 0.00724269\n",
      "Iteration 15, loss = 0.00713005\n",
      "Iteration 16, loss = 0.00575586\n",
      "Iteration 17, loss = 0.00414131\n",
      "Iteration 18, loss = 0.00319959\n",
      "Iteration 19, loss = 0.00328879\n",
      "Iteration 20, loss = 0.00405226\n",
      "Iteration 21, loss = 0.00461494\n",
      "Iteration 22, loss = 0.00456487\n",
      "Iteration 23, loss = 0.00401475\n",
      "Iteration 24, loss = 0.00335351\n",
      "Iteration 25, loss = 0.00299079\n",
      "Iteration 26, loss = 0.00312837\n",
      "Iteration 27, loss = 0.00348587\n",
      "Iteration 28, loss = 0.00362254\n",
      "Iteration 29, loss = 0.00343928\n",
      "Iteration 30, loss = 0.00312583\n",
      "Iteration 31, loss = 0.00290030\n",
      "Iteration 32, loss = 0.00291071\n",
      "Iteration 33, loss = 0.00302915\n",
      "Iteration 34, loss = 0.00306777\n",
      "Iteration 35, loss = 0.00300452\n",
      "Iteration 36, loss = 0.00289663\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04904318\n",
      "Iteration 2, loss = 0.08395588\n",
      "Iteration 3, loss = 0.00516672\n",
      "Iteration 4, loss = 0.03453979\n",
      "Iteration 5, loss = 0.02094274\n",
      "Iteration 6, loss = 0.00454486\n",
      "Iteration 7, loss = 0.00915853\n",
      "Iteration 8, loss = 0.01496251\n",
      "Iteration 9, loss = 0.01200324\n",
      "Iteration 10, loss = 0.00708763\n",
      "Iteration 11, loss = 0.00387053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.00400594\n",
      "Iteration 13, loss = 0.00590208\n",
      "Iteration 14, loss = 0.00730828\n",
      "Iteration 15, loss = 0.00712834\n",
      "Iteration 16, loss = 0.00569514\n",
      "Iteration 17, loss = 0.00406826\n",
      "Iteration 18, loss = 0.00316987\n",
      "Iteration 19, loss = 0.00332740\n",
      "Iteration 20, loss = 0.00412198\n",
      "Iteration 21, loss = 0.00465381\n",
      "Iteration 22, loss = 0.00455952\n",
      "Iteration 23, loss = 0.00397585\n",
      "Iteration 24, loss = 0.00331392\n",
      "Iteration 25, loss = 0.00297760\n",
      "Iteration 26, loss = 0.00314920\n",
      "Iteration 27, loss = 0.00350415\n",
      "Iteration 28, loss = 0.00361801\n",
      "Iteration 29, loss = 0.00342690\n",
      "Iteration 30, loss = 0.00311087\n",
      "Iteration 31, loss = 0.00289507\n",
      "Iteration 32, loss = 0.00291200\n",
      "Iteration 33, loss = 0.00302918\n",
      "Iteration 34, loss = 0.00306696\n",
      "Iteration 35, loss = 0.00300371\n",
      "Iteration 36, loss = 0.00289611\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04904354\n",
      "Iteration 2, loss = 0.08407009\n",
      "Iteration 3, loss = 0.00518867\n",
      "Iteration 4, loss = 0.03455280\n",
      "Iteration 5, loss = 0.02098561\n",
      "Iteration 6, loss = 0.00456601\n",
      "Iteration 7, loss = 0.00916710\n",
      "Iteration 8, loss = 0.01499418\n",
      "Iteration 9, loss = 0.01203223\n",
      "Iteration 10, loss = 0.00711646\n",
      "Iteration 11, loss = 0.00388622\n",
      "Iteration 12, loss = 0.00401909\n",
      "Iteration 13, loss = 0.00591784\n",
      "Iteration 14, loss = 0.00732569\n",
      "Iteration 15, loss = 0.00714651\n",
      "Iteration 16, loss = 0.00571288\n",
      "Iteration 17, loss = 0.00407904\n",
      "Iteration 18, loss = 0.00317433\n",
      "Iteration 19, loss = 0.00334104\n",
      "Iteration 20, loss = 0.00413303\n",
      "Iteration 21, loss = 0.00465783\n",
      "Iteration 22, loss = 0.00455235\n",
      "Iteration 23, loss = 0.00396511\n",
      "Iteration 24, loss = 0.00331017\n",
      "Iteration 25, loss = 0.00298064\n",
      "Iteration 26, loss = 0.00315199\n",
      "Iteration 27, loss = 0.00350082\n",
      "Iteration 28, loss = 0.00361629\n",
      "Iteration 29, loss = 0.00341808\n",
      "Iteration 30, loss = 0.00310379\n",
      "Iteration 31, loss = 0.00288981\n",
      "Iteration 32, loss = 0.00290570\n",
      "Iteration 33, loss = 0.00302116\n",
      "Iteration 34, loss = 0.00305582\n",
      "Iteration 35, loss = 0.00299091\n",
      "Iteration 36, loss = 0.00288254\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04887209\n",
      "Iteration 2, loss = 0.08335764\n",
      "Iteration 3, loss = 0.00539078\n",
      "Iteration 4, loss = 0.03501276\n",
      "Iteration 5, loss = 0.02143205\n",
      "Iteration 6, loss = 0.00468590\n",
      "Iteration 7, loss = 0.00936853\n",
      "Iteration 8, loss = 0.01553119\n",
      "Iteration 9, loss = 0.01205935\n",
      "Iteration 10, loss = 0.00694894\n",
      "Iteration 11, loss = 0.00376276\n",
      "Iteration 12, loss = 0.00398494\n",
      "Iteration 13, loss = 0.00595243\n",
      "Iteration 14, loss = 0.00735195\n",
      "Iteration 15, loss = 0.00714480\n",
      "Iteration 16, loss = 0.00572432\n",
      "Iteration 17, loss = 0.00413176\n",
      "Iteration 18, loss = 0.00323765\n",
      "Iteration 19, loss = 0.00333450\n",
      "Iteration 20, loss = 0.00407810\n",
      "Iteration 21, loss = 0.00467201\n",
      "Iteration 22, loss = 0.00467238\n",
      "Iteration 23, loss = 0.00411924\n",
      "Iteration 24, loss = 0.00341865\n",
      "Iteration 25, loss = 0.00299951\n",
      "Iteration 26, loss = 0.00310009\n",
      "Iteration 27, loss = 0.00347028\n",
      "Iteration 28, loss = 0.00364564\n",
      "Iteration 29, loss = 0.00346236\n",
      "Iteration 30, loss = 0.00311895\n",
      "Iteration 31, loss = 0.00287388\n",
      "Iteration 32, loss = 0.00287334\n",
      "Iteration 33, loss = 0.00300850\n",
      "Iteration 34, loss = 0.00305251\n",
      "Iteration 35, loss = 0.00296901\n",
      "Iteration 36, loss = 0.00284548\n",
      "Iteration 37, loss = 0.00275384\n",
      "Iteration 38, loss = 0.00272044\n",
      "Iteration 39, loss = 0.00273021\n",
      "Iteration 40, loss = 0.00275442\n",
      "Iteration 41, loss = 0.00275669\n",
      "Iteration 42, loss = 0.00272327\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04901287\n",
      "Iteration 2, loss = 0.08406241\n",
      "Iteration 3, loss = 0.00512408\n",
      "Iteration 4, loss = 0.03453203\n",
      "Iteration 5, loss = 0.02085491\n",
      "Iteration 6, loss = 0.00452062\n",
      "Iteration 7, loss = 0.00917405\n",
      "Iteration 8, loss = 0.01483867\n",
      "Iteration 9, loss = 0.01192586\n",
      "Iteration 10, loss = 0.00707134\n",
      "Iteration 11, loss = 0.00391762\n",
      "Iteration 12, loss = 0.00407444\n",
      "Iteration 13, loss = 0.00594569\n",
      "Iteration 14, loss = 0.00730336\n",
      "Iteration 15, loss = 0.00706415\n",
      "Iteration 16, loss = 0.00560607\n",
      "Iteration 17, loss = 0.00400806\n",
      "Iteration 18, loss = 0.00316427\n",
      "Iteration 19, loss = 0.00337406\n",
      "Iteration 20, loss = 0.00417171\n",
      "Iteration 21, loss = 0.00469158\n",
      "Iteration 22, loss = 0.00456812\n",
      "Iteration 23, loss = 0.00395655\n",
      "Iteration 24, loss = 0.00328351\n",
      "Iteration 25, loss = 0.00297364\n",
      "Iteration 26, loss = 0.00318202\n",
      "Iteration 27, loss = 0.00353283\n",
      "Iteration 28, loss = 0.00362249\n",
      "Iteration 29, loss = 0.00340885\n",
      "Iteration 30, loss = 0.00308257\n",
      "Iteration 31, loss = 0.00287401\n",
      "Iteration 32, loss = 0.00290140\n",
      "Iteration 33, loss = 0.00301626\n",
      "Iteration 34, loss = 0.00305262\n",
      "Iteration 35, loss = 0.00298938\n",
      "Iteration 36, loss = 0.00287625\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04904084\n",
      "Iteration 2, loss = 0.08391206\n",
      "Iteration 3, loss = 0.00514582\n",
      "Iteration 4, loss = 0.03454141\n",
      "Iteration 5, loss = 0.02099529\n",
      "Iteration 6, loss = 0.00455629\n",
      "Iteration 7, loss = 0.00918232\n",
      "Iteration 8, loss = 0.01501533\n",
      "Iteration 9, loss = 0.01201907\n",
      "Iteration 10, loss = 0.00710405\n",
      "Iteration 11, loss = 0.00388578\n",
      "Iteration 12, loss = 0.00401227\n",
      "Iteration 13, loss = 0.00589552\n",
      "Iteration 14, loss = 0.00729010\n",
      "Iteration 15, loss = 0.00710246\n",
      "Iteration 16, loss = 0.00567255\n",
      "Iteration 17, loss = 0.00405269\n",
      "Iteration 18, loss = 0.00316366\n",
      "Iteration 19, loss = 0.00334289\n",
      "Iteration 20, loss = 0.00413710\n",
      "Iteration 21, loss = 0.00464598\n",
      "Iteration 22, loss = 0.00451995\n",
      "Iteration 23, loss = 0.00392830\n",
      "Iteration 24, loss = 0.00328532\n",
      "Iteration 25, loss = 0.00297865\n",
      "Iteration 26, loss = 0.00316811\n",
      "Iteration 27, loss = 0.00351355\n",
      "Iteration 28, loss = 0.00361277\n",
      "Iteration 29, loss = 0.00340619\n",
      "Iteration 30, loss = 0.00309580\n",
      "Iteration 31, loss = 0.00288761\n",
      "Iteration 32, loss = 0.00291257\n",
      "Iteration 33, loss = 0.00302705\n",
      "Iteration 34, loss = 0.00305863\n",
      "Iteration 35, loss = 0.00299309\n",
      "Iteration 36, loss = 0.00288499\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04903940\n",
      "Iteration 2, loss = 0.08389562\n",
      "Iteration 3, loss = 0.00515316\n",
      "Iteration 4, loss = 0.03453283\n",
      "Iteration 5, loss = 0.02106101\n",
      "Iteration 6, loss = 0.00458637\n",
      "Iteration 7, loss = 0.00909241\n",
      "Iteration 8, loss = 0.01501596\n",
      "Iteration 9, loss = 0.01211431\n",
      "Iteration 10, loss = 0.00720116\n",
      "Iteration 11, loss = 0.00391842\n",
      "Iteration 12, loss = 0.00399182\n",
      "Iteration 13, loss = 0.00586641\n",
      "Iteration 14, loss = 0.00731070\n",
      "Iteration 15, loss = 0.00719182\n",
      "Iteration 16, loss = 0.00579645\n",
      "Iteration 17, loss = 0.00415357\n",
      "Iteration 18, loss = 0.00319744\n",
      "Iteration 19, loss = 0.00329946\n",
      "Iteration 20, loss = 0.00406568\n",
      "Iteration 21, loss = 0.00461401\n",
      "Iteration 22, loss = 0.00454843\n",
      "Iteration 23, loss = 0.00398148\n",
      "Iteration 24, loss = 0.00330588\n",
      "Iteration 25, loss = 0.00293468\n",
      "Iteration 26, loss = 0.00308549\n",
      "Iteration 27, loss = 0.00345688\n",
      "Iteration 28, loss = 0.00358415\n",
      "Iteration 29, loss = 0.00338914\n",
      "Iteration 30, loss = 0.00304864\n",
      "Iteration 31, loss = 0.00283109\n",
      "Iteration 32, loss = 0.00285667\n",
      "Iteration 33, loss = 0.00297817\n",
      "Iteration 34, loss = 0.00301247\n",
      "Iteration 35, loss = 0.00294127\n",
      "Iteration 36, loss = 0.00282660\n",
      "Iteration 37, loss = 0.00273101\n",
      "Iteration 38, loss = 0.00267678\n",
      "Iteration 39, loss = 0.00267479\n",
      "Iteration 40, loss = 0.00270186\n",
      "Iteration 41, loss = 0.00271376\n",
      "Iteration 42, loss = 0.00268635\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04904397\n",
      "Iteration 2, loss = 0.08417783\n",
      "Iteration 3, loss = 0.00517399\n",
      "Iteration 4, loss = 0.03455747\n",
      "Iteration 5, loss = 0.02118323\n",
      "Iteration 6, loss = 0.00465938\n",
      "Iteration 7, loss = 0.00898538\n",
      "Iteration 8, loss = 0.01491769\n",
      "Iteration 9, loss = 0.01209972\n",
      "Iteration 10, loss = 0.00722821\n",
      "Iteration 11, loss = 0.00394066\n",
      "Iteration 12, loss = 0.00396773\n",
      "Iteration 13, loss = 0.00582335\n",
      "Iteration 14, loss = 0.00727167\n",
      "Iteration 15, loss = 0.00717987\n",
      "Iteration 16, loss = 0.00581379\n",
      "Iteration 17, loss = 0.00418494\n",
      "Iteration 18, loss = 0.00322041\n",
      "Iteration 19, loss = 0.00330259\n",
      "Iteration 20, loss = 0.00405969\n",
      "Iteration 21, loss = 0.00461571\n",
      "Iteration 22, loss = 0.00456006\n",
      "Iteration 23, loss = 0.00400196\n",
      "Iteration 24, loss = 0.00333744\n",
      "Iteration 25, loss = 0.00295808\n",
      "Iteration 26, loss = 0.00308394\n",
      "Iteration 27, loss = 0.00345344\n",
      "Iteration 28, loss = 0.00360116\n",
      "Iteration 29, loss = 0.00340033\n",
      "Iteration 30, loss = 0.00306499\n",
      "Iteration 31, loss = 0.00284260\n",
      "Iteration 32, loss = 0.00286394\n",
      "Iteration 33, loss = 0.00298567\n",
      "Iteration 34, loss = 0.00301891\n",
      "Iteration 35, loss = 0.00294800\n",
      "Iteration 36, loss = 0.00283376\n",
      "Iteration 37, loss = 0.00274223\n",
      "Iteration 38, loss = 0.00269462\n",
      "Iteration 39, loss = 0.00269934\n",
      "Iteration 40, loss = 0.00272990\n",
      "Iteration 41, loss = 0.00274506\n",
      "Iteration 42, loss = 0.00272188\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04892614\n",
      "Iteration 2, loss = 0.08382865\n",
      "Iteration 3, loss = 0.00508734\n",
      "Iteration 4, loss = 0.03438491\n",
      "Iteration 5, loss = 0.02111585\n",
      "Iteration 6, loss = 0.00463172\n",
      "Iteration 7, loss = 0.00896815\n",
      "Iteration 8, loss = 0.01486714\n",
      "Iteration 9, loss = 0.01206708\n",
      "Iteration 10, loss = 0.00720208\n",
      "Iteration 11, loss = 0.00393389\n",
      "Iteration 12, loss = 0.00396868\n",
      "Iteration 13, loss = 0.00581923\n",
      "Iteration 14, loss = 0.00724428\n",
      "Iteration 15, loss = 0.00713308\n",
      "Iteration 16, loss = 0.00576410\n",
      "Iteration 17, loss = 0.00415400\n",
      "Iteration 18, loss = 0.00321080\n",
      "Iteration 19, loss = 0.00330694\n",
      "Iteration 20, loss = 0.00407862\n",
      "Iteration 21, loss = 0.00462883\n",
      "Iteration 22, loss = 0.00455408\n",
      "Iteration 23, loss = 0.00399456\n",
      "Iteration 24, loss = 0.00334378\n",
      "Iteration 25, loss = 0.00300003\n",
      "Iteration 26, loss = 0.00314431\n",
      "Iteration 27, loss = 0.00349524\n",
      "Iteration 28, loss = 0.00361679\n",
      "Iteration 29, loss = 0.00343544\n",
      "Iteration 30, loss = 0.00312570\n",
      "Iteration 31, loss = 0.00290882\n",
      "Iteration 32, loss = 0.00291970\n",
      "Iteration 33, loss = 0.00303487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34, loss = 0.00307287\n",
      "Iteration 35, loss = 0.00300994\n",
      "Iteration 36, loss = 0.00290213\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04900110\n",
      "Iteration 2, loss = 0.08421588\n",
      "Iteration 3, loss = 0.00497661\n",
      "Iteration 4, loss = 0.03459564\n",
      "Iteration 5, loss = 0.02130631\n",
      "Iteration 6, loss = 0.00463895\n",
      "Iteration 7, loss = 0.00903426\n",
      "Iteration 8, loss = 0.01494923\n",
      "Iteration 9, loss = 0.01211523\n",
      "Iteration 10, loss = 0.00726965\n",
      "Iteration 11, loss = 0.00400687\n",
      "Iteration 12, loss = 0.00401533\n",
      "Iteration 13, loss = 0.00581519\n",
      "Iteration 14, loss = 0.00722003\n",
      "Iteration 15, loss = 0.00708958\n",
      "Iteration 16, loss = 0.00570596\n",
      "Iteration 17, loss = 0.00409027\n",
      "Iteration 18, loss = 0.00317226\n",
      "Iteration 19, loss = 0.00331519\n",
      "Iteration 20, loss = 0.00409888\n",
      "Iteration 21, loss = 0.00462091\n",
      "Iteration 22, loss = 0.00452305\n",
      "Iteration 23, loss = 0.00394368\n",
      "Iteration 24, loss = 0.00329414\n",
      "Iteration 25, loss = 0.00295956\n",
      "Iteration 26, loss = 0.00312767\n",
      "Iteration 27, loss = 0.00347033\n",
      "Iteration 28, loss = 0.00357288\n",
      "Iteration 29, loss = 0.00337589\n",
      "Iteration 30, loss = 0.00305873\n",
      "Iteration 31, loss = 0.00285223\n",
      "Iteration 32, loss = 0.00287531\n",
      "Iteration 33, loss = 0.00298624\n",
      "Iteration 34, loss = 0.00301918\n",
      "Iteration 35, loss = 0.00295210\n",
      "Iteration 36, loss = 0.00284354\n",
      "Iteration 37, loss = 0.00275064\n",
      "Iteration 38, loss = 0.00269701\n",
      "Iteration 39, loss = 0.00269715\n",
      "Iteration 40, loss = 0.00272611\n",
      "Iteration 41, loss = 0.00273871\n",
      "Iteration 42, loss = 0.00271094\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04903494\n",
      "Iteration 2, loss = 0.08446245\n",
      "Iteration 3, loss = 0.00507589\n",
      "Iteration 4, loss = 0.03454091\n",
      "Iteration 5, loss = 0.02087287\n",
      "Iteration 6, loss = 0.00453467\n",
      "Iteration 7, loss = 0.00914159\n",
      "Iteration 8, loss = 0.01489523\n",
      "Iteration 9, loss = 0.01198216\n",
      "Iteration 10, loss = 0.00711996\n",
      "Iteration 11, loss = 0.00392945\n",
      "Iteration 12, loss = 0.00404917\n",
      "Iteration 13, loss = 0.00591308\n",
      "Iteration 14, loss = 0.00730279\n",
      "Iteration 15, loss = 0.00710458\n",
      "Iteration 16, loss = 0.00565830\n",
      "Iteration 17, loss = 0.00403248\n",
      "Iteration 18, loss = 0.00316324\n",
      "Iteration 19, loss = 0.00336344\n",
      "Iteration 20, loss = 0.00415781\n",
      "Iteration 21, loss = 0.00466930\n",
      "Iteration 22, loss = 0.00454337\n",
      "Iteration 23, loss = 0.00394330\n",
      "Iteration 24, loss = 0.00328329\n",
      "Iteration 25, loss = 0.00297120\n",
      "Iteration 26, loss = 0.00317453\n",
      "Iteration 27, loss = 0.00352057\n",
      "Iteration 28, loss = 0.00361408\n",
      "Iteration 29, loss = 0.00341580\n",
      "Iteration 30, loss = 0.00309516\n",
      "Iteration 31, loss = 0.00288975\n",
      "Iteration 32, loss = 0.00291592\n",
      "Iteration 33, loss = 0.00302998\n",
      "Iteration 34, loss = 0.00305953\n",
      "Iteration 35, loss = 0.00298970\n",
      "Iteration 36, loss = 0.00287195\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04902546\n",
      "Iteration 2, loss = 0.08435846\n",
      "Iteration 3, loss = 0.00510603\n",
      "Iteration 4, loss = 0.03457044\n",
      "Iteration 5, loss = 0.02112558\n",
      "Iteration 6, loss = 0.00458158\n",
      "Iteration 7, loss = 0.00893177\n",
      "Iteration 8, loss = 0.01494567\n",
      "Iteration 9, loss = 0.01205214\n",
      "Iteration 10, loss = 0.00713525\n",
      "Iteration 11, loss = 0.00384203\n",
      "Iteration 12, loss = 0.00386748\n",
      "Iteration 13, loss = 0.00574718\n",
      "Iteration 14, loss = 0.00718594\n",
      "Iteration 15, loss = 0.00706887\n",
      "Iteration 16, loss = 0.00567233\n",
      "Iteration 17, loss = 0.00402993\n",
      "Iteration 18, loss = 0.00308233\n",
      "Iteration 19, loss = 0.00320361\n",
      "Iteration 20, loss = 0.00398421\n",
      "Iteration 21, loss = 0.00452211\n",
      "Iteration 22, loss = 0.00443996\n",
      "Iteration 23, loss = 0.00387663\n",
      "Iteration 24, loss = 0.00323103\n",
      "Iteration 25, loss = 0.00289407\n",
      "Iteration 26, loss = 0.00305136\n",
      "Iteration 27, loss = 0.00339922\n",
      "Iteration 28, loss = 0.00352332\n",
      "Iteration 29, loss = 0.00334663\n",
      "Iteration 30, loss = 0.00304157\n",
      "Iteration 31, loss = 0.00282559\n",
      "Iteration 32, loss = 0.00283658\n",
      "Iteration 33, loss = 0.00295867\n",
      "Iteration 34, loss = 0.00299916\n",
      "Iteration 35, loss = 0.00294010\n",
      "Iteration 36, loss = 0.00283422\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04904288\n",
      "Iteration 2, loss = 0.08410825\n",
      "Iteration 3, loss = 0.00515717\n",
      "Iteration 4, loss = 0.03446620\n",
      "Iteration 5, loss = 0.02112948\n",
      "Iteration 6, loss = 0.00463185\n",
      "Iteration 7, loss = 0.00903120\n",
      "Iteration 8, loss = 0.01490766\n",
      "Iteration 9, loss = 0.01205557\n",
      "Iteration 10, loss = 0.00716727\n",
      "Iteration 11, loss = 0.00390145\n",
      "Iteration 12, loss = 0.00396014\n",
      "Iteration 13, loss = 0.00583504\n",
      "Iteration 14, loss = 0.00724344\n",
      "Iteration 15, loss = 0.00708500\n",
      "Iteration 16, loss = 0.00567225\n",
      "Iteration 17, loss = 0.00405199\n",
      "Iteration 18, loss = 0.00314763\n",
      "Iteration 19, loss = 0.00331423\n",
      "Iteration 20, loss = 0.00410926\n",
      "Iteration 21, loss = 0.00462057\n",
      "Iteration 22, loss = 0.00449404\n",
      "Iteration 23, loss = 0.00391104\n",
      "Iteration 24, loss = 0.00326505\n",
      "Iteration 25, loss = 0.00295142\n",
      "Iteration 26, loss = 0.00314120\n",
      "Iteration 27, loss = 0.00349113\n",
      "Iteration 28, loss = 0.00359444\n",
      "Iteration 29, loss = 0.00338484\n",
      "Iteration 30, loss = 0.00307425\n",
      "Iteration 31, loss = 0.00287174\n",
      "Iteration 32, loss = 0.00289919\n",
      "Iteration 33, loss = 0.00301462\n",
      "Iteration 34, loss = 0.00304622\n",
      "Iteration 35, loss = 0.00297981\n",
      "Iteration 36, loss = 0.00287133\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04900764\n",
      "Iteration 2, loss = 0.08395636\n",
      "Iteration 3, loss = 0.00514288\n",
      "Iteration 4, loss = 0.03451181\n",
      "Iteration 5, loss = 0.02113604\n",
      "Iteration 6, loss = 0.00464065\n",
      "Iteration 7, loss = 0.00905660\n",
      "Iteration 8, loss = 0.01497825\n",
      "Iteration 9, loss = 0.01211251\n",
      "Iteration 10, loss = 0.00720936\n",
      "Iteration 11, loss = 0.00392567\n",
      "Iteration 12, loss = 0.00399741\n",
      "Iteration 13, loss = 0.00586790\n",
      "Iteration 14, loss = 0.00731407\n",
      "Iteration 15, loss = 0.00718489\n",
      "Iteration 16, loss = 0.00578272\n",
      "Iteration 17, loss = 0.00414618\n",
      "Iteration 18, loss = 0.00321128\n",
      "Iteration 19, loss = 0.00332676\n",
      "Iteration 20, loss = 0.00411290\n",
      "Iteration 21, loss = 0.00465955\n",
      "Iteration 22, loss = 0.00457747\n",
      "Iteration 23, loss = 0.00400625\n",
      "Iteration 24, loss = 0.00334015\n",
      "Iteration 25, loss = 0.00297962\n",
      "Iteration 26, loss = 0.00312949\n",
      "Iteration 27, loss = 0.00349583\n",
      "Iteration 28, loss = 0.00363123\n",
      "Iteration 29, loss = 0.00343009\n",
      "Iteration 30, loss = 0.00309751\n",
      "Iteration 31, loss = 0.00287403\n",
      "Iteration 32, loss = 0.00289344\n",
      "Iteration 33, loss = 0.00300976\n",
      "Iteration 34, loss = 0.00303667\n",
      "Iteration 35, loss = 0.00295908\n",
      "Iteration 36, loss = 0.00284302\n",
      "Iteration 37, loss = 0.00275196\n",
      "Iteration 38, loss = 0.00270797\n",
      "Iteration 39, loss = 0.00271338\n",
      "Iteration 40, loss = 0.00273994\n",
      "Iteration 41, loss = 0.00274695\n",
      "Iteration 42, loss = 0.00271484\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04892024\n",
      "Iteration 2, loss = 0.08422615\n",
      "Iteration 3, loss = 0.00497699\n",
      "Iteration 4, loss = 0.03446030\n",
      "Iteration 5, loss = 0.02065374\n",
      "Iteration 6, loss = 0.00448567\n",
      "Iteration 7, loss = 0.00885229\n",
      "Iteration 8, loss = 0.01450164\n",
      "Iteration 9, loss = 0.01195286\n",
      "Iteration 10, loss = 0.00719985\n",
      "Iteration 11, loss = 0.00396109\n",
      "Iteration 12, loss = 0.00402359\n",
      "Iteration 13, loss = 0.00588643\n",
      "Iteration 14, loss = 0.00727679\n",
      "Iteration 15, loss = 0.00706867\n",
      "Iteration 16, loss = 0.00560623\n",
      "Iteration 17, loss = 0.00398554\n",
      "Iteration 18, loss = 0.00313765\n",
      "Iteration 19, loss = 0.00336113\n",
      "Iteration 20, loss = 0.00418413\n",
      "Iteration 21, loss = 0.00470396\n",
      "Iteration 22, loss = 0.00455159\n",
      "Iteration 23, loss = 0.00389911\n",
      "Iteration 24, loss = 0.00322145\n",
      "Iteration 25, loss = 0.00295781\n",
      "Iteration 26, loss = 0.00328084\n",
      "Iteration 27, loss = 0.00357298\n",
      "Iteration 28, loss = 0.00353886\n",
      "Iteration 29, loss = 0.00327125\n",
      "Iteration 30, loss = 0.00298417\n",
      "Iteration 31, loss = 0.00285434\n",
      "Iteration 32, loss = 0.00293022\n",
      "Iteration 33, loss = 0.00302691\n",
      "Iteration 34, loss = 0.00302599\n",
      "Iteration 35, loss = 0.00293762\n",
      "Iteration 36, loss = 0.00282986\n",
      "Iteration 37, loss = 0.00274814\n",
      "Iteration 38, loss = 0.00271237\n",
      "Iteration 39, loss = 0.00271755\n",
      "Iteration 40, loss = 0.00273834\n",
      "Iteration 41, loss = 0.00273825\n",
      "Iteration 42, loss = 0.00270454\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04859554\n",
      "Iteration 2, loss = 0.08352740\n",
      "Iteration 3, loss = 0.00498500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.03429744\n",
      "Iteration 5, loss = 0.02089301\n",
      "Iteration 6, loss = 0.00455608\n",
      "Iteration 7, loss = 0.00893662\n",
      "Iteration 8, loss = 0.01472619\n",
      "Iteration 9, loss = 0.01197084\n",
      "Iteration 10, loss = 0.00711105\n",
      "Iteration 11, loss = 0.00386719\n",
      "Iteration 12, loss = 0.00393325\n",
      "Iteration 13, loss = 0.00579248\n",
      "Iteration 14, loss = 0.00723838\n",
      "Iteration 15, loss = 0.00712481\n",
      "Iteration 16, loss = 0.00574408\n",
      "Iteration 17, loss = 0.00411347\n",
      "Iteration 18, loss = 0.00316120\n",
      "Iteration 19, loss = 0.00325553\n",
      "Iteration 20, loss = 0.00401102\n",
      "Iteration 21, loss = 0.00453871\n",
      "Iteration 22, loss = 0.00444838\n",
      "Iteration 23, loss = 0.00386605\n",
      "Iteration 24, loss = 0.00320056\n",
      "Iteration 25, loss = 0.00289233\n",
      "Iteration 26, loss = 0.00312717\n",
      "Iteration 27, loss = 0.00348567\n",
      "Iteration 28, loss = 0.00354059\n",
      "Iteration 29, loss = 0.00326602\n",
      "Iteration 30, loss = 0.00293965\n",
      "Iteration 31, loss = 0.00279859\n",
      "Iteration 32, loss = 0.00289545\n",
      "Iteration 33, loss = 0.00300906\n",
      "Iteration 34, loss = 0.00300871\n",
      "Iteration 35, loss = 0.00291401\n",
      "Iteration 36, loss = 0.00279660\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04818867\n",
      "Iteration 2, loss = 0.08265202\n",
      "Iteration 3, loss = 0.00499569\n",
      "Iteration 4, loss = 0.03477568\n",
      "Iteration 5, loss = 0.02061866\n",
      "Iteration 6, loss = 0.00437658\n",
      "Iteration 7, loss = 0.00867709\n",
      "Iteration 8, loss = 0.01431476\n",
      "Iteration 9, loss = 0.01194938\n",
      "Iteration 10, loss = 0.00677560\n",
      "Iteration 11, loss = 0.00353834\n",
      "Iteration 12, loss = 0.00377672\n",
      "Iteration 13, loss = 0.00584008\n",
      "Iteration 14, loss = 0.00727999\n",
      "Iteration 15, loss = 0.00706667\n",
      "Iteration 16, loss = 0.00560209\n",
      "Iteration 17, loss = 0.00395976\n",
      "Iteration 18, loss = 0.00306367\n",
      "Iteration 19, loss = 0.00321286\n",
      "Iteration 20, loss = 0.00399366\n",
      "Iteration 21, loss = 0.00461449\n",
      "Iteration 22, loss = 0.00450313\n",
      "Iteration 23, loss = 0.00383311\n",
      "Iteration 24, loss = 0.00309786\n",
      "Iteration 25, loss = 0.00277218\n",
      "Iteration 26, loss = 0.00304699\n",
      "Iteration 27, loss = 0.00342940\n",
      "Iteration 28, loss = 0.00348865\n",
      "Iteration 29, loss = 0.00320124\n",
      "Iteration 30, loss = 0.00283545\n",
      "Iteration 31, loss = 0.00264817\n",
      "Iteration 32, loss = 0.00273270\n",
      "Iteration 33, loss = 0.00286373\n",
      "Iteration 34, loss = 0.00288063\n",
      "Iteration 35, loss = 0.00279121\n",
      "Iteration 36, loss = 0.00266865\n",
      "Iteration 37, loss = 0.00257503\n",
      "Iteration 38, loss = 0.00253205\n",
      "Iteration 39, loss = 0.00254604\n",
      "Iteration 40, loss = 0.00258262\n",
      "Iteration 41, loss = 0.00259872\n",
      "Iteration 42, loss = 0.00257299\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04898438\n",
      "Iteration 2, loss = 0.08447608\n",
      "Iteration 3, loss = 0.00508555\n",
      "Iteration 4, loss = 0.03469710\n",
      "Iteration 5, loss = 0.02120242\n",
      "Iteration 6, loss = 0.00462916\n",
      "Iteration 7, loss = 0.00893121\n",
      "Iteration 8, loss = 0.01493694\n",
      "Iteration 9, loss = 0.01208647\n",
      "Iteration 10, loss = 0.00720174\n",
      "Iteration 11, loss = 0.00389580\n",
      "Iteration 12, loss = 0.00390020\n",
      "Iteration 13, loss = 0.00572669\n",
      "Iteration 14, loss = 0.00719222\n",
      "Iteration 15, loss = 0.00710781\n",
      "Iteration 16, loss = 0.00573493\n",
      "Iteration 17, loss = 0.00409767\n",
      "Iteration 18, loss = 0.00314418\n",
      "Iteration 19, loss = 0.00324862\n",
      "Iteration 20, loss = 0.00402403\n",
      "Iteration 21, loss = 0.00457538\n",
      "Iteration 22, loss = 0.00449757\n",
      "Iteration 23, loss = 0.00394078\n",
      "Iteration 24, loss = 0.00327659\n",
      "Iteration 25, loss = 0.00290948\n",
      "Iteration 26, loss = 0.00304648\n",
      "Iteration 27, loss = 0.00340928\n",
      "Iteration 28, loss = 0.00354496\n",
      "Iteration 29, loss = 0.00336115\n",
      "Iteration 30, loss = 0.00303913\n",
      "Iteration 31, loss = 0.00281261\n",
      "Iteration 32, loss = 0.00282724\n",
      "Iteration 33, loss = 0.00294895\n",
      "Iteration 34, loss = 0.00298006\n",
      "Iteration 35, loss = 0.00289991\n",
      "Iteration 36, loss = 0.00277911\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04904427\n",
      "Iteration 2, loss = 0.08458652\n",
      "Iteration 3, loss = 0.00495462\n",
      "Iteration 4, loss = 0.03470064\n",
      "Iteration 5, loss = 0.02112754\n",
      "Iteration 6, loss = 0.00444479\n",
      "Iteration 7, loss = 0.00882838\n",
      "Iteration 8, loss = 0.01489848\n",
      "Iteration 9, loss = 0.01209881\n",
      "Iteration 10, loss = 0.00712386\n",
      "Iteration 11, loss = 0.00373614\n",
      "Iteration 12, loss = 0.00375953\n",
      "Iteration 13, loss = 0.00567109\n",
      "Iteration 14, loss = 0.00716767\n",
      "Iteration 15, loss = 0.00707265\n",
      "Iteration 16, loss = 0.00565426\n",
      "Iteration 17, loss = 0.00397283\n",
      "Iteration 18, loss = 0.00299900\n",
      "Iteration 19, loss = 0.00309548\n",
      "Iteration 20, loss = 0.00388458\n",
      "Iteration 21, loss = 0.00447203\n",
      "Iteration 22, loss = 0.00443045\n",
      "Iteration 23, loss = 0.00386825\n",
      "Iteration 24, loss = 0.00319210\n",
      "Iteration 25, loss = 0.00281932\n",
      "Iteration 26, loss = 0.00296068\n",
      "Iteration 27, loss = 0.00332215\n",
      "Iteration 28, loss = 0.00347752\n",
      "Iteration 29, loss = 0.00331557\n",
      "Iteration 30, loss = 0.00300479\n",
      "Iteration 31, loss = 0.00277455\n",
      "Iteration 32, loss = 0.00275846\n",
      "Iteration 33, loss = 0.00288217\n",
      "Iteration 34, loss = 0.00293857\n",
      "Iteration 35, loss = 0.00288649\n",
      "Iteration 36, loss = 0.00277952\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04855820\n",
      "Iteration 2, loss = 0.08374323\n",
      "Iteration 3, loss = 0.00496346\n",
      "Iteration 4, loss = 0.03480656\n",
      "Iteration 5, loss = 0.02238353\n",
      "Iteration 6, loss = 0.00495845\n",
      "Iteration 7, loss = 0.00843794\n",
      "Iteration 8, loss = 0.01465580\n",
      "Iteration 9, loss = 0.01250670\n",
      "Iteration 10, loss = 0.00745033\n",
      "Iteration 11, loss = 0.00387389\n",
      "Iteration 12, loss = 0.00372753\n",
      "Iteration 13, loss = 0.00558728\n",
      "Iteration 14, loss = 0.00716974\n",
      "Iteration 15, loss = 0.00725781\n",
      "Iteration 16, loss = 0.00599378\n",
      "Iteration 17, loss = 0.00433509\n",
      "Iteration 18, loss = 0.00319943\n",
      "Iteration 19, loss = 0.00309773\n",
      "Iteration 20, loss = 0.00381968\n",
      "Iteration 21, loss = 0.00446155\n",
      "Iteration 22, loss = 0.00451861\n",
      "Iteration 23, loss = 0.00400180\n",
      "Iteration 24, loss = 0.00329141\n",
      "Iteration 25, loss = 0.00285117\n",
      "Iteration 26, loss = 0.00299873\n",
      "Iteration 27, loss = 0.00335243\n",
      "Iteration 28, loss = 0.00343501\n",
      "Iteration 29, loss = 0.00320604\n",
      "Iteration 30, loss = 0.00285975\n",
      "Iteration 31, loss = 0.00263984\n",
      "Iteration 32, loss = 0.00273427\n",
      "Iteration 33, loss = 0.00282154\n",
      "Iteration 34, loss = 0.00275619\n",
      "Iteration 35, loss = 0.00263211\n",
      "Iteration 36, loss = 0.00253674\n",
      "Iteration 37, loss = 0.00247579\n",
      "Iteration 38, loss = 0.00245960\n",
      "Iteration 39, loss = 0.00247640\n",
      "Iteration 40, loss = 0.00248893\n",
      "Iteration 41, loss = 0.00246930\n",
      "Iteration 42, loss = 0.00241942\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04861943\n",
      "Iteration 2, loss = 0.08425381\n",
      "Iteration 3, loss = 0.00482156\n",
      "Iteration 4, loss = 0.03475079\n",
      "Iteration 5, loss = 0.02234833\n",
      "Iteration 6, loss = 0.00494259\n",
      "Iteration 7, loss = 0.00872651\n",
      "Iteration 8, loss = 0.01492400\n",
      "Iteration 9, loss = 0.01218341\n",
      "Iteration 10, loss = 0.00741799\n",
      "Iteration 11, loss = 0.00410365\n",
      "Iteration 12, loss = 0.00394096\n",
      "Iteration 13, loss = 0.00557623\n",
      "Iteration 14, loss = 0.00700226\n",
      "Iteration 15, loss = 0.00702936\n",
      "Iteration 16, loss = 0.00580486\n",
      "Iteration 17, loss = 0.00423180\n",
      "Iteration 18, loss = 0.00323014\n",
      "Iteration 19, loss = 0.00322913\n",
      "Iteration 20, loss = 0.00394166\n",
      "Iteration 21, loss = 0.00447796\n",
      "Iteration 22, loss = 0.00443547\n",
      "Iteration 23, loss = 0.00388972\n",
      "Iteration 24, loss = 0.00324396\n",
      "Iteration 25, loss = 0.00290574\n",
      "Iteration 26, loss = 0.00308179\n",
      "Iteration 27, loss = 0.00341208\n",
      "Iteration 28, loss = 0.00348363\n",
      "Iteration 29, loss = 0.00325450\n",
      "Iteration 30, loss = 0.00293795\n",
      "Iteration 31, loss = 0.00277650\n",
      "Iteration 32, loss = 0.00284890\n",
      "Iteration 33, loss = 0.00295219\n",
      "Iteration 34, loss = 0.00293872\n",
      "Iteration 35, loss = 0.00283165\n",
      "Iteration 36, loss = 0.00271128\n",
      "Iteration 37, loss = 0.00263459\n",
      "Iteration 38, loss = 0.00261949\n",
      "Iteration 39, loss = 0.00265335\n",
      "Iteration 40, loss = 0.00267969\n",
      "Iteration 41, loss = 0.00265966\n",
      "Iteration 42, loss = 0.00260188\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04864640\n",
      "Iteration 2, loss = 0.08418777\n",
      "Iteration 3, loss = 0.00492002\n",
      "Iteration 4, loss = 0.03383380\n",
      "Iteration 5, loss = 0.02059956\n",
      "Iteration 6, loss = 0.00452355\n",
      "Iteration 7, loss = 0.00886896\n",
      "Iteration 8, loss = 0.01408010\n",
      "Iteration 9, loss = 0.01043370\n",
      "Iteration 10, loss = 0.00541359\n",
      "Iteration 11, loss = 0.00367768\n",
      "Iteration 12, loss = 0.00569444\n",
      "Iteration 13, loss = 0.00754599\n",
      "Iteration 14, loss = 0.00691854\n",
      "Iteration 15, loss = 0.00486540\n",
      "Iteration 16, loss = 0.00336202\n",
      "Iteration 17, loss = 0.00340132\n",
      "Iteration 18, loss = 0.00441784\n",
      "Iteration 19, loss = 0.00504687\n",
      "Iteration 20, loss = 0.00470676\n",
      "Iteration 21, loss = 0.00376413\n",
      "Iteration 22, loss = 0.00300661\n",
      "Iteration 23, loss = 0.00296147\n",
      "Iteration 24, loss = 0.00341894\n",
      "Iteration 25, loss = 0.00369570\n",
      "Iteration 26, loss = 0.00349783\n",
      "Iteration 27, loss = 0.00307516\n",
      "Iteration 28, loss = 0.00275319\n",
      "Iteration 29, loss = 0.00274074\n",
      "Iteration 30, loss = 0.00289560\n",
      "Iteration 31, loss = 0.00295637\n",
      "Iteration 32, loss = 0.00286594\n",
      "Iteration 33, loss = 0.00270027\n",
      "Iteration 34, loss = 0.00256994\n",
      "Iteration 35, loss = 0.00253611\n",
      "Iteration 36, loss = 0.00257978\n",
      "Iteration 37, loss = 0.00262323\n",
      "Iteration 38, loss = 0.00259859\n",
      "Iteration 39, loss = 0.00251330\n",
      "Iteration 40, loss = 0.00243220\n",
      "Iteration 41, loss = 0.00240652\n",
      "Iteration 42, loss = 0.00242746\n",
      "Iteration 43, loss = 0.00243891\n",
      "Iteration 44, loss = 0.00240468\n",
      "Iteration 45, loss = 0.00234542\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04821795\n",
      "Iteration 2, loss = 0.08548897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.00478796\n",
      "Iteration 4, loss = 0.03419799\n",
      "Iteration 5, loss = 0.02067027\n",
      "Iteration 6, loss = 0.00471690\n",
      "Iteration 7, loss = 0.00814642\n",
      "Iteration 8, loss = 0.01380145\n",
      "Iteration 9, loss = 0.01199048\n",
      "Iteration 10, loss = 0.00734969\n",
      "Iteration 11, loss = 0.00400876\n",
      "Iteration 12, loss = 0.00385588\n",
      "Iteration 13, loss = 0.00564405\n",
      "Iteration 14, loss = 0.00709203\n",
      "Iteration 15, loss = 0.00701122\n",
      "Iteration 16, loss = 0.00564503\n",
      "Iteration 17, loss = 0.00402786\n",
      "Iteration 18, loss = 0.00311980\n",
      "Iteration 19, loss = 0.00329730\n",
      "Iteration 20, loss = 0.00407373\n",
      "Iteration 21, loss = 0.00455489\n",
      "Iteration 22, loss = 0.00441683\n",
      "Iteration 23, loss = 0.00382123\n",
      "Iteration 24, loss = 0.00318802\n",
      "Iteration 25, loss = 0.00288322\n",
      "Iteration 26, loss = 0.00305994\n",
      "Iteration 27, loss = 0.00337055\n",
      "Iteration 28, loss = 0.00346111\n",
      "Iteration 29, loss = 0.00325750\n",
      "Iteration 30, loss = 0.00294985\n",
      "Iteration 31, loss = 0.00275010\n",
      "Iteration 32, loss = 0.00276385\n",
      "Iteration 33, loss = 0.00287103\n",
      "Iteration 34, loss = 0.00289614\n",
      "Iteration 35, loss = 0.00282093\n",
      "Iteration 36, loss = 0.00270275\n",
      "Iteration 37, loss = 0.00260262\n",
      "Iteration 38, loss = 0.00255129\n",
      "Iteration 39, loss = 0.00255933\n",
      "Iteration 40, loss = 0.00258649\n",
      "Iteration 41, loss = 0.00258965\n",
      "Iteration 42, loss = 0.00255221\n",
      "Iteration 43, loss = 0.00249540\n",
      "Iteration 44, loss = 0.00245307\n",
      "Iteration 45, loss = 0.00244004\n",
      "Iteration 46, loss = 0.00244566\n",
      "Iteration 47, loss = 0.00244202\n",
      "Iteration 48, loss = 0.00241467\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04815754\n",
      "Iteration 2, loss = 0.08429687\n",
      "Iteration 3, loss = 0.00513570\n",
      "Iteration 4, loss = 0.03511807\n",
      "Iteration 5, loss = 0.02101698\n",
      "Iteration 6, loss = 0.00471233\n",
      "Iteration 7, loss = 0.00870541\n",
      "Iteration 8, loss = 0.01437109\n",
      "Iteration 9, loss = 0.01195801\n",
      "Iteration 10, loss = 0.00705005\n",
      "Iteration 11, loss = 0.00379843\n",
      "Iteration 12, loss = 0.00386929\n",
      "Iteration 13, loss = 0.00579219\n",
      "Iteration 14, loss = 0.00722915\n",
      "Iteration 15, loss = 0.00710450\n",
      "Iteration 16, loss = 0.00573079\n",
      "Iteration 17, loss = 0.00412307\n",
      "Iteration 18, loss = 0.00319622\n",
      "Iteration 19, loss = 0.00327252\n",
      "Iteration 20, loss = 0.00400733\n",
      "Iteration 21, loss = 0.00455507\n",
      "Iteration 22, loss = 0.00447400\n",
      "Iteration 23, loss = 0.00386316\n",
      "Iteration 24, loss = 0.00319149\n",
      "Iteration 25, loss = 0.00290940\n",
      "Iteration 26, loss = 0.00317188\n",
      "Iteration 27, loss = 0.00351167\n",
      "Iteration 28, loss = 0.00353757\n",
      "Iteration 29, loss = 0.00325913\n",
      "Iteration 30, loss = 0.00293046\n",
      "Iteration 31, loss = 0.00278500\n",
      "Iteration 32, loss = 0.00288394\n",
      "Iteration 33, loss = 0.00300168\n",
      "Iteration 34, loss = 0.00300203\n",
      "Iteration 35, loss = 0.00290469\n",
      "Iteration 36, loss = 0.00278810\n",
      "Iteration 37, loss = 0.00270399\n",
      "Iteration 38, loss = 0.00267502\n",
      "Iteration 39, loss = 0.00269722\n",
      "Iteration 40, loss = 0.00272982\n",
      "Iteration 41, loss = 0.00273428\n",
      "Iteration 42, loss = 0.00269650\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04893295\n",
      "Iteration 2, loss = 0.08465436\n",
      "Iteration 3, loss = 0.00502514\n",
      "Iteration 4, loss = 0.03499724\n",
      "Iteration 5, loss = 0.02142955\n",
      "Iteration 6, loss = 0.00460200\n",
      "Iteration 7, loss = 0.00890753\n",
      "Iteration 8, loss = 0.01500725\n",
      "Iteration 9, loss = 0.01228586\n",
      "Iteration 10, loss = 0.00727620\n",
      "Iteration 11, loss = 0.00388739\n",
      "Iteration 12, loss = 0.00392885\n",
      "Iteration 13, loss = 0.00584500\n",
      "Iteration 14, loss = 0.00734686\n",
      "Iteration 15, loss = 0.00724476\n",
      "Iteration 16, loss = 0.00584473\n",
      "Iteration 17, loss = 0.00417674\n",
      "Iteration 18, loss = 0.00318831\n",
      "Iteration 19, loss = 0.00327387\n",
      "Iteration 20, loss = 0.00405575\n",
      "Iteration 21, loss = 0.00461563\n",
      "Iteration 22, loss = 0.00456326\n",
      "Iteration 23, loss = 0.00402645\n",
      "Iteration 24, loss = 0.00336233\n",
      "Iteration 25, loss = 0.00296173\n",
      "Iteration 26, loss = 0.00306361\n",
      "Iteration 27, loss = 0.00342325\n",
      "Iteration 28, loss = 0.00358490\n",
      "Iteration 29, loss = 0.00342449\n",
      "Iteration 30, loss = 0.00310228\n",
      "Iteration 31, loss = 0.00284925\n",
      "Iteration 32, loss = 0.00284105\n",
      "Iteration 33, loss = 0.00297235\n",
      "Iteration 34, loss = 0.00301039\n",
      "Iteration 35, loss = 0.00293135\n",
      "Iteration 36, loss = 0.00280683\n",
      "Iteration 37, loss = 0.00270980\n",
      "Iteration 38, loss = 0.00266752\n",
      "Iteration 39, loss = 0.00267356\n",
      "Iteration 40, loss = 0.00270015\n",
      "Iteration 41, loss = 0.00270914\n",
      "Iteration 42, loss = 0.00267841\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04902871\n",
      "Iteration 2, loss = 0.08465136\n",
      "Iteration 3, loss = 0.00488691\n",
      "Iteration 4, loss = 0.03458555\n",
      "Iteration 5, loss = 0.02115502\n",
      "Iteration 6, loss = 0.00445566\n",
      "Iteration 7, loss = 0.00883031\n",
      "Iteration 8, loss = 0.01495360\n",
      "Iteration 9, loss = 0.01206566\n",
      "Iteration 10, loss = 0.00707177\n",
      "Iteration 11, loss = 0.00371392\n",
      "Iteration 12, loss = 0.00379014\n",
      "Iteration 13, loss = 0.00574692\n",
      "Iteration 14, loss = 0.00722813\n",
      "Iteration 15, loss = 0.00709635\n",
      "Iteration 16, loss = 0.00566233\n",
      "Iteration 17, loss = 0.00398849\n",
      "Iteration 18, loss = 0.00303990\n",
      "Iteration 19, loss = 0.00317372\n",
      "Iteration 20, loss = 0.00396720\n",
      "Iteration 21, loss = 0.00451587\n",
      "Iteration 22, loss = 0.00443008\n",
      "Iteration 23, loss = 0.00384817\n",
      "Iteration 24, loss = 0.00317909\n",
      "Iteration 25, loss = 0.00282696\n",
      "Iteration 26, loss = 0.00297535\n",
      "Iteration 27, loss = 0.00333020\n",
      "Iteration 28, loss = 0.00346260\n",
      "Iteration 29, loss = 0.00328718\n",
      "Iteration 30, loss = 0.00296258\n",
      "Iteration 31, loss = 0.00273596\n",
      "Iteration 32, loss = 0.00274165\n",
      "Iteration 33, loss = 0.00286056\n",
      "Iteration 34, loss = 0.00290088\n",
      "Iteration 35, loss = 0.00283536\n",
      "Iteration 36, loss = 0.00272380\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04818327\n",
      "Iteration 2, loss = 0.08528316\n",
      "Iteration 3, loss = 0.00472005\n",
      "Iteration 4, loss = 0.03406190\n",
      "Iteration 5, loss = 0.02266178\n",
      "Iteration 6, loss = 0.00514589\n",
      "Iteration 7, loss = 0.00826278\n",
      "Iteration 8, loss = 0.01461259\n",
      "Iteration 9, loss = 0.01224644\n",
      "Iteration 10, loss = 0.00759185\n",
      "Iteration 11, loss = 0.00418923\n",
      "Iteration 12, loss = 0.00385623\n",
      "Iteration 13, loss = 0.00542846\n",
      "Iteration 14, loss = 0.00687456\n",
      "Iteration 15, loss = 0.00696259\n",
      "Iteration 16, loss = 0.00577146\n",
      "Iteration 17, loss = 0.00418525\n",
      "Iteration 18, loss = 0.00317355\n",
      "Iteration 19, loss = 0.00319883\n",
      "Iteration 20, loss = 0.00391593\n",
      "Iteration 21, loss = 0.00443130\n",
      "Iteration 22, loss = 0.00435986\n",
      "Iteration 23, loss = 0.00381436\n",
      "Iteration 24, loss = 0.00318320\n",
      "Iteration 25, loss = 0.00288256\n",
      "Iteration 26, loss = 0.00303766\n",
      "Iteration 27, loss = 0.00334323\n",
      "Iteration 28, loss = 0.00341510\n",
      "Iteration 29, loss = 0.00321171\n",
      "Iteration 30, loss = 0.00292613\n",
      "Iteration 31, loss = 0.00276231\n",
      "Iteration 32, loss = 0.00280490\n",
      "Iteration 33, loss = 0.00289265\n",
      "Iteration 34, loss = 0.00289297\n",
      "Iteration 35, loss = 0.00280578\n",
      "Iteration 36, loss = 0.00269722\n",
      "Iteration 37, loss = 0.00261580\n",
      "Iteration 38, loss = 0.00258180\n",
      "Iteration 39, loss = 0.00259476\n",
      "Iteration 40, loss = 0.00261710\n",
      "Iteration 41, loss = 0.00261339\n",
      "Iteration 42, loss = 0.00257629\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04828618\n",
      "Iteration 2, loss = 0.08482120\n",
      "Iteration 3, loss = 0.00499596\n",
      "Iteration 4, loss = 0.03443007\n",
      "Iteration 5, loss = 0.02120393\n",
      "Iteration 6, loss = 0.00479165\n",
      "Iteration 7, loss = 0.00858753\n",
      "Iteration 8, loss = 0.01442051\n",
      "Iteration 9, loss = 0.01206520\n",
      "Iteration 10, loss = 0.00724653\n",
      "Iteration 11, loss = 0.00392877\n",
      "Iteration 12, loss = 0.00386918\n",
      "Iteration 13, loss = 0.00572496\n",
      "Iteration 14, loss = 0.00718686\n",
      "Iteration 15, loss = 0.00711703\n",
      "Iteration 16, loss = 0.00577391\n",
      "Iteration 17, loss = 0.00415831\n",
      "Iteration 18, loss = 0.00319147\n",
      "Iteration 19, loss = 0.00328577\n",
      "Iteration 20, loss = 0.00403699\n",
      "Iteration 21, loss = 0.00456006\n",
      "Iteration 22, loss = 0.00449029\n",
      "Iteration 23, loss = 0.00393547\n",
      "Iteration 24, loss = 0.00327978\n",
      "Iteration 25, loss = 0.00291164\n",
      "Iteration 26, loss = 0.00306325\n",
      "Iteration 27, loss = 0.00342131\n",
      "Iteration 28, loss = 0.00353834\n",
      "Iteration 29, loss = 0.00332863\n",
      "Iteration 30, loss = 0.00300416\n",
      "Iteration 31, loss = 0.00280475\n",
      "Iteration 32, loss = 0.00284169\n",
      "Iteration 33, loss = 0.00295716\n",
      "Iteration 34, loss = 0.00298445\n",
      "Iteration 35, loss = 0.00291279\n",
      "Iteration 36, loss = 0.00280440\n",
      "Iteration 37, loss = 0.00271629\n",
      "Iteration 38, loss = 0.00266934\n",
      "Iteration 39, loss = 0.00267269\n",
      "Iteration 40, loss = 0.00270137\n",
      "Iteration 41, loss = 0.00271765\n",
      "Iteration 42, loss = 0.00269892\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04862398\n",
      "Iteration 2, loss = 0.08468382\n",
      "Iteration 3, loss = 0.00493212\n",
      "Iteration 4, loss = 0.03470599\n",
      "Iteration 5, loss = 0.02079851\n",
      "Iteration 6, loss = 0.00454371\n",
      "Iteration 7, loss = 0.00880133\n",
      "Iteration 8, loss = 0.01452038\n",
      "Iteration 9, loss = 0.01208970\n",
      "Iteration 10, loss = 0.00732719\n",
      "Iteration 11, loss = 0.00402026\n",
      "Iteration 12, loss = 0.00402658\n",
      "Iteration 13, loss = 0.00586451\n",
      "Iteration 14, loss = 0.00729169\n",
      "Iteration 15, loss = 0.00712225\n",
      "Iteration 16, loss = 0.00567092\n",
      "Iteration 17, loss = 0.00403121\n",
      "Iteration 18, loss = 0.00315772\n",
      "Iteration 19, loss = 0.00337243\n",
      "Iteration 20, loss = 0.00420431\n",
      "Iteration 21, loss = 0.00471697\n",
      "Iteration 22, loss = 0.00456469\n",
      "Iteration 23, loss = 0.00391123\n",
      "Iteration 24, loss = 0.00322615\n",
      "Iteration 25, loss = 0.00297110\n",
      "Iteration 26, loss = 0.00330346\n",
      "Iteration 27, loss = 0.00359506\n",
      "Iteration 28, loss = 0.00353269\n",
      "Iteration 29, loss = 0.00323400\n",
      "Iteration 30, loss = 0.00295506\n",
      "Iteration 31, loss = 0.00288773\n",
      "Iteration 32, loss = 0.00300118\n",
      "Iteration 33, loss = 0.00304007\n",
      "Iteration 34, loss = 0.00297628\n",
      "Iteration 35, loss = 0.00288252\n",
      "Iteration 36, loss = 0.00280870\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04822077\n",
      "Iteration 2, loss = 0.08508359\n",
      "Iteration 3, loss = 0.00499404\n",
      "Iteration 4, loss = 0.03464139\n",
      "Iteration 5, loss = 0.02131626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.00482942\n",
      "Iteration 7, loss = 0.00851126\n",
      "Iteration 8, loss = 0.01441189\n",
      "Iteration 9, loss = 0.01222242\n",
      "Iteration 10, loss = 0.00738705\n",
      "Iteration 11, loss = 0.00398661\n",
      "Iteration 12, loss = 0.00385156\n",
      "Iteration 13, loss = 0.00569985\n",
      "Iteration 14, loss = 0.00720343\n",
      "Iteration 15, loss = 0.00717567\n",
      "Iteration 16, loss = 0.00585146\n",
      "Iteration 17, loss = 0.00422115\n",
      "Iteration 18, loss = 0.00322014\n",
      "Iteration 19, loss = 0.00327593\n",
      "Iteration 20, loss = 0.00402001\n",
      "Iteration 21, loss = 0.00455236\n",
      "Iteration 22, loss = 0.00450047\n",
      "Iteration 23, loss = 0.00396390\n",
      "Iteration 24, loss = 0.00331520\n",
      "Iteration 25, loss = 0.00293341\n",
      "Iteration 26, loss = 0.00304005\n",
      "Iteration 27, loss = 0.00340148\n",
      "Iteration 28, loss = 0.00353337\n",
      "Iteration 29, loss = 0.00333842\n",
      "Iteration 30, loss = 0.00300995\n",
      "Iteration 31, loss = 0.00279598\n",
      "Iteration 32, loss = 0.00281662\n",
      "Iteration 33, loss = 0.00292390\n",
      "Iteration 34, loss = 0.00292438\n",
      "Iteration 35, loss = 0.00281789\n",
      "Iteration 36, loss = 0.00269387\n",
      "Iteration 37, loss = 0.00265219\n",
      "Iteration 38, loss = 0.00269432\n",
      "Iteration 39, loss = 0.00268339\n",
      "Iteration 40, loss = 0.00260652\n",
      "Iteration 41, loss = 0.00254020\n",
      "Iteration 42, loss = 0.00252690\n",
      "Iteration 43, loss = 0.00253444\n",
      "Iteration 44, loss = 0.00252492\n",
      "Iteration 45, loss = 0.00248834\n",
      "Iteration 46, loss = 0.00243977\n",
      "Iteration 47, loss = 0.00240499\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04852310\n",
      "Iteration 2, loss = 0.08449879\n",
      "Iteration 3, loss = 0.00480044\n",
      "Iteration 4, loss = 0.03448874\n",
      "Iteration 5, loss = 0.02217535\n",
      "Iteration 6, loss = 0.00491144\n",
      "Iteration 7, loss = 0.00841662\n",
      "Iteration 8, loss = 0.01452744\n",
      "Iteration 9, loss = 0.01222591\n",
      "Iteration 10, loss = 0.00743283\n",
      "Iteration 11, loss = 0.00397867\n",
      "Iteration 12, loss = 0.00377849\n",
      "Iteration 13, loss = 0.00550683\n",
      "Iteration 14, loss = 0.00699229\n",
      "Iteration 15, loss = 0.00701572\n",
      "Iteration 16, loss = 0.00573166\n",
      "Iteration 17, loss = 0.00410997\n",
      "Iteration 18, loss = 0.00312413\n",
      "Iteration 19, loss = 0.00321394\n",
      "Iteration 20, loss = 0.00397397\n",
      "Iteration 21, loss = 0.00451443\n",
      "Iteration 22, loss = 0.00443435\n",
      "Iteration 23, loss = 0.00385912\n",
      "Iteration 24, loss = 0.00320963\n",
      "Iteration 25, loss = 0.00286748\n",
      "Iteration 26, loss = 0.00302211\n",
      "Iteration 27, loss = 0.00335207\n",
      "Iteration 28, loss = 0.00345122\n",
      "Iteration 29, loss = 0.00325116\n",
      "Iteration 30, loss = 0.00291459\n",
      "Iteration 31, loss = 0.00270819\n",
      "Iteration 32, loss = 0.00274240\n",
      "Iteration 33, loss = 0.00285133\n",
      "Iteration 34, loss = 0.00287012\n",
      "Iteration 35, loss = 0.00278671\n",
      "Iteration 36, loss = 0.00266744\n",
      "Iteration 37, loss = 0.00257782\n",
      "Iteration 38, loss = 0.00254348\n",
      "Iteration 39, loss = 0.00256194\n",
      "Iteration 40, loss = 0.00259387\n",
      "Iteration 41, loss = 0.00259512\n",
      "Iteration 42, loss = 0.00255086\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04901843\n",
      "Iteration 2, loss = 0.08468159\n",
      "Iteration 3, loss = 0.00494772\n",
      "Iteration 4, loss = 0.03474011\n",
      "Iteration 5, loss = 0.02122875\n",
      "Iteration 6, loss = 0.00452338\n",
      "Iteration 7, loss = 0.00877776\n",
      "Iteration 8, loss = 0.01482238\n",
      "Iteration 9, loss = 0.01218552\n",
      "Iteration 10, loss = 0.00718605\n",
      "Iteration 11, loss = 0.00379630\n",
      "Iteration 12, loss = 0.00383850\n",
      "Iteration 13, loss = 0.00577776\n",
      "Iteration 14, loss = 0.00727304\n",
      "Iteration 15, loss = 0.00717076\n",
      "Iteration 16, loss = 0.00575575\n",
      "Iteration 17, loss = 0.00407933\n",
      "Iteration 18, loss = 0.00310284\n",
      "Iteration 19, loss = 0.00320443\n",
      "Iteration 20, loss = 0.00396913\n",
      "Iteration 21, loss = 0.00451133\n",
      "Iteration 22, loss = 0.00443822\n",
      "Iteration 23, loss = 0.00386578\n",
      "Iteration 24, loss = 0.00318001\n",
      "Iteration 25, loss = 0.00281979\n",
      "Iteration 26, loss = 0.00299440\n",
      "Iteration 27, loss = 0.00336461\n",
      "Iteration 28, loss = 0.00348577\n",
      "Iteration 29, loss = 0.00326544\n",
      "Iteration 30, loss = 0.00292803\n",
      "Iteration 31, loss = 0.00272541\n",
      "Iteration 32, loss = 0.00276778\n",
      "Iteration 33, loss = 0.00289148\n",
      "Iteration 34, loss = 0.00291827\n",
      "Iteration 35, loss = 0.00284274\n",
      "Iteration 36, loss = 0.00272743\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04900355\n",
      "Iteration 2, loss = 0.08458034\n",
      "Iteration 3, loss = 0.00497814\n",
      "Iteration 4, loss = 0.03468611\n",
      "Iteration 5, loss = 0.02115001\n",
      "Iteration 6, loss = 0.00454552\n",
      "Iteration 7, loss = 0.00900708\n",
      "Iteration 8, loss = 0.01503100\n",
      "Iteration 9, loss = 0.01217139\n",
      "Iteration 10, loss = 0.00720336\n",
      "Iteration 11, loss = 0.00385952\n",
      "Iteration 12, loss = 0.00393560\n",
      "Iteration 13, loss = 0.00586859\n",
      "Iteration 14, loss = 0.00735743\n",
      "Iteration 15, loss = 0.00722368\n",
      "Iteration 16, loss = 0.00578867\n",
      "Iteration 17, loss = 0.00411719\n",
      "Iteration 18, loss = 0.00316456\n",
      "Iteration 19, loss = 0.00329157\n",
      "Iteration 20, loss = 0.00408557\n",
      "Iteration 21, loss = 0.00464208\n",
      "Iteration 22, loss = 0.00456260\n",
      "Iteration 23, loss = 0.00398587\n",
      "Iteration 24, loss = 0.00330879\n",
      "Iteration 25, loss = 0.00294478\n",
      "Iteration 26, loss = 0.00308868\n",
      "Iteration 27, loss = 0.00345142\n",
      "Iteration 28, loss = 0.00359473\n",
      "Iteration 29, loss = 0.00339838\n",
      "Iteration 30, loss = 0.00306598\n",
      "Iteration 31, loss = 0.00283546\n",
      "Iteration 32, loss = 0.00284461\n",
      "Iteration 33, loss = 0.00296686\n",
      "Iteration 34, loss = 0.00300385\n",
      "Iteration 35, loss = 0.00292933\n",
      "Iteration 36, loss = 0.00281094\n",
      "Iteration 37, loss = 0.00271739\n",
      "Iteration 38, loss = 0.00267093\n",
      "Iteration 39, loss = 0.00267340\n",
      "Iteration 40, loss = 0.00270106\n",
      "Iteration 41, loss = 0.00271096\n",
      "Iteration 42, loss = 0.00268077\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04795553\n",
      "Iteration 2, loss = 0.08567178\n",
      "Iteration 3, loss = 0.00467072\n",
      "Iteration 4, loss = 0.03354891\n",
      "Iteration 5, loss = 0.02060227\n",
      "Iteration 6, loss = 0.00466828\n",
      "Iteration 7, loss = 0.00775299\n",
      "Iteration 8, loss = 0.01325852\n",
      "Iteration 9, loss = 0.01179961\n",
      "Iteration 10, loss = 0.00725570\n",
      "Iteration 11, loss = 0.00393366\n",
      "Iteration 12, loss = 0.00378915\n",
      "Iteration 13, loss = 0.00555066\n",
      "Iteration 14, loss = 0.00695054\n",
      "Iteration 15, loss = 0.00682883\n",
      "Iteration 16, loss = 0.00543746\n",
      "Iteration 17, loss = 0.00383037\n",
      "Iteration 18, loss = 0.00299639\n",
      "Iteration 19, loss = 0.00328568\n",
      "Iteration 20, loss = 0.00408562\n",
      "Iteration 21, loss = 0.00449895\n",
      "Iteration 22, loss = 0.00426297\n",
      "Iteration 23, loss = 0.00361777\n",
      "Iteration 24, loss = 0.00301357\n",
      "Iteration 25, loss = 0.00281325\n",
      "Iteration 26, loss = 0.00304985\n",
      "Iteration 27, loss = 0.00333148\n",
      "Iteration 28, loss = 0.00336525\n",
      "Iteration 29, loss = 0.00315108\n",
      "Iteration 30, loss = 0.00285321\n",
      "Iteration 31, loss = 0.00268619\n",
      "Iteration 32, loss = 0.00271698\n",
      "Iteration 33, loss = 0.00280252\n",
      "Iteration 34, loss = 0.00281753\n",
      "Iteration 35, loss = 0.00274269\n",
      "Iteration 36, loss = 0.00263091\n",
      "Iteration 37, loss = 0.00253359\n",
      "Iteration 38, loss = 0.00249312\n",
      "Iteration 39, loss = 0.00250444\n",
      "Iteration 40, loss = 0.00253446\n",
      "Iteration 41, loss = 0.00254283\n",
      "Iteration 42, loss = 0.00251028\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04820342\n",
      "Iteration 2, loss = 0.08461816\n",
      "Iteration 3, loss = 0.00487790\n",
      "Iteration 4, loss = 0.03417294\n",
      "Iteration 5, loss = 0.02098924\n",
      "Iteration 6, loss = 0.00470185\n",
      "Iteration 7, loss = 0.00858363\n",
      "Iteration 8, loss = 0.01431763\n",
      "Iteration 9, loss = 0.01201415\n",
      "Iteration 10, loss = 0.00732898\n",
      "Iteration 11, loss = 0.00400116\n",
      "Iteration 12, loss = 0.00393512\n",
      "Iteration 13, loss = 0.00574947\n",
      "Iteration 14, loss = 0.00719383\n",
      "Iteration 15, loss = 0.00709172\n",
      "Iteration 16, loss = 0.00570314\n",
      "Iteration 17, loss = 0.00407665\n",
      "Iteration 18, loss = 0.00317015\n",
      "Iteration 19, loss = 0.00334453\n",
      "Iteration 20, loss = 0.00411739\n",
      "Iteration 21, loss = 0.00460649\n",
      "Iteration 22, loss = 0.00448338\n",
      "Iteration 23, loss = 0.00390380\n",
      "Iteration 24, loss = 0.00327653\n",
      "Iteration 25, loss = 0.00296533\n",
      "Iteration 26, loss = 0.00316056\n",
      "Iteration 27, loss = 0.00346889\n",
      "Iteration 28, loss = 0.00353369\n",
      "Iteration 29, loss = 0.00333941\n",
      "Iteration 30, loss = 0.00303241\n",
      "Iteration 31, loss = 0.00283429\n",
      "Iteration 32, loss = 0.00285249\n",
      "Iteration 33, loss = 0.00294480\n",
      "Iteration 34, loss = 0.00296604\n",
      "Iteration 35, loss = 0.00289126\n",
      "Iteration 36, loss = 0.00276933\n",
      "Iteration 37, loss = 0.00266346\n",
      "Iteration 38, loss = 0.00260043\n",
      "Iteration 39, loss = 0.00259916\n",
      "Iteration 40, loss = 0.00262058\n",
      "Iteration 41, loss = 0.00262266\n",
      "Iteration 42, loss = 0.00259157\n",
      "Iteration 43, loss = 0.00254308\n",
      "Iteration 44, loss = 0.00250097\n",
      "Iteration 45, loss = 0.00248204\n",
      "Iteration 46, loss = 0.00248129\n",
      "Iteration 47, loss = 0.00248176\n",
      "Iteration 48, loss = 0.00246713\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04884801\n",
      "Iteration 2, loss = 0.08504489\n",
      "Iteration 3, loss = 0.00491410\n",
      "Iteration 4, loss = 0.03499415\n",
      "Iteration 5, loss = 0.02099836\n",
      "Iteration 6, loss = 0.00454263\n",
      "Iteration 7, loss = 0.00864415\n",
      "Iteration 8, loss = 0.01448732\n",
      "Iteration 9, loss = 0.01213339\n",
      "Iteration 10, loss = 0.00736455\n",
      "Iteration 11, loss = 0.00397773\n",
      "Iteration 12, loss = 0.00390156\n",
      "Iteration 13, loss = 0.00572096\n",
      "Iteration 14, loss = 0.00718946\n",
      "Iteration 15, loss = 0.00707873\n",
      "Iteration 16, loss = 0.00565201\n",
      "Iteration 17, loss = 0.00399834\n",
      "Iteration 18, loss = 0.00308431\n",
      "Iteration 19, loss = 0.00324754\n",
      "Iteration 20, loss = 0.00406012\n",
      "Iteration 21, loss = 0.00462283\n",
      "Iteration 22, loss = 0.00453091\n",
      "Iteration 23, loss = 0.00389337\n",
      "Iteration 24, loss = 0.00318914\n",
      "Iteration 25, loss = 0.00289831\n",
      "Iteration 26, loss = 0.00323093\n",
      "Iteration 27, loss = 0.00352575\n",
      "Iteration 28, loss = 0.00346429\n",
      "Iteration 29, loss = 0.00316793\n",
      "Iteration 30, loss = 0.00289151\n",
      "Iteration 31, loss = 0.00280260\n",
      "Iteration 32, loss = 0.00289727\n",
      "Iteration 33, loss = 0.00296838\n",
      "Iteration 34, loss = 0.00294011\n",
      "Iteration 35, loss = 0.00284787\n",
      "Iteration 36, loss = 0.00275453\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04499204\n",
      "Iteration 2, loss = 0.09006915\n",
      "Iteration 3, loss = 0.00312882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.03435660\n",
      "Iteration 5, loss = 0.02104810\n",
      "Iteration 6, loss = 0.00418175\n",
      "Iteration 7, loss = 0.00526570\n",
      "Iteration 8, loss = 0.01117712\n",
      "Iteration 9, loss = 0.01144520\n",
      "Iteration 10, loss = 0.00746266\n",
      "Iteration 11, loss = 0.00352731\n",
      "Iteration 12, loss = 0.00244598\n",
      "Iteration 13, loss = 0.00371854\n",
      "Iteration 14, loss = 0.00536419\n",
      "Iteration 15, loss = 0.00590297\n",
      "Iteration 16, loss = 0.00505426\n",
      "Iteration 17, loss = 0.00349300\n",
      "Iteration 18, loss = 0.00220697\n",
      "Iteration 19, loss = 0.00186665\n",
      "Iteration 20, loss = 0.00249585\n",
      "Iteration 21, loss = 0.00312872\n",
      "Iteration 22, loss = 0.00323507\n",
      "Iteration 23, loss = 0.00283117\n",
      "Iteration 24, loss = 0.00219771\n",
      "Iteration 25, loss = 0.00177272\n",
      "Iteration 26, loss = 0.00187941\n",
      "Iteration 27, loss = 0.00216203\n",
      "Iteration 28, loss = 0.00218312\n",
      "Iteration 29, loss = 0.00198717\n",
      "Iteration 30, loss = 0.00176674\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04723676\n",
      "Iteration 2, loss = 0.08939580\n",
      "Iteration 3, loss = 0.00467566\n",
      "Iteration 4, loss = 0.03574148\n",
      "Iteration 5, loss = 0.02196221\n",
      "Iteration 6, loss = 0.00506491\n",
      "Iteration 7, loss = 0.00728291\n",
      "Iteration 8, loss = 0.01318795\n",
      "Iteration 9, loss = 0.01258162\n",
      "Iteration 10, loss = 0.00818808\n",
      "Iteration 11, loss = 0.00438228\n",
      "Iteration 12, loss = 0.00365604\n",
      "Iteration 13, loss = 0.00519371\n",
      "Iteration 14, loss = 0.00681960\n",
      "Iteration 15, loss = 0.00711604\n",
      "Iteration 16, loss = 0.00602852\n",
      "Iteration 17, loss = 0.00440337\n",
      "Iteration 18, loss = 0.00323919\n",
      "Iteration 19, loss = 0.00308220\n",
      "Iteration 20, loss = 0.00374063\n",
      "Iteration 21, loss = 0.00435581\n",
      "Iteration 22, loss = 0.00443131\n",
      "Iteration 23, loss = 0.00396021\n",
      "Iteration 24, loss = 0.00329070\n",
      "Iteration 25, loss = 0.00283300\n",
      "Iteration 26, loss = 0.00285298\n",
      "Iteration 27, loss = 0.00318418\n",
      "Iteration 28, loss = 0.00335820\n",
      "Iteration 29, loss = 0.00320937\n",
      "Iteration 30, loss = 0.00289231\n",
      "Iteration 31, loss = 0.00265345\n",
      "Iteration 32, loss = 0.00265170\n",
      "Iteration 33, loss = 0.00275770\n",
      "Iteration 34, loss = 0.00276757\n",
      "Iteration 35, loss = 0.00265220\n",
      "Iteration 36, loss = 0.00251129\n",
      "Iteration 37, loss = 0.00246572\n",
      "Iteration 38, loss = 0.00252286\n",
      "Iteration 39, loss = 0.00251751\n",
      "Iteration 40, loss = 0.00243608\n",
      "Iteration 41, loss = 0.00237061\n",
      "Iteration 42, loss = 0.00236516\n",
      "Iteration 43, loss = 0.00237835\n",
      "Iteration 44, loss = 0.00236513\n",
      "Iteration 45, loss = 0.00231772\n",
      "Iteration 46, loss = 0.00226904\n",
      "Iteration 47, loss = 0.00224862\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Config of the regressors and cross val cross val leave one out\n",
    "####\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes = (100, 50), alpha = 0.001,\n",
    "    learning_rate_init = 0.01, max_iter = 1000,\n",
    "    random_state = 9, tol = 0.0001, verbose = True)\n",
    "svr = SVR(kernel = 'linear', C = 0.25, epsilon = 0.01, verbose = True, max_iter = 1000)\n",
    "lr = LinearRegression()\n",
    "\n",
    "full_predict_lr = cross_val_predict(lr, X, target, cv = 10)\n",
    "full_predict_mlp = cross_val_predict(mlp, X, target, cv = loo)\n",
    "full_predict_svr = cross_val_predict(svr, X, target, cv = loo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error in LR: 0.006598024911100442\n",
      "Mean Squared Error in MLP: 0.005762317354408195\n",
      "Mean Squared Error in SVR: 0.005986897033680813\n",
      "R² score in LR: 0.8472614539448069\n",
      "R² score in MLP: 0.8666073580382825\n",
      "R² score in SVR: 0.8614085335191553\n",
      "adjusted R² score in LR: 0.8247526155787784\n",
      "adjusted R² score in MLP: 0.8469494950123452\n",
      "adjusted R² score in SVR: 0.8409845279325044\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Printing some metrics of the regressors\n",
    "####\n",
    "\n",
    "print('Mean Squared Error in LR: %s' %(metrics.mean_squared_error(target, full_predict_lr)))\n",
    "print('Mean Squared Error in MLP: %s' %(metrics.mean_squared_error(target, full_predict_mlp)))\n",
    "print('Mean Squared Error in SVR: %s' %(metrics.mean_squared_error(target, full_predict_svr)))\n",
    "\n",
    "r_squared_lr = metrics.r2_score(target, full_predict_lr)\n",
    "r_squared_mlp = metrics.r2_score(target, full_predict_mlp)\n",
    "r_squared_svr = metrics.r2_score(target, full_predict_svr)\n",
    "\n",
    "print('R² score in LR: %s' %(r_squared_lr))\n",
    "print('R² score in MLP: %s' %(r_squared_mlp))\n",
    "print('R² score in SVR: %s' %(r_squared_svr))\n",
    "\n",
    "adjusted_r_squared_lr = 1 - (1 - r_squared_lr) * (len(target) - 1) / (len(target) - X.shape[1] - 1)\n",
    "adjusted_r_squared_mlp = 1 - (1 - r_squared_mlp) * (len(target) - 1) / (len(target) - X.shape[1] - 1)\n",
    "adjusted_r_squared_svr = 1 - (1 - r_squared_svr) * (len(target) - 1) / (len(target) - X.shape[1] - 1)\n",
    "\n",
    "print('adjusted R² score in LR: %s' %(adjusted_r_squared_lr))\n",
    "print('adjusted R² score in MLP: %s' %(adjusted_r_squared_mlp))\n",
    "print('adjusted R² score in SVR: %s' %(adjusted_r_squared_svr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "## Filling lists with NaN so the len is the same across all lists \n",
    "## so that a graph can be generated\n",
    "####\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "values_to_add = list()\n",
    "for i in range(0, window_size):\n",
    "    values_to_add.append(float('NaN'))\n",
    "    \n",
    "full_predict_svr = np.insert(full_predict_svr, 0, values_to_add)\n",
    "full_predict_svr.shape = (len(full_predict_svr), 1)\n",
    "    \n",
    "full_predict_mlp = np.insert(full_predict_mlp, 0, values_to_add)\n",
    "full_predict_mlp.shape = (len(full_predict_mlp), 1)\n",
    "\n",
    "full_predict_lr = np.insert(full_predict_lr, 0, values_to_add)\n",
    "full_predict_lr.shape = (len(full_predict_lr), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Casos</th>\n",
       "      <th>Taxa</th>\n",
       "      <th>CasosNormalizados</th>\n",
       "      <th>TaxaNormalizadas</th>\n",
       "      <th>Predict_lr</th>\n",
       "      <th>Predict_mlp</th>\n",
       "      <th>Predict_svr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26/2/20</th>\n",
       "      <td>1</td>\n",
       "      <td>24.7</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>27.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>31.4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/3/20</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.461333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17/6/20</th>\n",
       "      <td>34918</td>\n",
       "      <td>37.3</td>\n",
       "      <td>0.637527</td>\n",
       "      <td>0.336000</td>\n",
       "      <td>0.647398</td>\n",
       "      <td>0.550599</td>\n",
       "      <td>0.545715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18/6/20</th>\n",
       "      <td>32188</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0.587683</td>\n",
       "      <td>0.368000</td>\n",
       "      <td>0.710381</td>\n",
       "      <td>0.569856</td>\n",
       "      <td>0.625587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19/6/20</th>\n",
       "      <td>22765</td>\n",
       "      <td>34.7</td>\n",
       "      <td>0.415640</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.605114</td>\n",
       "      <td>0.540465</td>\n",
       "      <td>0.562175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20/6/20</th>\n",
       "      <td>54771</td>\n",
       "      <td>39.1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>0.489959</td>\n",
       "      <td>0.466195</td>\n",
       "      <td>0.434855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21/6/20</th>\n",
       "      <td>34666</td>\n",
       "      <td>47.3</td>\n",
       "      <td>0.632926</td>\n",
       "      <td>0.602667</td>\n",
       "      <td>0.658307</td>\n",
       "      <td>0.582858</td>\n",
       "      <td>0.587575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Casos  Taxa  CasosNormalizados  TaxaNormalizadas  Predict_lr  \\\n",
       "Data                                                                    \n",
       "26/2/20      1  24.7           0.000018          0.000000         NaN   \n",
       "27/2/20      0  27.5           0.000000          0.074667         NaN   \n",
       "28/2/20      0  26.6           0.000000          0.050667         NaN   \n",
       "29/2/20      0  31.4           0.000000          0.178667         NaN   \n",
       "1/3/20       1    42           0.000018          0.461333         NaN   \n",
       "...        ...   ...                ...               ...         ...   \n",
       "17/6/20  34918  37.3           0.637527          0.336000    0.647398   \n",
       "18/6/20  32188  38.5           0.587683          0.368000    0.710381   \n",
       "19/6/20  22765  34.7           0.415640          0.266667    0.605114   \n",
       "20/6/20  54771  39.1           1.000000          0.384000    0.489959   \n",
       "21/6/20  34666  47.3           0.632926          0.602667    0.658307   \n",
       "\n",
       "         Predict_mlp  Predict_svr  \n",
       "Data                               \n",
       "26/2/20          NaN          NaN  \n",
       "27/2/20          NaN          NaN  \n",
       "28/2/20          NaN          NaN  \n",
       "29/2/20          NaN          NaN  \n",
       "1/3/20           NaN          NaN  \n",
       "...              ...          ...  \n",
       "17/6/20     0.550599     0.545715  \n",
       "18/6/20     0.569856     0.625587  \n",
       "19/6/20     0.540465     0.562175  \n",
       "20/6/20     0.466195     0.434855  \n",
       "21/6/20     0.582858     0.587575  \n",
       "\n",
       "[117 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####\n",
    "## Adding the data to plot \n",
    "####\n",
    "\n",
    "data['Predict_lr'] = full_predict_lr\n",
    "data['Predict_mlp'] = full_predict_mlp\n",
    "data['Predict_svr'] = full_predict_svr\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEGCAYAAAB1iW6ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOy9eZwcdZ3///xU38dMz5n7DgkBMiSEhEMIBCIih8GIgCgii8e6LssPEQT3yyoiP0VXQbxQdl34iiwLuhJhURQ5hHAsJCGBkIvcmUwmmXumz+qq+nz/qK7q7pmemT5mMkms5+MRmOmjurqm+13ven3e79dbSClxcHBwcDj6UcZ6BxwcHBwcRgYnoDs4ODgcIzgB3cHBweEYwQnoDg4ODscITkB3cHBwOEZwj9ULNzQ0yBkzZozVyzs4ODgclaxdu7ZdStlY6L4xC+gzZsxgzZo1Y/XyDg4ODkclQog9g93nSC4ODg4OxwhOQHdwcHA4RnACuoODg8MxghPQHRwcHI4RnIDu4ODgcIwwbJWLEOI/gEuBQ1LK+QXuF8D9wMVAHLhOSrlupHfUwcHB4Whj8d3P0R5VB9zeEPay5o4LRvz1isnQHwY+PMT9FwFzMv++ADxQ+W45ODg4HP0UCuZD3V4pwwZ0KeXLQOcQD7kM+JU0eQOoEUJMHKkddCjM/q1ddB6IjfVuODg4HEGMhIY+GdiX83tz5rYBCCG+IIRYI4RY09bWNgIv/bfLC49sZs0fdo/1bjg4OBxBjERAFwVuKzg1Q0r5oJRysZRycWNjwc5VhyJJxTVScW2sd8PBweEIYiQCejMwNef3KUDLCGzXYRCklKgJDTXhBHQHB4csIxHQnwKuFSZnAD1SygMjsF2HQUindKQENekEdAeHI5mGsLek2yulmLLFx4BlQIMQohn4BuABkFL+HPgDZsnidsyyxb8blT11sEkndQAnQ3dwOMKxShM/dN9f2deZYPO3hioYrJxhA7qU8uph7pfAP47YHjkMi5WZOwHdweHoQNUMEmkdKSVm687o4HSKHoWkMoFcTelIo+D6s4ODwxFEWje/p8m0Maqv4wT0o5B0wpRckGZQd3BwOLJJaWYgT6RH9/vqBPSjkNzFUEd2cXA48knrZkCPq6P7fXUC+lFIKuEEdAeHowk1k6EnnQzdoT9WlQtUHtBb3u/ihUc2Y65tOzg4jAaqnaE7Ad2hH7mSS6rCgL5zfTubXz2APsqLNQ4Of6vohkTPFC8knIDu0J/cIJ6brZdDvNd0fVMr3I6Dg0NhLP0cIO5ILg79SSc0rFLWSjP0eG8KcLpOHRxGCzUnoCedDN2hP2pSJ1TjM3+uNKD3mBl6pZm+g4NDYawFUXA0dIcCqAmNYLUXoYjKA7otuTgZuoPDaJAb0J06dIcBqEkNb8CN1++qSPvW04Ztweto6A4Oo0Ouhu4sijoMQE3q+AJuvH53RRl6vC87BivtZOgODqOCk6E7DIma0PAE3HgD7ooWRS39HJwM3cFhtMhdFHU0dIcBqAkNr9+FN+CqLEPPVLiAo6E7OIwWuRm60ynqkIc0JGpKxxtw4wu4KwrE1oIoOFUuDg6jRX6Vi+Pl4pBDWtVBgtdvSi6VZehmQHd7FSdDd3AYJSzrXIDEKHdkDzvgwuHIwgrgXr8rsyhafmYd71HxhzyZgO5k6A4Oo4GqZ79biVHO0J2AfpRhBXBvIJuhlzsFJdaTIhjxIg3pVLk4OIwSqmZm6CGvy6lyccjHkkbMgO7CMGTZxlrxXpVgtdc8MYxAhr5vS6dj8uXg0A+ryiUS8DhVLg75ZCUXc1EUyvdzsQK6x+eqOEPvbU/w1A/Xs33doYq24+BwrGEtilYHPE5jkUM+Vibt9bvw+M2AXs7CqJTSDOgR34hk6LFuswQykdOs5ODgkO0UjQQ8juTikE+u5GJl6OUsjKpJHT1tmJKLz1VxlUsimgYqd390cDjWsDL0iJOhO/THllwyi6JQXlNQvMfMqIPVXjwBd8V16FZm7ozEc3DIJy9DdwK6Qy5WwPT4XNmAXkYQtWrQgxErQ9crGkPnBHQHh8KkNEdycRgENanj8blQFIHX7wLKkznsgJ6pcpGGRKugQiXRl5Fc4k5Ad3DIJXdRVDNkXufoSOME9KMMNaHZmXlFGXrGmCtU7cPjM08Mlcgudobu1LM7OOSR1g08LkHQa37PRjNLLyqgCyE+LITYKoTYLoS4vcD904QQLwoh3hZCvCOEuHjkd9UBMl7omcy8UslFcQl8QXdF27GwFkUr6Vx1cDgWUTUDr0sh6DW/Z6Opow8b0IUQLuCnwEXAicDVQogT+z3sDuAJKeUpwCeAn430jjqYqEndDsCKIvD4yhtyEe9N2VOP7Aw9VXmGnoqny96Gg8OxSFo38LgVAl4z3I51hn4asF1KuVNKqQL/BVzW7zESqM78HAFaRm4XHXKxrHMtyjXospqKrG1Y2y6XeN+Rl6FLKZ0TjMOYo+pmhh7wmN+z0XRcLCagTwb25fzenLktlzuBa4QQzcAfgH8qtCEhxBeEEGuEEGva2trK2F2HXA0dzAajsgN6xGdvA0AtM0OXhiSZU4deSbXMSLJrQzsP3/YqyZgT1B3GjpRm4HEpBDIa+mh6ohcT0Au5PvX/xl4NPCylnAJcDDwihBiwbSnlg1LKxVLKxY2NjaXvrYMpufhzAnqZU4viPTkZegUdp2BWtkhD2kZfmnpk+Ll0H4yjpY28yUwODoebtC7xuRV7UXQ0/VyKCejNwNSc36cwUFL5LPAEgJTydcAPNIzEDjrkYy6KZgO6r4y2fcOQJPqyAd3jr0xDT0TNgFkzLmjuY5EnBiklf/7lezRv6cy7vftQnJ62RFn7kosltzjdqw5jiarpeN0KAU+mymWMA/pbwBwhxEwhhBdz0fOpfo/ZCywHEEKcgBnQHU1lhDEMSTqp4w0U1tC7WmOs+/Me1v15D+v/spdoV6rgdhJ9KlIyYhm6tSBaMy4AFF+Lnk7pvP/WQXZv7Mi7/YVfbeav/7mlrH3JJRkz98NpdnIYS9K6zJNcRnNRdFg/dCmlJoS4AfgT4AL+Q0r5nhDiLmCNlPIp4CvAvwkhvowpx1wnjxQh9RjCyqAH09Bf/PUWDmzvse/btLqFj9+2OO/xAG17+wComxQCzIlFQlSQoWcWRCNWhl5kLbq1gJrozZdEop2pvJNWuaRiVobuaOgOY4eqGYctQy9qwIWU8g+Yi525t3095+dNwFkju2sO/cm1zrWwMvTOlhgHtvdw+mWzWHD+VFp39vD0jzfw3EObuPiLTQgluxTSurMHoQjGzTALk4QQePzlj7PLZuhmQC9W4rBeL3e2qeUCaeiVz16xFkNVp3v1bwojmUTduxf/3LljvSuAGdBzG4vGWkN3OEKwfVz6lS1qaYONf21GcQlOOnsSHp+LqSfUcfYVx7H7nXbeemZX3nZad/bQMCWMx5uzHb+r7CoXq2SxutGUXIoNoFYmnxvQ1YSGrhkkRyAIW5LLSGjoiaiKoR8Zi71HKgd39/LOi81jvRt0/+537L784xix2FjvCpApW3S78HuOkE5RhyMDa/HTF8jP0AE2v3aA2ac0Eqjy2vc1LZvCvDMn8NYzu+lqNT/chm5wcFcvE2ZH8rbt8btJlxn4kn0qvqCbQNgDVJahWz/raQOtwkzGWhStVEPX0wa//pc32PTqgYq2c6yz+dUWXvvd9rHeDfSOTmQ6jdbVPda7AlidogKfW0ERY78o6nCEkOuFbmHJL1ra4KSl+e0BQgjOuGw2CNi+1pwk1LE/hqYaTJyVH9ArzdADVd6SG5SswJ+MptEz2W9uiWGl9ePW8ys1DEtE06gJjd72yitvjmWSMQ09bYz5GEIjaf6d9J4jI6CndVNDF0IQ8IzuXFEnoB9FFJJcrGy9ZnyQSXNrBjwnVONj4uyIHdBbd5qLpv0z9HIblACSUZVAlcdcXFVEyRk6QKLXDL652bolmZSDltbtevhKM3SrLNMpfxwa+wQ6xsdJJsyAbvT0DPPIw4PVKQoQ8LodDd3BxApMuZKLL2j+fNLSSQhRqAcMZi8aR2dLjK7WGAd29BCq8RGu9eU9xuN3l13lYmXoQgizLr7ogJ59PWthNTegpyrI0HOz8twAI6Vk21utpEv4UlldsM7i6tCMlMRVKUbcytCPkICe6RQFCHiVMe8UdThCyM4TzQb0CbMjnH3FHE46p78bQ5bZp4wDTNmldWcPE2ZVDwj+lWfoli+Mq2iJI7e8MZaZoBTvzdbOJyvwYcmVa3L3p6s1znO/3MTOt4tvk0ge4+P1nvzBOt59qfLFzCMlQzeSSeDICeiW5AIQ9LjH3MvF4QjBCoCWOyKAy62wYPnUvIqV/oRrTdll0+oW+jqSTOinn0P5GbphSBLRtL0gag6cLl1ysTLzeI9p6wvZQFoOqYxc4wu6C75OKfq8Lbkcgxm6oRu0bO/mUKY3oRLsRq4xPk5GIg6A3n1kBPRUTobu97pIjOIagxPQjyLSKR23z5VXU14ssxeNsztH++vnkM3QS+0HS0bTILEz9P4BdCjUhIY/cyLIlVxqxmfq2SsIDFbArm4I5GWM5YzKy3q9H3sBPRnTQI5MJZCWSQjGOkOXR6Dk4rMzdBcJJ0N3AHOikNdXXgfl7EWmGZrLrdA4tWrA/V6/Gykp2VjLyl4DVR57O8UG4lRCM0fg+V12dUusV6W6IYDLrVRU5WLpudUN/ryM0ZZPSjhZJO3xesdex6l1xVLp1UeuPDbWJ74jWXIJeJ0qF4cM6ZSeJ7eUQrjWz+Tja5k0J4LLPfDPbm231BFyVtt/WRl6UscXcBOo9uZILimCES++kLuigG5d/lc3BNDSBnpmjqM9iKOMDP1IsgYeKUZquHcqpyJprDN0W3IZg4AuDYOdH/sYXU88AYCmGxiSnEVRl1Pl4mCSTmp5JYulcvE/NPHhv28qeJ9VQ17qXFErIORp6CVILt6Am2AmoBu6QSKaJljtxR/y5AWJUknF0ghF2NU81j6VI58kY+Z7NDQ55jXWI411Iq00oOeefMc6Q89KLoe/Dj21fTupTZuJv/G/gGnMBWQzdI+LpBPQHaCyDB1MOSS3QiaXkcrQvRk7X8MYPpPNBnQf8V7VDLYSQpmAXlmGnsYfcuMLZrpXM5JCoq90ycV6Dox99jnSjNRw77yqorHO0DOSy1jUocfXrAFA3bvX/H/mytDK0INeF3FHcnEAU6Lw+Co3rSqE3eVZaoYeVUFgL2767Ex/+C+1NfA6GDEzdEtHD1b78AXdFWnWyZiGP+Sx9yfblVp6RpqMpu0ro2Ot0iU7C7YyOcn6WwlFHAFVLpkMfQyqXBJr1gKg7tljjkDUM6XGORm60/rvAJgZurcCyWUorO0WE4hzSfSl8Yc8KJnKG+vEUEzgUxPmwOtglRc1odHXYWZWwUgmQ6+kbDGexhf0DLAjsPXwIk8WUprj9SKW8dgxlqFbkouhVyYnJaPmcamq841phi4Nw+4U1Xt6Duuah5SS+Nq1oCgYfX3oXV1ZySVTihvwukhpBnoRV7Dl4AT0o4h0UqtIchkKe8hFGRq6pZ9DNkMf7hJezyxUegNughFTrmlvNmuhg9VefCFPRY6LWckl/wRT6qKomtAwDJm1Bj7mMvSRkUqS8TSKIgjX+sf0pCdTZmmuEokgVRWZkV8OB+n9+9EOHiS09GzAzNItySU3Q4fRmyvqBPSjiHRKr2hRdCg8ZWbose6UHZABvMHiDLqs4OH1u+3JSe3NUQAC1V78IXdFjoupmIYvlJ+hS0PmNb8Uk71ZGb2VoVc6LGPbW62sfXZ3RdsYSfpbF5dLKpbGlzmBjuVJz5JbPBMmAIe30sXSz2tWfgwwA3o6YzrndZnfr9H2RHcC+lGClLLiRdGhKCdD13WD9uYoDVOyde2+IiUXK4P3BVx5Ad3rd+HxuvCHzKy/3IXRZDyNP+jJZugJzSw7NCShGh9SFjehyZJ9IuNK83oHczKUZWlgsfWNVjb+dX/R2xhtEn2q3UNQUYaeWbMopcppMFLxNL//4dtluVtaPi7uCeOB4gO6Ho3S+eijSLX8geKJtWtRqqsJn7cMFCUvQ/dkJBe/k6E7gGmPKyWDVqlUijWGrpRqh879MfS0wbgZ2YBe7HxSe/pSIJuh93UkCUbMMkOrOqUcx0VdN0gndXwhNx6fy3xfCa2s2ad2hl7iNCaAZ366gbee2Z13m1XNcyTUs0spifepRBpLG+5dCFPiMhehK9XQ25ujNG/p4uCu3pKfK5NWhj4RKH5hNPrCCxz81t20/ehHJb+mRXzNWoKLFqH4fHgmTya9Zy+pfpJL0OMCKUctQx+d6FAm6XSa5uZmkodR9zpaMAzJkmtqcYV62bx586i8xuJravH4YkVvP53UWHJNLelAB5s3dwEgM/uZ9LWzefPgdcBa2mDJNbVExSESB9pYck0tAC6PwubNm9E8Bos/VUtbVwuR8TPxeDyDbqs/Vv26P+RBCIE3YMoAucF5/7buogKYVRUTrvHhcitFywlmsEzbC70WsR4VPW1kFrjH9uunJjQMTVIzPkDrzp4898tSScXThGv9tpePNGRZFhVQmRmaLblMtCSX4mrR0wcPAtDxy/8gtPQcQqefVtLrah0dqLt2UXO5Kbd4p0/vJ7lkAroa48n/+WdSs2+BL1xb0msUwxEV0Jubm6mqqmLGjBmDWsH+raKldTpbYlTV+wmEvcM/oQza90dxuQQ144NFHf/ejgSpuEbDlLD9eCklbXv7CNX4CEV8gz43FU/T05agdkIIj89F274+pCHxBT1EGgOkVZ3OliiGJ0lzczMzZ84s+n1YFSy+kPnx9gXdpBJpu4Xfkk+KCRjWoqE/7MEbLD77TCd1pCHz3CMNQ5LMXCUko+kxD+jWe7O8cyrN0Bsmh02JS4Ka0vNsnkvarxIrkXKxJZfxpWno2qE2RDCIp7GRlttuY9bvV+GKDPQ86s/iu5+jParygZZ3+RfgU2+m2LL9GW4+JLjwwF7UdH7ZYjDai19PE3WNznf4iJJckskk9fX1TjAvgHWFXm7WUwzBai/plE6spzgdUUsZeLyuvL+XEAIhBHKYsiyr8ch6P5bDov1/xdxOpKqm5Cs2S6bxB3O7V3Xbd6bGkhiKyLaT0TQuj4LH5zK93ovM0C1vk9xjmehT7b9jbnXJWGEtiJY63LsQyQKL0GVvqwIzNFtyyWToxTYXaYcO4Rk/nknf/1e09nYOfvvbefd3tcZ465ldA6Sy9sxn6sTO3aQUN9trpgCww1dnly6CGdAX3/0cd/96NQB3v9bKjNufYcbtz7D47udKfp+DcUQFdMAJ5oNgBUhlFI9PIOzBH/IQ70kNmx0ZhkRLm+6P/REKw3aKykzJs5L5BCqZS1IroFuBvhyp2RqM4Qtlm51S8bQdREvK0GOmNbAQIpPpF2k8llMmaQ2Xzq0osU4uY4m1plDdEABRfhC2nBbzGrkqKTktw0DNwpJcXLV1CI+HLXs87Fh3aNjnaW1tuMeNI9DURN2119Lz9P+gNmcXr7evPcSbT+8adJ/qkr10BCJoivn+W0L1AMhms2PU41Joj6rUpsxKri5fdt2pfQQ/C0dcQHcoTDZDH73XEEJQVefH7XXR2560Da3M15f0tCXsQG+VE3q8A3fIzNCHfi3ZP0Pvl6kLYf5nuEy/EFZljD8juViVF4moisefraopJmAk+9S8LtiinSStx0mIFxivdyRl6MGI13TJLHfASTx7vEciQ0/EyveXsSQXJRhAqYnwfkcDW14ffri3dugQ7nHmIJi6az8NQtD16KP2/dbfc7CigbAap88TtH9vCTdmfjAHh1iSS03K7LXo9g10PB0JjtqAvvju5+xLltx/I3H50trayic+8Qlmz57NiSeeyMUXX8y2bdtGYK8Lc+eddxIMBjl0KJtJhMPhvMfYAXCUMvTdu3czf/58hCLYtnsjX/vGrXlfKF0zSMXTRLtSSCntgO4uMFhDKGYgvu666/jtb39b8PWkIW15BnIlF/MjKYRAKSLTL4QV0K1KGavyItFnZtvZoDN8UM0b3lGCk2TuFY6lo+cOwD5iMnRhXpmVMjqwP/YwkVB+mWi5VLQompFclEAAVySCariGLcWVUpoBvdEMwp4JE6i+8EK6f/tb9Ggsb18GWziuSieIes0rP+Hu5WAoDC4Xrv2ZgJ75XNemouhCoS/z2JHmqA3og12mVHr5IqVk5cqVLFu2jB07drBp0ya+/e1vczCzCj5aNDQ08IMf/GDI/YLCAV1KiWGMnAvg6Wecxj13/Wtenbb1s64ZJKNp0ikDxaUUtOJVFDFsIDakzLva6K+hQ+bEoJce0FNxDSGyNfHeoKl9W6PyXG4Ft7e4ipVENI0/swhtSTfF7oNFrDt/vJ6iCHuBdiyJW7YNLqWi+vHsFZGn6LLVIbdXgeRitf0rgQBKpJY03mFLcY3eXqSq4h7XaN9W95lrMfr66Fm1ytyXzHscKkOPeoKATmjmD1HGvYxn0iTcraZsk5uhd/vCyFG61D5iA/o3n36Pq37x+qD/hmKw53zz6feGfd0XX3wRj8fDF7/4Rfu2hQsXcsopp7B8+XIWLVpEU1MTv//97wGIxWJccsklLFiwgPnz5/P4448D8Pzzz3PKKafQ1NTE9ddfTyrTknz77bdz4okncvLJJ3PLLbfYr3H99dfz+OOP09nZOWCf7r33Xk497RTO+dAZ/OjH9wNmRn3CCSfwpS99iUWLFrFv3z7C4TC33XYbp556Kh/84Ad58803WbZsGbNmzeKpp56yn7d06VIWLVrEokWLeO211wa83l//+leuuf5K0imdiy++mIULF3L6mYs5rmkqv1n1X2ze+D4XXrKc5RefnbcNKSU33HADZ5xzKldd87G8K47+xyOZSCGEsI/HGWcv5pvfuSPvBFHMiaEQyZjp42LJOb6MA2S8N5ttF1svnYyms5JLRkMvpoY81/rXWhiN92Ykn4j3yMjQe8ubBduf3IDe32qhVBbf/Rzv7zUXMjft6S75ytuSXITfj1HdAAzfLKdlPqeejOQCEFiwgMCCBXQ+8iukYdgnqP4nqobMyb4qnaDPG0DxtiPccfxV2/FOn47XCuhWhp40A/poUVRdkRDiw8D9gAv4dynlPQUecyVwJyCBDVLKT47gfh42Nm7cyKmnnjrgdr/fz5NPPkl1dTXt7e2cccYZrFixgmeffZZJkybxzDPPANDT00MymeS6667j+eefZ+7cuVx77bU88MADXHvttTz55JNs2bIFIQTd3dka2XA4zPXXX8/999/PN7/5Tfv2tWvX8tBDD/HSX14h1p3k0isuYNl5y6itrWXr1q089NBD/OxnPwPMk8uyZcv47ne/y8qVK7njjjt47rnn2LRpE5/5zGdYsWIF48aN47nnnsPv9/P+++9z9dVXsybTspyLcAl0zeB/nv4fFJfC88++wo03/wNXXnU5se40TzyyivoJEVoO7bW38eSTT7J161Zef+Ut9u9t4ewPnmYG7wLH45cPPciVl1894HgoOVU8iiLydPxisdrQLSyJpac9QeN0U7v0Bj3DVqzouvlFzvV6tzzRC0lNefsQN/3YpZTE7QHYKsFqLx6fyy7NG0sSfSrB6uwJLtqdGuYZhbHLRIPukuSsQrRHVQLSb25Pirzbi8FIJhA+H8LlQq+qg96h7SwW3/0cU3Zu5DvAJ3+3nY0vm9/jhrCXF677DPu/fDOxV14hGS9szrbmjguQUvLe779KzBtE8bcAoPhbUKYswb9mHUiJx63QEPYSUaMDAnrDCJYhDxvQhRAu4KfABUAz8JYQ4ikp5aacx8wBvgacJaXsEkKMK7y14vnGR04a8v4Ztz8z6H2P//2Zlb78AKSU/PM//zMvv/wyiqKwf/9+Dh48SFNTE7fccgu33XYbl156KUuXLmXDhg3MnDmTuXPnAvCZz3yGn/70p9xwww34/X4+97nPcckll3DppZfmvcaNN97IwoUL+cpXvmLftnr1alauXEkwEIS0m5UrV/LKK6+wYsUKpk+fzhlnnGE/1uv18uEPfxiApqYmfD4fHo+HpqYmdu/eDZjNWzfccAPr16/H5XINujZgZbfplE5PXztf+qfP88j//U8axtcR7z3AV26/ic3bNuJ2u+1tvPzyy1x99dV4PG7GjZvI+eefD8DWrVsHHI/7fnA/n7/+i0MeD1Fuhh7XbOsAwM4atZReUoZuXfrbzwlmW+SHD+gavqAbIXIy9B4zoLu9rkEXRbe8fgCPz8XsRRV/hYYl3qtmT3ABN+qBWFnbsZwW/SGPKWd5FFJlNim5JXgRGEh8ZVQ4yUQCJWAGXy1UC72D695gnigWJM2O1A5/JO/28PILQFGIr1+PGl8CFM72jVgMlzRomNiIy28uwOpS52C9i2AyTkSN4nUprLnjAra98F0Sp3yA9772wSH7NMqlGMnlNGC7lHKnlFIF/gu4rN9jPg/8VErZBSClHL5O6AjlpJNOYu3atQNuf/TRR2lra2Pt2rWsX7+e8ePHk0wmmTt3LmvXrqWpqYmvfe1r3HXXXYNekrvdbt58800uv/xyVq1aZQdfi5qaGj75yU/aGTdktXMpzQCXq6GHQqG853s8nuwio6Lg8/nsnzXN/NLdd999jB8/ng0bNrBmzRrUQbwrrPLIZFzlE1dfzc03fpWFp5yMEIJ/f+TnTJgwng3rB25DCIHLo4CUdjAe7Hh4vEMfD8UlKGdpIJWRXCx8gezPVlNWMaPyrIDuz3kOFCcnmPa9boIRX78M3Ucg7LE7UPuz5o+7efu5vcNufyRI9KkEq3LWByqoclEUYRu8mWsW5WXo/sxHpVeReBC4SgzqRjyByAR0PVBt/l8zBlzpWUUVYJYcAnT6q1D8+wlM/SW+Cf+N4vVmWvj35CyKDjxGlr3A5qhE8bVgpBqQUvD997cCMDHWicdlXq0lelOsji1h1/q20t5YkRQT0CcD+3J+b87clstcYK4Q4lUhxBsZiWYAQogvCCHWCCHWtLVV9oYGu0yp9PLl/PPPJ5VK8W//9m/2bW+99RZ79uxh3LhxeDweXnzxRfbs2QNAS0sLwWCQa665hltuudiNGVwAACAASURBVIV169Yxb948du/ezfbt2wF45JFHOPfcc4lGo/T09HDxxRfzwx/+kPXr1w94/Ztvvplf/OIXdgA+55xzWLVqFbFojHgizpNPPsnSpUvLfn89PT1MnDgRRVF45JFH0PVBshdhVrDc8fX/w0knzGflio/bNefRaB8zZk/D5XblbeOcc87hv/7rv3B7BQcPtfLXl14CKHg8zjz9bGKx2JDHQygCpCzZ98SyzrWwHCAhO4jDW0QJYqJ/hl5CSZ6ZoXsIRbw5GXqKUMRLIOwtmKFLKYl2pehpG96UqrcjwVP3v122Z7yW1lGTev6kqYRelseMJXFZyYR5cigvQw8a5ja6FXM/Ss3SjWQym6F7s6WB/Ucr5ko4dcleYm4fYupTBGf8BFdoO56aNRyIHsA7fTqp3XsH1dABtIx02qn4UfwtaIkZGKmJtDd2ADAh1oEQAiMWI21kupf/8Pdw33x454nS3uAwFKOhF6qT63+Y3cAcYBkwBXhFCDFfSplnpCClfBB4EGDx4sUVuROtueOCSp4+KEIInnzySW666Sbuuece/H4/M2bM4M477+TGG29k8eLFLFy4kHnz5gHw7rvvcuutt6IoCh6PhwceeAC/389DDz3EFVdcgaZpLFmyhC9+8Yt0dnZy2WWXkUwmkVJy3333DXj9hoYGVq5cad+3aNEirrvuOs6/cClSwt9/8QuccsoptoRSKl/60pe4/PLL+c1vfsN55503IMvPxeNz8ZMH7mfe8Sfw0ksv4vIo3HXXXYNuY+XKlbzwwgssWLiAmdNnc+bpZyGlLHg8PvPJ64nF+/jUlVcNejyUMpuLUnHNbioC8lrQLWfBYjJSq/HGn1O2aG1/OJJxDX/QTSjio6M5Slo1A2gw4rWdHrW0jtuTlW6S0bTpE5827PF8g9G8uYt9m7to2d7NrIWNgz5u8PdmngismnxvwI00ZFkeM5bTooV5cijvRBOQ/QO6ID4g3AyOkYjbAT2dUxeuJjX779if+mQvnSEfnpq1qB1nke45ldCsH/HUjqe4bPp0ejdshowCViigRzvMQoZoQKK44xjJSWB46Rj/JgAT4ub9+quPkPaY35Xd/iRzevbB0zeaGzn5yqLf41AU85drBqbm/D4FaCnwmDeklGlglxBiK2aAf2tE9vIwM2nSJJ54YuCZ8/XXB1bXzJgxgwsvvHDA7cuXL+ftt9/Ou23ixIm8+eabAx5755135v1+7733cu+999q/33zzzVz/qS9iGJK6iSH7dTdu3Jj3vGg0Oug2rfvmzJnDO++8Y9/+ne98Z8D2li1bxrJly0jG0hzcbV5OBsJequr99vMKbUMIwU9+8hPAzJJ72xN2W3nu8bD8XmobJxU8HhZ2t2gJOrqhG6QywdQiNzBakotVyiilHLS2Pyu55GfoxZQupuJpIg1+e7yeVboYrPZiZEoxE31pquqyAT3alV2U7GlL0Dht8OaT7kPmZPvug/Fh96UQVlORdYLLLmaWE9DT+WsWAXfZw0nG+TwQG5ihF3vlLeMJRMD8nKYVP2Bm5kOVLtYle+kMuZFSkDp0EeBGi83i9zt+z8enfwI1589dSEOPd5h5ayxsNg39RH8Cme7jVl8jUb+XCTEzoGvP/wTNfSIAXZ4kaEA6Ac/fdVgD+lvAHCHETGA/8AmgfwXLKuBq4GEhRAOmBLNzRPbQATCDoDKKPi6FyPVeL9WH3RcwL8GTsfSATDPbJTr0NsrJ0HvbTd+X6oZs44YvWDhDNwyJphqDvrdE/4Deb+D0UGQlF9N7vXO/ueAYrPbZem4ymqaqLnuS7OvMetYMF9B7DpmyTLkBPWEF9Or89QE1oUFtaYt1ltOihTfoprejPMfUf7lgHq88vo0uxTxGT1x/OtNOrB/yOZZBFsAP32+l1xtk2e3PsLI7yXGYLp5DLYzWJXvZVg8yHcEKif7k6ezre4y9NRqaO5vpF7qqS3aYfi2Tq9+jDTg73UZCNz+7RiTB/IQZCrWODtJuMyFrIMcauKd5yPdXCsNq6FJKDbgB+BOwGXhCSvmeEOIuIcSKzMP+BHQIITYBLwK3Sik7RmwvHZBGph3+MOJyK3bnZiHPlqEQStb7pH+GXazRmHBlK23eX3OwqIEHXa1m4KyZkP0S5s5hza0ph6GDczJqLmy6MsfAG8gMih5GqpFS2lUuViVD277seD1Lk+9fix7tyg3oQwfqijP0jJwUzNHQoTQ/fItkLD3giqjcBdZEVEUi6cmshhZz8szVwxv1bs70bman75Msc22xbx/0fUlJY7IbV3Unp+utrPbeyM3j1/PqP91ESPHyp60/IJ0J6ELIgpJLqsvM0CORbUxLpwlJSYNuMC2dprUGpsTM9UJN1Nky0HiZE9AjU4Z9j8VS1LWVlPIPwB/63fb1nJ8lcHPmn8MoIKVEKIe/D8zrd6EmweUu/WziC3lIxtKoSS2v6sQo0sbA7Oh0oaV0/vzr90DAvNMnsOQjM6muL9w63dVqBrja8dmArrhMt0TDkHY2bg+zTqQJD5KRxnvVPN3V7XHh8ijD1q+nU7ptBWyN57MDesRrL9D1XxiNdqZwuRW8ARe9QyyMSkPaC6dd5WbofZbkkq1ygfIagiynRYtSXCn7092VJCkgIczPSCkdpyuU1TTq3YTcKRQBiuJBUVUMl3fAomhD2Et7VOUK/UXchkFLBKZpGlOUTj7X/UN+8811LB/fzap6H2d6zKuPsGhD3bMX3tmeJ5GoXd0IxcWOkM4JOfrMKckU2+r8zN0mkek0+oRzSTcHkRhMNMzPA54ALP86I8UR2ynqkI/lfXK4Cdf6qC3SH70/Xr8LxSWI9ah51RO2c+QwGbqiCOomhgjX+bjy/yxh4Qen8f6aQzz6jTd4/63CVgxdB+PmkOlg/gKYL+i2XROt32FoC92etrg9zcfeThHZpxUUfSE3oRrzZNG+t8/2TbFOEv0rVPq6koRrfUQaA/QMcTUS7U6hpw0ijQGS0XRZY/pi3WbXqn2CK7NlP9dp0cIbcKNrBloZY9a6upIkhCSV+WiUcoL5qvsJ0EG4MycDQviTplDQP0Nfc8cFPPflc7ghbXZ8769SmJI2HxMUKp9yvcDHo73E3C6SYTOgV7naUHWfuZCZU53ia3kXj1ej2evmhFT2amFRMsXeOgWkIH3gAJp7PKo/RMqdoFYaEJkKH/nRiOnn4AT0owZDjq7T4mAoLsWsKy8DIcwp8Jqa77He32mxmO00Tq3irMuP41N3nUFVrZ9Nr/Zflzfpbo1ROyE44HZvwG1no5CtTR8sOEtD0n0wnpfpA0UNQc7tnLSqSGI9KoGw6ZviC7oRirCzZItoZ4pwnY/qxoCtkRd8jxm5ZUaT2dpejuzSeSBG7YRshVP2iqW0gJ7rtGjhy1lgLZVor0pCSIIBD0aJ+zNJtGNoAsWSa4wqAmomoP/5uwNKBKMpza5B76oSTE1nT4wuDBakVIKGQW/YPE5hVxuqEcguZAK88wS1LeswfOZ7PT6nJ2N6WuNQplcp/fKj6B0dJANhNFcCcWc3fHnjiAZzcAL6UYGUEoaoxjiS8YeyHutWllSJFXBVnZ/Jc2vMCUeyvzYv6WqNUzNhYClm3cQQ9VOyLdd5i4AFiHan0FQjT4uH4krybPfBoNk5aWWv1rxUIQT+sGdA+3+0K0lVrZ9IY9DOwgthBfvpJ5uLhZbMVAqdLVHqJmWP03DHYzCS/bznc7dVzsShZCxNXMDxE6vQXKUN5d5vNCB1BSWToSdlmIDWCdJAjasDMuu4qtOeMCNuVximaNnXEsIMjserKl3hIELqhJROVBk0P7/WQubzd2GkJPHMmvC8jOQiJUzUNQ7WmN9Z9S8PoO3dSsoXRgZGzkivP0d3QH/nCbM4/86aESvSd7lcLFy4kPnz53PFFVcQj5enUQK89NJLdjv7U089xT33DLDAsenu7s7rEM2l2KoQizvvvJPvf//7pe3sKBKu86O4Ffo6khiGrNgKuHFaFamYllcVAqYmnYprA7JqgAs/P5/zPz3P/t07jGZsZb01ZWXomv1YgFCNmaWHqrNXCIGwJy9DN3SDWHeKcJ2fSGMApNk8VHDfDsVxeRQmHVeDooiSM/R4r0qiL019TkB3exWEIkoO6NbM1LwqlwoydD2hI72m74mq5Gfog1lmW5+i+1LmPE/hlkgJKSOMT0ZxG0nS0p+fWQPhbU/izmy/KwxTtYHv/Xg1TVcghFtL4NbiGHjQ8WYXMnua0VWFnoCgTtdpzDTZCQGNmk53WGIoknSPgXZgH6o7iOIvoWyrRI7egP7OE+YZt2cfIM3/9zsDl0MgEGD9+vVs3LgRr9fLz3/+87z7y7WqXbFiBbfffvug9w8Z0K2M9ijM0MHUwqvr/eiaQW97YsD4uVJpyJTzte+N5t3efdCscKmdODCgQ/7x8w0jMVhBcoDkUowHTI7kAtnMPJgb0Ks8eRq6uc6AraEDg3aM9hxKEGkM4HIrRMYFSg7onRnPltwM3Rym7SpZcrEqj6x9htxjW1qGLqVEUQ28ITchr5tUv6qSwQy6rPD4fHoxAEo4gir9GLjxiShuPYUqM39HK7N+5wlOWnsHnqSG6pUEFIOqAlbNJ6RU4t4Abi2BSzX7BFRXbXYhMzIFQxUcCiqckFLzujA9QIM0iFVJ0jE3etwgrfjwuMor6SyGIzugP3TJwH9vZlry//JN84ybSzoBf7zN/DnWMfC5JbJ06VK2b99e0Kr2z3/+M2eeeSaLFi3iiiuusBt3nn32WebNm8fZZ5/N7373O3tbDz/8MDfccAMABw8eZOXKlSxYsIAFCxbw2muvcfvtt7Njxw4WLlzIrbfemrcfVkb7yuqXOffcc7nyyiuZO3cut99+O48++iinnXYaTU1N7NixY8B7WLZsGTfddBMf+MAHmD9//pCNPKOJ1+8mXOs3JwdlMtNyz08Nk8MIRdiVIxZ2hUsByaU/Lo8yZMVK18E4Hp/LrlKx8AY9w2axVoZuzTS1MvPcbQXC3jzJJZq52gjX+e0a+sF09J5Dcbthq2Z8sORKl84WM6DXT8p3/StnyEVPWwK3z2XX90P5GXo6paNICFV5CfncJJAlLYr6NPNz9W7ThVw/3jSC84koLj2BamQCupVZP38Xbj2JlnTRl5FbCn0ej1dVdHcQdzqOSGYC+rJvZ7Xv5V9HUxVagwonZvRzmRPWJ2kaHRFI9bgxNAUpgvj8juQykN79hW9PDPQTLwdN0/jjH/9IU1MTYDoGXnvttbz99tuEQiHuvvtu/vKXv7Bu3ToWL17MvffeSzKZ5POf/zxPP/00r7zyCq2trQW3feONN3LuueeyYcMG1q1bx0knncQ999zD7NmzWb9+Pf/6r/+a93hbKhawYcMG7r//ft59910eeeQRtm3bxptvvsnnPvc5fvzjHxd8vVgsxmuvvcbPfvYzrr/++hE5PuUQqDL1dEOXA4zGSsHtdVE7IVgwoLu9CuGa4hpjhsq2uw/GqSlQ3WONoRvK88SyzrXMqoI1Voae3a/+kktfpga9qtZPoMqDx+8qWOliGJKe9oQ9F7VmfJCetnhJrpSdLdGMcVi/k1WZAT3SGMi/+ilTQ49mTMwiNT5CPhcJKYfM8lcoq1ntvZGdvk+y2nsjF8k3APjdgb+wX5gndb8SxZVOocpAfolgJlPXEi46wjA1Xfh9HyfdKARxGQnEtA8CoE7/kH2/PGElUlPoDQhOSKUhMhV14XXEpXlsJ2g6B2oEqR43EoEQQYJVwycc5VJaj+/h5u8Gt8glMiUjt/S/PeNSEKof+vmDkEgkWLhwIWBm6J/97GdpaWnJs6p944032LRpE2eddRYAqqpy5plnsmXLFmbOnMmcOXMAuOaaa3jwwQcHvMYLL7zAr371K8DU7CORCF2Z6eCFsKcVKYIlS5YwceJEAGbPns2HPmR+uJqamnjxxRcLPv/qq68GTPOs3t5euru7qampKe3AjADWzFItbQx0AyqRxmlV7Nucf/Luas0E4SKlHEsPl1Ly4q+3MHluLcefbk6L726NM2F2pOBzDF2ipQ08g1jopuKa3SkLEMoEzlzJxV/lJRXX0HUDl0sh2mkGs3CdDyEEkcZAwVr0aGcSQ5N5GbqhSfo6EgNKLAejsyVG3aRQ4ZNVqZJLW4LaifkBqtwMvTkjmdXXB+jzus3yxUEy9BXKau7x/DtBYZ4Up4h2bua/aaGWPg/MD50MgOZRcaWTqKIqv0QwEz/SCYWD45Wsfh6oA28I2dPMQdHAhI98m/CvFKSrGdGyH9xzSUUTQMbJsc9MKmJ+OOEf3oKqKXil5CtrQnwj+FsmaEl21ymciUR3+RAoVNXWlXRcSuHozdCXf9084+YyAkX6loa+fv16fvzjH+P1Zha0ckyspJRccMEF9uM2bdrEL3/5S2B0dG57ERFsS1wY3CK3P/33aSy1eKEIascHqRlf2UzFxqlVxHtUYj1Z/5Pug7Gi5BYLq2Jl+9pDbH71AOv/YtrWplWdvs5kwfJHq1V/6xuFr74g64VuESqkoferRY92Js0BEZl68EhDoKCGbpUsWhm6pfEXW+kipaSjJUbdpIFTc0rN0KUh6W1PEmnI/1t6fC6EKF1D399qypYTG4OEfaaGPtgJ5qvuJ+xgbrEv03i31NC42j8fgGeqJuHWk6ju8fklgsu/TtLwoSVctFdnMnRPAC76Lnx5Iz87dw1nJO6nZ85K/FqIpCuG3G86rKoH99ib0XtMr6O0TzA5bJrQCiF4PXQ+d895gke5ntZMpYtlzFVbN7SVQSUcvQH95CvNM25kKiBGpUh/MM444wxeffVV2w42Ho+zbds25s2bx65du2wt+7HHHiv4/OXLl/PAAw8AoOs6vb29VFVV0dfXV/DxlZT5AfZYvNWrVxOJRIhEBmaehxOhCNtSoFwap5kBqW2vecw0Vae3o3AQHgxf0E28V+X1J3cgBLTvi5r2tYcKV7gAzD51HNNOquOVx7dxYHv3gPsh64VuMe2kek5fMYsJx2WPu1UTbwX0vq5UXsdqZFwgbwHZwtLV7Qw98367D8bRNYONL+8fIEXlEutWURNaXoWLRakt+9HuFLpmUN2YH9CFEASqvQOqkIbjULt53KdMCBP0ukgK0NWsl3muQdck0Q6AAWz3eHg6HOTXIXOxfEVfO8G1ZmN7W2grLi1JLKbnj7JruoJX9UvBEGybLJjkGc/mOb9ANl0BwImTzAx8y4FeXGkvfd4o8pB5ElcPZb1XjAO7APD4qvMSpfqwl/Zoiolq0i5dtCwE6iOjd3V8ZEsuw3HylYclgPensbGRhx9+mKuvvtqeFXr33Xczd+5cHnzwQS655BIaGho4++yzBzgiAtx///184Qtf4Je//CUul4sHHniAM888k7POOov58+dz0UUX5enothdKmZl1bW0tH/jAB+jt7eU//uM/ytrGkUbDlEyly74oM5oazMxVFg7Cg+ELuNn7ninbLL1qLq88vo09G9ttyaDQthRFcMH1J/Hbe9bwxwc3cuXXFueV7MFA+16Pz8Xii2fkPcb2c8no6NGuJOEco67qhgCGLol2JfNsDroPxXHnLNYGwl58ITf7Nney7c2D9gluzpLxnPaRmXbgt+hsMbPgukECeikyiXUFYV0t5DJxVoQD7/cUtR3LXOvUpIvz8fLRf3+DlAKnCFPSUhMagSqvbZn9/T9tpeXVBqaIdr5VX8tvq83PwjltOiAJuQwMw9yn3lAMt54kjR+Q2UqZju0s2PYnDlHNlikCTniMFx5roeHMKI1TqzhxohnQ32vuBl3Q60/gTpmSkNqRvTpL7TANZV3hXENaaAj76Iip9KXGoWYSci3j41I/inLn0R3QR4FcC1qLQla1559/Pm+9NdAd+MMf/jBbtmwZcPt1113HddddB8D48ePtIdO5/Od//mfBfbI09PPOO4/ly8+3b38pM0ACspa3MNA69/LLL7ctbo8VvAE3kXEBOxstpcLF3kamCmXGyQ00LZvMhuf3svvdDsZlxrL1D4YW/pCHi//hZH773TU8/383c9lNp+Tdn4preVbDBbdRZRl0WZJLivEzsxm8VQbY25bIC+hWyWJuNlg7Psje9zrxhzxc8NkT6dwfY8ML+9ix7hBnXDabhR+caq8rdLQMLFm0MIdpm2ZqxaxDWBp/f8kFYNLcGna83UZvR6Kg706uQ6JFQJqj5+y2/5z2/9wu35iq8WPlar7r/ne2eb2ckFL5dlsHdQfcHKQWxS1JyTAuVLoZj1tLYkgfyGR2usOeV4kf8tLSEEQNSejNP8GOq/JRH/KybV8vU4GeYAKXbl5xqF1Z38EDmH9nV92CvPdSH/KxrbWPdnU6vgDoPoVEyDxJBEIjP3rO4uiVXP6GkJkqp6O0DH3UaJxWZWekXQdiIChJmw9We1FcgrMuPw4hBNObGmje3El7c5RwrW9Iy+C6SSFOXzGL5i1dNG/NX9A2JZfCwxQsLF/2vs4kaVUnGUtTVZf9olsLje3N/WrtD8Wp6ZcRH3/GROYsHscn/uU05i6ZwBkfnc013zqT6fPree132/n9/ettr/XOliiBaq/9+rl4A26QoKaKy9J72hIoLpF3ZWExaY5pW9vyfmFZqlBNeVAKEgI76CYtg65+PizxlM5fvefB0i/T4XIxM53muHQaMpa1iluSNMK4lTjdYgouPWnaUJBdb5E7VxPv8LFlWpjJ4cn0dZj3WYZpQghOnFTNrgMZSa/WgyINXKRRo5m1jXeeoPWVXwDwydZf5fXAWOZf6bSPkFToq9KJ+82/b67vzUjjBPQjkHfffZeFCxfa/85cehoXffT8shYzX3rpJRYvXjwKezn2NE6toq8jyar73mbNH3ZTPymcNwFoOBYun8pV/+c0W1qZ0VSPljbY/U57UdLNSedMIlTj482ndubMfpWkYvmLooUIhD00Tqvirf/ZxfY1ptFYrnQTivioqvfTujMrWyRjaXraEtRPzl/QnH/OZD70ufm2EZj1/Iu+2MR518zj4K4efnPPW7Tt66OzJVZQP4fSRuyBGdCr6v0FTdbqJ4Xwhdy0bCsc0AsRkMJ2WQQGNeiKqhpBnwsWfJJOr5/6qR8ATwCpZQK6SxI3qukVbnrldNyZzPoW1ypWKKvhvvkkX16F1AQ7pySZWjXVbpCy1jQW3/0cr7zfzr6MHfP7hhcD0Awd9aRr7MbGzqR5YpqnH8hrbGwI+1B1AymhVoTYMkdnf+aKb7jPRiU4Af0wIqXZ+Tbc3Mampia7gmb9+vWsfukN/vRU4ZLEv2UmzTG1yL6OBKdeNINL/vHkkp7vDbjzpIfJc2px+1wYuixKunF7XCy+aDoHdvSwb5OpxadTOoYhh/3SCkVw6Q0LCEV8vPCIKdHlZugAE2ZFaN3RY39eDmzvBpl938MhhODEsyfx8dsWoyiCJ3+wjo79sYJyC2TLDd9fcxBDH775pZAbZe77m3RcDfsHydALETDoF9ALe6LHUxohr5t4qJ6E1KifeR585EcYbvO4iLrJbNbncIgAffpMRCagXyrWcL/nZ9Czj3ibeYXyztQo09SUbWFgSS7WFYQ1MSkmI3RVgVuLob7zB/jd5yGdoC/tIuWRuBTyrAXqc66A6vHzhzMVNk0LIYWKe8t/F31MSsUJ6IcRTdXpPhQnXeQlrYU0ym+TP5aZMCvCdd89i2u+dSanr5iVN/2nHFwehWknmDXCxS6unnDWJKrq/PxvJkvv3yU6FMFqLytuWmg3QvVfXJ04O0KsR7Xlkv3vd+NyK4yfWV30ewKonxzm8q8utu0X6iYWDujTTqxjwqwIr/9uB4/d9SZ7Ng4+o0ZKSW+mqWgwJs2pobctkTdabyjCUhBTsgE9OYgnekzVCXpddOw317DqA/Vw8pUYC/8O4fMhvvIeSVFLUoDUa0hnWu0N/LZsGT/kQ43oHKxWOPf9tUQzIwL7G6b5MzNOY4mZtEXAp/eScQBAB9S0gp47yTrTsFQfNv+mK5TVzI3updnjBj2EcMVGxKJkMJyAfhjRNfMPbxTwjBiKoeZe/q0TivhG9NhMbzJLEootf3S5FRZfMoNDe/rY/U77AGOu4aiuD/DRm0/hA5cfN2AhdcIsc5G0dYcpuxx4v5vxM6tLkpUswrU+Vt5yKmd9/DjmLBlf8DHegJuP3bqIi/+hCSHgDw+8k1ea2dMWZ9OrLUgpM4NL9GEDOkDL9sGb5izcEiKGoFMZXnKJqxphn5uO5/4ZgHq/+TeTiYQ9IHpa2M9HTpvCXZedhFAyi5mZ9n8pId7mZdNUwbR0muPbUnajW39LYytWJ9L1JMPgSidJZXxhtng9BJICxZvzfc5YC1glll91P8HkdIqoouDVg7iU6ACTsJHkmA7oiaiKppbu+DZaWOWHw0kuA54n5Zh4of8tcvxpEzjvmnlMPr626OfMO2MCkcYA//v0rqydbAk6aaQxyCkXTBtwYqqfHMLtVTiwswc1odG2t69ouaUQvoCbhR+cNuQQaCEEMxc08rFbT6Wqzs8fH9xItCtF684efnvPWl58ZAvb1x7KliwOEdAbplbh9bsK6uj9hz7X6QIFgRpysfueS9j1nYsxFIFkYINSPGVm6F1tB/jqb3Tqf2j2WRjxBCIT0K2xeOOq/HgUswKqFzMQp3rcGGmFV2a4ubwvStR/EmCenPsPHfFlMvSUgEn+JL5Ukj5pXuH8pLaG6qSkypM54eQ0NjZkMvRJop2JmhmD/FoQr5JZ5B7BOaK5HLNhQkpJX0dywCXUWGJl5rJEbx5znqiToR8OXB6FE8+eVNJAbsWlsOTSmXQ0R9m02hy8MVyVS7HbHT+zmtYdPRzY0YMsQT+vFH/Iw0X/0ISW0nnq/rf5/X1v4w26qZ0Y4vXf7bANvvo3FeXtvyKYeFxNwUqXNXdcwJZvfRgh4P9bPodfX7kIgJ//o2mvIYQg5HMhPWKAgVpMlb5CHAAAIABJREFU1Thh/9vU/9bD4u0S8aeXMVIpjGQSJRBAS+toqoEv6GFctQ9PJkPf4zLLUeOHzJPJ+1PhkphG3+xPA9AwNVxAcoE0El3AeI8bj5YkKkO8HPCzOhhgSkzH5zHQqqbkNTbWhczXaJENTMp0cHu1IEGRCegjOEc0l2M4oFs/lPa80fRD//693zN3qYCR0tD2ucXVBTuMHXOWjKd2YsgejTdSlQwTZkVob46y570OFEXYMszhoH5SmA9edyJdrXHqJoW4/NZTWXrVHPo6k7z59C4QUN0w9LrFpDk1dLXGbcveXJq74kgJMxtCdLbEEApE6rInwpDPje4WA7pXG1v3cO5D96K54bFzFEinSWzYgJGIowQC2XWMkJvx1X5ahSnJ7FPMgB7r8tIdghPcaXac9A16/fNRXIKGKWFbcrGuIHxSkMx89X7vOxe3lkAaAb5XX8sMNU1VUvCmex7xf1yf1+TocSnUBD18T7uSRml+FvxakLDoG/E5orkcwwG9PHljNP3Qb/qnW8xtlBrQjcpq0MvdZ4fiURTBaZfOtH/3jVCt8YRZEaQh2fzaAcbNqBqyNn40mHVKI5/4+mms/MoigtVeps6rY8bJDeYwjhrfsHr+caeOwx/ysOredRzc1Zt33+5Mq//0+iCdB2KEtG5av3a7Pbjm1dTljNN2oR7MmvBJKZl6aDeKYfDXFSqvnREGIYi/9RYynkAE/NmJUSEPjWEf7yuTADjorgUEh1JBDtTCMx3/SMesy+jtSBCu8xOszhqmrbnjAnbfcwnnz2ogJSQ/v2YRX/7GN3DpSVwywF63l1tTbtKqh/fcMwl7B57A60NenjLOpn3hnbikxKcFqfYxqhYlR2yn6Hff/C5bOgd2XBaLlBItZSBcAndmJua8unncdtptRW9j6dKlvPPOO+zevZuLLrqI8847j9dff51Vq1axdetWvvGNb5BKpZg9ezYPPfQQ4XCYZ599lptuuomGhgYWLVpkb+vhhx/m1Zff4P//xvdoPdjKrZ/9Mjt37gTggQce4Ec/+pHth37BBRfkt/5nMvQDBw5w1VVX0dvbi6ZpPPDAA2zcuJFdu3bxve99z36dtWvX8pWvfGXAPk+fPr3s4+kwPLNPaaR+SpjO/VG8IxR4J2S6R7WUbjfrHG76+6Z/4GOz2buxw/ZtH4rqhgCXf/VUnv7xelbdu46TzplMb3uCnrYEfSeY251RH+K93a0EOnfR9/KzpMQj+CJpFCAoekgdSJhB/uQrSaYNajNzQPefsIigouKbN5n4W2swkklcNTXZOadBD163gh4KIQyNLncDqz/zP/CHLxCbVYUen0XY56a9PUl1vT/PX8cyVPMZkFLgvZZeLjx+ll3Tft6E5Zy97Dts/ckpqMFwQYmuIexjR1uM7uM+xsTo7/EYPoJLPwcnzyjr71AMx2yGTpmSi0WuH3oyqo6IH7p1tfDVr91StB+6lObINiFMa4ALL7yQ9evXs2HDBhYuXMjHP/7xvEEajz/+OFdddRWQ7+HuBPPRRyiC5Z85gaVXzR0xicwf9tgVN5PmHn7L40LUTgix/LoTBvjTDEbN+CCXf3Ux9VPCvPPCProPxuntSNK7qZtIwEPV1lX09iiEogcA6NiSrTDyiRgpI2BXhcRUjbpkH+lwNW3BMPWBBoJLFpNYvx69t8eUXOw5p5l8NVKDW0+ixzS+/KcbqO2TzJ5zMaAQ8rnp60hQ3RDA388BEyCd1HH5XGzc34PwePBmtPE7F99lOy1qwYHOlZBdGPW4BFO85vdvNJuK4AjO0EvJpAuhJjW6M1NnSvH3KOSHvnXjTqZOnlaxH7oV0F9e/Vcee/xRoAg/dNtp0fRCv/7660mn03z0ox9l4cKFVFVVMWvWLN544w3mzJnD1q1bOeuss9izZ0+eh7vD4aFxahWNU6tGdJsTZkXoPpRgYgF/9rFi7mkTSnp8sNrL5V89FUOXuDb9lud+vYu9rXNZO+6zdP/3VCT3EYodINCQondPkMaT+/AEDHxKjLhaQ3dHmpp3niA++VJqk70oYR+dsVaOb2wiuGQJXb96hPSevQQXLiQZszR0M0B7ayK4DiUJqn7Och+PIt9BHz8dmsGH2e5flZOh5y6MpuJpQmEP77WYVwX+GjOWGCmBHjMDuhHO/3v396n55L/9Lw3u01jI6Lb9wxEc0CslWyJY2vMsDb3/tgLBoF0Pbvmh97fHXb9+/aDVKFLKnKuG4nfKsIZbCME555zDyy+/zDPPPMOnP/1pbr31Vq699lquuuoqnnjiCebNm8fKlSuzgxVCozcZxeHwseTSmcw+ddyQ5YZHA0IIXJt+A0/fyFTjNLbJJXRqM+jWzIEtocQBJi7tYeezjXRtCzFuQR/j3O+zWZ7Po+0PMO7ft7Og6Tucrb6L36PS0RekPjiZ4JlZawsRCBDNTH+y1jH8dbW4DySZH57PRdNP4yBfIlozDpolStwsKaxu8A9wwARz5mxkcoBDh3o41JfEX18NMbPZSaxZA0C8cWLe+yzkU+NLmifA0c7Qj1nJxSoNLHVRtOC2rG1k/leOH3rufpxz9rKi/dCtLjm3R2HPnj2MGzeOz3/+83z2s59l3bp1AHzsYx9j1apVPPbYY7bc4nDsUFXnZ/pJozcU4bDy/F3o8eT/Y++846Sqzv//Pnd62953WYrSYakCKiii2ECM3RhrLGiiSWyJieVni/rFxPaNKQY7KkFjwY6ifhUVAUHQpUnfxvbpfe75/XFnZjvbEBXn/XrxYnfuueeeuzPz3Oc85zmfh8jyKpAqu0MTaYqWImSMDF01pvQojpIgzdtsVK7IxPDOBmZ+fRNHOJ7CF81g1VfDISRQLCoeRSFr20foK97DFJ8VC7OVratqKRiSntSmseVmoYsGMUXNiJo6AFwZuQCoXu37lZZt6aBRL1VJ2B8lL64WWV7txpKXFW8TxPmfJVTlluIdMKTb2zbHv/77a7G8K3pk0IUQJwohtgghtgkhuixdL4Q4UwghhRDfuxpUeyPcH9R2G4Ja66GXlZUxbdo0Nm/ejNlsTuqhT58+vU3cuvVz5e47FvDhhx8yduxYJk2aRHl5OdnZ2Uk99ESRaCklfncYvUHBYNbx0UcfMX78eCZMmMB///tffvvb3wKa3vmoUaPYvXs3U6ZM6f8Np0jxXeGqJNhkQB/wkxHaxZ7QBJqjA7AG67GkaZ519kgvalQQaDTic0iURh/jTEs5Jv3vuGJF7M6aRcSqfaGyI0FYfifWww4DoFFm46z1M/qoouQl0/Jz0MeCBANRIpUVCIOBJqsmnxB1a8Y7LceCOR5zT3jo4VAMKeGFdVr94kueXM3/fq0V1nj8b68S2rKFj4dNx27u3kgnJAQS1/iu6LZ3IYQOeBSYDVQCq4UQS6WUG9u1cwC/Ab74LgbaW9ob4Z7SmR76gOKBfLxsJaoKSjx5obd66Bf84kJOOe5MdHqF3OzcHumhhwNRYhGVtBxN//qiiy7ioosu6nTcb7zxRpvfO9NwT5Hieye9hOBmTcgss34Tu8wnYhFObN49VORChdnE4YPzGP4LH2qkiVu8RVz0JtQH9Ax0rKNEv46dpSeTF/oAgOyYCp5KrNOm0Pz882x352Ky6jl0Yl7yklkFOTRHg/hDknBlFYaiInwRiSIg4AyiNypYHAaEEJhs+mQM3dOo7Yb1tJIjqDU7GAVMrNiEYrXyQckEZvTEoMcjBvtjw9m+6ImHPgXYJqXcIaUMA4uBUztpdxewAOhd3anviGQe+n5Iv+7rlv3O+tDpFaQqu+1LSonPFUanV77zuFuKFAeMw68m2KwZtaz6TUh0+GU2Nl8Njw+y8ccBQwheswZl3v/wiS2Nb7M0D2pnWNvANFX3DFG9mW9tPwMgOxaD9BKsUw4j4silymljxLRC9K0KeOfmZ6GLhYhGIVJZiWHAAHyhGDaTHndDEEd2S8EQi92Y1ERvrNI2QzXoWoxIlUVbmLaqKmnzTqE+piOtB2sbZimQyKSi5XdFT3ovBipa/V4JTG3dQAgxARggpXxDCHFDVx0JIa4ArgAoLS3t/Wh7QesYen/ErRLna3323aAntv3r9PFnqKSleko7vv76a84//3xiEYmiFyiKwGQy8cUXP4jJT4oUQOdVh0DbZZkoF5dkwxIt9dBVQbA5F1NODEfjbgyqj4hiw+bfS0W+jsaoj5e2vsT5Zefz+NeLCDZo61F7g1p82xGqprj6U6qKj2VEbSNZvAnH3oY+K4vwzY+hvlPNqBlFbS6dn25hq4wSlQqRigrMY0bji0vwuhu1HPQEFoeBYPyemqp9xJA0t/PQCatE9WbsZ55FaNEuHO0MeqK4RWvMUhAW9EpSoi/0xKB3NoLkHQohFOBB4OLuOpJSPgY8BjB58uT9EN3ex7VaG999GM/e9NOf9VU16aFrA1GlRNfFoMaOHcvH739OJBgju9ie2vaf4gdJa6M1T1nB7/VLKBINVIdzYMO9Lbsh48UgiARQo4KwR0/6qACVkSyEuxIyhqMP1TDnyEv5qnE9T3zzBDMHzOTr6G6CkZlElQ/wBjSvPhrQMWTna+wdnMXMHT/nq0GnoViOwLmimvIvGikamtFBHjjHbiSKikRPxO3BWFKCLxzFYVBwVwYoOrQlv99sMySFx5qqvTQpErXV10/V6VBiIRqsuYx9dhcmCX9ZtpW/LNuavFaHhxmw7PFyanf2rMZqf+iJQa8EWldALQGqW/3uAMYAH8W94AJgqRBinpRyzf4aaG9pHdKQUiL6aNFb75jvr4cuhEDRiZa+9rGZMBJSMVr0KWOe4odH3NveYaqgWuawXB3PWbqPsQrNwJeIBs2Ag2bUl9+pScYCQacepMCRGaQo2oTc8z5bjXoarA38suxSNjZu5NJll/Lr5b9GRSXsmoI3ayWqP4xMH0Ak2IwhGsB79JeUb9+NsnsOu/+qZXsZzTomzxnUYbh6nUIs7mXHdCYMJQPwVsc4NKgQCcUYPC4n2dbiMCarRDVW+9qEWxJ49HoCuRP5jUtgQrDYFqLCoLXrbMYCEPJFvvMcdOiZQV8NDBVCDAaqgHOB8xIHpZQuIPkXEUJ8BNzwfRpzaBs7749n3dZD718MXehE0kDvK7avqhI1pqIzfPcfgBQpuqN1aGWesoL7DAuxijCK0Iz3BeJ9OvgdCc3vsrPbSMUm4ufmzDDpOheRLRvJW7WRd4cNxG60c1jBYUzMm8jaurUMT5/AmkgO4dwc0p17qLl8Oas2PshA3TLqsnOpFeWce+EdNO/1k11sIy3b0qUDJOPyHzGdGUNJCb4dTkY3qWQV2SgZ0SKpYLEbCPqihANRPI1B/Gkdlxm/MUFRVFKvUxkT1jEioksa9K7wNAV7tcGxr3S7KCqljAJXA+8Cm4AlUspyIcSdQoh53/UA+4q6vwyx3E8hl5hEUUQylr+vMSU03BMaNClSfJ+09jp/r1+S9MQTdDmJTBjyVlKxwWYDOlOMF/OtnDMhh0h8lrrDpJUPFELwq/G/AuCEAadpB3NLyHXBNw3fYHA24bKm0RhqItucTWaBjSHjc0nPte5zNivii6RRvRljSTEmZxR7QFJ2TEmb9TWLw4hUJTXxoiK3nDeOXffNYdd9c5gyKItJAzN55uETWOwIs9waYYchxqERXZv06MYqbxuFSCkl7sYgjm6UKfcHPbIYUsq3pJTDpJSHSCn/HH/tNinl0k7azvy+vfP4OFr93I9+Wj8Y+hNyUSVKGw+9675iEe1p35fKNClSfJcUiYaeN7ZkwoNjwFVBYhEr2GwglB3j3twsImoum7KLAdiVVpw8bWrhVJadsYxZpfFYdN4QMn2wseYrjO5mPNZ0GgONWum5HqJYNV0V1ZGFkp5OaUOMmF4wbGpbCYOEnkvVFk2Ko3X91RGFDrbs9bRxFrcZVOxSUBTTTKldhSX3rmbtO7uSbfzusJZ+nN29mFl/OWhdQKmSjFf3xqK310P3elt0nHv7YGith/7W22/w0P/+NVl5qH1freVzoxFVi7fr23ocu3btYsyYMb0bRIoU+5FqmdN9IwDFAGFv3JgDSGRMEHIZWFVkxBg1UVHxO1bnjicidOxMb7t9vtBeiC0uSRvI1IxuxbZ1WDxOfPYMmoJNydJz3TH57vfY7NZmFeWFxzP3+ncp9kvWGSIYjG2dJosjbtC3NqM3KG0UJUcUpOENRalyBpKv7dDHiCE5NKJ9sQ8LGVCjksbqFruRKEDdnXb8/uAgNugSRafEf+75ee310P+9sJW4Vjx/vLfa4lJKjj/uZG647saWkEs7D729QdcZlB6nWkaj0e4bpUixH/hL9Cy6n6gKMDkg1jY0E3LrQBV8UaLgbpgN0sCrh8zg6mOuxZTVURrYFpcgdqVr2/Sbd23F5nUScKTjDDl77KE3eMM0Eaa46v8I2Es42a+lQK4xdPzeWOKFLer3eMgstLVJMxxRqIlwbapxJysShRTYo1cZGtFhVmFcWBuzs7alMI67QXsAHAgP/Qe7Y2XvPfcQ2tQ3PXQJREIxFEWgqhKfQdFyuUeOoOBPf+pxPzNmzODL1evYU7Gb8y45k6NmHM2X61b3Wg9dSli8ZBHlWzfwj3/+nbr6Oi799fXsqdgFdNRDnz7taO65+759ju2pp57izTffJBgM4vP5+OCDD/r0t0qRojd8pI7nazmYEhrIwtNF4RUJgY7qocEmzQjuyLFx5iFncu9pk/Z5LWvcQ29O0/RTcmqDmMMqgXQrEtljDx3AbbQwfMMivpH1fDj8FKDtDtAECQ9dSsguaruIOTxfM+hb9nq457SxXLnoS/571eGYdvn5vxe28vCwgWxbU8fgcTns+rqRWExFp1NwN2ge+g8mhv5TJKGHPmrkaIQQbNv+Leee9Ys+6aHLWEIxUft38+2/Z/qR0zvVQ1/75Vpu++Nd6I3dvzWff/45Tz/9dMqYp/hOybEbmaesYIXxN6w1zScbD3dELqAqHn6JBjv5rKYVdXip0mnGb4Q9saMZV5zb7XV1isBi0NFkTgedwtBq7Xvkjxef6E0M3WPUNNb3WjPYblDZ3kVWiqVV8erMdgbdZtIzMNvK5r0e1uxqwqhXGFOczqAy7V62raljUFkOQ8bnIlWJJ27I3Y0BLA5Dh/DOd8EP1kPvjSfdnlhUpbHKizXNiN8d1rSO21UZ74r2eugXnHcRe3ZXMqCklMmTNAGg3uqhJxZRhNCyXD79/GMe//eTQEc99Gh8QVTXgwXR2bNnk5WV1dM/S4oUfWLVKc2EXnkcCyFAS1W8z7CQpa5pHP5NOf5KE9kjPeSWeRB6E8RCMPZs+OIfENWMWt0uK8q3JjaM0hN2Hs6Y4p5pu9tMOrwx0OcXMLx6LyD52vwm0DuDXm3LJioUvs3Yd3FmXVwILxKMdajUBDCiwMGmvW4qnQHGl2Rg0uswZerIG5RG3S43k05sEeRz1vrJyLfiaQz2qLrT/uAHa9D7QyLDJRlD78ViZns9dFedH0UBq9XaRtOlN3roakwz0q0PdZXlEg0nMly699BTeucpvksS+ecrjH+kRAm1ORbcpmfcum9xCSvmvAiNmxz4dFYG33QTvHEtWNLh+D/D8juo3xKlYWUG5QMFG35+Hfp1Zobmd17lpz02kx5fKIqxqIjcNdp+RmPmWA4vNDM0Y2iP76XWls2Zc+4mpO/esbPYDUSCsTYZLglGFKTx3sZaFCG4/KgW2dzDTh5EzXYXBUPSk/K7zjotju5uCJA/KK3HY+0PB2XIJbEI2mZXZh9RVdmSathHPfRkmlPcos+YPpOFT2qLre310GORGEJp2VGaIsX3RSL/vLNUxcaNDsyZES4+7k+ccPhfWFY6meA3OipWR2H06eAohCmX4Zn7BnVfaMbc/uA9NHpGMyzfgUnfs/CD1ajHF4phKG5Jaywr/TWPHf8YdmPPHgo58dl5e2PelaiWxWHEaNZhzzS1eX3y3e/x8PJvUSVEVck/PtrOoJveZPLd7zGoLIfDTzsE0FIfTTY9zlo/qirxNoVwHCAP/SA16AkPPZG22I++ZEuoJNFvb/XQW8fQAe67ewGfrPi4Uz30KUdO4s57b+2zmFiKFPub9qmKsYggGtChFio0WdKRQuHhCWfzRf5IPP/4O/KMx2HMmbDlHdZ/uBhFgv7W33HiyFMpr3Ixuqjn3qrdpMMXimIo1mLyMaGgz+xdmHHNLbPZdd8c3vrNDABOm6A9HBZedFin7XMHOCgZkdXhO9jVtv7OXs/Is+Ks8+NtDqKqso0A2HfJQR1yEYq2CtmbnaLt9dClKhk0aBArV6xJVg+C3umhe5uDnHv2L5K1JvPzClj05H86iAg999xzNFR6MXehmdxa4/ziiy/m4osv7vF9pUjRVxZEz05u9wcIezSz8VjhABRzFWqwCFUorMkfwdTaTdx0w/Xc4HiJXOGm6dtC0hXBnKkXUusO0egL98igt1dyvHl3I9cCzSY7NkvfJDFGFDjItBr4cItWtSiRFtmeo88b3u9KZxn5Vio3NycXRg9UDP2g9NBbFiG1f/3RRE+EXDQPvY99xCSK0pJXLpTOHzJqTCJVia4HGS4pUhwolqrTeSB6BqDNWNf5NA95/aA6bIP/F+vghzBkrKLWqi10/jH0ArlCK6oc8Uo8dskN/+8Opt27HIDbX9+YDFV0RXuvt86q5ak3mdOS6Yy9RVEEUwdn4/RrMW67qet++jtDzsi34nOGkhuMDsSmIjhIDXoyhp4wxH182kqpGVihiKQR7ktfiW3/CYTS+UMmkeGyeUs548ePb/Nv6tSpHU9IkeIA8Z/YsVwV/i3TQo+wPJJFTIHHL3mDW6fdCugwF76Me8QrACiBuLIhYPAoRB0qv9cv6dBnVyGMzqiNG/RmkwNbP9L/pg1pCdfY9mHQ+0tGnpYmWbGpCSHAnpkKufSZZMhFCM1D7+PsKXGeIlrC8FpMvXf9JIS5ErSOx7cmFtFEucZPGNcm0yZFiu+D1oUaPFh5W52KPv1LSja5iRXkUJw1iLOzBvH7J63obN9Sn/424MLj15MG7DQYyHaDvjBKkWjq11jqLRmoCJrMaQzpoyFuH8aZfPf7yfvsTMO8P2Tkawa9aksztgxTS2Gb75iD06CrLTH0rsIbve0n+ZrsfbUMqUpEqze0tbffemoXDasoOpFMt0yRYn/TmypDq28+jsP+/D4zh+fxl/F1+Ew25q78kCHvmsgc2VpTSBDzDcOlGnFb/oYnaKIYL9/ojQz1gsESpVr2PGe8M6KKnudGzGZDziEc3UXsuzt6s6jZGZ1VIkq83p70PC1mHgnFyC119GKU/eMgNegtRlgI+pzl0sagty5D14vPk5SSWExiaiW0pbQS6Grt7UcjakphMcV3yr6M2qCbtA07CeNe5wnR4A0zpigN3rqQxTkFNIXryW0UmA85pEMfsUApDWkCo19btNwVMTEcSDOp3BM9u99jf37E8QB9jqH3l9548QajlvbobQ4dsAwXOFhj6LIlxNGfGHpicVVRRIdc9B6PRZUg28XQOxHoklLGDfpB+Zak+BGRMPrfVGma4GPz9NC8izeEn5m6UYhIFOPglk01LR6qwl5bBsKrY4+aQ228bNzTphNZqk7v1Rg683oT2L4ng95bEmGXA5WDDgerQVdbQhm9yXJpbGxMLkIWFBQweMhAZp00ncOmTSYSiST77g2J4tCJMEpJSQkXXPyLZF+LFy/msssuIxZVQR6YDJdbbrmFhx56CICbb76ZDz/8sF/9RaNRMjIyum+Y4kdFebUbgaTolX+zJWhiW8TJ8UILtRiHDE62S+R577pvDrXmAnLckmOUa/H4tY05r1pmdNr/vox26z5LMi2cMq6IO08dDYC1jyGXA01iYfRAZbjAQRpyUaVM6o5reeg9s+jZ2dnJxcjbb78dk8HCpedfRVaRDTUmCRDttYfeYtBbPPTVa1azbfu3HFYwLvlay5b/fX9Yo9Eoev3+e9v+/Oc/77e+Uvxw6Sp2vi++qXIxwRLB+cSLVA9LRzdKocyfhRcwDR7c6Tlu26GYopvItX5AXmUMFW1B02xQ0AnB17ef0CZBoCcMz3ewda+HUYVa/vqPzUM/ELK5CQ5SD51WOd99L0GX1IRRBKef8TNmzz2KCRPLWLhwIQA7duxg6NChNDU1EYvFOOKII5LKh6eccgqTJk1i/MQyFi1+uo1B/91vf8fDj/61zcwhGlFpbGrkjDNPo6ysjCOOOCK5ieiWW25h/vz5zJ49m0suuYSFCxdy+umnM3fuXAYPHsw//vEP7r//fiZMmMARRxyB0+kE4J///CeHHXYY48aN46yzziIQaBHmT3D++efz6quv8sUXXyRnJ2PGjMEQr2faVR/bt29n6tSpHHbYYdx+++3J/txuN7NmzWLixImUlZXxxhtvAODxeDjppJMYN24cY8aM4aWXXurTe5Kib/TWmIPmoZ8XWwFAzg49M11RdOs/R5edja6LGdnNl5wEQEFsKzkuCKdnsG3BqYwryWB4gaPXxhxgWIGD7fVenAGtjqm5j2HJrmYE+5op9IeBY7IZOCabnAE9kyjYH/xgH3WfLNlKQ4W3+4adEAnHEEKgNyjEoipqTGIw6cgZYGfG2cN63Z9QBE8+8RQyaERnUZkx8wjOOOMMhgwZwvXXX8+vfvUrxo0bx4QJE5g1axYATz/9NFlZWdTXNDP9qMO56LJfkJ2t5cCee+7PefTRR9mxc0fyGrFwjAUP/pmp06ay9PWlLFu2jIsvvpg1a7RqfuvWrePjjz/GbDazcOFCysvLWbt2LV6vl6FDh/LAAw+wbt06rrnmGhYtWsTVV1/NWWedxZVXXgnATTfdxFNPPcVVV13V6T1OnTo1OTu59tprOeUUTTO6qz6uueYafvvb33Leeefx8MMPJ/uxWCy89tprOBwO6urqOPLII5k7dy5vvfUWgwYN4u233wbA5XL1+n0/GwOYAAAgAElEQVRIceBo9oWZ5H6PWXuXUIsdYxRO/9JLePsqTHkdF0QTJDRX8lySXJeCMy0XKSVbaj2cNKawy/P2xfB8B1FVsrHajc2o7/Omn/2dmtgdGflW5l49rvuG+5GD0kOnQ65439MWEzouDz/yEMeceCQzjz2KysrKpADXlVdeSX19PU8++SQLFixInvvggw8ybtw4Zs0+muq91exsZbyNBgNXXnY19/+lpX00orJqzUouuOACAI4//niqq6vx+bSdZqeeeipmc0ssbtasWdhsNvLz87Hb7UkDPHbsWHbt2gXAhg0bmDFjBmPHjmXx4sWUl5d3e8/PP/885eXlyVBMV318/vnnnHPOOQDJMYM2q/nDH/5AWVkZxx9/PBUVFTQ0NFBWVsY777zDTTfdxKeffkp6es/kU1N8P0y46z1+r1+C6tOmkVXZULTJQNilYGRPl+clNFdyXJDn1lFlTKfWHcLpjzCysG/pe8PihSW+2uP80cTPvy9+sB56XzzpBPV7PJjtBhxZZnzOED5XiNxSR6+f7FJqIZv333+fT1Z8wluvvk9OQQYnzDmWYFDTaPB6vdTU1BCLxfB6vdhsNt5//30+/vhjVq5cSdgr27QHrc9zz/wFR58wlVGjR2qpjVG1Q2io9e/tpXJNphYlOEVRkr8ripIsSXfhhRfy9ttvM2bMGBYuXMjKlSv3eb8bNmzg7rvv5pNPPkGJ51Z21UfiQdeeZ555BpfLxdq1a9Hr9ZSUlBAMBhk5ciRr1qzhrbfe4sYbb2Tu3Ln8qR+a9yn6x+jGnYyr/5bnh8/mrtPGcuur37DqT8cy5Z7lyTZFooG9vnR8ZthVFqH4Qy0MZzI7u+xXZ7ejpKcxJmQm29PA/+U6WLdH0/pPVPzpLUNybegUgScUJddh6v6EnzAHnYee2LCTTFvsoihzDztDKAKXy0VWVhYWs4WNG8vbiHLdeOONXHzxxdx2223Mnz8foKW9xUL5xnLWrV/bpluhCIxGI1fO/zUPP/xwMnNmxowZPPfcc4D2ECkpKemX5rnP56OgoIBIJMLzzz+/z7bNzc38/Oc/59lnnyU7u2UTSFd9TJs2jSVLtO3ciTEn7j0vLw+9Xs97771HVVUVAFVVVdjtdi644AKuu+461q5t+zdJcWA5f9M7XLB5GYfXlFOcYWaesoKMf01kh+k8Vhh/wzxlBdUyB09AT1065A/wI3Sat24s6Fj/szWG4mKmNWWhi6nUWjN5eZ32GRhR0DdNcLNBx6BsbYEx5aHvm4PQoGv/Jwx5Mue7DxY9IZ07Z84c/H4/x5x0JPctuCepq7J8+XLWr1/P9ddfz0UXXYSqqjz77LPJ9uPGjWPBX+9j8sS2Mp0irhp28QWXEA6HSdScvuOOO/nss88oKyvjtttu48knn+zbHyHOnXfeyZQpU5g9ezajRo3aZ9uXX36ZyspKLr30UsaPH8/kyZP32ccjjzzCgw8+yJQpU9ooVF5wwQV89tlnTJ48mRdffDFZvWn9+vUcdthhjB8/ngULFqS88wNMa+3vzKCbsQ1aCHD+xtcZVv0G9xkWYvRVoQgoUbSKRMvV8QR9ehrSBEMIk1aqzTKNc36zz2sZi4sJbdkCQJ0lkw8311GYbia9CxXRnpAIu3xfm4p+LIj+ykT2lcmTJ8vEgl+CTZs2MXLkyH71myg/58gyY3EYCXjDeBqDZBfZ0fVydbypxoeiiGT6UUOlF6NZ12MpTCkl9RUerA5jB3GehgoPJqsBR7YZT1OQoDdCzgB7Sge9E/bH5+Knzh9f3sDr62tYc8tx+Jcspvauu8n/0x+pvedecqYIcodUdTinOpZJ46tm3itTuGJoDSZdER7jCWT+/q/7/JzW3nsfTU8/DcD8WTewJ60geayvuikPvreVh5d/y6wReTxxcec65j8VhBBfSiknd3bsoHvctddf6ZeH3l6DpZdCX9ouUTrVZhGKSO5EjUZi6AxKypin2K90lns+4tZ3eOrTf5KXBa803sxxpRYa1woyChUMlrb7NfKiTlyRQhrsZsYEF7Hrvjn0pLREYmEUWmRvE/QlfbL1fXywua6DREGKFg4+gy7bG/TE633oS5VJ3ZVEn715MHS2qah9X6oqiYZVTNaD7q1I8T3T4A0zT1nB7/VLKBINNEs7ukCMmvoMXj1S4fmsDD6bHeaGJwWNm+wUTHS3Od/p09Zv9lqye5WrnUhddBusBPX9X8Tsr6jWT4kexSCEECcKIbYIIbYJIW7q5Ph1QoiNQogNQojlQoiBnfXTE/obAkrEoxOGvEWDpXf9SinjO05ba7D0but/bF8GPd6XpzGIVCVmW9/jiwcz31dI8GBgnrKC+wwLKVEaUARkK17cVTpA8NVwyV31jezI0rH6UEFdpbWN0yMl6AOa3MWRI3J6J0wVN+jtvfMU3z3duoVCCB3wKDAbqARWCyGWSik3tmq2DpgspfQLIa4CFgDn9HYwZrOZxsZGsrOz+xx+6Bhyaft6zzvS/rUx6IpIet09QY1pT5euQi5aSbsYtgwTxi4K1v6UkVLS2NjYJv/+p8C+JG6hc880x25kzbxmWH4nuCohvYTbDc1YRRh/g4GAy8C2InDVOJC5cGu0gTHhMEf7Azw1OAfDVj3v+NM4yeZGon1vGoLa9fIaP4MNS6CsZ4qJhiIt5JIy6AeenliRKcA2KeUOACHEYuBUIGnQpZSt1Z1WAuf3ZTAlJSVUVlZSX1/fl9MBTX846I1Q7zGh6ASxqIrfFcbsNGDoRaUTVZX4mkOYmvRJYxvwhInFJLXOnk0jQ4EoYX+UBp+pwwMq4I0QDcXQGxUsfiNU9/wef0qYzWZKSkq+72EcUPoSYjjC/wG8/iRE4vIOrgoyAe9eE7s/zkJRBWlAGuCeFGBMWOsrU1WZb69nh1LAhloH4bIYp3q1zWzNQQNGHQzCrz0oemjQdWlp7LVmsiOtbztDU/Sdnhj0YqCi1e+VwL7qoV0KvN3ZASHEFcAVAKWlpR2OGwwGBnch+tNT1i+vYPWL33LpX2dgthlw1QdY9OjnHHvxSEaM6/kHrHmvj+f/9wtmXzqKYSO1VfoPF21m19cNXPI/43vUx8cvbGHr6mYue2BCh2Pln1RRua6ek64c06sHTYoUrUnEyItFA0TaHgu59FR8mklFtuDj4yOcUhFmYKPKsNK2khpWg8SeH+LozSauOCaTEaEwwyMRAj4d7jSYFo1oXn8vuG3uTdQEO85mvyvdlBQaPTHoncU+Oo07CCHOByYDR3d2XEr5GPAYaGmLPRxjr4iEtF2SBrNmJPVxOdqEmmFPCXi1b4epVe6swaQjEor1uA+fO4wto3NvfvSMYkbPKO7VmFKkaE0iRm4VHT33SECh4uMsAkbB/WcpPO9qIr8oBkXtGhpsoDeRXhIksNrMkFp4zWHj901O8OnwOMAmJaQP6NXYPrlzXj/urC29qRT0U6cnBr0SaP1ultBJgEAIcRxwM3C0lDK0f4bXe8IBLQVQF49bJ7zfaLjnhhjAudcPQGY8Bx1aDHr70nFd4XOGsKWnPnQp9i+tvfKuPoaNm+xEgzr+fKHCZCVMbkwFSzzpMNAM6SVw7G3JMIpvZzWxk47j+C1p/Ge65LomJyavIJqtgsGitf2eSKUm9pyeGPTVwFAhxGCgCjgXOK91AyHEBOBfwIlSyrr9PspeEPJHMFpabkuX9NB7Z9Abq73ojQqOrJYFOYNJB1Lz9g092ILsc4XIyE8tDKXoO63TDqtlDsvV8Zyl+7hTrzyBGgXXTit1Q6JsLbRwR0UzvwtfxSO339vlOQWlBazIPYShmxpomqnjk/RiCnwSfZoCpzzS4/h5iu+XbtMWpZRR4GrgXWATsERKWS6EuFMIkZhX3Q/YgReFEF8JIZZ+ZyPuBm9zCEdmS5hDp1NQdIJIL0MuTdU+sgptbbJcjPEwTk/CLlKV+F1hbOkpMaEUvSMRSmifdliiNHCB7v19GnMAT6UFNaLw3CQDR/kDjIn6+b1+yT7P0esUvhkykRKnkyGVRl7K0nbmOqaenjLmPyJ6lIcupXxLSjlMSnmIlPLP8dduk1Iujf98nJQyX0o5Pv5v/wXQeomnKYg9q22am96o64OH7iOruK0wfcIrT8Tp90XQF0GNSWwZqZBLit7x1CVTAPi9fkkH491dfQgpoXG7FXe6ZOVgHRe7tM1CRaKx2+vunngUzSY7l72vp2nXZgByBo3owx2k+L44qJKfpZR4mkOUjspu87reqPRqUTTgDRNwh8kuaqt0aDBpf65wsPuHg8+lfRFTHnqK3rJmVxPzlBVkNbtwuS1EQwpCJ0krDaA3SdQoNH9rx11hpmhaM5tydZSbjAw0ZiIrvOTUm3jjaMF8p5vJQW05q1pm01XyZ+u8d+PoudywdjFnfKodKx026QDccYr9xUFl0EP+KNFQDHtWWyPaWw+9qVrLw80qbGfQexFy8bm0L1JXWS4pUnTJ1y9yW+2z7P40t83LdevSsRcHCTQYiQZ0CEXy1aocrrzEQDSuOXRBjZWTFcnJ+U0c7tRy0v3SyILo2TzSxeVaZ5AsHzCJk3atZPSeXahA+oCuqxOl+OFxUMnnepo0eU9Hu5CLwah0MMLRSIwPn9tMxaamDv0kDXpRVyGXrg163W43u8sbqYz3a01LhVxS9BwpJbP3PEbTl1ZMmWFqz3Bx6zWSGy7V8f44QX2dmRqH4IOfhVk8VyWtQccVy9NYdsYynin5EzPXG9hRmM8AbKhSUKnmcFPkMj6zzurZAITg0XGnExOCZosFxZj6/P6YOKg8dG/coHcaQ4+0Dbls/nwvGz+pZvOnNRxzwQhGHN6y6aix2ofJqu8Q/04a9C5CLs5aPy/etyaZpa83KqmQy0+Q1iGM1lkqtSKHwtPv7XqRccMSYu/dgVgfJhqysvB0ydsDsxkSjnBWyEnlEXoemmmgSdHhV4zogNmjYOZaJ/4b78b84QfYCwvZdtnNTF8fZNWfjqUkzdylZ94VO9OLeHrkSdgiQab36y+R4kBzUBn0rjz09iEXNaaybtlucksdmKx6lj+9CZ8rxKQTBwHQVO0lq8jWIde89aJoOBClZruL0tFZyXY7vqoHCXOvHofJqseaZuy1BnuKHz+tjfl9hoVYCBNsNlCY1QCvx4tDlJ3N5Lvf4wj/B0mDD/C1y4Z5ezpvTBGsKRbc2tDE6R5v/IsqaL2nL4CRmw+9mBP2rKL4o4949dCjWTTieNRvwgzIspCX1ncNnBeHaR797X3uIcX3wUFm0EPo9AoWR1vlQoNRwedqyUzZtrYOd0OQk+YPZeDYbJY/vYmVr+6gdFQ2OQPsNFX7OHRSXof+E5ouzXv9vPyXL2ms8nHKNeMoHa0twu74qp7cUgcDx2R3ODfFT49ElkrjJht169MZcHQj9sIAvHw5LL+Ta4LDOcvQklPuQ1C7Nh1HGhwyysnblT5MCfttsMC48+DbZUnxrT/Un8JSZTrLZ0zGGg1SY8vR2sZUJpWm9j/8FDmoDLq3KYg9q6MQVmsPXUrJ2nf2kFlgZfC4HIQiOPq84ez+uoG17+5m+llDCfmjHeLn0OKhr1u2B4NZh8mqp/yTakpHZ+Nzhajd6WbqvP5p0aT4cdJeIXGesoJi0UDYo6P+G62WpnOHFXthfBO1q4ILdBVt0hDf9aQzsg6iR3g4Mqit40gJImNAm12dCZbGCz24THZcpraf10kDe27QU1vrDx4OKoPuaQp2CLeAFsuOxA367m8aaazyMuvCkclNQyaLnjFHl7Bu2W6KhmYAdEhZBFD0Ar1BwWjVM/fqcWxbU8u69yrwOUPs3KBNmQePy+1wXoofJ40LFxLavoOie+/ptm37YhIJatZkIBSJKAnhrTATDQn0cbe7tTEPCYhtsuKzSiYWe5KvV8kcSq79ptdjn9gLg57aWn/wcFAZdG9TkAGjO4Y7NA9dWxRdt2wP9kwTw6bkt2kz7tgBrF9ewcpXtwOQ1YlBF0Iw95pxZORZ4xrmOta+u4eNn1azd4eLtFxLp+el+PEhIxEaH3+CSLOTkwNlOM2ONscT5c8SnnlnQlnOnRb8tSbWHB3hP4fYuP+JGI17rOQP9XW43luRNMbsgND4AEpcVaK7dMN9MeeRFW3GmeKnwUGzYheLqvjc4Tbb/hMkQi71FR6qv3VSdswAdPq2t25NMzLyiELCwRgWhwGLo/PpZvGwzGRueXqulQEjMyn/uIrKzc0MGZ+bqgt6kOD7/HNizc0oSKbUbupwPBGiSPzffldnLCyoXZdGXaHK/YebKUoLsiMfyivSqNG11QGKAE1b7UR0kpED3UhJMt1wqdq/PJNUmbafFgeNh+5zhkB2TFmElp2iGz6oQG9UGHlk57roE44vpXxFdafx864YPaOYdx7TpsRDxuX0bfApfnC4nn0UxQiKLsaNdYsxDw53aVwNsSj6dWHCw3QYHVpor2pjGrGwwv0n6bnK6eIqp5uVgzJI/8LKFcZCRnqinLxK4jfCxkKF2eUQOSTMjborWRpquc6+4thdxb5T/HQ5aAy6pzGespjd0aAnJHS3rqpl1JFFXdbvTMuxcPTPh3XaR1cMGpeDNc2IlJL8Iel9GHmKHxrqqkV4Pl9PeqkfFHDttPCA8nceNvydKpnDgujZLFWnJ6vPj27aSfO3dnx7zQw8rp53DHYGbrXxSZngTL2Ti50e/NLI9rzxTFC2cNfiKKaAQpMd0nQwZxtIIRl6iJtH7rm3xyGW1qGUxFhS/LQ5eAx6c9ygZ3bmoWsGXY1Jymbtu5xZb4tO6HQKx148EjUqUbpTTkqxX1EDAZxLlpBx9tkoFst+69fz9D3IqCBtYAAZEzi32QjWmbAXhSgRDdxnWAgRkh770GatoFfIp2PZmgIaLQoDdCqzDm1kuCtEZeIhoJvOtSWLOX7vavLGuxg61IdeB9GgghoR1Nkyse5rYClSdMNBY9CTu0Q7jaFr8fLSUVlkFuz/Rcv2YmApDgzNLyymbsECog2N5F1/XduDG5a0KZjcWdpfB+LnuDcF0FsMbBogcKNQ9JmKp9qMvSiEGgO9P8aD9r9zO8+QgZcdrlwa0/Q8O0PHr99UGYzEMDrGoef+DcrOpoSWFMOHx5/FTqWIe0xPoI8///VmFb/JyIJI3xZAU6RI8KM26CF/BJ1eQW/U4WkKYXEYkt54a8zxMnJls3peRivm9dHw6KNkX3E5+szUJo0fGlJVaX7hBRCCxieeIN31OCaliojMpPr/9KSXesgY0lIwufUOzc64+Y5buVn9J8ZwBO/eAmrGRLi2SNtc9nyhH2+1iYhPofLTLILNBg6ZU0eWXavN6Xfq+LpUoXSgl7QxMYJVZp4edRZ3dnItVdHxKkehRpR4imMj1TI7Gcbpq0FP5ZKngB+5QX/toa8wmHT87NoJeLvIQQcYODabn107gaJhGT3u2/XKKzQ9+SS6jAxy5l/Rr3HG3G6EyYRiOvh0XcKVlQghMBT3MFTVF8+5E3zP3EOkooL8iS7qv3ZQ+38Rig4X7PlAR9itx783A0UvSSvVZm5EAl1Xrt+whDvUv6GTKrtXZqFK+MthFo7xByg3GXlppJGzlwl2vJOHVAEp8NaYyBrqJxJUMPgUXHkxbmxyIsYAYzzcmf460HWFoKXqdJaG959SSio1MQX8iNMWw8FoMg1x02c1nRa2SKDTKxQPz+xVSqHz5ZcBcL32GlL2vZ51zOVixynz2HvXXX3u47tAqr2r4NQZ/tWr2TnvVHZfeBEyEun+hA1LNE/ZVQHIFs95wxJiXi+Bb8p71Efz7cU0Pf04enOMzEN95I5146s1sfOdXCI+HQOObsSSE6ZqZSbevS0PUdVVqY3hwTFwe4b2/xvXweu/QYdK+ZdZBGrMPHaijjkGNw/UNXBnfSPvjdCjKpKwRXLPhQp7M2BXnRbtXuvVQnhlDl/bauquyjbDTnnKKQ4EP1qD3lDhAQlmu4HPXt6GuzHY6YJoV6iBAHsuvQzfyi86HAuUlxPatAlzWRnhHTsIfrPvnXoyGkX1+zs9VnvPvURrawmsXtPjsX3XRGpq2DplKs7/vtxtWxmLEfjqK5yvvEr9I4/gfPVVovX1+D77jD2XX4GwWolUVeFa2n3Vwegbd+Ddo9JQbqduvQN3hZlQQ4T6+25n26xj2XXmmQQ3dcz5ThJ/INg8AXw1ZjIO9bHXoGPP6CiRrCiRsELJjCbshSEGHNWEKS1K5YpMIgHtYy6khJevaPNAaVr5JCtq9by1Pg/ddjNvHQk/z6/n104XCjDA78AeHsT1l+q55HIjzizJroESZa+RCqGj3GNDFXCU2dt2rOltF9/X3DKbXffNYdd9c7o07imjn6K//GhDLnW7te3RJ80fw2sPf4UalR0KW+yLaGMTkZoa9vzyl+T+5hqyr7gCoWhffNd/X0YYjRQ/8AA7Tj4Z16uvYRk7tsu+qq69Fs9776PPz8c4eDAZZ55J2pyT8X70Ea7XXkNfUEB4925iHg86h6PLfg4UzUuWoHq91N57L7Yjj8BQUNBl25pbbsX1yisdDygKpkMPpfSJx6m48ioa/vkv0ufNQxg6TwkNbtrErudiyFh8AVlIkAmfVmIf5MbrlbhuOwPzrbd3HhpZfidEAjRvS0MKyd2HW/kgR6tk77hAYg9Ahj2LC1xupumCFB3ZxM4383Bus5E71oM2QWuZbb21O5vBn5vIBjKBb8dFmT+gHntQa5PYqbm5dgqDix/ll95tXO5qpjLTQjCSyQPRPCbXKwQyo5j0rWZxBosWSuqCVHgkxXfFj9ZDr9vtwZ5pomhoZlL2Ni2756lrxpJiBr+4hLSTT6b+oYepuGI+0YYG1FAI1xtv4Jg9G2NJMfZjZ+F+801kuPMNHNHmZjwffIjtiMOxHX440dpaqm+4gV3nnMve2/4fpmHDKLhN+3IHyzf2+777i4xEcL70EuaxY5HRKHtvv6PLkFJ4zx5cr71G+qyJDDnVxYizqhl8Qh25ZW4yh/op/cNp6HNyyPnVr4hUVOB6/Y0ur+t64w2khNKZDQw7vYZhZ9aQfUID4nAPBSfVM2BaNY6iIK6tUeRLV8Dt6VpIZEOr4sauSuqdRmq32/h0pMJXmQZ+2+Tk73vr+HfzXuarTYSE4E95OcwqLeHwsUVsHgx1O2yo7STsP4jYKFlloqJUJXCCi0POqGHeyDrscYMflUrLTk1pZGfltWyu/zm1ag4DskOoiqRwt8LQGklutgRLFiAgfQCc8kiqsHKK74UfrYdev8dDbqnm7U46cSD2TBOlY7J61Ydis1F0/wKskydTe8897PjZaaSdcAKq203GGacDkH7qqXjefgfvJ5/gOPbYDn14lr0HsRh5N96IeeRIZCyG67Wl1D/0ENHmZkr++Q8MRUUABMvLsU2b2s877x+e5R8Qq2+g8K67CO/YSd2CBbjfeov0OXM6tG188kmETkdu0VcYopr+iDkzijkzHl54/zpwWLEfcxamUSNp+Oc/SZ93CkLf8rFKaH7/+r1XID/CouFmPrVmsNloJBCfEZlUlRmBIGcMD5NdqcO314i9KNQSY9+zEr5dxvaIgfoV2YStAsskD+9UeLDEH0aqhFGhCEe5BRsseuqMMb7V2VkxEUb8V+HfzlxOyG+mNBpli8GA87N0MvSS6RPqsFrarif4pbHTbfetFzL/nP0vjlq/G0coTNov7oJzz90/b1CKFP3gR2nQw4Eozlo/w6dqAls6vcKoI4v61JcQgsxzz8EycQLV119P83PPoS8qxDptGgD2tBp0FnDdexmODbYOWRnut9/GOGgQphFadXSh05Fx+mmknXQi0YYGjAO0VElDURHBT98G9wP9zvDoD83/WYyhqAj7jBkwfTrut99m7513oc/Kwnb44cl20cZGXC+/QvrPTsUQ/V+cisKHVm0GlBeLkR+NURSNYn35CsTLl5NbUkLlMhXnS/8l89xztE42LOHd8LXYgwF2ePJ5fJqZd7OsjAyFOd3jpSQaIzsWY53JxPs2Cx9OsvD0p1Gcu6yaQQeIBAivfpy3zTZMn+SQFwbzic2cH9FSEqWEZuzcHrmwxQC3mkwtN/yG5kw9JV8bOGVSIRYpOXqD5Jd7JPYp7jbGXEra7ATddV/LQ669PO6a/BFMrP8WAPOYrsNxKVIcSH6UBr1+jxY/zx2Ytt/6NA8bxqAXX6Tx3wsxjx6F+OYlePsPiEAT6QPTaNpsI7BjLxZ/Sz5ztKEB/6pV5Fw5v0MGjWKxaMY8nqZnVrwENuyB0jqtQQ9yo9vQOt3PEs+LDzT36sEQ2rkT/+cryf3d72gKO0kzplH8179QcdWv2PPLS7W1hPnzEYrC/fPvYl4ozNk+G6MPKeQLm45oJ1lCmbEYk4IhrjDUYspLZ/fdd3Pc5zGOtH3N/xgXkq2E2bFXm0mFSsN8uKeBnJiq6XzHuzvJ5+cPTc3cn5XJ8tFWjv/KTE5YsNuuZ5XBzI49DmZ9AVkeiW2mk0HWQAfj2xUPx87m5iGLsH1p54GVHjx+A0PW6yE3SsngtqqHEsH0cOeZ4O3j3qFtw9kx93WEwYB52NBu//YpUhwIfpQGvS5u0PNK9+8Co2I2k3vN1S3pdXEvMGeUB/duCzWrMhh8fD0ins/sfvddUFUcJ57YeYet+jFn2fFUWoiFBTpjPGYdCcArV2o/78sgtxsPgVaFrTt7MHSR6938wgug1/P8IXv595KZWPQWJuVPYubdZ3H4ovXUP/wITc8/j7kkk59t2Uq0NExo7CLWqibOdLs5zesmTVWp0+mp1euo0uupNOhZZrXyfomV0070c+5zEV7++mYGTG9CAaLAt7V2yIM/hhrJiXWeLqkD/tDUzNODQPelhX/vLEKoMGG7ZLIXIrlRBk5yYs/XvOR9Gd8Eu+6bw6CbQF8c5fINr1PykZZqaMiJUjqlifbPp2rZsuO3u4wT4yGHoC8qRJ+Ti0gVUlb4ZgkAABasSURBVE7xA+FHadDrd7uxZ5m6lLjtN/FsigQ6o6RgspPKT7Jp3GwnZ7SWY+x++22Mhx6CediwbvsxZ2p52sEmA7aCVjEBGWsTJ+7UAw/72oynA5EAlS/9kenP2zrqcrsqkC9fTu2/bqDxAxsfjbTy7+qXCDsnE1YNfOzbzIqqFagj7EzzTmHOzirKdmxConDXMWbyo1EW1lRji+mxEEUnJCXRGIRaLn+d0syitDQWFTgIz4SLlpt5ypmD55AIjRE951QLnOOCFEVbViabpB0L4TaSs0i4UNfMhgwjx63ToRokhoIwRZO82ApCbQxwa+PbHS8rM6mYlEehr5HPisZyhO0b7jMsxEjLePzSyD9057UJs+wLIQTFf/1rypin+EHRI4MuhDgReBjNkVoopbyv3XET8AwwCWgEzpFS7tq/Q22hbreHvH6GW6SULNq0iCkFUxiWOaxtyMRVSQTYbDQyIhzGADiKQzgGBGgod6CkpSNeeIHAl2vJufrX1Pvr2dCwAQWF65ZsYKprN7eI1ygWDUkjZM7SDFew2djWoANEAqirHwcB3xoNDAk0kUz+c1W0CU90RbFoYIXxN1hFEKsI06BT2Gg0UqnXs1MaOfotM7E0eG22n3tqAhwZWEYGXqqbc/iT4VhW59Sw6oi1rDoCDNKEBEoiUR7fW0eeGgMRIyR1xKTAKKJtrp2mSn7ldPFLl5uPSi3U5qcx7iMji3QmIopAkSoTc1uq8PilkTuiFwJ02P4OcMORS8gJNWHJiiA6ycNKpBPui4SHndgSv7pgVPLYUnU6RFquXSuyKTzjXv7cy/UM64QJvWqfIsV3jehuF6QQQgdsBWYDlcBq4OdSyo2t2vwKKJNSXimEOBc4TUp5zr76nTx5slyzpuebbRKLUiYVfuO28LE5whfmaPcndoEwNGI/9H4A8iIqM/0+RgWgLBRlsyXGPzLTqTAYGBiJcF2Tk2P8AWIBhR3v5hILxvVihCQyJ8IlIzIJG1pcViElY0JhRoXDOBWFer2OsaEw8542Ys2KUHJkc6djesiSgW+LjYZM7QEwxhzgRJ+f7F7s6pQSmnUKc0uK8OgUkJIbXokxeauk8hQf0+we0tW277lfGvlD5FLeMhdxm30BjTodEQG/dLnJbRciaVTtBDAny6x1JjAZ9uqo+SIDf70JFIneqHLoqbVAywLmZ9ZZbSr+tKezCkCdLoCSqsqT4qeFEOJLKeXkzo71xEOfAmyTUu6Id7YYOBVonVR9KnB7/OeXgL8JIYTsz575diS+9PkxzWWr1fVv67qMZHPEtuM4Jv2/fGo18LrDxpL0FndweCjMTY1NLHE4+G1+LlP9QW5qbOLQubXEwgpIiOkl8wflougCPFrTQHYsRlBRWGMx8bHFwpt2G9mxGOkxlWfSHBQWxxhbIygG2tvBd2xWGrbb+cVnifsysH6QkePOzmBqKIRRShp1OoxScrnTzRHBYKf3JQQsTnPg0Sn8bW8dh24XuLdkklvmYbTVC5382awizB/0L/K6/xGOixopVhq7/LtlCh+TQo8BJGtotp6JABjtMUpnNeLeZaF2fRppg/xdZo90pend3otuLWCVoKfhkRQpfir0xKAXAxWtfq8E2idTJ9tIKaNCCBeQDTSwnymIG/S9/TTo85QVPKA8hd6rcq5XKwO20WRknclESTTKLH8ABTjL5WWho4Bns4ycVVLIeW4Pc7w+cmMxFqZnsNZs5n/qGjiqlYGdFAox3+luc72vjUaW5+Zg2Krjd2l5/MnXSH5Mi+FuMxi4LSeL25ZHMaTFGHBkM+49FsaVO7jl0xBPTTOgR5ITi7FHb2B+YR5T/GFubWhgUKztLMUvBC+k2Znp83N0IEjVrkx0phjZI9ptTW9HsWhgrekKMvHuM8TTOnadyMvuzJsWAtIHB9APjPHHyKUsDc/o9j1prxi4LwGr1Db5FCk60hOD3tlXu73n3ZM2CCGuAK4AKC0t7cGlO7LFEMOtSIL92OOaMEB60fJQMADjQmHGhdpO//UIFjQ8wBrf+fwtK4Nn0xw8k94Sv7/A5eZkX+c6Lq3JDKbxEZdzAv/G2WTm1CFFnOr1Uq3Xsd5swh5ROaQS7IPDmNKj5IzxEGw2MHqlkf9m1KG3xqj/Oo1As8rrc1X+lZ3GlQV6Xq2uwtxqIvSq3YZTp+PnTj+xKHirTaSVBjqNRbdGCMiixeirUntT2xh2g4WSU+5lV1l7z3gObJhAzct/JF82oKKgQ23xymXPSqqlwiYpUvSPnhj0SqC1kHgJUN1Fm0ohhB5IB5ratUFK+RjwGGgx9L4M2KWTuHSx7hvug/YFffdFwiMNxrK4vbGBi11uthsNNOh06JD8zNOxgnt7Eot4G7OGENQZGPXVWFYVeVmStp1BkQiTgyEu/dYH0UyseSFiElzYyZvkZNc7eVR9nomMCUIubanUu3U6gQFjqBq4kAczsvhjsxYiiQLPpKcxPhhifCjK6zVTGR6twFYSRJWdx7u7QhHxeLk0J0MeJafc23V6ZdnZFMaPJZ4dJcAj8X8pUqT47umJn7saGCqEGCyEMALnAu2l9ZYCF8V/PhP4YH/Gz/c3iQW97midTbEgejZ+aWRQNMqx/gDneLyc6fHt84kopZael9hGHtYZ+Cp3KFOqdxLYfTnTt87lucomHqhrIL9KW2gVOZJrI79iYugxhusW8X7ZJIJNRqIhBfPhUWJCEK3REfMfSrh5Gs+n21hjNBMFnktzUGXQc4nLjVWEGVO9C5/ezKT0f/O7yK+oVHNQEXHdke7JFD6mhx9hSOg5fmb8Z0qfJEWKHzjdeujxmPjVwLtoaYtPSCnLhRB3AmuklEuBx/9/e+ceXVV15/HPL9yEEEJIMAFTwFBasSQOYItQ21KjtoC2M3bhkrbq1KnYrrZLOmhZM1qltc9ldbW1DmsW05ev6VhbLTOd6VRFChaHCr54JhJAmRah4AMsIdyYx69/7H3DzvUGEnJzT+7J77PWXffcvX/7nP3dZ+/f3Wfvc/YB7heRXbieedYXtsjmG873aSUTMjj1VM+4nKNvmYTrPkl3fFhB6d7zTQ1V9PQU44bTa3nvnxs448gB/rtsDmUFhXyrbCUtB1sYPgZGXbmcu6YdfxWZ6iU0r1nLiBnTSYwZwx+vWcQVf/4Ty277CEfb6lnwXwu4cVw7ChxMJPibZCv1LcfQTujcB9XzLmLnHZfi5q2DFy58/2y/jGzPFJRPYM/1NvFoGPnCSW9bHCj6ettiVkl/8hLckqenskpeH9/A03bgALvOr6fqSzdQ+ZnPAG4FxB2zZlO+YAGnL7vlhId7/b77OfDtb/OORx+hqKaGDfs3cN2jizj3WJLLjjTzwZZjFAJHDxbxx99VMv7OOymbP693ZRByquVhGMaAcqLbFvN2+dx+MW2hc1ajJ9LvJU+nLYTrt8Gth933SfZROG4cxbW1NK99oivs2NZt6LFjlMyaddLDlV5QD0DzEy797OrZbDxnGf/6ejMXeWcOcGRfKVKYoHROD+ucpJfBiDG2BKxh5Dl5+eh/Vpi2MDKHVVpfz6srVtB+6BCJigpaNm4EoGTWuSdNWzRxIkXvfAdH1qxhzKfc05Yy/ePudhR/paBl42l+rYyRHziHgpEje95ZhGVgGEb2GZo99IgpvaAeOjs5um4dAC0bNzJ8yhQSFRW9Sj+qvp6Wp5+hozm4tzy4Umi/YjVtrxxm5PveNwC5NwxjsGIOPQKK6+oYVlnJaz/+CS9dvpCj69d3W4v8ZJTW10N7O0ef/L+M8cmGRn+c2ozxhmHEE3PoESAFBZTNnUtrUxNSUEDVkiVUfXFxr9OPmDGDgtGjaV73+4zxycYGEKH4rLOylWXDMPKAoTuGHjHjbrqRqi8uZlh5eZ/TSiLBiOnTSG7dljE+2dhIUU3NicfPDcOIHdZDjwgpLDwlZ56iuK6O1t276cywSFdrQyPFtVP7kz3DMPIQc+h5SnFtLXR00LpjR7fwjsOHadu3j+FTzaEbxlDDHHqeMqKuDoBj27d3C0++8AIAxVNtQtQwhhrm0POURHU1wyoqSKY79NQdLlPfFUW2DMOIEHPoeYqIUFxb2+XAUyQbG0mMHUvitN6/c9MwjHhgDj2PKa6ro3XnTjpbj7/+LtnYQLGNnxvGkMQceh5TXFsL7e20Nu0EoDOZ5M0XX2K43eFiGEMSc+h5TPHZbmI0NY7e2tQEnZ3WQzeMIYo59DymcPx4CkaP7nLoXROitXaHi2EMRcyh5zFuYnQqyYYGOt54gzdWrqRg9GgKx4+POmuGYUSAOfQ8Z0RdHa1NTez5xCdJNjRw+leWIdKHl4cahhEbbC2XPKe4rg5ta6Pj0CHOuPunlMzM+CITwzCGAObQ85zS88/ntGsXUX755RTV1ESdHcMwIsQcep5TUFLC2KVLo86GYRiDABtDNwzDiAnm0A3DMGKCOXTDMIyYYA7dMAwjJphDNwzDiAnm0A3DMGKCOXTDMIyYYA7dMAwjJoiqRnNgkVeA/z/F5JXAq1nMzmAgbpripgfipylueiB+mjLpqVHVqkzGkTn0/iAiz6hqrBYtiZumuOmB+GmKmx6In6a+6rEhF8MwjJhgDt0wDCMm5KtD/2HUGRgA4qYpbnogfpripgfip6lPevJyDN0wDMN4K/naQzcMwzDSMIduGIYRE3Lq0EVkooisEZFGEdkuIv8YxC0WkR0+/Pa0dM+KSJmI/EZEXvA2t6XZVIvIYyIyQ0T+4G22iMjHA5u3i8gGEdkpIg+KSFEWNP1URA6KyLYg7EER2eQ/e0RkUwY9RSLyiIhs9nldISLDApvzRORHIvJhb7/Vf18Y2LzHh+8SkbtkAF4mKiLlIvKQL/dGETkvzF9gd4aINIvI0rT0/yYi7xeRO/w+tojIShEpD2xu8hp2iMi8LOe/P3WuKPj96/Ach2WQ63OUqc71Vo+IrPU2qfo5NrCJpA1l0LfHl9kmEXkmCO+qcyIyLcjjVhEpDuxuEpErReQGEWnwGlaLSE1gc7XXsFNErs62hshQ1Zx9gGrg3X57FNAE1AIXAI8Dw33c2CDNJODXQAlwgQ8rAtYBFwd2nwa+BEwBzvRhbwP2A+X+9y+AT/jtFcDns6Dpg8C7gW09xH8X+Eq6Hr9d5r8FeDiVNx/2NeAy4BzgbT7sbODlwGYjcJ5P/9uwPLJ4zu4Frg3KvTzMX2D3MPBLYGla+k3AMGAukPBh3wG+47drgc3AcODtwG5g2GCoc8HvBcB/pJ/jqM5RpjrXWz3AWmBmD/uNpA1lyMceoDJDeKq8E8AWYLoPPy2sM8AaoMqXSYkP+zzwoN8eA7zovyv8dkW2dUTxyWkPXVX3q+pzfvsI0AiM94V9m6q2+riDQbKLgUdUtUVV1/j4N4HngAmB3Xzgt6rapKo7vd0+4CBQ5XtGFwIPeft7gY9lQdPvgdczxfljLgQeSNfj0/7FhyVwzjKcob4IeFxVn/c6ALYDxSIyXESqcX8If1BXS+/Lhp60/JfhnMdPfH7fVNXDYf683cdwjWJ7WvqpQJOqdqjqY6ra7qOe4vi5uxT4uaq2qupLwC5gVrY09KfOeQ2lwA3ANzPsPpJz1EOd65WekxBJG+oDqTo3F9iiqpt9Hl9T1Q7oqrNFqvqKqq5R1RafNqxz84BVqvq6qh4CVuG05z2RjaGLyCRcz2YDrkcwx1/KPSEi5wam80mrjP5y/W+B1f73MOAsVW1Is5uFc5S7cf/ihwOnshfXsAeSOcCBVOPwdNMjIo/iGswRfEMRkUqgTVXfSNvfZcDzvtGOx2lIMRB6JgOvAHeLyPMi8mMRGRnmT0RGAv+M6z2l05MjuQbXW8Xn+U9B3ICdl1Osc9/AXWW1BPGD6Ryl6EsbutsPZyxLDQENsjakwGN+mOizPh9heU8BVEQeFZHnROSfgrQfwvuFNBYRQZ3LNZG8JNr3eh4GlqjqX0Qkgbv0eS9wLvALEZkMFAITVPXFIG0C1+O9KwifjWuk4TGqgfuBq1W1s4exy4G+Z/OTBL1zP97YTY+qzvPjfz/D9X5W4Xogj4U7EpE63FDF3FRQhuNlW08Cd2m/WFU3iMgPgBtxvdxU/r4GfF9VmzMU8TzcZXwXInIz0I7TC7nRcUp1TkRmAO9U1ev9n0HIYDlHKXrbhq5U1ZdFZBSuPP4ed+UwmNrQ+1V1nx/fXyUiL+B616nyTgAfwOlsAVaLyLOquhr353V3mo6rgJnA+amgDMeMxf3bOe+hi0ghriL9TFV/5YP3Ar9Sx0agE7cozRzgybRd/BDYqap3BmHdeoL+sus3wC2q+pQPfhUo9w0ZXAXZxwDhj7MAeDAIzqQHVU3i5gku9UHpeiYAK4FPqepuH7yX7kNOA6FnL7BXVVMN/SGcgw/zNxu4XUT2AEuAL4vIdSJSght37cqTn3z6KM6paHCMiQOpox917jzgPV7bk8AUEVnr4wbLOUrRqzakqi/77yO4eYHU8NagaUOpOuOHjVb6PIb52ws8oaqv+iGV/8XVS7ztxkDHh4Cbgb9LDUeRgzoXGf0ZgO/rB/fPeB9wZ1r454Cv++0puMshAe4A5gd238Q1zIK09Os5PsFYhLvkWpLh+L+k+4TOF7KkaxJvnTCbj6t0YViXHqAUqPbbCZzjv87r3szxh77K/e/LMhz3aVyPLDXhdskAnLN1uEtxgFu9hq78pdneip8UBT6CG9MNy6MBqEpLU0f3SdEXye6kaL/qXKZzPBjOUXqd640eX88q/XYh7g/6c4OhDQX7HwmMCrbX45x5WN4VuDm0Eq/pcV/f6nDzMal9nYMbKjoz7RhjgJf8fir89phst50oPrk9mLtMUtwM9Sb/ucRXoH8HtvkTdaG3fxoY4bcn+LSNQdprcbPZvwuOcRXQFthsAmb4uMm4f+9dvmIOz4KmB3B3AbTh/vkX+fB7Uo0lsA31jPO/t+Am0v7FV86ZwD1BmluAo2l6xvq4mb7MdgPLyeBks6BvBvCMz+d/4i5z7+nB9laOO/TlQH0QtwvnZFIaVgRxN3sNO8jynTr9qXNp+5nEcYce6TnKVOd6owfnIJ8N6twPcHcgRdqG0rRNxjnvzT6PN6eXd5DH7V7v7T5sKfAPgc3jwIFAQ3jn0jVewy7g09luN1F9Bu2j//4S9keqevFJ7K7CjRHediK7qOmDnluAXar689zkrG/0Nn8i8hwwW1XbcpOz/hOXc5QiLm2oD3VuFW7Ia39ucjb4GLQO3TAMw+gb9ui/YRhGTDCHbhiGERPMoRuGYcQEc+iGYRgxwRy6MWQQkQ7/yPt2catc3iAiJ2wDIjJJRK7IVR4Noz+YQzeGEsdUdYaq1gEfxt2P/tWTpJkEmEM38gK7bdEYMohIs6qWBr8n4x68qQRqcOuWjPTR16nqehF5CpiKe5rwXtyj6G+xy5EEwzgh5tCNIUO6Q/dhh4B34Va77FTVpIicCTygqjNFpB739OtHvX1JJrvcKjGMzESy2qJhDCJSK+8VAsv9CosduPVQMtFbO8PIOebQjSGLH3LpwK1H/1Xcuh/TcXNLyR6SXd9LO8PIOTYpagxJRKQKt1rgcnXjjqOB/araiVsjPPV+1yO4V9el6MnOMCLHxtCNIYOIdABbccMm7bjJze+pe3nDmbilmVtw76RcrKqlfi31R3ATp/cA/5PJLtdaDCMT5tANwzBigg25GIZhxARz6IZhGDHBHLphGEZMMIduGIYRE8yhG4ZhxARz6IZhGDHBHLphGEZM+CtOrd2/TquHOQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "####\n",
    "## Ploting the graph\n",
    "####\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "data.plot(y=['CasosNormalizados', 'Predict_mlp', 'Predict_svr', 'Predict_lr', 'TaxaNormalizadas'], style=['-s', '--o'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
