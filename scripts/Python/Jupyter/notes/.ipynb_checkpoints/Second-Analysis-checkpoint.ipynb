{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "## Celso Antonio Uliana Junior\n",
    "## July 2 2020\n",
    "####\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#####\n",
    "## Consuming and shaping the data to analysis\n",
    "## Covid-19 numbers in Brazil by date\n",
    "## Isolation percentage in Brazil by date\n",
    "#####\n",
    "\n",
    "data_raw_covid = pd.read_csv(\"C:/Users/PCDOMILHAO/Documents/GitHub/trab-siad/scripts/Python/Jupyter/dados/covidBrasil.csv\", sep = \";\", decimal = \",\")\n",
    "data_covid = data_raw_covid['Data'].values.copy()\n",
    "data_covid = data_raw_covid.dropna().set_index(\"Data\")\n",
    "\n",
    "####\n",
    "## Shaping a central pandas dataFrame for all our ML needs\n",
    "####\n",
    "\n",
    "data2 = data_covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Casos</th>\n",
       "      <th>CasosNormalizados</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26/2/20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/3/20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/3/20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6/3/20</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7/3/20</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9/3/20</th>\n",
       "      <td>12</td>\n",
       "      <td>0.000219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/3/20</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/3/20</th>\n",
       "      <td>18</td>\n",
       "      <td>0.000329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13/3/20</th>\n",
       "      <td>25</td>\n",
       "      <td>0.000456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14/3/20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.000383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15/3/20</th>\n",
       "      <td>23</td>\n",
       "      <td>0.000420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16/3/20</th>\n",
       "      <td>79</td>\n",
       "      <td>0.001442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Casos  CasosNormalizados\n",
       "Data                             \n",
       "26/2/20      1           0.000018\n",
       "27/2/20      0           0.000000\n",
       "28/2/20      0           0.000000\n",
       "29/2/20      0           0.000000\n",
       "1/3/20       1           0.000018\n",
       "2/3/20       0           0.000000\n",
       "3/3/20       0           0.000000\n",
       "4/3/20       0           0.000000\n",
       "5/3/20       1           0.000018\n",
       "6/3/20       5           0.000091\n",
       "7/3/20       5           0.000091\n",
       "8/3/20       0           0.000000\n",
       "9/3/20      12           0.000219\n",
       "10/3/20      0           0.000000\n",
       "11/3/20      9           0.000164\n",
       "12/3/20     18           0.000329\n",
       "13/3/20     25           0.000456\n",
       "14/3/20     21           0.000383\n",
       "15/3/20     23           0.000420\n",
       "16/3/20     79           0.001442"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "####\n",
    "## normalizing values for both covid and isolation percentage \n",
    "## between range [0,1] using sklearn MinMaxScaler\n",
    "####\n",
    "\n",
    "covid_norm = data_covid[\"Casos\"].values.copy()\n",
    "covid_norm.shape = (len(covid_norm), 1)\n",
    "\n",
    "####\n",
    "## Shaping the central dataFrame with normalized values\n",
    "####\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "covid_norm = min_max_scaler.fit_transform(covid_norm)\n",
    "\n",
    "data2[\"CasosNormalizados\"] = covid_norm\n",
    "data = data2.copy()\n",
    "#data = data.iloc[20:]\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               E0        E1        E2        E3        E4        E5        E6  \\\n",
      "Data                                                                            \n",
      "26/2/20  0.000018  0.000000  0.000000  0.000000  0.000018  0.000000  0.000000   \n",
      "27/2/20  0.000000  0.000000  0.000000  0.000018  0.000000  0.000000  0.000000   \n",
      "28/2/20  0.000000  0.000000  0.000018  0.000000  0.000000  0.000000  0.000018   \n",
      "29/2/20  0.000000  0.000018  0.000000  0.000000  0.000000  0.000018  0.000091   \n",
      "1/3/20   0.000018  0.000000  0.000000  0.000000  0.000018  0.000091  0.000091   \n",
      "...           ...       ...       ...       ...       ...       ...       ...   \n",
      "10/6/20  0.585912  0.600920  0.555257  0.474375  0.396268  0.312392  0.376970   \n",
      "11/6/20  0.600920  0.555257  0.474375  0.396268  0.312392  0.376970  0.637527   \n",
      "12/6/20  0.555257  0.474375  0.396268  0.312392  0.376970  0.637527  0.587683   \n",
      "13/6/20  0.474375  0.396268  0.312392  0.376970  0.637527  0.587683  0.415640   \n",
      "14/6/20  0.396268  0.312392  0.376970  0.637527  0.587683  0.415640  1.000000   \n",
      "\n",
      "               E7  \n",
      "Data               \n",
      "26/2/20  0.000000  \n",
      "27/2/20  0.000018  \n",
      "28/2/20  0.000091  \n",
      "29/2/20  0.000091  \n",
      "1/3/20   0.000000  \n",
      "...           ...  \n",
      "10/6/20  0.637527  \n",
      "11/6/20  0.587683  \n",
      "12/6/20  0.415640  \n",
      "13/6/20  1.000000  \n",
      "14/6/20  0.632926  \n",
      "\n",
      "[110 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Sliding window\n",
    "####\n",
    "df = pd.DataFrame()\n",
    "window_size = 7\n",
    "for i in range(0, window_size + 1):\n",
    "    df['E{}'.format(i)] = data['CasosNormalizados'].shift(-i)\n",
    "df = df.iloc[: -window_size]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.82578372e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.82578372e-05 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.82578372e-05\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.82578372e-05 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.82578372e-05]\n",
      " [0.00000000e+00 1.82578372e-05 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.82578372e-05 9.12891859e-05]\n",
      " [1.82578372e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.82578372e-05 9.12891859e-05 9.12891859e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.82578372e-05\n",
      "  9.12891859e-05 9.12891859e-05 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.82578372e-05 9.12891859e-05\n",
      "  9.12891859e-05 0.00000000e+00 2.19094046e-04]\n",
      " [0.00000000e+00 1.82578372e-05 9.12891859e-05 9.12891859e-05\n",
      "  0.00000000e+00 2.19094046e-04 0.00000000e+00]\n",
      " [1.82578372e-05 9.12891859e-05 9.12891859e-05 0.00000000e+00\n",
      "  2.19094046e-04 0.00000000e+00 1.64320535e-04]\n",
      " [9.12891859e-05 9.12891859e-05 0.00000000e+00 2.19094046e-04\n",
      "  0.00000000e+00 1.64320535e-04 3.28641069e-04]\n",
      " [9.12891859e-05 0.00000000e+00 2.19094046e-04 0.00000000e+00\n",
      "  1.64320535e-04 3.28641069e-04 4.56445929e-04]\n",
      " [0.00000000e+00 2.19094046e-04 0.00000000e+00 1.64320535e-04\n",
      "  3.28641069e-04 4.56445929e-04 3.83414581e-04]\n",
      " [2.19094046e-04 0.00000000e+00 1.64320535e-04 3.28641069e-04\n",
      "  4.56445929e-04 3.83414581e-04 4.19930255e-04]\n",
      " [0.00000000e+00 1.64320535e-04 3.28641069e-04 4.56445929e-04\n",
      "  3.83414581e-04 4.19930255e-04 1.44236914e-03]\n",
      " [1.64320535e-04 3.28641069e-04 4.56445929e-04 3.83414581e-04\n",
      "  4.19930255e-04 1.44236914e-03 6.20766464e-04]\n",
      " [3.28641069e-04 4.56445929e-04 3.83414581e-04 4.19930255e-04\n",
      "  1.44236914e-03 6.20766464e-04 1.04069672e-03]\n",
      " [4.56445929e-04 3.83414581e-04 4.19930255e-04 1.44236914e-03\n",
      "  6.20766464e-04 1.04069672e-03 2.50132369e-03]\n",
      " [3.83414581e-04 4.19930255e-04 1.44236914e-03 6.20766464e-04\n",
      "  1.04069672e-03 2.50132369e-03 3.52376258e-03]\n",
      " [4.19930255e-04 1.44236914e-03 6.20766464e-04 1.04069672e-03\n",
      "  2.50132369e-03 3.52376258e-03 5.16696792e-03]\n",
      " [1.44236914e-03 6.20766464e-04 1.04069672e-03 2.50132369e-03\n",
      "  3.52376258e-03 5.16696792e-03 4.08975553e-03]\n",
      " [6.20766464e-04 1.04069672e-03 2.50132369e-03 3.52376258e-03\n",
      "  5.16696792e-03 4.08975553e-03 7.63177594e-03]\n",
      " [1.04069672e-03 2.50132369e-03 3.52376258e-03 5.16696792e-03\n",
      "  4.08975553e-03 7.63177594e-03 6.29895383e-03]\n",
      " [2.50132369e-03 3.52376258e-03 5.16696792e-03 4.08975553e-03\n",
      "  7.63177594e-03 6.29895383e-03 5.65992952e-03]\n",
      " [3.52376258e-03 5.16696792e-03 4.08975553e-03 7.63177594e-03\n",
      "  6.29895383e-03 5.65992952e-03 4.23581822e-03]\n",
      " [5.16696792e-03 4.08975553e-03 7.63177594e-03 6.29895383e-03\n",
      "  5.65992952e-03 4.23581822e-03 8.80027752e-03]\n",
      " [4.08975553e-03 7.63177594e-03 6.29895383e-03 5.65992952e-03\n",
      "  4.23581822e-03 8.80027752e-03 9.16543426e-03]\n",
      " [7.63177594e-03 6.29895383e-03 5.65992952e-03 4.23581822e-03\n",
      "  8.80027752e-03 9.16543426e-03 8.89156671e-03]\n",
      " [6.29895383e-03 5.65992952e-03 4.23581822e-03 8.80027752e-03\n",
      "  9.16543426e-03 8.89156671e-03 6.42675869e-03]\n",
      " [5.65992952e-03 4.23581822e-03 8.80027752e-03 9.16543426e-03\n",
      "  8.89156671e-03 6.42675869e-03 5.89728141e-03]\n",
      " [4.23581822e-03 8.80027752e-03 9.16543426e-03 8.89156671e-03\n",
      "  6.42675869e-03 5.89728141e-03 2.07774187e-02]\n",
      " [8.80027752e-03 9.16543426e-03 8.89156671e-03 6.42675869e-03\n",
      "  5.89728141e-03 2.07774187e-02 2.04305198e-02]\n",
      " [9.16543426e-03 8.89156671e-03 6.42675869e-03 5.89728141e-03\n",
      "  2.07774187e-02 2.04305198e-02 1.96089171e-02]\n",
      " [8.89156671e-03 6.42675869e-03 5.89728141e-03 2.07774187e-02\n",
      "  2.04305198e-02 1.96089171e-02 2.09234814e-02]\n",
      " [6.42675869e-03 5.89728141e-03 2.07774187e-02 2.04305198e-02\n",
      "  1.96089171e-02 2.09234814e-02 2.23110770e-02]\n",
      " [5.89728141e-03 2.07774187e-02 2.04305198e-02 1.96089171e-02\n",
      "  2.09234814e-02 2.23110770e-02 1.55556773e-02]\n",
      " [2.07774187e-02 2.04305198e-02 1.96089171e-02 2.09234814e-02\n",
      "  2.23110770e-02 1.55556773e-02 1.69067572e-02]\n",
      " [2.04305198e-02 1.96089171e-02 2.09234814e-02 2.23110770e-02\n",
      "  1.55556773e-02 1.69067572e-02 3.03262676e-02]\n",
      " [1.96089171e-02 2.09234814e-02 2.23110770e-02 1.55556773e-02\n",
      "  1.69067572e-02 3.03262676e-02 4.03498202e-02]\n",
      " [2.09234814e-02 2.23110770e-02 1.55556773e-02 1.69067572e-02\n",
      "  3.03262676e-02 4.03498202e-02 3.52376258e-02]\n",
      " [2.23110770e-02 1.55556773e-02 1.69067572e-02 3.03262676e-02\n",
      "  4.03498202e-02 3.52376258e-02 3.25172080e-02]\n",
      " [1.55556773e-02 1.69067572e-02 3.03262676e-02 4.03498202e-02\n",
      "  3.52376258e-02 3.25172080e-02 1.98827847e-02]\n",
      " [1.69067572e-02 3.03262676e-02 4.03498202e-02 3.52376258e-02\n",
      "  3.25172080e-02 1.98827847e-02 2.63278012e-02]\n",
      " [3.03262676e-02 4.03498202e-02 3.52376258e-02 3.25172080e-02\n",
      "  1.98827847e-02 2.63278012e-02 2.30231327e-02]\n",
      " [4.03498202e-02 3.52376258e-02 3.25172080e-02 1.98827847e-02\n",
      "  2.63278012e-02 2.30231327e-02 3.34483577e-02]\n",
      " [3.52376258e-02 3.25172080e-02 1.98827847e-02 2.63278012e-02\n",
      "  2.30231327e-02 3.34483577e-02 5.58324661e-02]\n",
      " [3.25172080e-02 1.98827847e-02 2.63278012e-02 2.30231327e-02\n",
      "  3.34483577e-02 5.58324661e-02 3.84327473e-02]\n",
      " [1.98827847e-02 2.63278012e-02 2.30231327e-02 3.34483577e-02\n",
      "  5.58324661e-02 3.84327473e-02 5.94657757e-02]\n",
      " [2.63278012e-02 2.30231327e-02 3.34483577e-02 5.58324661e-02\n",
      "  3.84327473e-02 5.94657757e-02 5.32581110e-02]\n",
      " [2.30231327e-02 3.34483577e-02 5.58324661e-02 3.84327473e-02\n",
      "  5.94657757e-02 5.32581110e-02 3.75198554e-02]\n",
      " [3.34483577e-02 5.58324661e-02 3.84327473e-02 5.94657757e-02\n",
      "  5.32581110e-02 3.75198554e-02 3.51828522e-02]\n",
      " [5.58324661e-02 3.84327473e-02 5.94657757e-02 5.32581110e-02\n",
      "  3.75198554e-02 3.51828522e-02 4.56080773e-02]\n",
      " [3.84327473e-02 5.94657757e-02 5.32581110e-02 3.75198554e-02\n",
      "  3.51828522e-02 4.56080773e-02 4.88944880e-02]\n",
      " [5.94657757e-02 5.32581110e-02 3.75198554e-02 3.51828522e-02\n",
      "  4.56080773e-02 4.88944880e-02 6.81930219e-02]\n",
      " [5.32581110e-02 3.75198554e-02 3.51828522e-02 4.56080773e-02\n",
      "  4.88944880e-02 6.81930219e-02 6.39572036e-02]\n",
      " [3.75198554e-02 3.51828522e-02 4.56080773e-02 4.88944880e-02\n",
      "  6.81930219e-02 6.39572036e-02 1.00673714e-01]\n",
      " [3.51828522e-02 4.56080773e-02 4.88944880e-02 6.81930219e-02\n",
      "  6.39572036e-02 1.00673714e-01 6.16932318e-02]\n",
      " [4.56080773e-02 4.88944880e-02 6.81930219e-02 6.39572036e-02\n",
      "  1.00673714e-01 6.16932318e-02 8.42234029e-02]\n",
      " [4.88944880e-02 6.81930219e-02 6.39572036e-02 1.00673714e-01\n",
      "  6.16932318e-02 8.42234029e-02 9.83184532e-02]\n",
      " [6.81930219e-02 6.39572036e-02 1.00673714e-01 6.16932318e-02\n",
      "  8.42234029e-02 9.83184532e-02 1.14586186e-01]\n",
      " [6.39572036e-02 1.00673714e-01 6.16932318e-02 8.42234029e-02\n",
      "  9.83184532e-02 1.14586186e-01 1.31785069e-01]\n",
      " [1.00673714e-01 6.16932318e-02 8.42234029e-02 9.83184532e-02\n",
      "  1.14586186e-01 1.31785069e-01 1.13362911e-01]\n",
      " [6.16932318e-02 8.42234029e-02 9.83184532e-02 1.14586186e-01\n",
      "  1.31785069e-01 1.13362911e-01 9.07414508e-02]\n",
      " [8.42234029e-02 9.83184532e-02 1.14586186e-01 1.31785069e-01\n",
      "  1.13362911e-01 9.07414508e-02 8.37669570e-02]\n",
      " [9.83184532e-02 1.14586186e-01 1.31785069e-01 1.13362911e-01\n",
      "  9.07414508e-02 8.37669570e-02 1.21104234e-01]\n",
      " [1.14586186e-01 1.31785069e-01 1.13362911e-01 9.07414508e-02\n",
      "  8.37669570e-02 1.21104234e-01 1.26618101e-01]\n",
      " [1.31785069e-01 1.13362911e-01 9.07414508e-02 8.37669570e-02\n",
      "  1.21104234e-01 1.26618101e-01 1.91762064e-01]\n",
      " [1.13362911e-01 9.07414508e-02 8.37669570e-02 1.21104234e-01\n",
      "  1.26618101e-01 1.91762064e-01 1.80533494e-01]\n",
      " [9.07414508e-02 8.37669570e-02 1.21104234e-01 1.26618101e-01\n",
      "  1.91762064e-01 1.80533494e-01 1.86631612e-01]\n",
      " [8.37669570e-02 1.21104234e-01 1.26618101e-01 1.91762064e-01\n",
      "  1.80533494e-01 1.86631612e-01 1.93733910e-01]\n",
      " [1.21104234e-01 1.26618101e-01 1.91762064e-01 1.80533494e-01\n",
      "  1.86631612e-01 1.93733910e-01 1.23422979e-01]\n",
      " [1.26618101e-01 1.91762064e-01 1.80533494e-01 1.86631612e-01\n",
      "  1.93733910e-01 1.23422979e-01 1.02828139e-01]\n",
      " [1.91762064e-01 1.80533494e-01 1.86631612e-01 1.93733910e-01\n",
      "  1.23422979e-01 1.02828139e-01 1.69031057e-01]\n",
      " [1.80533494e-01 1.86631612e-01 1.93733910e-01 1.23422979e-01\n",
      "  1.02828139e-01 1.69031057e-01 2.07865476e-01]\n",
      " [1.86631612e-01 1.93733910e-01 1.23422979e-01 1.02828139e-01\n",
      "  1.69031057e-01 2.07865476e-01 2.54587282e-01]\n",
      " [1.93733910e-01 1.23422979e-01 1.02828139e-01 1.69031057e-01\n",
      "  2.07865476e-01 2.54587282e-01 2.79436198e-01]\n",
      " [1.23422979e-01 1.02828139e-01 1.69031057e-01 2.07865476e-01\n",
      "  2.54587282e-01 2.79436198e-01 2.72388673e-01]\n",
      " [1.02828139e-01 1.69031057e-01 2.07865476e-01 2.54587282e-01\n",
      "  2.79436198e-01 2.72388673e-01 1.44930712e-01]\n",
      " [1.69031057e-01 2.07865476e-01 2.54587282e-01 2.79436198e-01\n",
      "  2.72388673e-01 1.44930712e-01 2.39907981e-01]\n",
      " [2.07865476e-01 2.54587282e-01 2.79436198e-01 2.72388673e-01\n",
      "  1.44930712e-01 2.39907981e-01 3.17832430e-01]\n",
      " [2.54587282e-01 2.79436198e-01 2.72388673e-01 1.44930712e-01\n",
      "  2.39907981e-01 3.17832430e-01 3.64262110e-01]\n",
      " [2.79436198e-01 2.72388673e-01 1.44930712e-01 2.39907981e-01\n",
      "  3.17832430e-01 3.64262110e-01 3.37916050e-01]\n",
      " [2.72388673e-01 1.44930712e-01 2.39907981e-01 3.17832430e-01\n",
      "  3.64262110e-01 3.37916050e-01 3.79817787e-01]\n",
      " [1.44930712e-01 2.39907981e-01 3.17832430e-01 3.64262110e-01\n",
      "  3.37916050e-01 3.79817787e-01 3.01400376e-01]\n",
      " [2.39907981e-01 3.17832430e-01 3.64262110e-01 3.37916050e-01\n",
      "  3.79817787e-01 3.01400376e-01 2.88711179e-01]\n",
      " [3.17832430e-01 3.64262110e-01 3.37916050e-01 3.79817787e-01\n",
      "  3.01400376e-01 2.88711179e-01 2.13379343e-01]\n",
      " [3.64262110e-01 3.37916050e-01 3.79817787e-01 3.01400376e-01\n",
      "  2.88711179e-01 2.13379343e-01 2.98040934e-01]\n",
      " [3.37916050e-01 3.79817787e-01 3.01400376e-01 2.88711179e-01\n",
      "  2.13379343e-01 2.98040934e-01 3.76093188e-01]\n",
      " [3.79817787e-01 3.01400376e-01 2.88711179e-01 2.13379343e-01\n",
      "  2.98040934e-01 3.76093188e-01 4.82317285e-01]\n",
      " [3.01400376e-01 2.88711179e-01 2.13379343e-01 2.98040934e-01\n",
      "  3.76093188e-01 4.82317285e-01 4.91647039e-01]\n",
      " [2.88711179e-01 2.13379343e-01 2.98040934e-01 3.76093188e-01\n",
      "  4.82317285e-01 4.91647039e-01 6.07511274e-01]\n",
      " [2.13379343e-01 2.98040934e-01 3.76093188e-01 4.82317285e-01\n",
      "  4.91647039e-01 6.07511274e-01 2.99592850e-01]\n",
      " [2.98040934e-01 3.76093188e-01 4.82317285e-01 4.91647039e-01\n",
      "  6.07511274e-01 2.99592850e-01 2.11754396e-01]\n",
      " [3.76093188e-01 4.82317285e-01 4.91647039e-01 6.07511274e-01\n",
      "  2.99592850e-01 2.11754396e-01 5.28308777e-01]\n",
      " [4.82317285e-01 4.91647039e-01 6.07511274e-01 2.99592850e-01\n",
      "  2.11754396e-01 5.28308777e-01 5.22776652e-01]\n",
      " [4.91647039e-01 6.07511274e-01 2.99592850e-01 2.11754396e-01\n",
      "  5.28308777e-01 5.22776652e-01 5.64459294e-01]\n",
      " [6.07511274e-01 2.99592850e-01 2.11754396e-01 5.28308777e-01\n",
      "  5.22776652e-01 5.64459294e-01 5.62889120e-01]\n",
      " [2.99592850e-01 2.11754396e-01 5.28308777e-01 5.22776652e-01\n",
      "  5.64459294e-01 5.62889120e-01 4.94330942e-01]\n",
      " [2.11754396e-01 5.28308777e-01 5.22776652e-01 5.64459294e-01\n",
      "  5.62889120e-01 4.94330942e-01 3.45456537e-01]\n",
      " [5.28308777e-01 5.22776652e-01 5.64459294e-01 5.62889120e-01\n",
      "  4.94330942e-01 3.45456537e-01 2.85808183e-01]\n",
      " [5.22776652e-01 5.64459294e-01 5.62889120e-01 4.94330942e-01\n",
      "  3.45456537e-01 2.85808183e-01 5.85912253e-01]\n",
      " [5.64459294e-01 5.62889120e-01 4.94330942e-01 3.45456537e-01\n",
      "  2.85808183e-01 5.85912253e-01 6.00920195e-01]\n",
      " [5.62889120e-01 4.94330942e-01 3.45456537e-01 2.85808183e-01\n",
      "  5.85912253e-01 6.00920195e-01 5.55257344e-01]\n",
      " [4.94330942e-01 3.45456537e-01 2.85808183e-01 5.85912253e-01\n",
      "  6.00920195e-01 5.55257344e-01 4.74375126e-01]\n",
      " [3.45456537e-01 2.85808183e-01 5.85912253e-01 6.00920195e-01\n",
      "  5.55257344e-01 4.74375126e-01 3.96268098e-01]\n",
      " [2.85808183e-01 5.85912253e-01 6.00920195e-01 5.55257344e-01\n",
      "  4.74375126e-01 3.96268098e-01 3.12391594e-01]\n",
      " [5.85912253e-01 6.00920195e-01 5.55257344e-01 4.74375126e-01\n",
      "  3.96268098e-01 3.12391594e-01 3.76969564e-01]\n",
      " [6.00920195e-01 5.55257344e-01 4.74375126e-01 3.96268098e-01\n",
      "  3.12391594e-01 3.76969564e-01 6.37527159e-01]\n",
      " [5.55257344e-01 4.74375126e-01 3.96268098e-01 3.12391594e-01\n",
      "  3.76969564e-01 6.37527159e-01 5.87683263e-01]\n",
      " [4.74375126e-01 3.96268098e-01 3.12391594e-01 3.76969564e-01\n",
      "  6.37527159e-01 5.87683263e-01 4.15639663e-01]\n",
      " [3.96268098e-01 3.12391594e-01 3.76969564e-01 6.37527159e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5.87683263e-01 4.15639663e-01 1.00000000e+00]]\n",
      "[0.00000000e+00 1.82578372e-05 9.12891859e-05 9.12891859e-05\n",
      " 0.00000000e+00 2.19094046e-04 0.00000000e+00 1.64320535e-04\n",
      " 3.28641069e-04 4.56445929e-04 3.83414581e-04 4.19930255e-04\n",
      " 1.44236914e-03 6.20766464e-04 1.04069672e-03 2.50132369e-03\n",
      " 3.52376258e-03 5.16696792e-03 4.08975553e-03 7.63177594e-03\n",
      " 6.29895383e-03 5.65992952e-03 4.23581822e-03 8.80027752e-03\n",
      " 9.16543426e-03 8.89156671e-03 6.42675869e-03 5.89728141e-03\n",
      " 2.07774187e-02 2.04305198e-02 1.96089171e-02 2.09234814e-02\n",
      " 2.23110770e-02 1.55556773e-02 1.69067572e-02 3.03262676e-02\n",
      " 4.03498202e-02 3.52376258e-02 3.25172080e-02 1.98827847e-02\n",
      " 2.63278012e-02 2.30231327e-02 3.34483577e-02 5.58324661e-02\n",
      " 3.84327473e-02 5.94657757e-02 5.32581110e-02 3.75198554e-02\n",
      " 3.51828522e-02 4.56080773e-02 4.88944880e-02 6.81930219e-02\n",
      " 6.39572036e-02 1.00673714e-01 6.16932318e-02 8.42234029e-02\n",
      " 9.83184532e-02 1.14586186e-01 1.31785069e-01 1.13362911e-01\n",
      " 9.07414508e-02 8.37669570e-02 1.21104234e-01 1.26618101e-01\n",
      " 1.91762064e-01 1.80533494e-01 1.86631612e-01 1.93733910e-01\n",
      " 1.23422979e-01 1.02828139e-01 1.69031057e-01 2.07865476e-01\n",
      " 2.54587282e-01 2.79436198e-01 2.72388673e-01 1.44930712e-01\n",
      " 2.39907981e-01 3.17832430e-01 3.64262110e-01 3.37916050e-01\n",
      " 3.79817787e-01 3.01400376e-01 2.88711179e-01 2.13379343e-01\n",
      " 2.98040934e-01 3.76093188e-01 4.82317285e-01 4.91647039e-01\n",
      " 6.07511274e-01 2.99592850e-01 2.11754396e-01 5.28308777e-01\n",
      " 5.22776652e-01 5.64459294e-01 5.62889120e-01 4.94330942e-01\n",
      " 3.45456537e-01 2.85808183e-01 5.85912253e-01 6.00920195e-01\n",
      " 5.55257344e-01 4.74375126e-01 3.96268098e-01 3.12391594e-01\n",
      " 3.76969564e-01 6.37527159e-01 5.87683263e-01 4.15639663e-01\n",
      " 1.00000000e+00 6.32926184e-01]\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Manipulating the data to split into X(a window size of values)\n",
    "## and target, or Y, the value X \"produces\"\n",
    "####\n",
    "\n",
    "arr = df.values\n",
    "\n",
    "X = arr[:, : -1]\n",
    "target = arr[:, -1]\n",
    "print(X)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.05712943\n",
      "Iteration 2, loss = 0.02704520\n",
      "Iteration 3, loss = 0.01275256\n",
      "Iteration 4, loss = 0.01177414\n",
      "Iteration 5, loss = 0.01615151\n",
      "Iteration 6, loss = 0.01765848\n",
      "Iteration 7, loss = 0.01465899\n",
      "Iteration 8, loss = 0.00976919\n",
      "Iteration 9, loss = 0.00579194\n",
      "Iteration 10, loss = 0.00416758\n",
      "Iteration 11, loss = 0.00482966\n",
      "Iteration 12, loss = 0.00665769\n",
      "Iteration 13, loss = 0.00830555\n",
      "Iteration 14, loss = 0.00892619\n",
      "Iteration 15, loss = 0.00840973\n",
      "Iteration 16, loss = 0.00716727\n",
      "Iteration 17, loss = 0.00578697\n",
      "Iteration 18, loss = 0.00477423\n",
      "Iteration 19, loss = 0.00438674\n",
      "Iteration 20, loss = 0.00456287\n",
      "Iteration 21, loss = 0.00500301\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712943\n",
      "Iteration 2, loss = 0.02704520\n",
      "Iteration 3, loss = 0.01275257\n",
      "Iteration 4, loss = 0.01177415\n",
      "Iteration 5, loss = 0.01615151\n",
      "Iteration 6, loss = 0.01765848\n",
      "Iteration 7, loss = 0.01465898\n",
      "Iteration 8, loss = 0.00976918\n",
      "Iteration 9, loss = 0.00579193\n",
      "Iteration 10, loss = 0.00416758\n",
      "Iteration 11, loss = 0.00482966\n",
      "Iteration 12, loss = 0.00665770\n",
      "Iteration 13, loss = 0.00830556\n",
      "Iteration 14, loss = 0.00892619\n",
      "Iteration 15, loss = 0.00840972\n",
      "Iteration 16, loss = 0.00716727\n",
      "Iteration 17, loss = 0.00578696\n",
      "Iteration 18, loss = 0.00477423\n",
      "Iteration 19, loss = 0.00438674\n",
      "Iteration 20, loss = 0.00456287\n",
      "Iteration 21, loss = 0.00500302\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712940\n",
      "Iteration 2, loss = 0.02704524\n",
      "Iteration 3, loss = 0.01275264\n",
      "Iteration 4, loss = 0.01177421\n",
      "Iteration 5, loss = 0.01615152\n",
      "Iteration 6, loss = 0.01765843\n",
      "Iteration 7, loss = 0.01465892\n",
      "Iteration 8, loss = 0.00976913\n",
      "Iteration 9, loss = 0.00579191\n",
      "Iteration 10, loss = 0.00416759\n",
      "Iteration 11, loss = 0.00482968\n",
      "Iteration 12, loss = 0.00665773\n",
      "Iteration 13, loss = 0.00830558\n",
      "Iteration 14, loss = 0.00892619\n",
      "Iteration 15, loss = 0.00840971\n",
      "Iteration 16, loss = 0.00716725\n",
      "Iteration 17, loss = 0.00578694\n",
      "Iteration 18, loss = 0.00477422\n",
      "Iteration 19, loss = 0.00438675\n",
      "Iteration 20, loss = 0.00456288\n",
      "Iteration 21, loss = 0.00500302\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712941\n",
      "Iteration 2, loss = 0.02704523\n",
      "Iteration 3, loss = 0.01275263\n",
      "Iteration 4, loss = 0.01177419\n",
      "Iteration 5, loss = 0.01615151\n",
      "Iteration 6, loss = 0.01765843\n",
      "Iteration 7, loss = 0.01465893\n",
      "Iteration 8, loss = 0.00976914\n",
      "Iteration 9, loss = 0.00579191\n",
      "Iteration 10, loss = 0.00416759\n",
      "Iteration 11, loss = 0.00482968\n",
      "Iteration 12, loss = 0.00665772\n",
      "Iteration 13, loss = 0.00830557\n",
      "Iteration 14, loss = 0.00892619\n",
      "Iteration 15, loss = 0.00840972\n",
      "Iteration 16, loss = 0.00716726\n",
      "Iteration 17, loss = 0.00578695\n",
      "Iteration 18, loss = 0.00477422\n",
      "Iteration 19, loss = 0.00438675\n",
      "Iteration 20, loss = 0.00456288\n",
      "Iteration 21, loss = 0.00500302\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712944\n",
      "Iteration 2, loss = 0.02704518\n",
      "Iteration 3, loss = 0.01275252\n",
      "Iteration 4, loss = 0.01177409\n",
      "Iteration 5, loss = 0.01615149\n",
      "Iteration 6, loss = 0.01765849\n",
      "Iteration 7, loss = 0.01465901\n",
      "Iteration 8, loss = 0.00976921\n",
      "Iteration 9, loss = 0.00579195\n",
      "Iteration 10, loss = 0.00416759\n",
      "Iteration 11, loss = 0.00482965\n",
      "Iteration 12, loss = 0.00665768\n",
      "Iteration 13, loss = 0.00830554\n",
      "Iteration 14, loss = 0.00892619\n",
      "Iteration 15, loss = 0.00840974\n",
      "Iteration 16, loss = 0.00716730\n",
      "Iteration 17, loss = 0.00578699\n",
      "Iteration 18, loss = 0.00477424\n",
      "Iteration 19, loss = 0.00438674\n",
      "Iteration 20, loss = 0.00456286\n",
      "Iteration 21, loss = 0.00500300\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712936\n",
      "Iteration 2, loss = 0.02704528\n",
      "Iteration 3, loss = 0.01275273\n",
      "Iteration 4, loss = 0.01177427\n",
      "Iteration 5, loss = 0.01615151\n",
      "Iteration 6, loss = 0.01765837\n",
      "Iteration 7, loss = 0.01465885\n",
      "Iteration 8, loss = 0.00976908\n",
      "Iteration 9, loss = 0.00579188\n",
      "Iteration 10, loss = 0.00416759\n",
      "Iteration 11, loss = 0.00482971\n",
      "Iteration 12, loss = 0.00665776\n",
      "Iteration 13, loss = 0.00830560\n",
      "Iteration 14, loss = 0.00892620\n",
      "Iteration 15, loss = 0.00840970\n",
      "Iteration 16, loss = 0.00716723\n",
      "Iteration 17, loss = 0.00578693\n",
      "Iteration 18, loss = 0.00477422\n",
      "Iteration 19, loss = 0.00438675\n",
      "Iteration 20, loss = 0.00456289\n",
      "Iteration 21, loss = 0.00500303\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712945\n",
      "Iteration 2, loss = 0.02704518\n",
      "Iteration 3, loss = 0.01275250\n",
      "Iteration 4, loss = 0.01177406\n",
      "Iteration 5, loss = 0.01615147\n",
      "Iteration 6, loss = 0.01765848\n",
      "Iteration 7, loss = 0.01465902\n",
      "Iteration 8, loss = 0.00976922\n",
      "Iteration 9, loss = 0.00579196\n",
      "Iteration 10, loss = 0.00416759\n",
      "Iteration 11, loss = 0.00482964\n",
      "Iteration 12, loss = 0.00665767\n",
      "Iteration 13, loss = 0.00830554\n",
      "Iteration 14, loss = 0.00892620\n",
      "Iteration 15, loss = 0.00840977\n",
      "Iteration 16, loss = 0.00716732\n",
      "Iteration 17, loss = 0.00578701\n",
      "Iteration 18, loss = 0.00477425\n",
      "Iteration 19, loss = 0.00438674\n",
      "Iteration 20, loss = 0.00456286\n",
      "Iteration 21, loss = 0.00500300\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712939\n",
      "Iteration 2, loss = 0.02704524\n",
      "Iteration 3, loss = 0.01275264\n",
      "Iteration 4, loss = 0.01177418\n",
      "Iteration 5, loss = 0.01615148\n",
      "Iteration 6, loss = 0.01765840\n",
      "Iteration 7, loss = 0.01465891\n",
      "Iteration 8, loss = 0.00976914\n",
      "Iteration 9, loss = 0.00579191\n",
      "Iteration 10, loss = 0.00416759\n",
      "Iteration 11, loss = 0.00482968\n",
      "Iteration 12, loss = 0.00665772\n",
      "Iteration 13, loss = 0.00830557\n",
      "Iteration 14, loss = 0.00892620\n",
      "Iteration 15, loss = 0.00840973\n",
      "Iteration 16, loss = 0.00716728\n",
      "Iteration 17, loss = 0.00578697\n",
      "Iteration 18, loss = 0.00477424\n",
      "Iteration 19, loss = 0.00438675\n",
      "Iteration 20, loss = 0.00456287\n",
      "Iteration 21, loss = 0.00500302\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712931\n",
      "Iteration 2, loss = 0.02704534\n",
      "Iteration 3, loss = 0.01275285\n",
      "Iteration 4, loss = 0.01177435\n",
      "Iteration 5, loss = 0.01615148\n",
      "Iteration 6, loss = 0.01765827\n",
      "Iteration 7, loss = 0.01465874\n",
      "Iteration 8, loss = 0.00976900\n",
      "Iteration 9, loss = 0.00579185\n",
      "Iteration 10, loss = 0.00416759\n",
      "Iteration 11, loss = 0.00482973\n",
      "Iteration 12, loss = 0.00665778\n",
      "Iteration 13, loss = 0.00830561\n",
      "Iteration 14, loss = 0.00892620\n",
      "Iteration 15, loss = 0.00840970\n",
      "Iteration 16, loss = 0.00716723\n",
      "Iteration 17, loss = 0.00578693\n",
      "Iteration 18, loss = 0.00477422\n",
      "Iteration 19, loss = 0.00438677\n",
      "Iteration 20, loss = 0.00456290\n",
      "Iteration 21, loss = 0.00500304\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712931\n",
      "Iteration 2, loss = 0.02704534\n",
      "Iteration 3, loss = 0.01275286\n",
      "Iteration 4, loss = 0.01177434\n",
      "Iteration 5, loss = 0.01615145\n",
      "Iteration 6, loss = 0.01765823\n",
      "Iteration 7, loss = 0.01465871\n",
      "Iteration 8, loss = 0.00976898\n",
      "Iteration 9, loss = 0.00579184\n",
      "Iteration 10, loss = 0.00416760\n",
      "Iteration 11, loss = 0.00482975\n",
      "Iteration 12, loss = 0.00665780\n",
      "Iteration 13, loss = 0.00830563\n",
      "Iteration 14, loss = 0.00892622\n",
      "Iteration 15, loss = 0.00840972\n",
      "Iteration 16, loss = 0.00716724\n",
      "Iteration 17, loss = 0.00578694\n",
      "Iteration 18, loss = 0.00477432\n",
      "Iteration 19, loss = 0.00438686\n",
      "Iteration 20, loss = 0.00456297\n",
      "Iteration 21, loss = 0.00500307\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712933\n",
      "Iteration 2, loss = 0.02704531\n",
      "Iteration 3, loss = 0.01275277\n",
      "Iteration 4, loss = 0.01177425\n",
      "Iteration 5, loss = 0.01615141\n",
      "Iteration 6, loss = 0.01765826\n",
      "Iteration 7, loss = 0.01465878\n",
      "Iteration 8, loss = 0.00976904\n",
      "Iteration 9, loss = 0.00579188\n",
      "Iteration 10, loss = 0.00416760\n",
      "Iteration 11, loss = 0.00482971\n",
      "Iteration 12, loss = 0.00665774\n",
      "Iteration 13, loss = 0.00830558\n",
      "Iteration 14, loss = 0.00892622\n",
      "Iteration 15, loss = 0.00840975\n",
      "Iteration 16, loss = 0.00716729\n",
      "Iteration 17, loss = 0.00578699\n",
      "Iteration 18, loss = 0.00477435\n",
      "Iteration 19, loss = 0.00438686\n",
      "Iteration 20, loss = 0.00456295\n",
      "Iteration 21, loss = 0.00500305\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712934\n",
      "Iteration 2, loss = 0.02704530\n",
      "Iteration 3, loss = 0.01275274\n",
      "Iteration 4, loss = 0.01177419\n",
      "Iteration 5, loss = 0.01615137\n",
      "Iteration 6, loss = 0.01765826\n",
      "Iteration 7, loss = 0.01465879\n",
      "Iteration 8, loss = 0.00976906\n",
      "Iteration 9, loss = 0.00579189\n",
      "Iteration 10, loss = 0.00416761\n",
      "Iteration 11, loss = 0.00482971\n",
      "Iteration 12, loss = 0.00665774\n",
      "Iteration 13, loss = 0.00830558\n",
      "Iteration 14, loss = 0.00892624\n",
      "Iteration 15, loss = 0.00840978\n",
      "Iteration 16, loss = 0.00716733\n",
      "Iteration 17, loss = 0.00578702\n",
      "Iteration 18, loss = 0.00477437\n",
      "Iteration 19, loss = 0.00438687\n",
      "Iteration 20, loss = 0.00456294\n",
      "Iteration 21, loss = 0.00500304\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712898\n",
      "Iteration 2, loss = 0.02704572\n",
      "Iteration 3, loss = 0.01275367\n",
      "Iteration 4, loss = 0.01177495\n",
      "Iteration 5, loss = 0.01615139\n",
      "Iteration 6, loss = 0.01765768\n",
      "Iteration 7, loss = 0.01465805\n",
      "Iteration 8, loss = 0.00976847\n",
      "Iteration 9, loss = 0.00579159\n",
      "Iteration 10, loss = 0.00416762\n",
      "Iteration 11, loss = 0.00482998\n",
      "Iteration 12, loss = 0.00665807\n",
      "Iteration 13, loss = 0.00830580\n",
      "Iteration 14, loss = 0.00892625\n",
      "Iteration 15, loss = 0.00840959\n",
      "Iteration 16, loss = 0.00716707\n",
      "Iteration 17, loss = 0.00578680\n",
      "Iteration 18, loss = 0.00477427\n",
      "Iteration 19, loss = 0.00438691\n",
      "Iteration 20, loss = 0.00456307\n",
      "Iteration 21, loss = 0.00500315\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.02704531\n",
      "Iteration 3, loss = 0.01275271\n",
      "Iteration 4, loss = 0.01177408\n",
      "Iteration 5, loss = 0.01615121\n",
      "Iteration 6, loss = 0.01765813\n",
      "Iteration 7, loss = 0.01465873\n",
      "Iteration 8, loss = 0.00976904\n",
      "Iteration 9, loss = 0.00579190\n",
      "Iteration 10, loss = 0.00416762\n",
      "Iteration 11, loss = 0.00482970\n",
      "Iteration 12, loss = 0.00665770\n",
      "Iteration 13, loss = 0.00830557\n",
      "Iteration 14, loss = 0.00892629\n",
      "Iteration 15, loss = 0.00840990\n",
      "Iteration 16, loss = 0.00716746\n",
      "Iteration 17, loss = 0.00578715\n",
      "Iteration 18, loss = 0.00477444\n",
      "Iteration 19, loss = 0.00438689\n",
      "Iteration 20, loss = 0.00456292\n",
      "Iteration 21, loss = 0.00500301\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712919\n",
      "Iteration 2, loss = 0.02704546\n",
      "Iteration 3, loss = 0.01275302\n",
      "Iteration 4, loss = 0.01177430\n",
      "Iteration 5, loss = 0.01615117\n",
      "Iteration 6, loss = 0.01765792\n",
      "Iteration 7, loss = 0.01465850\n",
      "Iteration 8, loss = 0.00976888\n",
      "Iteration 9, loss = 0.00579183\n",
      "Iteration 10, loss = 0.00416763\n",
      "Iteration 11, loss = 0.00482977\n",
      "Iteration 12, loss = 0.00665777\n",
      "Iteration 13, loss = 0.00830560\n",
      "Iteration 14, loss = 0.00892630\n",
      "Iteration 15, loss = 0.00840985\n",
      "Iteration 16, loss = 0.00716741\n",
      "Iteration 17, loss = 0.00578712\n",
      "Iteration 18, loss = 0.00477444\n",
      "Iteration 19, loss = 0.00438691\n",
      "Iteration 20, loss = 0.00456295\n",
      "Iteration 21, loss = 0.00500303\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712858\n",
      "Iteration 2, loss = 0.02704612\n",
      "Iteration 3, loss = 0.01275449\n",
      "Iteration 4, loss = 0.01177548\n",
      "Iteration 5, loss = 0.01615117\n",
      "Iteration 6, loss = 0.01765697\n",
      "Iteration 7, loss = 0.01465729\n",
      "Iteration 8, loss = 0.00976792\n",
      "Iteration 9, loss = 0.00579133\n",
      "Iteration 10, loss = 0.00416765\n",
      "Iteration 11, loss = 0.00483017\n",
      "Iteration 12, loss = 0.00665827\n",
      "Iteration 13, loss = 0.00830591\n",
      "Iteration 14, loss = 0.00892628\n",
      "Iteration 15, loss = 0.00840958\n",
      "Iteration 16, loss = 0.00716703\n",
      "Iteration 17, loss = 0.00578679\n",
      "Iteration 18, loss = 0.00477430\n",
      "Iteration 19, loss = 0.00438699\n",
      "Iteration 20, loss = 0.00456315\n",
      "Iteration 21, loss = 0.00500320\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712842\n",
      "Iteration 2, loss = 0.02704633\n",
      "Iteration 3, loss = 0.01275492\n",
      "Iteration 4, loss = 0.01177571\n",
      "Iteration 5, loss = 0.01615097\n",
      "Iteration 6, loss = 0.01765651\n",
      "Iteration 7, loss = 0.01465682\n",
      "Iteration 8, loss = 0.00976756\n",
      "Iteration 9, loss = 0.00579116\n",
      "Iteration 10, loss = 0.00416768\n",
      "Iteration 11, loss = 0.00483035\n",
      "Iteration 12, loss = 0.00665849\n",
      "Iteration 13, loss = 0.00830609\n",
      "Iteration 14, loss = 0.00892641\n",
      "Iteration 15, loss = 0.00840963\n",
      "Iteration 16, loss = 0.00716702\n",
      "Iteration 17, loss = 0.00578677\n",
      "Iteration 18, loss = 0.00477429\n",
      "Iteration 19, loss = 0.00438701\n",
      "Iteration 20, loss = 0.00456318\n",
      "Iteration 21, loss = 0.00500320\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712786\n",
      "Iteration 2, loss = 0.02704689\n",
      "Iteration 3, loss = 0.01275613\n",
      "Iteration 4, loss = 0.01177655\n",
      "Iteration 5, loss = 0.01615075\n",
      "Iteration 6, loss = 0.01765556\n",
      "Iteration 7, loss = 0.01465575\n",
      "Iteration 8, loss = 0.00976676\n",
      "Iteration 9, loss = 0.00579078\n",
      "Iteration 10, loss = 0.00416770\n",
      "Iteration 11, loss = 0.00483064\n",
      "Iteration 12, loss = 0.00665881\n",
      "Iteration 13, loss = 0.00830627\n",
      "Iteration 14, loss = 0.00892642\n",
      "Iteration 15, loss = 0.00840946\n",
      "Iteration 16, loss = 0.00716680\n",
      "Iteration 17, loss = 0.00578661\n",
      "Iteration 18, loss = 0.00477424\n",
      "Iteration 19, loss = 0.00438708\n",
      "Iteration 20, loss = 0.00456328\n",
      "Iteration 21, loss = 0.00500324\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712843\n",
      "Iteration 2, loss = 0.02704626\n",
      "Iteration 3, loss = 0.01275448\n",
      "Iteration 4, loss = 0.01177488\n",
      "Iteration 5, loss = 0.01615021\n",
      "Iteration 6, loss = 0.01765619\n",
      "Iteration 7, loss = 0.01465693\n",
      "Iteration 8, loss = 0.00976782\n",
      "Iteration 9, loss = 0.00579140\n",
      "Iteration 10, loss = 0.00416776\n",
      "Iteration 11, loss = 0.00483017\n",
      "Iteration 12, loss = 0.00665811\n",
      "Iteration 13, loss = 0.00830583\n",
      "Iteration 14, loss = 0.00892668\n",
      "Iteration 15, loss = 0.00841018\n",
      "Iteration 16, loss = 0.00716772\n",
      "Iteration 17, loss = 0.00578746\n",
      "Iteration 18, loss = 0.00477468\n",
      "Iteration 19, loss = 0.00438709\n",
      "Iteration 20, loss = 0.00456302\n",
      "Iteration 21, loss = 0.00500301\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712727\n",
      "Iteration 2, loss = 0.02704740\n",
      "Iteration 3, loss = 0.01275705\n",
      "Iteration 4, loss = 0.01177680\n",
      "Iteration 5, loss = 0.01614996\n",
      "Iteration 6, loss = 0.01765432\n",
      "Iteration 7, loss = 0.01465471\n",
      "Iteration 8, loss = 0.00976610\n",
      "Iteration 9, loss = 0.00579055\n",
      "Iteration 10, loss = 0.00416782\n",
      "Iteration 11, loss = 0.00483090\n",
      "Iteration 12, loss = 0.00665901\n",
      "Iteration 13, loss = 0.00830642\n",
      "Iteration 14, loss = 0.00892675\n",
      "Iteration 15, loss = 0.00840973\n",
      "Iteration 16, loss = 0.00716709\n",
      "Iteration 17, loss = 0.00578692\n",
      "Iteration 18, loss = 0.00477445\n",
      "Iteration 19, loss = 0.00438720\n",
      "Iteration 20, loss = 0.00456330\n",
      "Iteration 21, loss = 0.00500318\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712778\n",
      "Iteration 2, loss = 0.02704678\n",
      "Iteration 3, loss = 0.01275541\n",
      "Iteration 4, loss = 0.01177399\n",
      "Iteration 5, loss = 0.01614758\n",
      "Iteration 6, loss = 0.01765265\n",
      "Iteration 7, loss = 0.01465344\n",
      "Iteration 8, loss = 0.00976536\n",
      "Iteration 9, loss = 0.00579029\n",
      "Iteration 10, loss = 0.00416818\n",
      "Iteration 11, loss = 0.00483197\n",
      "Iteration 12, loss = 0.00666061\n",
      "Iteration 13, loss = 0.00830788\n",
      "Iteration 14, loss = 0.00892768\n",
      "Iteration 15, loss = 0.00841011\n",
      "Iteration 16, loss = 0.00716701\n",
      "Iteration 17, loss = 0.00578651\n",
      "Iteration 18, loss = 0.00477412\n",
      "Iteration 19, loss = 0.00438696\n",
      "Iteration 20, loss = 0.00456309\n",
      "Iteration 21, loss = 0.00500293\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712824\n",
      "Iteration 2, loss = 0.02704630\n",
      "Iteration 3, loss = 0.01275422\n",
      "Iteration 4, loss = 0.01177244\n",
      "Iteration 5, loss = 0.01614679\n",
      "Iteration 6, loss = 0.01765287\n",
      "Iteration 7, loss = 0.01465422\n",
      "Iteration 8, loss = 0.00976615\n",
      "Iteration 9, loss = 0.00579079\n",
      "Iteration 10, loss = 0.00416824\n",
      "Iteration 11, loss = 0.00483159\n",
      "Iteration 12, loss = 0.00666003\n",
      "Iteration 13, loss = 0.00830750\n",
      "Iteration 14, loss = 0.00892782\n",
      "Iteration 15, loss = 0.00841068\n",
      "Iteration 16, loss = 0.00716780\n",
      "Iteration 17, loss = 0.00578726\n",
      "Iteration 18, loss = 0.00477448\n",
      "Iteration 19, loss = 0.00438690\n",
      "Iteration 20, loss = 0.00456275\n",
      "Iteration 21, loss = 0.00500257\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712810\n",
      "Iteration 2, loss = 0.02704621\n",
      "Iteration 3, loss = 0.01275357\n",
      "Iteration 4, loss = 0.01177178\n",
      "Iteration 5, loss = 0.01614633\n",
      "Iteration 6, loss = 0.01765275\n",
      "Iteration 7, loss = 0.01465438\n",
      "Iteration 8, loss = 0.00976639\n",
      "Iteration 9, loss = 0.00579096\n",
      "Iteration 10, loss = 0.00416823\n",
      "Iteration 11, loss = 0.00483130\n",
      "Iteration 12, loss = 0.00665954\n",
      "Iteration 13, loss = 0.00830711\n",
      "Iteration 14, loss = 0.00892783\n",
      "Iteration 15, loss = 0.00841100\n",
      "Iteration 16, loss = 0.00716828\n",
      "Iteration 17, loss = 0.00578774\n",
      "Iteration 18, loss = 0.00477476\n",
      "Iteration 19, loss = 0.00438694\n",
      "Iteration 20, loss = 0.00456263\n",
      "Iteration 21, loss = 0.00500242\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712636\n",
      "Iteration 2, loss = 0.02704789\n",
      "Iteration 3, loss = 0.01275772\n",
      "Iteration 4, loss = 0.01177510\n",
      "Iteration 5, loss = 0.01614632\n",
      "Iteration 6, loss = 0.01765004\n",
      "Iteration 7, loss = 0.01465088\n",
      "Iteration 8, loss = 0.00976358\n",
      "Iteration 9, loss = 0.00578950\n",
      "Iteration 10, loss = 0.00416828\n",
      "Iteration 11, loss = 0.00483257\n",
      "Iteration 12, loss = 0.00666120\n",
      "Iteration 13, loss = 0.00830832\n",
      "Iteration 14, loss = 0.00892797\n",
      "Iteration 15, loss = 0.00841001\n",
      "Iteration 16, loss = 0.00716682\n",
      "Iteration 17, loss = 0.00578644\n",
      "Iteration 18, loss = 0.00477416\n",
      "Iteration 19, loss = 0.00438713\n",
      "Iteration 20, loss = 0.00456322\n",
      "Iteration 21, loss = 0.00500290\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712588\n",
      "Iteration 2, loss = 0.02704820\n",
      "Iteration 3, loss = 0.01275839\n",
      "Iteration 4, loss = 0.01177537\n",
      "Iteration 5, loss = 0.01614585\n",
      "Iteration 6, loss = 0.01764916\n",
      "Iteration 7, loss = 0.01465002\n",
      "Iteration 8, loss = 0.00976298\n",
      "Iteration 9, loss = 0.00578920\n",
      "Iteration 10, loss = 0.00416824\n",
      "Iteration 11, loss = 0.00483266\n",
      "Iteration 12, loss = 0.00666125\n",
      "Iteration 13, loss = 0.00830841\n",
      "Iteration 14, loss = 0.00892805\n",
      "Iteration 15, loss = 0.00841002\n",
      "Iteration 16, loss = 0.00716683\n",
      "Iteration 17, loss = 0.00578647\n",
      "Iteration 18, loss = 0.00477421\n",
      "Iteration 19, loss = 0.00438719\n",
      "Iteration 20, loss = 0.00456325\n",
      "Iteration 21, loss = 0.00500289\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712641\n",
      "Iteration 2, loss = 0.02704791\n",
      "Iteration 3, loss = 0.01275701\n",
      "Iteration 4, loss = 0.01177268\n",
      "Iteration 5, loss = 0.01614331\n",
      "Iteration 6, loss = 0.01764693\n",
      "Iteration 7, loss = 0.01464799\n",
      "Iteration 8, loss = 0.00976159\n",
      "Iteration 9, loss = 0.00578856\n",
      "Iteration 10, loss = 0.00416863\n",
      "Iteration 11, loss = 0.00483429\n",
      "Iteration 12, loss = 0.00666369\n",
      "Iteration 13, loss = 0.00831042\n",
      "Iteration 14, loss = 0.00892866\n",
      "Iteration 15, loss = 0.00840954\n",
      "Iteration 16, loss = 0.00716568\n",
      "Iteration 17, loss = 0.00578504\n",
      "Iteration 18, loss = 0.00477302\n",
      "Iteration 19, loss = 0.00438640\n",
      "Iteration 20, loss = 0.00456272\n",
      "Iteration 21, loss = 0.00500236\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712713\n",
      "Iteration 2, loss = 0.02704711\n",
      "Iteration 3, loss = 0.01275478\n",
      "Iteration 4, loss = 0.01177042\n",
      "Iteration 5, loss = 0.01614259\n",
      "Iteration 6, loss = 0.01764773\n",
      "Iteration 7, loss = 0.01464944\n",
      "Iteration 8, loss = 0.00976289\n",
      "Iteration 9, loss = 0.00578931\n",
      "Iteration 10, loss = 0.00416867\n",
      "Iteration 11, loss = 0.00483370\n",
      "Iteration 12, loss = 0.00666285\n",
      "Iteration 13, loss = 0.00830982\n",
      "Iteration 14, loss = 0.00892864\n",
      "Iteration 15, loss = 0.00841008\n",
      "Iteration 16, loss = 0.00716646\n",
      "Iteration 17, loss = 0.00578575\n",
      "Iteration 18, loss = 0.00477333\n",
      "Iteration 19, loss = 0.00438625\n",
      "Iteration 20, loss = 0.00456228\n",
      "Iteration 21, loss = 0.00500191\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712778\n",
      "Iteration 2, loss = 0.02704653\n",
      "Iteration 3, loss = 0.01275338\n",
      "Iteration 4, loss = 0.01176902\n",
      "Iteration 5, loss = 0.01614232\n",
      "Iteration 6, loss = 0.01764845\n",
      "Iteration 7, loss = 0.01465050\n",
      "Iteration 8, loss = 0.00976369\n",
      "Iteration 9, loss = 0.00578975\n",
      "Iteration 10, loss = 0.00416875\n",
      "Iteration 11, loss = 0.00483350\n",
      "Iteration 12, loss = 0.00666261\n",
      "Iteration 13, loss = 0.00830974\n",
      "Iteration 14, loss = 0.00892876\n",
      "Iteration 15, loss = 0.00841032\n",
      "Iteration 16, loss = 0.00716676\n",
      "Iteration 17, loss = 0.00578599\n",
      "Iteration 18, loss = 0.00477340\n",
      "Iteration 19, loss = 0.00438614\n",
      "Iteration 20, loss = 0.00456207\n",
      "Iteration 21, loss = 0.00500173\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712041\n",
      "Iteration 2, loss = 0.02705150\n",
      "Iteration 3, loss = 0.01276736\n",
      "Iteration 4, loss = 0.01178245\n",
      "Iteration 5, loss = 0.01614500\n",
      "Iteration 6, loss = 0.01764191\n",
      "Iteration 7, loss = 0.01464099\n",
      "Iteration 8, loss = 0.00975572\n",
      "Iteration 9, loss = 0.00578525\n",
      "Iteration 10, loss = 0.00416786\n",
      "Iteration 11, loss = 0.00483495\n",
      "Iteration 12, loss = 0.00666428\n",
      "Iteration 13, loss = 0.00831005\n",
      "Iteration 14, loss = 0.00892716\n",
      "Iteration 15, loss = 0.00840703\n",
      "Iteration 16, loss = 0.00716317\n",
      "Iteration 17, loss = 0.00578326\n",
      "Iteration 18, loss = 0.00477250\n",
      "Iteration 19, loss = 0.00438699\n",
      "Iteration 20, loss = 0.00456378\n",
      "Iteration 21, loss = 0.00500313\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712127\n",
      "Iteration 2, loss = 0.02705132\n",
      "Iteration 3, loss = 0.01276607\n",
      "Iteration 4, loss = 0.01177926\n",
      "Iteration 5, loss = 0.01614151\n",
      "Iteration 6, loss = 0.01763905\n",
      "Iteration 7, loss = 0.01463864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.00975438\n",
      "Iteration 9, loss = 0.00578486\n",
      "Iteration 10, loss = 0.00416865\n",
      "Iteration 11, loss = 0.00483721\n",
      "Iteration 12, loss = 0.00666792\n",
      "Iteration 13, loss = 0.00831343\n",
      "Iteration 14, loss = 0.00892871\n",
      "Iteration 15, loss = 0.00840738\n",
      "Iteration 16, loss = 0.00716272\n",
      "Iteration 17, loss = 0.00578239\n",
      "Iteration 18, loss = 0.00477190\n",
      "Iteration 19, loss = 0.00438683\n",
      "Iteration 20, loss = 0.00456378\n",
      "Iteration 21, loss = 0.00500303\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712304\n",
      "Iteration 2, loss = 0.02705058\n",
      "Iteration 3, loss = 0.01276333\n",
      "Iteration 4, loss = 0.01177602\n",
      "Iteration 5, loss = 0.01613926\n",
      "Iteration 6, loss = 0.01763942\n",
      "Iteration 7, loss = 0.01464054\n",
      "Iteration 8, loss = 0.00975656\n",
      "Iteration 9, loss = 0.00578644\n",
      "Iteration 10, loss = 0.00416923\n",
      "Iteration 11, loss = 0.00483687\n",
      "Iteration 12, loss = 0.00666701\n",
      "Iteration 13, loss = 0.00831279\n",
      "Iteration 14, loss = 0.00892884\n",
      "Iteration 15, loss = 0.00840835\n",
      "Iteration 16, loss = 0.00716417\n",
      "Iteration 17, loss = 0.00578375\n",
      "Iteration 18, loss = 0.00477263\n",
      "Iteration 19, loss = 0.00438676\n",
      "Iteration 20, loss = 0.00456311\n",
      "Iteration 21, loss = 0.00500226\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712208\n",
      "Iteration 2, loss = 0.02705085\n",
      "Iteration 3, loss = 0.01276361\n",
      "Iteration 4, loss = 0.01177559\n",
      "Iteration 5, loss = 0.01613819\n",
      "Iteration 6, loss = 0.01763832\n",
      "Iteration 7, loss = 0.01463979\n",
      "Iteration 8, loss = 0.00975629\n",
      "Iteration 9, loss = 0.00578651\n",
      "Iteration 10, loss = 0.00416948\n",
      "Iteration 11, loss = 0.00483720\n",
      "Iteration 12, loss = 0.00666905\n",
      "Iteration 13, loss = 0.00831561\n",
      "Iteration 14, loss = 0.00893116\n",
      "Iteration 15, loss = 0.00840999\n",
      "Iteration 16, loss = 0.00716505\n",
      "Iteration 17, loss = 0.00578402\n",
      "Iteration 18, loss = 0.00477283\n",
      "Iteration 19, loss = 0.00438712\n",
      "Iteration 20, loss = 0.00456351\n",
      "Iteration 21, loss = 0.00500263\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712317\n",
      "Iteration 2, loss = 0.02705040\n",
      "Iteration 3, loss = 0.01276250\n",
      "Iteration 4, loss = 0.01177555\n",
      "Iteration 5, loss = 0.01613956\n",
      "Iteration 6, loss = 0.01764162\n",
      "Iteration 7, loss = 0.01464476\n",
      "Iteration 8, loss = 0.00976065\n",
      "Iteration 9, loss = 0.00578914\n",
      "Iteration 10, loss = 0.00416941\n",
      "Iteration 11, loss = 0.00483415\n",
      "Iteration 12, loss = 0.00666313\n",
      "Iteration 13, loss = 0.00831017\n",
      "Iteration 14, loss = 0.00892949\n",
      "Iteration 15, loss = 0.00841164\n",
      "Iteration 16, loss = 0.00716892\n",
      "Iteration 17, loss = 0.00578854\n",
      "Iteration 18, loss = 0.00477595\n",
      "Iteration 19, loss = 0.00438829\n",
      "Iteration 20, loss = 0.00456341\n",
      "Iteration 21, loss = 0.00500234\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712525\n",
      "Iteration 2, loss = 0.02704853\n",
      "Iteration 3, loss = 0.01275602\n",
      "Iteration 4, loss = 0.01176711\n",
      "Iteration 5, loss = 0.01613392\n",
      "Iteration 6, loss = 0.01763967\n",
      "Iteration 7, loss = 0.01464392\n",
      "Iteration 8, loss = 0.00976046\n",
      "Iteration 9, loss = 0.00578917\n",
      "Iteration 10, loss = 0.00417011\n",
      "Iteration 11, loss = 0.00483637\n",
      "Iteration 12, loss = 0.00666848\n",
      "Iteration 13, loss = 0.00831616\n",
      "Iteration 14, loss = 0.00893291\n",
      "Iteration 15, loss = 0.00841305\n",
      "Iteration 16, loss = 0.00716852\n",
      "Iteration 17, loss = 0.00578637\n",
      "Iteration 18, loss = 0.00477387\n",
      "Iteration 19, loss = 0.00438704\n",
      "Iteration 20, loss = 0.00456276\n",
      "Iteration 21, loss = 0.00500181\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712271\n",
      "Iteration 2, loss = 0.02704993\n",
      "Iteration 3, loss = 0.01275957\n",
      "Iteration 4, loss = 0.01176967\n",
      "Iteration 5, loss = 0.01613362\n",
      "Iteration 6, loss = 0.01763676\n",
      "Iteration 7, loss = 0.01464039\n",
      "Iteration 8, loss = 0.00975775\n",
      "Iteration 9, loss = 0.00578783\n",
      "Iteration 10, loss = 0.00417017\n",
      "Iteration 11, loss = 0.00483757\n",
      "Iteration 12, loss = 0.00666977\n",
      "Iteration 13, loss = 0.00831667\n",
      "Iteration 14, loss = 0.00893265\n",
      "Iteration 15, loss = 0.00841195\n",
      "Iteration 16, loss = 0.00716716\n",
      "Iteration 17, loss = 0.00578535\n",
      "Iteration 18, loss = 0.00477346\n",
      "Iteration 19, loss = 0.00438711\n",
      "Iteration 20, loss = 0.00456302\n",
      "Iteration 21, loss = 0.00500189\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05711305\n",
      "Iteration 2, loss = 0.02705362\n",
      "Iteration 3, loss = 0.01277254\n",
      "Iteration 4, loss = 0.01178006\n",
      "Iteration 5, loss = 0.01613185\n",
      "Iteration 6, loss = 0.01762446\n",
      "Iteration 7, loss = 0.01462451\n",
      "Iteration 8, loss = 0.00974476\n",
      "Iteration 9, loss = 0.00578055\n",
      "Iteration 10, loss = 0.00416967\n",
      "Iteration 11, loss = 0.00484319\n",
      "Iteration 12, loss = 0.00667883\n",
      "Iteration 13, loss = 0.00832356\n",
      "Iteration 14, loss = 0.00893318\n",
      "Iteration 15, loss = 0.00840732\n",
      "Iteration 16, loss = 0.00716041\n",
      "Iteration 17, loss = 0.00577910\n",
      "Iteration 18, loss = 0.00476992\n",
      "Iteration 19, loss = 0.00438708\n",
      "Iteration 20, loss = 0.00456475\n",
      "Iteration 21, loss = 0.00500330\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05710827\n",
      "Iteration 2, loss = 0.02705457\n",
      "Iteration 3, loss = 0.01277931\n",
      "Iteration 4, loss = 0.01178934\n",
      "Iteration 5, loss = 0.01613707\n",
      "Iteration 6, loss = 0.01762627\n",
      "Iteration 7, loss = 0.01462651\n",
      "Iteration 8, loss = 0.00974611\n",
      "Iteration 9, loss = 0.00578084\n",
      "Iteration 10, loss = 0.00416739\n",
      "Iteration 11, loss = 0.00483713\n",
      "Iteration 12, loss = 0.00666734\n",
      "Iteration 13, loss = 0.00831209\n",
      "Iteration 14, loss = 0.00892749\n",
      "Iteration 15, loss = 0.00840620\n",
      "Iteration 16, loss = 0.00716265\n",
      "Iteration 17, loss = 0.00578337\n",
      "Iteration 18, loss = 0.00477353\n",
      "Iteration 19, loss = 0.00438862\n",
      "Iteration 20, loss = 0.00456501\n",
      "Iteration 21, loss = 0.00500326\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05711341\n",
      "Iteration 2, loss = 0.02705379\n",
      "Iteration 3, loss = 0.01277363\n",
      "Iteration 4, loss = 0.01178305\n",
      "Iteration 5, loss = 0.01613554\n",
      "Iteration 6, loss = 0.01762865\n",
      "Iteration 7, loss = 0.01463148\n",
      "Iteration 8, loss = 0.00975114\n",
      "Iteration 9, loss = 0.00578447\n",
      "Iteration 10, loss = 0.00416904\n",
      "Iteration 11, loss = 0.00483696\n",
      "Iteration 12, loss = 0.00666651\n",
      "Iteration 13, loss = 0.00831188\n",
      "Iteration 14, loss = 0.00892875\n",
      "Iteration 15, loss = 0.00840894\n",
      "Iteration 16, loss = 0.00716605\n",
      "Iteration 17, loss = 0.00578642\n",
      "Iteration 18, loss = 0.00477541\n",
      "Iteration 19, loss = 0.00438902\n",
      "Iteration 20, loss = 0.00456435\n",
      "Iteration 21, loss = 0.00500247\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05711616\n",
      "Iteration 2, loss = 0.02705296\n",
      "Iteration 3, loss = 0.01276909\n",
      "Iteration 4, loss = 0.01177775\n",
      "Iteration 5, loss = 0.01613364\n",
      "Iteration 6, loss = 0.01763028\n",
      "Iteration 7, loss = 0.01463478\n",
      "Iteration 8, loss = 0.00975458\n",
      "Iteration 9, loss = 0.00578690\n",
      "Iteration 10, loss = 0.00416992\n",
      "Iteration 11, loss = 0.00483630\n",
      "Iteration 12, loss = 0.00666545\n",
      "Iteration 13, loss = 0.00831168\n",
      "Iteration 14, loss = 0.00893001\n",
      "Iteration 15, loss = 0.00841154\n",
      "Iteration 16, loss = 0.00716913\n",
      "Iteration 17, loss = 0.00578867\n",
      "Iteration 18, loss = 0.00477657\n",
      "Iteration 19, loss = 0.00438925\n",
      "Iteration 20, loss = 0.00456409\n",
      "Iteration 21, loss = 0.00500218\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05712350\n",
      "Iteration 2, loss = 0.02704919\n",
      "Iteration 3, loss = 0.01275571\n",
      "Iteration 4, loss = 0.01176397\n",
      "Iteration 5, loss = 0.01612945\n",
      "Iteration 6, loss = 0.01763508\n",
      "Iteration 7, loss = 0.01464276\n",
      "Iteration 8, loss = 0.00976168\n",
      "Iteration 9, loss = 0.00579075\n",
      "Iteration 10, loss = 0.00416985\n",
      "Iteration 11, loss = 0.00483321\n",
      "Iteration 12, loss = 0.00666145\n",
      "Iteration 13, loss = 0.00830902\n",
      "Iteration 14, loss = 0.00892993\n",
      "Iteration 15, loss = 0.00841395\n",
      "Iteration 16, loss = 0.00717245\n",
      "Iteration 17, loss = 0.00579127\n",
      "Iteration 18, loss = 0.00477701\n",
      "Iteration 19, loss = 0.00438730\n",
      "Iteration 20, loss = 0.00456051\n",
      "Iteration 21, loss = 0.00499860\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05711933\n",
      "Iteration 2, loss = 0.02705126\n",
      "Iteration 3, loss = 0.01276214\n",
      "Iteration 4, loss = 0.01177013\n",
      "Iteration 5, loss = 0.01613081\n",
      "Iteration 6, loss = 0.01763269\n",
      "Iteration 7, loss = 0.01463953\n",
      "Iteration 8, loss = 0.00975883\n",
      "Iteration 9, loss = 0.00578944\n",
      "Iteration 10, loss = 0.00417053\n",
      "Iteration 11, loss = 0.00483579\n",
      "Iteration 12, loss = 0.00666443\n",
      "Iteration 13, loss = 0.00831087\n",
      "Iteration 14, loss = 0.00893001\n",
      "Iteration 15, loss = 0.00841244\n",
      "Iteration 16, loss = 0.00717043\n",
      "Iteration 17, loss = 0.00578976\n",
      "Iteration 18, loss = 0.00477679\n",
      "Iteration 19, loss = 0.00438836\n",
      "Iteration 20, loss = 0.00456239\n",
      "Iteration 21, loss = 0.00500045\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05711652\n",
      "Iteration 2, loss = 0.02705210\n",
      "Iteration 3, loss = 0.01276397\n",
      "Iteration 4, loss = 0.01176929\n",
      "Iteration 5, loss = 0.01612578\n",
      "Iteration 6, loss = 0.01762545\n",
      "Iteration 7, loss = 0.01463044\n",
      "Iteration 8, loss = 0.00975156\n",
      "Iteration 9, loss = 0.00578504\n",
      "Iteration 10, loss = 0.00417119\n",
      "Iteration 11, loss = 0.00484197\n",
      "Iteration 12, loss = 0.00667607\n",
      "Iteration 13, loss = 0.00832149\n",
      "Iteration 14, loss = 0.00893373\n",
      "Iteration 15, loss = 0.00841043\n",
      "Iteration 16, loss = 0.00716508\n",
      "Iteration 17, loss = 0.00578308\n",
      "Iteration 18, loss = 0.00477204\n",
      "Iteration 19, loss = 0.00438680\n",
      "Iteration 20, loss = 0.00456268\n",
      "Iteration 21, loss = 0.00500074\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05710867\n",
      "Iteration 2, loss = 0.02705427\n",
      "Iteration 3, loss = 0.01277525\n",
      "Iteration 4, loss = 0.01178227\n",
      "Iteration 5, loss = 0.01613124\n",
      "Iteration 6, loss = 0.01762393\n",
      "Iteration 7, loss = 0.01462745\n",
      "Iteration 8, loss = 0.00974867\n",
      "Iteration 9, loss = 0.00578340\n",
      "Iteration 10, loss = 0.00416950\n",
      "Iteration 11, loss = 0.00483861\n",
      "Iteration 12, loss = 0.00666834\n",
      "Iteration 13, loss = 0.00831319\n",
      "Iteration 14, loss = 0.00892942\n",
      "Iteration 15, loss = 0.00840906\n",
      "Iteration 16, loss = 0.00716623\n",
      "Iteration 17, loss = 0.00578677\n",
      "Iteration 18, loss = 0.00477585\n",
      "Iteration 19, loss = 0.00438936\n",
      "Iteration 20, loss = 0.00456430\n",
      "Iteration 21, loss = 0.00500196\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05708856\n",
      "Iteration 2, loss = 0.02705410\n",
      "Iteration 3, loss = 0.01279013\n",
      "Iteration 4, loss = 0.01179676\n",
      "Iteration 5, loss = 0.01613002\n",
      "Iteration 6, loss = 0.01760845\n",
      "Iteration 7, loss = 0.01460712\n",
      "Iteration 8, loss = 0.00973083\n",
      "Iteration 9, loss = 0.00577158\n",
      "Iteration 10, loss = 0.00416431\n",
      "Iteration 11, loss = 0.00483847\n",
      "Iteration 12, loss = 0.00666978\n",
      "Iteration 13, loss = 0.00831258\n",
      "Iteration 14, loss = 0.00892467\n",
      "Iteration 15, loss = 0.00840041\n",
      "Iteration 16, loss = 0.00715623\n",
      "Iteration 17, loss = 0.00577804\n",
      "Iteration 18, loss = 0.00477059\n",
      "Iteration 19, loss = 0.00438787\n",
      "Iteration 20, loss = 0.00456513\n",
      "Iteration 21, loss = 0.00500261\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05711014\n",
      "Iteration 2, loss = 0.02705421\n",
      "Iteration 3, loss = 0.01277462\n",
      "Iteration 4, loss = 0.01178109\n",
      "Iteration 5, loss = 0.01613046\n",
      "Iteration 6, loss = 0.01762297\n",
      "Iteration 7, loss = 0.01462571\n",
      "Iteration 8, loss = 0.00974817\n",
      "Iteration 9, loss = 0.00578370\n",
      "Iteration 10, loss = 0.00416985\n",
      "Iteration 11, loss = 0.00483862\n",
      "Iteration 12, loss = 0.00666823\n",
      "Iteration 13, loss = 0.00831331\n",
      "Iteration 14, loss = 0.00892997\n",
      "Iteration 15, loss = 0.00841000\n",
      "Iteration 16, loss = 0.00716737\n",
      "Iteration 17, loss = 0.00578733\n",
      "Iteration 18, loss = 0.00477610\n",
      "Iteration 19, loss = 0.00438953\n",
      "Iteration 20, loss = 0.00456445\n",
      "Iteration 21, loss = 0.00500201\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05709740\n",
      "Iteration 2, loss = 0.02705479\n",
      "Iteration 3, loss = 0.01278485\n",
      "Iteration 4, loss = 0.01179076\n",
      "Iteration 5, loss = 0.01613020\n",
      "Iteration 6, loss = 0.01761411\n",
      "Iteration 7, loss = 0.01461468\n",
      "Iteration 8, loss = 0.00973878\n",
      "Iteration 9, loss = 0.00577800\n",
      "Iteration 10, loss = 0.00416784\n",
      "Iteration 11, loss = 0.00483925\n",
      "Iteration 12, loss = 0.00666963\n",
      "Iteration 13, loss = 0.00831350\n",
      "Iteration 14, loss = 0.00892774\n",
      "Iteration 15, loss = 0.00840554\n",
      "Iteration 16, loss = 0.00716207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.00578322\n",
      "Iteration 18, loss = 0.00477392\n",
      "Iteration 19, loss = 0.00438922\n",
      "Iteration 20, loss = 0.00456515\n",
      "Iteration 21, loss = 0.00500253\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05710207\n",
      "Iteration 2, loss = 0.02705482\n",
      "Iteration 3, loss = 0.01277951\n",
      "Iteration 4, loss = 0.01178417\n",
      "Iteration 5, loss = 0.01612814\n",
      "Iteration 6, loss = 0.01761724\n",
      "Iteration 7, loss = 0.01461964\n",
      "Iteration 8, loss = 0.00974383\n",
      "Iteration 9, loss = 0.00578161\n",
      "Iteration 10, loss = 0.00417032\n",
      "Iteration 11, loss = 0.00484041\n",
      "Iteration 12, loss = 0.00667023\n",
      "Iteration 13, loss = 0.00831478\n",
      "Iteration 14, loss = 0.00893051\n",
      "Iteration 15, loss = 0.00840967\n",
      "Iteration 16, loss = 0.00716682\n",
      "Iteration 17, loss = 0.00578710\n",
      "Iteration 18, loss = 0.00477654\n",
      "Iteration 19, loss = 0.00439055\n",
      "Iteration 20, loss = 0.00456562\n",
      "Iteration 21, loss = 0.00500283\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05711692\n",
      "Iteration 2, loss = 0.02705203\n",
      "Iteration 3, loss = 0.01276196\n",
      "Iteration 4, loss = 0.01176471\n",
      "Iteration 5, loss = 0.01612191\n",
      "Iteration 6, loss = 0.01762482\n",
      "Iteration 7, loss = 0.01463278\n",
      "Iteration 8, loss = 0.00975416\n",
      "Iteration 9, loss = 0.00578745\n",
      "Iteration 10, loss = 0.00417039\n",
      "Iteration 11, loss = 0.00483574\n",
      "Iteration 12, loss = 0.00666398\n",
      "Iteration 13, loss = 0.00831053\n",
      "Iteration 14, loss = 0.00893050\n",
      "Iteration 15, loss = 0.00841363\n",
      "Iteration 16, loss = 0.00717252\n",
      "Iteration 17, loss = 0.00579159\n",
      "Iteration 18, loss = 0.00477752\n",
      "Iteration 19, loss = 0.00438760\n",
      "Iteration 20, loss = 0.00455984\n",
      "Iteration 21, loss = 0.00499715\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05711045\n",
      "Iteration 2, loss = 0.02705332\n",
      "Iteration 3, loss = 0.01276668\n",
      "Iteration 4, loss = 0.01176866\n",
      "Iteration 5, loss = 0.01612157\n",
      "Iteration 6, loss = 0.01762058\n",
      "Iteration 7, loss = 0.01462978\n",
      "Iteration 8, loss = 0.00975407\n",
      "Iteration 9, loss = 0.00578805\n",
      "Iteration 10, loss = 0.00417154\n",
      "Iteration 11, loss = 0.00483741\n",
      "Iteration 12, loss = 0.00666548\n",
      "Iteration 13, loss = 0.00831105\n",
      "Iteration 14, loss = 0.00892992\n",
      "Iteration 15, loss = 0.00841230\n",
      "Iteration 16, loss = 0.00717136\n",
      "Iteration 17, loss = 0.00579123\n",
      "Iteration 18, loss = 0.00477808\n",
      "Iteration 19, loss = 0.00438896\n",
      "Iteration 20, loss = 0.00456154\n",
      "Iteration 21, loss = 0.00499825\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05710006\n",
      "Iteration 2, loss = 0.02705479\n",
      "Iteration 3, loss = 0.01277741\n",
      "Iteration 4, loss = 0.01177901\n",
      "Iteration 5, loss = 0.01612191\n",
      "Iteration 6, loss = 0.01761254\n",
      "Iteration 7, loss = 0.01461809\n",
      "Iteration 8, loss = 0.00974400\n",
      "Iteration 9, loss = 0.00578312\n",
      "Iteration 10, loss = 0.00417157\n",
      "Iteration 11, loss = 0.00484092\n",
      "Iteration 12, loss = 0.00666987\n",
      "Iteration 13, loss = 0.00831381\n",
      "Iteration 14, loss = 0.00892973\n",
      "Iteration 15, loss = 0.00840938\n",
      "Iteration 16, loss = 0.00716752\n",
      "Iteration 17, loss = 0.00578818\n",
      "Iteration 18, loss = 0.00477706\n",
      "Iteration 19, loss = 0.00439024\n",
      "Iteration 20, loss = 0.00456397\n",
      "Iteration 21, loss = 0.00500020\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05709448\n",
      "Iteration 2, loss = 0.02705485\n",
      "Iteration 3, loss = 0.01278151\n",
      "Iteration 4, loss = 0.01178295\n",
      "Iteration 5, loss = 0.01612105\n",
      "Iteration 6, loss = 0.01760796\n",
      "Iteration 7, loss = 0.01461337\n",
      "Iteration 8, loss = 0.00974001\n",
      "Iteration 9, loss = 0.00578078\n",
      "Iteration 10, loss = 0.00417104\n",
      "Iteration 11, loss = 0.00484171\n",
      "Iteration 12, loss = 0.00667113\n",
      "Iteration 13, loss = 0.00831459\n",
      "Iteration 14, loss = 0.00892950\n",
      "Iteration 15, loss = 0.00840817\n",
      "Iteration 16, loss = 0.00716597\n",
      "Iteration 17, loss = 0.00578690\n",
      "Iteration 18, loss = 0.00477678\n",
      "Iteration 19, loss = 0.00439070\n",
      "Iteration 20, loss = 0.00456479\n",
      "Iteration 21, loss = 0.00500082\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05707591\n",
      "Iteration 2, loss = 0.02705255\n",
      "Iteration 3, loss = 0.01279284\n",
      "Iteration 4, loss = 0.01179505\n",
      "Iteration 5, loss = 0.01612071\n",
      "Iteration 6, loss = 0.01759558\n",
      "Iteration 7, loss = 0.01459680\n",
      "Iteration 8, loss = 0.00972553\n",
      "Iteration 9, loss = 0.00577135\n",
      "Iteration 10, loss = 0.00416730\n",
      "Iteration 11, loss = 0.00484250\n",
      "Iteration 12, loss = 0.00667313\n",
      "Iteration 13, loss = 0.00831440\n",
      "Iteration 14, loss = 0.00892548\n",
      "Iteration 15, loss = 0.00840128\n",
      "Iteration 16, loss = 0.00715801\n",
      "Iteration 17, loss = 0.00578052\n",
      "Iteration 18, loss = 0.00477340\n",
      "Iteration 19, loss = 0.00439030\n",
      "Iteration 20, loss = 0.00456629\n",
      "Iteration 21, loss = 0.00500230\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05708269\n",
      "Iteration 2, loss = 0.02705365\n",
      "Iteration 3, loss = 0.01278935\n",
      "Iteration 4, loss = 0.01179016\n",
      "Iteration 5, loss = 0.01611874\n",
      "Iteration 6, loss = 0.01759886\n",
      "Iteration 7, loss = 0.01460217\n",
      "Iteration 8, loss = 0.00973051\n",
      "Iteration 9, loss = 0.00577555\n",
      "Iteration 10, loss = 0.00416986\n",
      "Iteration 11, loss = 0.00484295\n",
      "Iteration 12, loss = 0.00667251\n",
      "Iteration 13, loss = 0.00831462\n",
      "Iteration 14, loss = 0.00892804\n",
      "Iteration 15, loss = 0.00840625\n",
      "Iteration 16, loss = 0.00716435\n",
      "Iteration 17, loss = 0.00578603\n",
      "Iteration 18, loss = 0.00477661\n",
      "Iteration 19, loss = 0.00439104\n",
      "Iteration 20, loss = 0.00456531\n",
      "Iteration 21, loss = 0.00500094\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05705292\n",
      "Iteration 2, loss = 0.02704614\n",
      "Iteration 3, loss = 0.01279972\n",
      "Iteration 4, loss = 0.01180084\n",
      "Iteration 5, loss = 0.01611308\n",
      "Iteration 6, loss = 0.01757798\n",
      "Iteration 7, loss = 0.01457637\n",
      "Iteration 8, loss = 0.00970819\n",
      "Iteration 9, loss = 0.00576038\n",
      "Iteration 10, loss = 0.00416268\n",
      "Iteration 11, loss = 0.00484190\n",
      "Iteration 12, loss = 0.00667243\n",
      "Iteration 13, loss = 0.00831096\n",
      "Iteration 14, loss = 0.00891866\n",
      "Iteration 15, loss = 0.00839180\n",
      "Iteration 16, loss = 0.00714800\n",
      "Iteration 17, loss = 0.00577157\n",
      "Iteration 18, loss = 0.00476711\n",
      "Iteration 19, loss = 0.00438565\n",
      "Iteration 20, loss = 0.00456232\n",
      "Iteration 21, loss = 0.00499710\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05709658\n",
      "Iteration 2, loss = 0.02705487\n",
      "Iteration 3, loss = 0.01277835\n",
      "Iteration 4, loss = 0.01177537\n",
      "Iteration 5, loss = 0.01611379\n",
      "Iteration 6, loss = 0.01760613\n",
      "Iteration 7, loss = 0.01461424\n",
      "Iteration 8, loss = 0.00974110\n",
      "Iteration 9, loss = 0.00578188\n",
      "Iteration 10, loss = 0.00417230\n",
      "Iteration 11, loss = 0.00484167\n",
      "Iteration 12, loss = 0.00667018\n",
      "Iteration 13, loss = 0.00831418\n",
      "Iteration 14, loss = 0.00893091\n",
      "Iteration 15, loss = 0.00841152\n",
      "Iteration 16, loss = 0.00717065\n",
      "Iteration 17, loss = 0.00579111\n",
      "Iteration 18, loss = 0.00477868\n",
      "Iteration 19, loss = 0.00438977\n",
      "Iteration 20, loss = 0.00456185\n",
      "Iteration 21, loss = 0.00499805\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05708001\n",
      "Iteration 2, loss = 0.02705355\n",
      "Iteration 3, loss = 0.01278758\n",
      "Iteration 4, loss = 0.01178243\n",
      "Iteration 5, loss = 0.01611070\n",
      "Iteration 6, loss = 0.01759494\n",
      "Iteration 7, loss = 0.01460273\n",
      "Iteration 8, loss = 0.00973259\n",
      "Iteration 9, loss = 0.00577799\n",
      "Iteration 10, loss = 0.00417306\n",
      "Iteration 11, loss = 0.00484581\n",
      "Iteration 12, loss = 0.00667488\n",
      "Iteration 13, loss = 0.00831687\n",
      "Iteration 14, loss = 0.00893064\n",
      "Iteration 15, loss = 0.00840938\n",
      "Iteration 16, loss = 0.00716794\n",
      "Iteration 17, loss = 0.00578931\n",
      "Iteration 18, loss = 0.00477873\n",
      "Iteration 19, loss = 0.00439137\n",
      "Iteration 20, loss = 0.00456416\n",
      "Iteration 21, loss = 0.00499944\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05704725\n",
      "Iteration 2, loss = 0.02704701\n",
      "Iteration 3, loss = 0.01279859\n",
      "Iteration 4, loss = 0.01179543\n",
      "Iteration 5, loss = 0.01610778\n",
      "Iteration 6, loss = 0.01757513\n",
      "Iteration 7, loss = 0.01457695\n",
      "Iteration 8, loss = 0.00971205\n",
      "Iteration 9, loss = 0.00576715\n",
      "Iteration 10, loss = 0.00417068\n",
      "Iteration 11, loss = 0.00484922\n",
      "Iteration 12, loss = 0.00667963\n",
      "Iteration 13, loss = 0.00831907\n",
      "Iteration 14, loss = 0.00892827\n",
      "Iteration 15, loss = 0.00840358\n",
      "Iteration 16, loss = 0.00716053\n",
      "Iteration 17, loss = 0.00578316\n",
      "Iteration 18, loss = 0.00477585\n",
      "Iteration 19, loss = 0.00439197\n",
      "Iteration 20, loss = 0.00456619\n",
      "Iteration 21, loss = 0.00500054\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05704018\n",
      "Iteration 2, loss = 0.02704373\n",
      "Iteration 3, loss = 0.01280067\n",
      "Iteration 4, loss = 0.01179674\n",
      "Iteration 5, loss = 0.01610339\n",
      "Iteration 6, loss = 0.01756758\n",
      "Iteration 7, loss = 0.01456962\n",
      "Iteration 8, loss = 0.00970695\n",
      "Iteration 9, loss = 0.00576437\n",
      "Iteration 10, loss = 0.00417177\n",
      "Iteration 11, loss = 0.00485359\n",
      "Iteration 12, loss = 0.00668503\n",
      "Iteration 13, loss = 0.00832329\n",
      "Iteration 14, loss = 0.00893024\n",
      "Iteration 15, loss = 0.00840359\n",
      "Iteration 16, loss = 0.00715970\n",
      "Iteration 17, loss = 0.00578210\n",
      "Iteration 18, loss = 0.00477462\n",
      "Iteration 19, loss = 0.00439146\n",
      "Iteration 20, loss = 0.00456665\n",
      "Iteration 21, loss = 0.00500087\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05699704\n",
      "Iteration 2, loss = 0.02703066\n",
      "Iteration 3, loss = 0.01280437\n",
      "Iteration 4, loss = 0.01180058\n",
      "Iteration 5, loss = 0.01609272\n",
      "Iteration 6, loss = 0.01754420\n",
      "Iteration 7, loss = 0.01454318\n",
      "Iteration 8, loss = 0.00968535\n",
      "Iteration 9, loss = 0.00575139\n",
      "Iteration 10, loss = 0.00416701\n",
      "Iteration 11, loss = 0.00485418\n",
      "Iteration 12, loss = 0.00668606\n",
      "Iteration 13, loss = 0.00832088\n",
      "Iteration 14, loss = 0.00892284\n",
      "Iteration 15, loss = 0.00839195\n",
      "Iteration 16, loss = 0.00714651\n",
      "Iteration 17, loss = 0.00577137\n",
      "Iteration 18, loss = 0.00476888\n",
      "Iteration 19, loss = 0.00438849\n",
      "Iteration 20, loss = 0.00456521\n",
      "Iteration 21, loss = 0.00499834\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05703783\n",
      "Iteration 2, loss = 0.02704542\n",
      "Iteration 3, loss = 0.01279883\n",
      "Iteration 4, loss = 0.01178870\n",
      "Iteration 5, loss = 0.01609299\n",
      "Iteration 6, loss = 0.01756206\n",
      "Iteration 7, loss = 0.01456936\n",
      "Iteration 8, loss = 0.00970972\n",
      "Iteration 9, loss = 0.00576769\n",
      "Iteration 10, loss = 0.00417390\n",
      "Iteration 11, loss = 0.00485472\n",
      "Iteration 12, loss = 0.00668530\n",
      "Iteration 13, loss = 0.00832354\n",
      "Iteration 14, loss = 0.00893150\n",
      "Iteration 15, loss = 0.00840641\n",
      "Iteration 16, loss = 0.00716463\n",
      "Iteration 17, loss = 0.00578637\n",
      "Iteration 18, loss = 0.00477741\n",
      "Iteration 19, loss = 0.00439161\n",
      "Iteration 20, loss = 0.00456453\n",
      "Iteration 21, loss = 0.00499773\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05706921\n",
      "Iteration 2, loss = 0.02705345\n",
      "Iteration 3, loss = 0.01278070\n",
      "Iteration 4, loss = 0.01176198\n",
      "Iteration 5, loss = 0.01608239\n",
      "Iteration 6, loss = 0.01757096\n",
      "Iteration 7, loss = 0.01458861\n",
      "Iteration 8, loss = 0.00972772\n",
      "Iteration 9, loss = 0.00577648\n",
      "Iteration 10, loss = 0.00417020\n",
      "Iteration 11, loss = 0.00484125\n",
      "Iteration 12, loss = 0.00666902\n",
      "Iteration 13, loss = 0.00831049\n",
      "Iteration 14, loss = 0.00892529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.00840698\n",
      "Iteration 16, loss = 0.00716928\n",
      "Iteration 17, loss = 0.00579011\n",
      "Iteration 18, loss = 0.00477642\n",
      "Iteration 19, loss = 0.00438385\n",
      "Iteration 20, loss = 0.00455235\n",
      "Iteration 21, loss = 0.00498587\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05707259\n",
      "Iteration 2, loss = 0.02705434\n",
      "Iteration 3, loss = 0.01277755\n",
      "Iteration 4, loss = 0.01175599\n",
      "Iteration 5, loss = 0.01607879\n",
      "Iteration 6, loss = 0.01757125\n",
      "Iteration 7, loss = 0.01458998\n",
      "Iteration 8, loss = 0.00972844\n",
      "Iteration 9, loss = 0.00577599\n",
      "Iteration 10, loss = 0.00416911\n",
      "Iteration 11, loss = 0.00483849\n",
      "Iteration 12, loss = 0.00666497\n",
      "Iteration 13, loss = 0.00830715\n",
      "Iteration 14, loss = 0.00892400\n",
      "Iteration 15, loss = 0.00840711\n",
      "Iteration 16, loss = 0.00717059\n",
      "Iteration 17, loss = 0.00579147\n",
      "Iteration 18, loss = 0.00477655\n",
      "Iteration 19, loss = 0.00438262\n",
      "Iteration 20, loss = 0.00455040\n",
      "Iteration 21, loss = 0.00498432\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05701141\n",
      "Iteration 2, loss = 0.02703988\n",
      "Iteration 3, loss = 0.01280174\n",
      "Iteration 4, loss = 0.01178940\n",
      "Iteration 5, loss = 0.01608552\n",
      "Iteration 6, loss = 0.01754917\n",
      "Iteration 7, loss = 0.01455741\n",
      "Iteration 8, loss = 0.00970203\n",
      "Iteration 9, loss = 0.00576470\n",
      "Iteration 10, loss = 0.00417484\n",
      "Iteration 11, loss = 0.00485679\n",
      "Iteration 12, loss = 0.00668731\n",
      "Iteration 13, loss = 0.00832483\n",
      "Iteration 14, loss = 0.00893166\n",
      "Iteration 15, loss = 0.00840563\n",
      "Iteration 16, loss = 0.00716267\n",
      "Iteration 17, loss = 0.00578546\n",
      "Iteration 18, loss = 0.00477725\n",
      "Iteration 19, loss = 0.00439250\n",
      "Iteration 20, loss = 0.00456568\n",
      "Iteration 21, loss = 0.00499883\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05699025\n",
      "Iteration 2, loss = 0.02703247\n",
      "Iteration 3, loss = 0.01280524\n",
      "Iteration 4, loss = 0.01179464\n",
      "Iteration 5, loss = 0.01608311\n",
      "Iteration 6, loss = 0.01753869\n",
      "Iteration 7, loss = 0.01454397\n",
      "Iteration 8, loss = 0.00969068\n",
      "Iteration 9, loss = 0.00575804\n",
      "Iteration 10, loss = 0.00417357\n",
      "Iteration 11, loss = 0.00485962\n",
      "Iteration 12, loss = 0.00669138\n",
      "Iteration 13, loss = 0.00832738\n",
      "Iteration 14, loss = 0.00893116\n",
      "Iteration 15, loss = 0.00840248\n",
      "Iteration 16, loss = 0.00715815\n",
      "Iteration 17, loss = 0.00578138\n",
      "Iteration 18, loss = 0.00477507\n",
      "Iteration 19, loss = 0.00439258\n",
      "Iteration 20, loss = 0.00456717\n",
      "Iteration 21, loss = 0.00500017\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05686805\n",
      "Iteration 2, loss = 0.02697138\n",
      "Iteration 3, loss = 0.01280089\n",
      "Iteration 4, loss = 0.01180695\n",
      "Iteration 5, loss = 0.01605964\n",
      "Iteration 6, loss = 0.01747552\n",
      "Iteration 7, loss = 0.01446482\n",
      "Iteration 8, loss = 0.00961937\n",
      "Iteration 9, loss = 0.00570893\n",
      "Iteration 10, loss = 0.00415050\n",
      "Iteration 11, loss = 0.00485661\n",
      "Iteration 12, loss = 0.00669584\n",
      "Iteration 13, loss = 0.00832010\n",
      "Iteration 14, loss = 0.00890528\n",
      "Iteration 15, loss = 0.00836244\n",
      "Iteration 16, loss = 0.00711240\n",
      "Iteration 17, loss = 0.00573996\n",
      "Iteration 18, loss = 0.00474708\n",
      "Iteration 19, loss = 0.00437784\n",
      "Iteration 20, loss = 0.00456095\n",
      "Iteration 21, loss = 0.00499238\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05691367\n",
      "Iteration 2, loss = 0.02699673\n",
      "Iteration 3, loss = 0.01280700\n",
      "Iteration 4, loss = 0.01180405\n",
      "Iteration 5, loss = 0.01606781\n",
      "Iteration 6, loss = 0.01749922\n",
      "Iteration 7, loss = 0.01449698\n",
      "Iteration 8, loss = 0.00965100\n",
      "Iteration 9, loss = 0.00573376\n",
      "Iteration 10, loss = 0.00416606\n",
      "Iteration 11, loss = 0.00486655\n",
      "Iteration 12, loss = 0.00670157\n",
      "Iteration 13, loss = 0.00832681\n",
      "Iteration 14, loss = 0.00891707\n",
      "Iteration 15, loss = 0.00837698\n",
      "Iteration 16, loss = 0.00712842\n",
      "Iteration 17, loss = 0.00575641\n",
      "Iteration 18, loss = 0.00476067\n",
      "Iteration 19, loss = 0.00438683\n",
      "Iteration 20, loss = 0.00456559\n",
      "Iteration 21, loss = 0.00499674\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05694821\n",
      "Iteration 2, loss = 0.02701519\n",
      "Iteration 3, loss = 0.01280795\n",
      "Iteration 4, loss = 0.01179717\n",
      "Iteration 5, loss = 0.01607004\n",
      "Iteration 6, loss = 0.01751608\n",
      "Iteration 7, loss = 0.01452183\n",
      "Iteration 8, loss = 0.00967511\n",
      "Iteration 9, loss = 0.00575118\n",
      "Iteration 10, loss = 0.00417413\n",
      "Iteration 11, loss = 0.00486647\n",
      "Iteration 12, loss = 0.00669937\n",
      "Iteration 13, loss = 0.00833113\n",
      "Iteration 14, loss = 0.00893017\n",
      "Iteration 15, loss = 0.00839646\n",
      "Iteration 16, loss = 0.00714905\n",
      "Iteration 17, loss = 0.00577326\n",
      "Iteration 18, loss = 0.00477061\n",
      "Iteration 19, loss = 0.00439117\n",
      "Iteration 20, loss = 0.00456687\n",
      "Iteration 21, loss = 0.00499816\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05693843\n",
      "Iteration 2, loss = 0.02701445\n",
      "Iteration 3, loss = 0.01280725\n",
      "Iteration 4, loss = 0.01179254\n",
      "Iteration 5, loss = 0.01606318\n",
      "Iteration 6, loss = 0.01751096\n",
      "Iteration 7, loss = 0.01452010\n",
      "Iteration 8, loss = 0.00967613\n",
      "Iteration 9, loss = 0.00575332\n",
      "Iteration 10, loss = 0.00417576\n",
      "Iteration 11, loss = 0.00486675\n",
      "Iteration 12, loss = 0.00669885\n",
      "Iteration 13, loss = 0.00833055\n",
      "Iteration 14, loss = 0.00893006\n",
      "Iteration 15, loss = 0.00839714\n",
      "Iteration 16, loss = 0.00715030\n",
      "Iteration 17, loss = 0.00577470\n",
      "Iteration 18, loss = 0.00477157\n",
      "Iteration 19, loss = 0.00439126\n",
      "Iteration 20, loss = 0.00456596\n",
      "Iteration 21, loss = 0.00499674\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05705731\n",
      "Iteration 2, loss = 0.02705431\n",
      "Iteration 3, loss = 0.01276504\n",
      "Iteration 4, loss = 0.01171277\n",
      "Iteration 5, loss = 0.01602147\n",
      "Iteration 6, loss = 0.01752591\n",
      "Iteration 7, loss = 0.01456420\n",
      "Iteration 8, loss = 0.00971466\n",
      "Iteration 9, loss = 0.00576269\n",
      "Iteration 10, loss = 0.00414774\n",
      "Iteration 11, loss = 0.00480795\n",
      "Iteration 12, loss = 0.00663171\n",
      "Iteration 13, loss = 0.00827456\n",
      "Iteration 14, loss = 0.00889476\n",
      "Iteration 15, loss = 0.00838411\n",
      "Iteration 16, loss = 0.00715237\n",
      "Iteration 17, loss = 0.00577395\n",
      "Iteration 18, loss = 0.00475738\n",
      "Iteration 19, loss = 0.00435675\n",
      "Iteration 20, loss = 0.00451786\n",
      "Iteration 21, loss = 0.00494939\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05705569\n",
      "Iteration 2, loss = 0.02705451\n",
      "Iteration 3, loss = 0.01275885\n",
      "Iteration 4, loss = 0.01169926\n",
      "Iteration 5, loss = 0.01600589\n",
      "Iteration 6, loss = 0.01751402\n",
      "Iteration 7, loss = 0.01455665\n",
      "Iteration 8, loss = 0.00970910\n",
      "Iteration 9, loss = 0.00575635\n",
      "Iteration 10, loss = 0.00413880\n",
      "Iteration 11, loss = 0.00479609\n",
      "Iteration 12, loss = 0.00661892\n",
      "Iteration 13, loss = 0.00826356\n",
      "Iteration 14, loss = 0.00888694\n",
      "Iteration 15, loss = 0.00837876\n",
      "Iteration 16, loss = 0.00714791\n",
      "Iteration 17, loss = 0.00576834\n",
      "Iteration 18, loss = 0.00474971\n",
      "Iteration 19, loss = 0.00434673\n",
      "Iteration 20, loss = 0.00450574\n",
      "Iteration 21, loss = 0.00493704\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05690246\n",
      "Iteration 2, loss = 0.02700653\n",
      "Iteration 3, loss = 0.01280742\n",
      "Iteration 4, loss = 0.01178378\n",
      "Iteration 5, loss = 0.01604550\n",
      "Iteration 6, loss = 0.01749234\n",
      "Iteration 7, loss = 0.01450617\n",
      "Iteration 8, loss = 0.00966830\n",
      "Iteration 9, loss = 0.00575061\n",
      "Iteration 10, loss = 0.00417633\n",
      "Iteration 11, loss = 0.00486697\n",
      "Iteration 12, loss = 0.00669691\n",
      "Iteration 13, loss = 0.00832809\n",
      "Iteration 14, loss = 0.00892732\n",
      "Iteration 15, loss = 0.00839624\n",
      "Iteration 16, loss = 0.00715192\n",
      "Iteration 17, loss = 0.00577573\n",
      "Iteration 18, loss = 0.00477149\n",
      "Iteration 19, loss = 0.00438946\n",
      "Iteration 20, loss = 0.00456301\n",
      "Iteration 21, loss = 0.00499387\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05681010\n",
      "Iteration 2, loss = 0.02695965\n",
      "Iteration 3, loss = 0.01280386\n",
      "Iteration 4, loss = 0.01180097\n",
      "Iteration 5, loss = 0.01604392\n",
      "Iteration 6, loss = 0.01746194\n",
      "Iteration 7, loss = 0.01446142\n",
      "Iteration 8, loss = 0.00962735\n",
      "Iteration 9, loss = 0.00572539\n",
      "Iteration 10, loss = 0.00417073\n",
      "Iteration 11, loss = 0.00487883\n",
      "Iteration 12, loss = 0.00671520\n",
      "Iteration 13, loss = 0.00833602\n",
      "Iteration 14, loss = 0.00891989\n",
      "Iteration 15, loss = 0.00837439\n",
      "Iteration 16, loss = 0.00712299\n",
      "Iteration 17, loss = 0.00575177\n",
      "Iteration 18, loss = 0.00475892\n",
      "Iteration 19, loss = 0.00438804\n",
      "Iteration 20, loss = 0.00456783\n",
      "Iteration 21, loss = 0.00499779\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05669821\n",
      "Iteration 2, loss = 0.02689204\n",
      "Iteration 3, loss = 0.01277966\n",
      "Iteration 4, loss = 0.01179774\n",
      "Iteration 5, loss = 0.01602056\n",
      "Iteration 6, loss = 0.01740934\n",
      "Iteration 7, loss = 0.01439483\n",
      "Iteration 8, loss = 0.00956564\n",
      "Iteration 9, loss = 0.00568081\n",
      "Iteration 10, loss = 0.00414767\n",
      "Iteration 11, loss = 0.00487473\n",
      "Iteration 12, loss = 0.00672074\n",
      "Iteration 13, loss = 0.00833833\n",
      "Iteration 14, loss = 0.00890888\n",
      "Iteration 15, loss = 0.00834979\n",
      "Iteration 16, loss = 0.00708849\n",
      "Iteration 17, loss = 0.00571913\n",
      "Iteration 18, loss = 0.00473500\n",
      "Iteration 19, loss = 0.00437491\n",
      "Iteration 20, loss = 0.00456511\n",
      "Iteration 21, loss = 0.00499573\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05664371\n",
      "Iteration 2, loss = 0.02687170\n",
      "Iteration 3, loss = 0.01277369\n",
      "Iteration 4, loss = 0.01179310\n",
      "Iteration 5, loss = 0.01600960\n",
      "Iteration 6, loss = 0.01739191\n",
      "Iteration 7, loss = 0.01437532\n",
      "Iteration 8, loss = 0.00954896\n",
      "Iteration 9, loss = 0.00566992\n",
      "Iteration 10, loss = 0.00414389\n",
      "Iteration 11, loss = 0.00487614\n",
      "Iteration 12, loss = 0.00672434\n",
      "Iteration 13, loss = 0.00833784\n",
      "Iteration 14, loss = 0.00890341\n",
      "Iteration 15, loss = 0.00834072\n",
      "Iteration 16, loss = 0.00707727\n",
      "Iteration 17, loss = 0.00570865\n",
      "Iteration 18, loss = 0.00472674\n",
      "Iteration 19, loss = 0.00437021\n",
      "Iteration 20, loss = 0.00456341\n",
      "Iteration 21, loss = 0.00499335\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05674021\n",
      "Iteration 2, loss = 0.02693508\n",
      "Iteration 3, loss = 0.01280097\n",
      "Iteration 4, loss = 0.01179436\n",
      "Iteration 5, loss = 0.01602615\n",
      "Iteration 6, loss = 0.01743803\n",
      "Iteration 7, loss = 0.01443891\n",
      "Iteration 8, loss = 0.00961180\n",
      "Iteration 9, loss = 0.00571881\n",
      "Iteration 10, loss = 0.00417187\n",
      "Iteration 11, loss = 0.00488477\n",
      "Iteration 12, loss = 0.00672142\n",
      "Iteration 13, loss = 0.00833636\n",
      "Iteration 14, loss = 0.00891371\n",
      "Iteration 15, loss = 0.00836462\n",
      "Iteration 16, loss = 0.00711261\n",
      "Iteration 17, loss = 0.00574400\n",
      "Iteration 18, loss = 0.00475448\n",
      "Iteration 19, loss = 0.00438609\n",
      "Iteration 20, loss = 0.00456683\n",
      "Iteration 21, loss = 0.00499555\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05703619\n",
      "Iteration 2, loss = 0.02705431\n",
      "Iteration 3, loss = 0.01274350\n",
      "Iteration 4, loss = 0.01164935\n",
      "Iteration 5, loss = 0.01594324\n",
      "Iteration 6, loss = 0.01746047\n",
      "Iteration 7, loss = 0.01452229\n",
      "Iteration 8, loss = 0.00968674\n",
      "Iteration 9, loss = 0.00573243\n",
      "Iteration 10, loss = 0.00410644\n",
      "Iteration 11, loss = 0.00475418\n",
      "Iteration 12, loss = 0.00657314\n",
      "Iteration 13, loss = 0.00821928\n",
      "Iteration 14, loss = 0.00885166\n",
      "Iteration 15, loss = 0.00835536\n",
      "Iteration 16, loss = 0.00713143\n",
      "Iteration 17, loss = 0.00575216\n",
      "Iteration 18, loss = 0.00472904\n",
      "Iteration 19, loss = 0.00431824\n",
      "Iteration 20, loss = 0.00447104\n",
      "Iteration 21, loss = 0.00489912\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.05683080\n",
      "Iteration 2, loss = 0.02699595\n",
      "Iteration 3, loss = 0.01280627\n",
      "Iteration 4, loss = 0.01177226\n",
      "Iteration 5, loss = 0.01601562\n",
      "Iteration 6, loss = 0.01745326\n",
      "Iteration 7, loss = 0.01447150\n",
      "Iteration 8, loss = 0.00964789\n",
      "Iteration 9, loss = 0.00574311\n",
      "Iteration 10, loss = 0.00417503\n",
      "Iteration 11, loss = 0.00486897\n",
      "Iteration 12, loss = 0.00670074\n",
      "Iteration 13, loss = 0.00832435\n",
      "Iteration 14, loss = 0.00891579\n",
      "Iteration 15, loss = 0.00837958\n",
      "Iteration 16, loss = 0.00713543\n",
      "Iteration 17, loss = 0.00576590\n",
      "Iteration 18, loss = 0.00476797\n",
      "Iteration 19, loss = 0.00438857\n",
      "Iteration 20, loss = 0.00456138\n",
      "Iteration 21, loss = 0.00498909\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05647991\n",
      "Iteration 2, loss = 0.02681405\n",
      "Iteration 3, loss = 0.01276933\n",
      "Iteration 4, loss = 0.01179105\n",
      "Iteration 5, loss = 0.01598648\n",
      "Iteration 6, loss = 0.01735409\n",
      "Iteration 7, loss = 0.01434018\n",
      "Iteration 8, loss = 0.00953134\n",
      "Iteration 9, loss = 0.00567147\n",
      "Iteration 10, loss = 0.00416071\n",
      "Iteration 11, loss = 0.00489942\n",
      "Iteration 12, loss = 0.00674543\n",
      "Iteration 13, loss = 0.00834922\n",
      "Iteration 14, loss = 0.00890454\n",
      "Iteration 15, loss = 0.00833490\n",
      "Iteration 16, loss = 0.00706805\n",
      "Iteration 17, loss = 0.00570211\n",
      "Iteration 18, loss = 0.00472609\n",
      "Iteration 19, loss = 0.00437384\n",
      "Iteration 20, loss = 0.00456848\n",
      "Iteration 21, loss = 0.00499623\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05632688\n",
      "Iteration 2, loss = 0.02669637\n",
      "Iteration 3, loss = 0.01271469\n",
      "Iteration 4, loss = 0.01179525\n",
      "Iteration 5, loss = 0.01599005\n",
      "Iteration 6, loss = 0.01732732\n",
      "Iteration 7, loss = 0.01429418\n",
      "Iteration 8, loss = 0.00948196\n",
      "Iteration 9, loss = 0.00563411\n",
      "Iteration 10, loss = 0.00414176\n",
      "Iteration 11, loss = 0.00489154\n",
      "Iteration 12, loss = 0.00673162\n",
      "Iteration 13, loss = 0.00832746\n",
      "Iteration 14, loss = 0.00887586\n",
      "Iteration 15, loss = 0.00830503\n",
      "Iteration 16, loss = 0.00704647\n",
      "Iteration 17, loss = 0.00568281\n",
      "Iteration 18, loss = 0.00471041\n",
      "Iteration 19, loss = 0.00436397\n",
      "Iteration 20, loss = 0.00455900\n",
      "Iteration 21, loss = 0.00498711\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05637036\n",
      "Iteration 2, loss = 0.02674360\n",
      "Iteration 3, loss = 0.01274569\n",
      "Iteration 4, loss = 0.01180062\n",
      "Iteration 5, loss = 0.01598069\n",
      "Iteration 6, loss = 0.01732792\n",
      "Iteration 7, loss = 0.01431251\n",
      "Iteration 8, loss = 0.00951043\n",
      "Iteration 9, loss = 0.00566051\n",
      "Iteration 10, loss = 0.00416305\n",
      "Iteration 11, loss = 0.00491230\n",
      "Iteration 12, loss = 0.00676003\n",
      "Iteration 13, loss = 0.00835860\n",
      "Iteration 14, loss = 0.00890659\n",
      "Iteration 15, loss = 0.00833204\n",
      "Iteration 16, loss = 0.00706574\n",
      "Iteration 17, loss = 0.00570150\n",
      "Iteration 18, loss = 0.00472992\n",
      "Iteration 19, loss = 0.00438282\n",
      "Iteration 20, loss = 0.00458006\n",
      "Iteration 21, loss = 0.00500802\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05627693\n",
      "Iteration 2, loss = 0.02669738\n",
      "Iteration 3, loss = 0.01273044\n",
      "Iteration 4, loss = 0.01179857\n",
      "Iteration 5, loss = 0.01596776\n",
      "Iteration 6, loss = 0.01729714\n",
      "Iteration 7, loss = 0.01427762\n",
      "Iteration 8, loss = 0.00948435\n",
      "Iteration 9, loss = 0.00564820\n",
      "Iteration 10, loss = 0.00416212\n",
      "Iteration 11, loss = 0.00491581\n",
      "Iteration 12, loss = 0.00676258\n",
      "Iteration 13, loss = 0.00835714\n",
      "Iteration 14, loss = 0.00890066\n",
      "Iteration 15, loss = 0.00832396\n",
      "Iteration 16, loss = 0.00705640\n",
      "Iteration 17, loss = 0.00569302\n",
      "Iteration 18, loss = 0.00472352\n",
      "Iteration 19, loss = 0.00437914\n",
      "Iteration 20, loss = 0.00457762\n",
      "Iteration 21, loss = 0.00500480\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05665233\n",
      "Iteration 2, loss = 0.02692796\n",
      "Iteration 3, loss = 0.01280838\n",
      "Iteration 4, loss = 0.01176778\n",
      "Iteration 5, loss = 0.01595076\n",
      "Iteration 6, loss = 0.01735574\n",
      "Iteration 7, loss = 0.01437904\n",
      "Iteration 8, loss = 0.00957319\n",
      "Iteration 9, loss = 0.00568881\n",
      "Iteration 10, loss = 0.00414301\n",
      "Iteration 11, loss = 0.00484640\n",
      "Iteration 12, loss = 0.00667683\n",
      "Iteration 13, loss = 0.00829158\n",
      "Iteration 14, loss = 0.00887178\n",
      "Iteration 15, loss = 0.00832994\n",
      "Iteration 16, loss = 0.00708588\n",
      "Iteration 17, loss = 0.00572052\n",
      "Iteration 18, loss = 0.00473026\n",
      "Iteration 19, loss = 0.00435775\n",
      "Iteration 20, loss = 0.00453751\n",
      "Iteration 21, loss = 0.00496730\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05665591\n",
      "Iteration 2, loss = 0.02693331\n",
      "Iteration 3, loss = 0.01280873\n",
      "Iteration 4, loss = 0.01176368\n",
      "Iteration 5, loss = 0.01595074\n",
      "Iteration 6, loss = 0.01736474\n",
      "Iteration 7, loss = 0.01439500\n",
      "Iteration 8, loss = 0.00958969\n",
      "Iteration 9, loss = 0.00569781\n",
      "Iteration 10, loss = 0.00413453\n",
      "Iteration 11, loss = 0.00482630\n",
      "Iteration 12, loss = 0.00665361\n",
      "Iteration 13, loss = 0.00827946\n",
      "Iteration 14, loss = 0.00887235\n",
      "Iteration 15, loss = 0.00833995\n",
      "Iteration 16, loss = 0.00710024\n",
      "Iteration 17, loss = 0.00573400\n",
      "Iteration 18, loss = 0.00474321\n",
      "Iteration 19, loss = 0.00437213\n",
      "Iteration 20, loss = 0.00455203\n",
      "Iteration 21, loss = 0.00498229\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05670865\n",
      "Iteration 2, loss = 0.02697201\n",
      "Iteration 3, loss = 0.01280364\n",
      "Iteration 4, loss = 0.01170888\n",
      "Iteration 5, loss = 0.01587926\n",
      "Iteration 6, loss = 0.01731772\n",
      "Iteration 7, loss = 0.01437874\n",
      "Iteration 8, loss = 0.00957874\n",
      "Iteration 9, loss = 0.00565779\n",
      "Iteration 10, loss = 0.00406040\n",
      "Iteration 11, loss = 0.00473295\n",
      "Iteration 12, loss = 0.00655855\n",
      "Iteration 13, loss = 0.00819426\n",
      "Iteration 14, loss = 0.00880097\n",
      "Iteration 15, loss = 0.00827962\n",
      "Iteration 16, loss = 0.00703859\n",
      "Iteration 17, loss = 0.00566452\n",
      "Iteration 18, loss = 0.00466306\n",
      "Iteration 19, loss = 0.00428349\n",
      "Iteration 20, loss = 0.00445889\n",
      "Iteration 21, loss = 0.00488694\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05638123\n",
      "Iteration 2, loss = 0.02679768\n",
      "Iteration 3, loss = 0.01278732\n",
      "Iteration 4, loss = 0.01180287\n",
      "Iteration 5, loss = 0.01597456\n",
      "Iteration 6, loss = 0.01733717\n",
      "Iteration 7, loss = 0.01433957\n",
      "Iteration 8, loss = 0.00954132\n",
      "Iteration 9, loss = 0.00568032\n",
      "Iteration 10, loss = 0.00415644\n",
      "Iteration 11, loss = 0.00487581\n",
      "Iteration 12, loss = 0.00670404\n",
      "Iteration 13, loss = 0.00830871\n",
      "Iteration 14, loss = 0.00887607\n",
      "Iteration 15, loss = 0.00832088\n",
      "Iteration 16, loss = 0.00707252\n",
      "Iteration 17, loss = 0.00571068\n",
      "Iteration 18, loss = 0.00473340\n",
      "Iteration 19, loss = 0.00437801\n",
      "Iteration 20, loss = 0.00456662\n",
      "Iteration 21, loss = 0.00499433\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05598121\n",
      "Iteration 2, loss = 0.02654013\n",
      "Iteration 3, loss = 0.01267679\n",
      "Iteration 4, loss = 0.01178911\n",
      "Iteration 5, loss = 0.01595292\n",
      "Iteration 6, loss = 0.01725971\n",
      "Iteration 7, loss = 0.01422649\n",
      "Iteration 8, loss = 0.00943671\n",
      "Iteration 9, loss = 0.00562053\n",
      "Iteration 10, loss = 0.00415652\n",
      "Iteration 11, loss = 0.00492102\n",
      "Iteration 12, loss = 0.00675944\n",
      "Iteration 13, loss = 0.00834107\n",
      "Iteration 14, loss = 0.00887722\n",
      "Iteration 15, loss = 0.00830037\n",
      "Iteration 16, loss = 0.00704084\n",
      "Iteration 17, loss = 0.00568172\n",
      "Iteration 18, loss = 0.00471771\n",
      "Iteration 19, loss = 0.00438252\n",
      "Iteration 20, loss = 0.00458539\n",
      "Iteration 21, loss = 0.00501417\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05552645\n",
      "Iteration 2, loss = 0.02619750\n",
      "Iteration 3, loss = 0.01246171\n",
      "Iteration 4, loss = 0.01168119\n",
      "Iteration 5, loss = 0.01584476\n",
      "Iteration 6, loss = 0.01708407\n",
      "Iteration 7, loss = 0.01401127\n",
      "Iteration 8, loss = 0.00923517\n",
      "Iteration 9, loss = 0.00546875\n",
      "Iteration 10, loss = 0.00406630\n",
      "Iteration 11, loss = 0.00487270\n",
      "Iteration 12, loss = 0.00671816\n",
      "Iteration 13, loss = 0.00827519\n",
      "Iteration 14, loss = 0.00877325\n",
      "Iteration 15, loss = 0.00816476\n",
      "Iteration 16, loss = 0.00689465\n",
      "Iteration 17, loss = 0.00554922\n",
      "Iteration 18, loss = 0.00461162\n",
      "Iteration 19, loss = 0.00430183\n",
      "Iteration 20, loss = 0.00451898\n",
      "Iteration 21, loss = 0.00494747\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05560385\n",
      "Iteration 2, loss = 0.02625261\n",
      "Iteration 3, loss = 0.01251201\n",
      "Iteration 4, loss = 0.01171698\n",
      "Iteration 5, loss = 0.01587274\n",
      "Iteration 6, loss = 0.01712141\n",
      "Iteration 7, loss = 0.01406444\n",
      "Iteration 8, loss = 0.00929562\n",
      "Iteration 9, loss = 0.00552452\n",
      "Iteration 10, loss = 0.00410636\n",
      "Iteration 11, loss = 0.00489969\n",
      "Iteration 12, loss = 0.00674330\n",
      "Iteration 13, loss = 0.00830541\n",
      "Iteration 14, loss = 0.00881723\n",
      "Iteration 15, loss = 0.00822245\n",
      "Iteration 16, loss = 0.00695385\n",
      "Iteration 17, loss = 0.00560193\n",
      "Iteration 18, loss = 0.00465444\n",
      "Iteration 19, loss = 0.00433714\n",
      "Iteration 20, loss = 0.00455078\n",
      "Iteration 21, loss = 0.00498021\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05512533\n",
      "Iteration 2, loss = 0.02592788\n",
      "Iteration 3, loss = 0.01232227\n",
      "Iteration 4, loss = 0.01161995\n",
      "Iteration 5, loss = 0.01576068\n",
      "Iteration 6, loss = 0.01694675\n",
      "Iteration 7, loss = 0.01385668\n",
      "Iteration 8, loss = 0.00910330\n",
      "Iteration 9, loss = 0.00538011\n",
      "Iteration 10, loss = 0.00401297\n",
      "Iteration 11, loss = 0.00483280\n",
      "Iteration 12, loss = 0.00666916\n",
      "Iteration 13, loss = 0.00820681\n",
      "Iteration 14, loss = 0.00868439\n",
      "Iteration 15, loss = 0.00806230\n",
      "Iteration 16, loss = 0.00678899\n",
      "Iteration 17, loss = 0.00544747\n",
      "Iteration 18, loss = 0.00451926\n",
      "Iteration 19, loss = 0.00421867\n",
      "Iteration 20, loss = 0.00444054\n",
      "Iteration 21, loss = 0.00486612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05661583\n",
      "Iteration 2, loss = 0.02694559\n",
      "Iteration 3, loss = 0.01280338\n",
      "Iteration 4, loss = 0.01165906\n",
      "Iteration 5, loss = 0.01575771\n",
      "Iteration 6, loss = 0.01719911\n",
      "Iteration 7, loss = 0.01429272\n",
      "Iteration 8, loss = 0.00951399\n",
      "Iteration 9, loss = 0.00558825\n",
      "Iteration 10, loss = 0.00398061\n",
      "Iteration 11, loss = 0.00464299\n",
      "Iteration 12, loss = 0.00646487\n",
      "Iteration 13, loss = 0.00810567\n",
      "Iteration 14, loss = 0.00872425\n",
      "Iteration 15, loss = 0.00821835\n",
      "Iteration 16, loss = 0.00699015\n",
      "Iteration 17, loss = 0.00562204\n",
      "Iteration 18, loss = 0.00461983\n",
      "Iteration 19, loss = 0.00423362\n",
      "Iteration 20, loss = 0.00440131\n",
      "Iteration 21, loss = 0.00482728\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05684738\n",
      "Iteration 2, loss = 0.02703680\n",
      "Iteration 3, loss = 0.01273371\n",
      "Iteration 4, loss = 0.01148569\n",
      "Iteration 5, loss = 0.01558485\n",
      "Iteration 6, loss = 0.01708036\n",
      "Iteration 7, loss = 0.01422029\n",
      "Iteration 8, loss = 0.00944334\n",
      "Iteration 9, loss = 0.00547910\n",
      "Iteration 10, loss = 0.00380066\n",
      "Iteration 11, loss = 0.00441010\n",
      "Iteration 12, loss = 0.00622226\n",
      "Iteration 13, loss = 0.00788879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 0.00855010\n",
      "Iteration 15, loss = 0.00807908\n",
      "Iteration 16, loss = 0.00686752\n",
      "Iteration 17, loss = 0.00550248\n",
      "Iteration 18, loss = 0.00448914\n",
      "Iteration 19, loss = 0.00408648\n",
      "Iteration 20, loss = 0.00424475\n",
      "Iteration 21, loss = 0.00467182\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05516195\n",
      "Iteration 2, loss = 0.02606234\n",
      "Iteration 3, loss = 0.01248019\n",
      "Iteration 4, loss = 0.01171711\n",
      "Iteration 5, loss = 0.01582053\n",
      "Iteration 6, loss = 0.01702727\n",
      "Iteration 7, loss = 0.01397485\n",
      "Iteration 8, loss = 0.00924375\n",
      "Iteration 9, loss = 0.00551192\n",
      "Iteration 10, loss = 0.00411855\n",
      "Iteration 11, loss = 0.00491563\n",
      "Iteration 12, loss = 0.00674235\n",
      "Iteration 13, loss = 0.00828293\n",
      "Iteration 14, loss = 0.00877150\n",
      "Iteration 15, loss = 0.00816398\n",
      "Iteration 16, loss = 0.00690082\n",
      "Iteration 17, loss = 0.00556411\n",
      "Iteration 18, loss = 0.00463246\n",
      "Iteration 19, loss = 0.00432305\n",
      "Iteration 20, loss = 0.00453584\n",
      "Iteration 21, loss = 0.00495711\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05506990\n",
      "Iteration 2, loss = 0.02599102\n",
      "Iteration 3, loss = 0.01241947\n",
      "Iteration 4, loss = 0.01171393\n",
      "Iteration 5, loss = 0.01585266\n",
      "Iteration 6, loss = 0.01704778\n",
      "Iteration 7, loss = 0.01396911\n",
      "Iteration 8, loss = 0.00922175\n",
      "Iteration 9, loss = 0.00549217\n",
      "Iteration 10, loss = 0.00411119\n",
      "Iteration 11, loss = 0.00492227\n",
      "Iteration 12, loss = 0.00675589\n",
      "Iteration 13, loss = 0.00829709\n",
      "Iteration 14, loss = 0.00877490\n",
      "Iteration 15, loss = 0.00815701\n",
      "Iteration 16, loss = 0.00688698\n",
      "Iteration 17, loss = 0.00555220\n",
      "Iteration 18, loss = 0.00463333\n",
      "Iteration 19, loss = 0.00434184\n",
      "Iteration 20, loss = 0.00456976\n",
      "Iteration 21, loss = 0.00499722\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05487448\n",
      "Iteration 2, loss = 0.02579571\n",
      "Iteration 3, loss = 0.01227874\n",
      "Iteration 4, loss = 0.01164579\n",
      "Iteration 5, loss = 0.01579764\n",
      "Iteration 6, loss = 0.01697020\n",
      "Iteration 7, loss = 0.01388363\n",
      "Iteration 8, loss = 0.00914975\n",
      "Iteration 9, loss = 0.00544496\n",
      "Iteration 10, loss = 0.00408577\n",
      "Iteration 11, loss = 0.00490937\n",
      "Iteration 12, loss = 0.00674390\n",
      "Iteration 13, loss = 0.00827588\n",
      "Iteration 14, loss = 0.00874364\n",
      "Iteration 15, loss = 0.00812042\n",
      "Iteration 16, loss = 0.00685092\n",
      "Iteration 17, loss = 0.00552040\n",
      "Iteration 18, loss = 0.00460852\n",
      "Iteration 19, loss = 0.00432327\n",
      "Iteration 20, loss = 0.00455777\n",
      "Iteration 21, loss = 0.00498918\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05465073\n",
      "Iteration 2, loss = 0.02578955\n",
      "Iteration 3, loss = 0.01236235\n",
      "Iteration 4, loss = 0.01169979\n",
      "Iteration 5, loss = 0.01580980\n",
      "Iteration 6, loss = 0.01693762\n",
      "Iteration 7, loss = 0.01382852\n",
      "Iteration 8, loss = 0.00910999\n",
      "Iteration 9, loss = 0.00546037\n",
      "Iteration 10, loss = 0.00413217\n",
      "Iteration 11, loss = 0.00495590\n",
      "Iteration 12, loss = 0.00677517\n",
      "Iteration 13, loss = 0.00828637\n",
      "Iteration 14, loss = 0.00874440\n",
      "Iteration 15, loss = 0.00812256\n",
      "Iteration 16, loss = 0.00686915\n",
      "Iteration 17, loss = 0.00555986\n",
      "Iteration 18, loss = 0.00466453\n",
      "Iteration 19, loss = 0.00438512\n",
      "Iteration 20, loss = 0.00461198\n",
      "Iteration 21, loss = 0.00502991\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05563616\n",
      "Iteration 2, loss = 0.02642609\n",
      "Iteration 3, loss = 0.01268951\n",
      "Iteration 4, loss = 0.01180241\n",
      "Iteration 5, loss = 0.01588282\n",
      "Iteration 6, loss = 0.01715958\n",
      "Iteration 7, loss = 0.01415990\n",
      "Iteration 8, loss = 0.00941515\n",
      "Iteration 9, loss = 0.00561076\n",
      "Iteration 10, loss = 0.00413206\n",
      "Iteration 11, loss = 0.00487062\n",
      "Iteration 12, loss = 0.00668885\n",
      "Iteration 13, loss = 0.00827016\n",
      "Iteration 14, loss = 0.00881258\n",
      "Iteration 15, loss = 0.00824817\n",
      "Iteration 16, loss = 0.00700134\n",
      "Iteration 17, loss = 0.00565358\n",
      "Iteration 18, loss = 0.00469655\n",
      "Iteration 19, loss = 0.00436113\n",
      "Iteration 20, loss = 0.00456001\n",
      "Iteration 21, loss = 0.00498387\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05641763\n",
      "Iteration 2, loss = 0.02690046\n",
      "Iteration 3, loss = 0.01280457\n",
      "Iteration 4, loss = 0.01165513\n",
      "Iteration 5, loss = 0.01570432\n",
      "Iteration 6, loss = 0.01711719\n",
      "Iteration 7, loss = 0.01421923\n",
      "Iteration 8, loss = 0.00946394\n",
      "Iteration 9, loss = 0.00555457\n",
      "Iteration 10, loss = 0.00395423\n",
      "Iteration 11, loss = 0.00460994\n",
      "Iteration 12, loss = 0.00642676\n",
      "Iteration 13, loss = 0.00806527\n",
      "Iteration 14, loss = 0.00868465\n",
      "Iteration 15, loss = 0.00818308\n",
      "Iteration 16, loss = 0.00696074\n",
      "Iteration 17, loss = 0.00560271\n",
      "Iteration 18, loss = 0.00461124\n",
      "Iteration 19, loss = 0.00423443\n",
      "Iteration 20, loss = 0.00440817\n",
      "Iteration 21, loss = 0.00483440\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05628192\n",
      "Iteration 2, loss = 0.02684449\n",
      "Iteration 3, loss = 0.01280726\n",
      "Iteration 4, loss = 0.01164609\n",
      "Iteration 5, loss = 0.01567312\n",
      "Iteration 6, loss = 0.01707158\n",
      "Iteration 7, loss = 0.01416769\n",
      "Iteration 8, loss = 0.00941305\n",
      "Iteration 9, loss = 0.00551034\n",
      "Iteration 10, loss = 0.00392037\n",
      "Iteration 11, loss = 0.00459149\n",
      "Iteration 12, loss = 0.00641297\n",
      "Iteration 13, loss = 0.00804653\n",
      "Iteration 14, loss = 0.00865643\n",
      "Iteration 15, loss = 0.00814452\n",
      "Iteration 16, loss = 0.00691929\n",
      "Iteration 17, loss = 0.00556445\n",
      "Iteration 18, loss = 0.00457889\n",
      "Iteration 19, loss = 0.00421026\n",
      "Iteration 20, loss = 0.00438857\n",
      "Iteration 21, loss = 0.00481187\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05458153\n",
      "Iteration 2, loss = 0.02564328\n",
      "Iteration 3, loss = 0.01224324\n",
      "Iteration 4, loss = 0.01163732\n",
      "Iteration 5, loss = 0.01577416\n",
      "Iteration 6, loss = 0.01693722\n",
      "Iteration 7, loss = 0.01385950\n",
      "Iteration 8, loss = 0.00914110\n",
      "Iteration 9, loss = 0.00544275\n",
      "Iteration 10, loss = 0.00408716\n",
      "Iteration 11, loss = 0.00490929\n",
      "Iteration 12, loss = 0.00673965\n",
      "Iteration 13, loss = 0.00827101\n",
      "Iteration 14, loss = 0.00873878\n",
      "Iteration 15, loss = 0.00811106\n",
      "Iteration 16, loss = 0.00683727\n",
      "Iteration 17, loss = 0.00550590\n",
      "Iteration 18, loss = 0.00459426\n",
      "Iteration 19, loss = 0.00431143\n",
      "Iteration 20, loss = 0.00454635\n",
      "Iteration 21, loss = 0.00497625\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05428547\n",
      "Iteration 2, loss = 0.02544795\n",
      "Iteration 3, loss = 0.01213432\n",
      "Iteration 4, loss = 0.01159146\n",
      "Iteration 5, loss = 0.01574620\n",
      "Iteration 6, loss = 0.01686870\n",
      "Iteration 7, loss = 0.01374492\n",
      "Iteration 8, loss = 0.00901922\n",
      "Iteration 9, loss = 0.00536323\n",
      "Iteration 10, loss = 0.00405312\n",
      "Iteration 11, loss = 0.00489824\n",
      "Iteration 12, loss = 0.00672746\n",
      "Iteration 13, loss = 0.00823878\n",
      "Iteration 14, loss = 0.00868248\n",
      "Iteration 15, loss = 0.00803651\n",
      "Iteration 16, loss = 0.00676336\n",
      "Iteration 17, loss = 0.00544760\n",
      "Iteration 18, loss = 0.00456204\n",
      "Iteration 19, loss = 0.00430572\n",
      "Iteration 20, loss = 0.00455663\n",
      "Iteration 21, loss = 0.00499166\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05487347\n",
      "Iteration 2, loss = 0.02581192\n",
      "Iteration 3, loss = 0.01234883\n",
      "Iteration 4, loss = 0.01171130\n",
      "Iteration 5, loss = 0.01584435\n",
      "Iteration 6, loss = 0.01701277\n",
      "Iteration 7, loss = 0.01393165\n",
      "Iteration 8, loss = 0.00920455\n",
      "Iteration 9, loss = 0.00550274\n",
      "Iteration 10, loss = 0.00413602\n",
      "Iteration 11, loss = 0.00494253\n",
      "Iteration 12, loss = 0.00676431\n",
      "Iteration 13, loss = 0.00829924\n",
      "Iteration 14, loss = 0.00877993\n",
      "Iteration 15, loss = 0.00816975\n",
      "Iteration 16, loss = 0.00690808\n",
      "Iteration 17, loss = 0.00558098\n",
      "Iteration 18, loss = 0.00466456\n",
      "Iteration 19, loss = 0.00437188\n",
      "Iteration 20, loss = 0.00459844\n",
      "Iteration 21, loss = 0.00502758\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05533667\n",
      "Iteration 2, loss = 0.02629611\n",
      "Iteration 3, loss = 0.01265417\n",
      "Iteration 4, loss = 0.01179975\n",
      "Iteration 5, loss = 0.01586165\n",
      "Iteration 6, loss = 0.01708084\n",
      "Iteration 7, loss = 0.01404049\n",
      "Iteration 8, loss = 0.00930376\n",
      "Iteration 9, loss = 0.00556282\n",
      "Iteration 10, loss = 0.00413673\n",
      "Iteration 11, loss = 0.00489594\n",
      "Iteration 12, loss = 0.00670706\n",
      "Iteration 13, loss = 0.00826036\n",
      "Iteration 14, loss = 0.00877251\n",
      "Iteration 15, loss = 0.00819247\n",
      "Iteration 16, loss = 0.00695107\n",
      "Iteration 17, loss = 0.00562212\n",
      "Iteration 18, loss = 0.00469101\n",
      "Iteration 19, loss = 0.00437552\n",
      "Iteration 20, loss = 0.00458184\n",
      "Iteration 21, loss = 0.00500172\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05608402\n",
      "Iteration 2, loss = 0.02672178\n",
      "Iteration 3, loss = 0.01279340\n",
      "Iteration 4, loss = 0.01174275\n",
      "Iteration 5, loss = 0.01578253\n",
      "Iteration 6, loss = 0.01712462\n",
      "Iteration 7, loss = 0.01417899\n",
      "Iteration 8, loss = 0.00942820\n",
      "Iteration 9, loss = 0.00556474\n",
      "Iteration 10, loss = 0.00401828\n",
      "Iteration 11, loss = 0.00471364\n",
      "Iteration 12, loss = 0.00653106\n",
      "Iteration 13, loss = 0.00814335\n",
      "Iteration 14, loss = 0.00873001\n",
      "Iteration 15, loss = 0.00820002\n",
      "Iteration 16, loss = 0.00696672\n",
      "Iteration 17, loss = 0.00561475\n",
      "Iteration 18, loss = 0.00464160\n",
      "Iteration 19, loss = 0.00428532\n",
      "Iteration 20, loss = 0.00446976\n",
      "Iteration 21, loss = 0.00489176\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05640059\n",
      "Iteration 2, loss = 0.02688808\n",
      "Iteration 3, loss = 0.01280622\n",
      "Iteration 4, loss = 0.01163918\n",
      "Iteration 5, loss = 0.01568709\n",
      "Iteration 6, loss = 0.01709543\n",
      "Iteration 7, loss = 0.01419956\n",
      "Iteration 8, loss = 0.00944582\n",
      "Iteration 9, loss = 0.00553447\n",
      "Iteration 10, loss = 0.00391625\n",
      "Iteration 11, loss = 0.00456378\n",
      "Iteration 12, loss = 0.00637635\n",
      "Iteration 13, loss = 0.00801284\n",
      "Iteration 14, loss = 0.00863292\n",
      "Iteration 15, loss = 0.00812953\n",
      "Iteration 16, loss = 0.00690617\n",
      "Iteration 17, loss = 0.00555414\n",
      "Iteration 18, loss = 0.00456885\n",
      "Iteration 19, loss = 0.00420016\n",
      "Iteration 20, loss = 0.00437814\n",
      "Iteration 21, loss = 0.00480198\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05578307\n",
      "Iteration 2, loss = 0.02648367\n",
      "Iteration 3, loss = 0.01272226\n",
      "Iteration 4, loss = 0.01179210\n",
      "Iteration 5, loss = 0.01584979\n",
      "Iteration 6, loss = 0.01715387\n",
      "Iteration 7, loss = 0.01418135\n",
      "Iteration 8, loss = 0.00944157\n",
      "Iteration 9, loss = 0.00562253\n",
      "Iteration 10, loss = 0.00412006\n",
      "Iteration 11, loss = 0.00483889\n",
      "Iteration 12, loss = 0.00665373\n",
      "Iteration 13, loss = 0.00824383\n",
      "Iteration 14, loss = 0.00880115\n",
      "Iteration 15, loss = 0.00824755\n",
      "Iteration 16, loss = 0.00700585\n",
      "Iteration 17, loss = 0.00566100\n",
      "Iteration 18, loss = 0.00470015\n",
      "Iteration 19, loss = 0.00436076\n",
      "Iteration 20, loss = 0.00455518\n",
      "Iteration 21, loss = 0.00497599\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05399473\n",
      "Iteration 2, loss = 0.02515858\n",
      "Iteration 3, loss = 0.01192511\n",
      "Iteration 4, loss = 0.01144517\n",
      "Iteration 5, loss = 0.01561298\n",
      "Iteration 6, loss = 0.01671803\n",
      "Iteration 7, loss = 0.01359460\n",
      "Iteration 8, loss = 0.00888785\n",
      "Iteration 9, loss = 0.00525540\n",
      "Iteration 10, loss = 0.00397080\n",
      "Iteration 11, loss = 0.00483335\n",
      "Iteration 12, loss = 0.00667618\n",
      "Iteration 13, loss = 0.00818436\n",
      "Iteration 14, loss = 0.00861274\n",
      "Iteration 15, loss = 0.00795277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.00666821\n",
      "Iteration 17, loss = 0.00535250\n",
      "Iteration 18, loss = 0.00447869\n",
      "Iteration 19, loss = 0.00423748\n",
      "Iteration 20, loss = 0.00449918\n",
      "Iteration 21, loss = 0.00493525\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05450276\n",
      "Iteration 2, loss = 0.02555858\n",
      "Iteration 3, loss = 0.01216724\n",
      "Iteration 4, loss = 0.01161446\n",
      "Iteration 5, loss = 0.01576823\n",
      "Iteration 6, loss = 0.01690575\n",
      "Iteration 7, loss = 0.01379501\n",
      "Iteration 8, loss = 0.00906976\n",
      "Iteration 9, loss = 0.00540293\n",
      "Iteration 10, loss = 0.00407992\n",
      "Iteration 11, loss = 0.00491755\n",
      "Iteration 12, loss = 0.00674522\n",
      "Iteration 13, loss = 0.00826316\n",
      "Iteration 14, loss = 0.00871521\n",
      "Iteration 15, loss = 0.00808243\n",
      "Iteration 16, loss = 0.00681659\n",
      "Iteration 17, loss = 0.00549933\n",
      "Iteration 18, loss = 0.00460448\n",
      "Iteration 19, loss = 0.00433459\n",
      "Iteration 20, loss = 0.00457627\n",
      "Iteration 21, loss = 0.00500955\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05575901\n",
      "Iteration 2, loss = 0.02647479\n",
      "Iteration 3, loss = 0.01271632\n",
      "Iteration 4, loss = 0.01180108\n",
      "Iteration 5, loss = 0.01587566\n",
      "Iteration 6, loss = 0.01716304\n",
      "Iteration 7, loss = 0.01416912\n",
      "Iteration 8, loss = 0.00941875\n",
      "Iteration 9, loss = 0.00560200\n",
      "Iteration 10, loss = 0.00410835\n",
      "Iteration 11, loss = 0.00483496\n",
      "Iteration 12, loss = 0.00665009\n",
      "Iteration 13, loss = 0.00823347\n",
      "Iteration 14, loss = 0.00878021\n",
      "Iteration 15, loss = 0.00821987\n",
      "Iteration 16, loss = 0.00697468\n",
      "Iteration 17, loss = 0.00562657\n",
      "Iteration 18, loss = 0.00466553\n",
      "Iteration 19, loss = 0.00432405\n",
      "Iteration 20, loss = 0.00451612\n",
      "Iteration 21, loss = 0.00493545\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05149115\n",
      "Iteration 2, loss = 0.02333727\n",
      "Iteration 3, loss = 0.01066278\n",
      "Iteration 4, loss = 0.01053042\n",
      "Iteration 5, loss = 0.01465860\n",
      "Iteration 6, loss = 0.01546915\n",
      "Iteration 7, loss = 0.01216115\n",
      "Iteration 8, loss = 0.00752497\n",
      "Iteration 9, loss = 0.00413956\n",
      "Iteration 10, loss = 0.00309609\n",
      "Iteration 11, loss = 0.00407952\n",
      "Iteration 12, loss = 0.00587881\n",
      "Iteration 13, loss = 0.00723474\n",
      "Iteration 14, loss = 0.00751162\n",
      "Iteration 15, loss = 0.00676638\n",
      "Iteration 16, loss = 0.00547539\n",
      "Iteration 17, loss = 0.00421295\n",
      "Iteration 18, loss = 0.00341310\n",
      "Iteration 19, loss = 0.00322392\n",
      "Iteration 20, loss = 0.00349947\n",
      "Iteration 21, loss = 0.00390423\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05482246\n",
      "Iteration 2, loss = 0.02599997\n",
      "Iteration 3, loss = 0.01253053\n",
      "Iteration 4, loss = 0.01177900\n",
      "Iteration 5, loss = 0.01583134\n",
      "Iteration 6, loss = 0.01697189\n",
      "Iteration 7, loss = 0.01389014\n",
      "Iteration 8, loss = 0.00917733\n",
      "Iteration 9, loss = 0.00550745\n",
      "Iteration 10, loss = 0.00414731\n",
      "Iteration 11, loss = 0.00494433\n",
      "Iteration 12, loss = 0.00675466\n",
      "Iteration 13, loss = 0.00827420\n",
      "Iteration 14, loss = 0.00874638\n",
      "Iteration 15, loss = 0.00813878\n",
      "Iteration 16, loss = 0.00689251\n",
      "Iteration 17, loss = 0.00558403\n",
      "Iteration 18, loss = 0.00468381\n",
      "Iteration 19, loss = 0.00439743\n",
      "Iteration 20, loss = 0.00461942\n",
      "Iteration 21, loss = 0.00503545\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Config of the regressors and cross val leave one out\n",
    "####\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes = (50,), alpha = 0.001,\n",
    "    learning_rate_init = 0.01, max_iter = 1000,\n",
    "    random_state = 9, verbose = True)\n",
    "svr = SVR(kernel = 'linear', C = 0.25, epsilon = 0.01, verbose = True, max_iter = 1000)\n",
    "lr = LinearRegression()\n",
    "\n",
    "full_predict_lr = cross_val_predict(lr, X, target, cv = 10)\n",
    "full_predict_mlp = cross_val_predict(mlp, X, target, cv = loo)\n",
    "full_predict_svr = cross_val_predict(svr, X, target, cv = loo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error in LR: 0.00599490799826504\n",
      "Mean Squared Error in MLP: 0.011419006423488113\n",
      "Mean Squared Error in SVR: 0.005935750349804434\n",
      "R score in LR: 0.8612230866468593\n",
      "R score in MLP: 0.7356599191397122\n",
      "R score in SVR: 0.8625925348280439\n",
      "adjusted R score in LR: 0.8516991808285065\n",
      "adjusted R score in MLP: 0.7175189331983198\n",
      "adjusted R score in SVR: 0.8531626107476156\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Printing some metrics of the regressors\n",
    "####\n",
    "\n",
    "print('Mean Squared Error in LR: %s' %(metrics.mean_squared_error(target, full_predict_lr)))\n",
    "print('Mean Squared Error in MLP: %s' %(metrics.mean_squared_error(target, full_predict_mlp)))\n",
    "print('Mean Squared Error in SVR: %s' %(metrics.mean_squared_error(target, full_predict_svr)))\n",
    "\n",
    "r_squared_lr = metrics.r2_score(target, full_predict_lr)\n",
    "r_squared_mlp = metrics.r2_score(target, full_predict_mlp)\n",
    "r_squared_svr = metrics.r2_score(target, full_predict_svr)\n",
    "\n",
    "print('R score in LR: %s' %(r_squared_lr))\n",
    "print('R score in MLP: %s' %(r_squared_mlp))\n",
    "print('R score in SVR: %s' %(r_squared_svr))\n",
    "\n",
    "adjusted_r_squared_lr = 1 - (1 - r_squared_lr) * (len(target) - 1) / (len(target) - X.shape[1] - 1)\n",
    "adjusted_r_squared_mlp = 1 - (1 - r_squared_mlp) * (len(target) - 1) / (len(target) - X.shape[1] - 1)\n",
    "adjusted_r_squared_svr = 1 - (1 - r_squared_svr) * (len(target) - 1) / (len(target) - X.shape[1] - 1)\n",
    "\n",
    "print('adjusted R score in LR: %s' %(adjusted_r_squared_lr))\n",
    "print('adjusted R score in MLP: %s' %(adjusted_r_squared_mlp))\n",
    "print('adjusted R score in SVR: %s' %(adjusted_r_squared_svr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "## Filling lists with NaN so the len is the same across all lists \n",
    "## so that a graph can be generated\n",
    "####\n",
    "import numpy as np\n",
    "\n",
    "values_to_add = list()\n",
    "for i in range(0, window_size):\n",
    "    values_to_add.append(float('NaN'))\n",
    "    \n",
    "full_predict_svr = np.insert(full_predict_svr, 0, values_to_add)\n",
    "full_predict_svr.shape = (len(full_predict_svr), 1)\n",
    "    \n",
    "full_predict_mlp = np.insert(full_predict_mlp, 0, values_to_add)\n",
    "full_predict_mlp.shape = (len(full_predict_mlp), 1)\n",
    "\n",
    "full_predict_lr = np.insert(full_predict_lr, 0, values_to_add)\n",
    "full_predict_lr.shape = (len(full_predict_lr), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "## Adding the data to plot \n",
    "####\n",
    "\n",
    "data['Predict_lr'] = full_predict_lr\n",
    "data['Predict_mlp'] = full_predict_mlp\n",
    "data['Predict_svr'] = full_predict_svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data.plot(y=['CasosNormalizados', 'Predict_mlp', 'Predict_svr', 'Predict_lr'], style=['-s', '--o'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
