{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "## Celso Antonio Uliana Junior\n",
    "## July 2 2020\n",
    "####\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#####\n",
    "## Consuming and shaping the data to analysis\n",
    "## Covid-19 numbers in Brazil by date\n",
    "## Isolation percentage in Brazil by date\n",
    "#####\n",
    "\n",
    "data_raw_covid = pd.read_csv(\"C:/Users/PCDOMILHAO/Documents/GitHub/trab-siad/scripts/Python/Jupyter/dados/covidBrasil.csv\", sep = \";\", decimal = \",\")\n",
    "data_raw_isolation = pd.read_csv(\"C:/Users/PCDOMILHAO/Documents/GitHub/trab-siad/scripts/Python/Jupyter/dados/isolamento.csv\", sep = \";\", decimal = \",\")\n",
    "data_covid = data_raw_covid['Data'].values.copy()\n",
    "data_covid = data_raw_covid.dropna().set_index(\"Data\")\n",
    "data_isolation = data_raw_isolation['Data'].values.copy()\n",
    "data_isolation = data_raw_isolation.dropna().set_index(\"Data\")\n",
    "\n",
    "####\n",
    "## Shaping a central pandas dataFrame for all our ML needs\n",
    "####\n",
    "\n",
    "data2 = data_covid\n",
    "data2['Taxa'] = data_isolation['Taxa'].values.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Casos</th>\n",
       "      <th>Taxa</th>\n",
       "      <th>CasosNormalizados</th>\n",
       "      <th>TaxaNormalizadas</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26/2/20</th>\n",
       "      <td>1</td>\n",
       "      <td>24.7</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>27.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>31.4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/3/20</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.461333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>27.7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>30.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/3/20</th>\n",
       "      <td>1</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6/3/20</th>\n",
       "      <td>5</td>\n",
       "      <td>28.5</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.101333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7/3/20</th>\n",
       "      <td>5</td>\n",
       "      <td>31.8</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.189333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.434667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9/3/20</th>\n",
       "      <td>12</td>\n",
       "      <td>29.2</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/3/20</th>\n",
       "      <td>9</td>\n",
       "      <td>28.3</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.096000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/3/20</th>\n",
       "      <td>18</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13/3/20</th>\n",
       "      <td>25</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14/3/20</th>\n",
       "      <td>21</td>\n",
       "      <td>35.7</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.293333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15/3/20</th>\n",
       "      <td>23</td>\n",
       "      <td>42.6</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.477333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16/3/20</th>\n",
       "      <td>79</td>\n",
       "      <td>32.1</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.197333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Casos  Taxa  CasosNormalizados  TaxaNormalizadas\n",
       "Data                                                     \n",
       "26/2/20      1  24.7           0.000018          0.000000\n",
       "27/2/20      0  27.5           0.000000          0.074667\n",
       "28/2/20      0  26.6           0.000000          0.050667\n",
       "29/2/20      0  31.4           0.000000          0.178667\n",
       "1/3/20       1    42           0.000018          0.461333\n",
       "2/3/20       0  27.7           0.000000          0.080000\n",
       "3/3/20       0    29           0.000000          0.114667\n",
       "4/3/20       0  30.2           0.000000          0.146667\n",
       "5/3/20       1  29.7           0.000018          0.133333\n",
       "6/3/20       5  28.5           0.000091          0.101333\n",
       "7/3/20       5  31.8           0.000091          0.189333\n",
       "8/3/20       0    41           0.000000          0.434667\n",
       "9/3/20      12  29.2           0.000219          0.120000\n",
       "10/3/20      0  29.7           0.000000          0.133333\n",
       "11/3/20      9  28.3           0.000164          0.096000\n",
       "12/3/20     18  30.1           0.000329          0.144000\n",
       "13/3/20     25  30.1           0.000456          0.144000\n",
       "14/3/20     21  35.7           0.000383          0.293333\n",
       "15/3/20     23  42.6           0.000420          0.477333\n",
       "16/3/20     79  32.1           0.001442          0.197333"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "####\n",
    "## normalizing values for both covid and isolation percentage \n",
    "## between range [0,1] using sklearn MinMaxScaler\n",
    "####\n",
    "\n",
    "covid_norm = data_covid[\"Casos\"].values.copy()\n",
    "covid_norm.shape = (len(covid_norm), 1)\n",
    "\n",
    "isolation_norm = data_isolation[\"Taxa\"].values.copy()\n",
    "isolation_norm.shape = (len(isolation_norm), 1)\n",
    "\n",
    "####\n",
    "## Shaping the central dataFrame with normalized values\n",
    "####\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "covid_norm = min_max_scaler.fit_transform(covid_norm)\n",
    "isolation_norm = min_max_scaler.fit_transform(isolation_norm)\n",
    "\n",
    "data2[\"CasosNormalizados\"] = covid_norm\n",
    "data2[\"TaxaNormalizadas\"] = isolation_norm\n",
    "data = data2.copy()\n",
    "#data = data.iloc[20:]\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               E0        E1        E3\n",
      "Data                                 \n",
      "26/2/20  0.000018  0.000000  0.000000\n",
      "27/2/20  0.000000  0.074667  0.000000\n",
      "28/2/20  0.000000  0.050667  0.000000\n",
      "29/2/20  0.000000  0.178667  0.000018\n",
      "1/3/20   0.000018  0.461333  0.000000\n",
      "...           ...       ...       ...\n",
      "16/6/20  0.376970  0.384000  0.637527\n",
      "17/6/20  0.637527  0.336000  0.587683\n",
      "18/6/20  0.587683  0.368000  0.415640\n",
      "19/6/20  0.415640  0.266667  1.000000\n",
      "20/6/20  1.000000  0.384000  0.632926\n",
      "\n",
      "[116 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Sliding window\n",
    "####\n",
    "\n",
    "df = pd.DataFrame()\n",
    "window_size = 7\n",
    "for i in range(0, window_size):\n",
    "    df['E{}'.format(i)] = data['CasosNormalizados'].shift(-i)\n",
    "    if(i == window_size - 1):\n",
    "        for j in range(0, window_size):\n",
    "             df['E{}'.format(j + i + 1)] = data['TaxaNormalizadas'].shift(-j)\n",
    "        df['E{}'.format(window_size * 2 + 1)] = data['CasosNormalizados'].shift(-window_size)\n",
    "df = df.iloc[:-window_size]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.82578372e-05 0.00000000e+00]\n",
      " [0.00000000e+00 7.46666667e-02]\n",
      " [0.00000000e+00 5.06666667e-02]\n",
      " [0.00000000e+00 1.78666667e-01]\n",
      " [1.82578372e-05 4.61333333e-01]\n",
      " [0.00000000e+00 8.00000000e-02]\n",
      " [0.00000000e+00 1.14666667e-01]\n",
      " [0.00000000e+00 1.46666667e-01]\n",
      " [1.82578372e-05 1.33333333e-01]\n",
      " [9.12891859e-05 1.01333333e-01]\n",
      " [9.12891859e-05 1.89333333e-01]\n",
      " [0.00000000e+00 4.34666667e-01]\n",
      " [2.19094046e-04 1.20000000e-01]\n",
      " [0.00000000e+00 1.33333333e-01]\n",
      " [1.64320535e-04 9.60000000e-02]\n",
      " [3.28641069e-04 1.44000000e-01]\n",
      " [4.56445929e-04 1.44000000e-01]\n",
      " [3.83414581e-04 2.93333333e-01]\n",
      " [4.19930255e-04 4.77333333e-01]\n",
      " [1.44236914e-03 1.97333333e-01]\n",
      " [6.20766464e-04 2.16000000e-01]\n",
      " [1.04069672e-03 3.01333333e-01]\n",
      " [2.50132369e-03 3.89333333e-01]\n",
      " [3.52376258e-03 4.21333333e-01]\n",
      " [5.16696792e-03 7.30666667e-01]\n",
      " [4.08975553e-03 1.00000000e+00]\n",
      " [7.63177594e-03 7.38666667e-01]\n",
      " [6.29895383e-03 7.76000000e-01]\n",
      " [5.65992952e-03 7.76000000e-01]\n",
      " [4.23581822e-03 7.57333333e-01]\n",
      " [8.80027752e-03 7.12000000e-01]\n",
      " [9.16543426e-03 7.78666667e-01]\n",
      " [8.89156671e-03 9.38666667e-01]\n",
      " [6.42675869e-03 6.53333333e-01]\n",
      " [5.89728141e-03 6.34666667e-01]\n",
      " [2.07774187e-02 5.68000000e-01]\n",
      " [2.04305198e-02 6.34666667e-01]\n",
      " [1.96089171e-02 6.05333333e-01]\n",
      " [2.09234814e-02 6.77333333e-01]\n",
      " [2.23110770e-02 8.56000000e-01]\n",
      " [1.55556773e-02 6.05333333e-01]\n",
      " [1.69067572e-02 5.62666667e-01]\n",
      " [3.03262676e-02 5.41333333e-01]\n",
      " [4.03498202e-02 5.22666667e-01]\n",
      " [3.52376258e-02 7.84000000e-01]\n",
      " [3.25172080e-02 6.21333333e-01]\n",
      " [1.98827847e-02 7.81333333e-01]\n",
      " [2.63278012e-02 5.70666667e-01]\n",
      " [2.30231327e-02 5.68000000e-01]\n",
      " [3.34483577e-02 5.60000000e-01]\n",
      " [5.58324661e-02 4.82666667e-01]\n",
      " [3.84327473e-02 4.88000000e-01]\n",
      " [5.94657757e-02 5.84000000e-01]\n",
      " [5.32581110e-02 7.97333333e-01]\n",
      " [3.75198554e-02 5.52000000e-01]\n",
      " [3.51828522e-02 7.36000000e-01]\n",
      " [4.56080773e-02 4.93333333e-01]\n",
      " [4.88944880e-02 5.20000000e-01]\n",
      " [6.81930219e-02 4.69333333e-01]\n",
      " [6.39572036e-02 5.68000000e-01]\n",
      " [1.00673714e-01 7.57333333e-01]\n",
      " [6.16932318e-02 5.17333333e-01]\n",
      " [8.42234029e-02 4.98666667e-01]\n",
      " [9.83184532e-02 4.69333333e-01]\n",
      " [1.14586186e-01 4.40000000e-01]\n",
      " [1.31785069e-01 6.66666667e-01]\n",
      " [1.13362911e-01 5.14666667e-01]\n",
      " [9.07414508e-02 6.74666667e-01]\n",
      " [8.37669570e-02 4.90666667e-01]\n",
      " [1.21104234e-01 4.74666667e-01]\n",
      " [1.26618101e-01 6.05333333e-01]\n",
      " [1.91762064e-01 4.96000000e-01]\n",
      " [1.80533494e-01 4.29333333e-01]\n",
      " [1.86631612e-01 4.90666667e-01]\n",
      " [1.93733910e-01 5.94666667e-01]\n",
      " [1.23422979e-01 4.96000000e-01]\n",
      " [1.02828139e-01 5.06666667e-01]\n",
      " [1.69031057e-01 4.98666667e-01]\n",
      " [2.07865476e-01 4.88000000e-01]\n",
      " [2.54587282e-01 4.74666667e-01]\n",
      " [2.79436198e-01 5.62666667e-01]\n",
      " [2.72388673e-01 7.33333333e-01]\n",
      " [1.44930712e-01 4.80000000e-01]\n",
      " [2.39907981e-01 4.72000000e-01]\n",
      " [3.17832430e-01 4.53333333e-01]\n",
      " [3.64262110e-01 4.66666667e-01]\n",
      " [3.37916050e-01 4.40000000e-01]\n",
      " [3.79817787e-01 5.49333333e-01]\n",
      " [3.01400376e-01 7.60000000e-01]\n",
      " [2.88711179e-01 5.12000000e-01]\n",
      " [2.13379343e-01 4.50666667e-01]\n",
      " [2.98040934e-01 4.37333333e-01]\n",
      " [3.76093188e-01 4.40000000e-01]\n",
      " [4.82317285e-01 3.86666667e-01]\n",
      " [4.91647039e-01 4.77333333e-01]\n",
      " [6.07511274e-01 6.69333333e-01]\n",
      " [2.99592850e-01 4.26666667e-01]\n",
      " [2.11754396e-01 4.21333333e-01]\n",
      " [5.28308777e-01 3.94666667e-01]\n",
      " [5.22776652e-01 3.84000000e-01]\n",
      " [5.64459294e-01 3.86666667e-01]\n",
      " [5.62889120e-01 4.24000000e-01]\n",
      " [4.94330942e-01 6.40000000e-01]\n",
      " [3.45456537e-01 3.60000000e-01]\n",
      " [2.85808183e-01 3.54666667e-01]\n",
      " [5.85912253e-01 3.89333333e-01]\n",
      " [6.00920195e-01 5.06666667e-01]\n",
      " [5.55257344e-01 3.22666667e-01]\n",
      " [4.74375126e-01 4.10666667e-01]\n",
      " [3.96268098e-01 6.45333333e-01]\n",
      " [3.12391594e-01 3.81333333e-01]\n",
      " [3.76969564e-01 3.84000000e-01]\n",
      " [6.37527159e-01 3.36000000e-01]\n",
      " [5.87683263e-01 3.68000000e-01]\n",
      " [4.15639663e-01 2.66666667e-01]\n",
      " [1.00000000e+00 3.84000000e-01]]\n",
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 1.82578372e-05\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.82578372e-05\n",
      " 9.12891859e-05 9.12891859e-05 0.00000000e+00 2.19094046e-04\n",
      " 0.00000000e+00 1.64320535e-04 3.28641069e-04 4.56445929e-04\n",
      " 3.83414581e-04 4.19930255e-04 1.44236914e-03 6.20766464e-04\n",
      " 1.04069672e-03 2.50132369e-03 3.52376258e-03 5.16696792e-03\n",
      " 4.08975553e-03 7.63177594e-03 6.29895383e-03 5.65992952e-03\n",
      " 4.23581822e-03 8.80027752e-03 9.16543426e-03 8.89156671e-03\n",
      " 6.42675869e-03 5.89728141e-03 2.07774187e-02 2.04305198e-02\n",
      " 1.96089171e-02 2.09234814e-02 2.23110770e-02 1.55556773e-02\n",
      " 1.69067572e-02 3.03262676e-02 4.03498202e-02 3.52376258e-02\n",
      " 3.25172080e-02 1.98827847e-02 2.63278012e-02 2.30231327e-02\n",
      " 3.34483577e-02 5.58324661e-02 3.84327473e-02 5.94657757e-02\n",
      " 5.32581110e-02 3.75198554e-02 3.51828522e-02 4.56080773e-02\n",
      " 4.88944880e-02 6.81930219e-02 6.39572036e-02 1.00673714e-01\n",
      " 6.16932318e-02 8.42234029e-02 9.83184532e-02 1.14586186e-01\n",
      " 1.31785069e-01 1.13362911e-01 9.07414508e-02 8.37669570e-02\n",
      " 1.21104234e-01 1.26618101e-01 1.91762064e-01 1.80533494e-01\n",
      " 1.86631612e-01 1.93733910e-01 1.23422979e-01 1.02828139e-01\n",
      " 1.69031057e-01 2.07865476e-01 2.54587282e-01 2.79436198e-01\n",
      " 2.72388673e-01 1.44930712e-01 2.39907981e-01 3.17832430e-01\n",
      " 3.64262110e-01 3.37916050e-01 3.79817787e-01 3.01400376e-01\n",
      " 2.88711179e-01 2.13379343e-01 2.98040934e-01 3.76093188e-01\n",
      " 4.82317285e-01 4.91647039e-01 6.07511274e-01 2.99592850e-01\n",
      " 2.11754396e-01 5.28308777e-01 5.22776652e-01 5.64459294e-01\n",
      " 5.62889120e-01 4.94330942e-01 3.45456537e-01 2.85808183e-01\n",
      " 5.85912253e-01 6.00920195e-01 5.55257344e-01 4.74375126e-01\n",
      " 3.96268098e-01 3.12391594e-01 3.76969564e-01 6.37527159e-01\n",
      " 5.87683263e-01 4.15639663e-01 1.00000000e+00 6.32926184e-01]\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Manipulating the data to split into X(a window size of values)\n",
    "## and target, or Y, the value X \"produces\"\n",
    "####\n",
    "\n",
    "arr = df.values\n",
    "#print(arr)\n",
    "\n",
    "\n",
    "X = arr[:, : -1]\n",
    "target = arr[:, -1]\n",
    "print(X)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.01797606\n",
      "Iteration 2, loss = 0.01523016\n",
      "Iteration 3, loss = 0.01335825\n",
      "Iteration 4, loss = 0.01020597\n",
      "Iteration 5, loss = 0.00894277\n",
      "Iteration 6, loss = 0.00839802\n",
      "Iteration 7, loss = 0.00700109\n",
      "Iteration 8, loss = 0.00555849\n",
      "Iteration 9, loss = 0.00504264\n",
      "Iteration 10, loss = 0.00506703\n",
      "Iteration 11, loss = 0.00477855\n",
      "Iteration 12, loss = 0.00437938\n",
      "Iteration 13, loss = 0.00447077\n",
      "Iteration 14, loss = 0.00485258\n",
      "Iteration 15, loss = 0.00491443\n",
      "Iteration 16, loss = 0.00471926\n",
      "Iteration 17, loss = 0.00469075\n",
      "Iteration 18, loss = 0.00482206\n",
      "Iteration 19, loss = 0.00478158\n",
      "Iteration 20, loss = 0.00452092\n",
      "Iteration 21, loss = 0.00431060\n",
      "Iteration 22, loss = 0.00426595\n",
      "Iteration 23, loss = 0.00421916\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01801413\n",
      "Iteration 2, loss = 0.01525983\n",
      "Iteration 3, loss = 0.01338045\n",
      "Iteration 4, loss = 0.01020361\n",
      "Iteration 5, loss = 0.00890085\n",
      "Iteration 6, loss = 0.00832010\n",
      "Iteration 7, loss = 0.00689276\n",
      "Iteration 8, loss = 0.00542734\n",
      "Iteration 9, loss = 0.00489552\n",
      "Iteration 10, loss = 0.00490954\n",
      "Iteration 11, loss = 0.00461408\n",
      "Iteration 12, loss = 0.00422430\n",
      "Iteration 13, loss = 0.00435819\n",
      "Iteration 14, loss = 0.00477467\n",
      "Iteration 15, loss = 0.00488087\n",
      "Iteration 16, loss = 0.00474059\n",
      "Iteration 17, loss = 0.00471378\n",
      "Iteration 18, loss = 0.00476889\n",
      "Iteration 19, loss = 0.00465614\n",
      "Iteration 20, loss = 0.00438613\n",
      "Iteration 21, loss = 0.00418766\n",
      "Iteration 22, loss = 0.00411677\n",
      "Iteration 23, loss = 0.00405132\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01800173\n",
      "Iteration 2, loss = 0.01524985\n",
      "Iteration 3, loss = 0.01336960\n",
      "Iteration 4, loss = 0.01019657\n",
      "Iteration 5, loss = 0.00889818\n",
      "Iteration 6, loss = 0.00831684\n",
      "Iteration 7, loss = 0.00688875\n",
      "Iteration 8, loss = 0.00542560\n",
      "Iteration 9, loss = 0.00489675\n",
      "Iteration 10, loss = 0.00491095\n",
      "Iteration 11, loss = 0.00461448\n",
      "Iteration 12, loss = 0.00422602\n",
      "Iteration 13, loss = 0.00435948\n",
      "Iteration 14, loss = 0.00477465\n",
      "Iteration 15, loss = 0.00487895\n",
      "Iteration 16, loss = 0.00474129\n",
      "Iteration 17, loss = 0.00471747\n",
      "Iteration 18, loss = 0.00477077\n",
      "Iteration 19, loss = 0.00465582\n",
      "Iteration 20, loss = 0.00438413\n",
      "Iteration 21, loss = 0.00418785\n",
      "Iteration 22, loss = 0.00411717\n",
      "Iteration 23, loss = 0.00405007\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01807040\n",
      "Iteration 2, loss = 0.01529841\n",
      "Iteration 3, loss = 0.01342168\n",
      "Iteration 4, loss = 0.01023784\n",
      "Iteration 5, loss = 0.00892219\n",
      "Iteration 6, loss = 0.00833929\n",
      "Iteration 7, loss = 0.00690949\n",
      "Iteration 8, loss = 0.00543731\n",
      "Iteration 9, loss = 0.00489826\n",
      "Iteration 10, loss = 0.00491012\n",
      "Iteration 11, loss = 0.00461590\n",
      "Iteration 12, loss = 0.00422511\n",
      "Iteration 13, loss = 0.00435707\n",
      "Iteration 14, loss = 0.00477411\n",
      "Iteration 15, loss = 0.00488469\n",
      "Iteration 16, loss = 0.00474853\n",
      "Iteration 17, loss = 0.00472220\n",
      "Iteration 18, loss = 0.00477625\n",
      "Iteration 19, loss = 0.00466481\n",
      "Iteration 20, loss = 0.00439376\n",
      "Iteration 21, loss = 0.00419466\n",
      "Iteration 22, loss = 0.00412201\n",
      "Iteration 23, loss = 0.00405517\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01818912\n",
      "Iteration 2, loss = 0.01536516\n",
      "Iteration 3, loss = 0.01350797\n",
      "Iteration 4, loss = 0.01030660\n",
      "Iteration 5, loss = 0.00895621\n",
      "Iteration 6, loss = 0.00837094\n",
      "Iteration 7, loss = 0.00695099\n",
      "Iteration 8, loss = 0.00546494\n",
      "Iteration 9, loss = 0.00490321\n",
      "Iteration 10, loss = 0.00491250\n",
      "Iteration 11, loss = 0.00462772\n",
      "Iteration 12, loss = 0.00423138\n",
      "Iteration 13, loss = 0.00434931\n",
      "Iteration 14, loss = 0.00477052\n",
      "Iteration 15, loss = 0.00489183\n",
      "Iteration 16, loss = 0.00475224\n",
      "Iteration 17, loss = 0.00471943\n",
      "Iteration 18, loss = 0.00478211\n",
      "Iteration 19, loss = 0.00468232\n",
      "Iteration 20, loss = 0.00441967\n",
      "Iteration 21, loss = 0.00421344\n",
      "Iteration 22, loss = 0.00413762\n",
      "Iteration 23, loss = 0.00407514\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01801710\n",
      "Iteration 2, loss = 0.01526203\n",
      "Iteration 3, loss = 0.01338065\n",
      "Iteration 4, loss = 0.01020466\n",
      "Iteration 5, loss = 0.00890216\n",
      "Iteration 6, loss = 0.00832048\n",
      "Iteration 7, loss = 0.00689261\n",
      "Iteration 8, loss = 0.00542760\n",
      "Iteration 9, loss = 0.00489608\n",
      "Iteration 10, loss = 0.00490921\n",
      "Iteration 11, loss = 0.00461353\n",
      "Iteration 12, loss = 0.00422406\n",
      "Iteration 13, loss = 0.00435840\n",
      "Iteration 14, loss = 0.00477478\n",
      "Iteration 15, loss = 0.00488062\n",
      "Iteration 16, loss = 0.00474044\n",
      "Iteration 17, loss = 0.00471401\n",
      "Iteration 18, loss = 0.00476925\n",
      "Iteration 19, loss = 0.00465617\n",
      "Iteration 20, loss = 0.00438628\n",
      "Iteration 21, loss = 0.00418804\n",
      "Iteration 22, loss = 0.00411729\n",
      "Iteration 23, loss = 0.00405166\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01803608\n",
      "Iteration 2, loss = 0.01527545\n",
      "Iteration 3, loss = 0.01339431\n",
      "Iteration 4, loss = 0.01021629\n",
      "Iteration 5, loss = 0.00891008\n",
      "Iteration 6, loss = 0.00832790\n",
      "Iteration 7, loss = 0.00689988\n",
      "Iteration 8, loss = 0.00543191\n",
      "Iteration 9, loss = 0.00489663\n",
      "Iteration 10, loss = 0.00490904\n",
      "Iteration 11, loss = 0.00461347\n",
      "Iteration 12, loss = 0.00422373\n",
      "Iteration 13, loss = 0.00435788\n",
      "Iteration 14, loss = 0.00477458\n",
      "Iteration 15, loss = 0.00488429\n",
      "Iteration 16, loss = 0.00474889\n",
      "Iteration 17, loss = 0.00472337\n",
      "Iteration 18, loss = 0.00477460\n",
      "Iteration 19, loss = 0.00466061\n",
      "Iteration 20, loss = 0.00438713\n",
      "Iteration 21, loss = 0.00418896\n",
      "Iteration 22, loss = 0.00411663\n",
      "Iteration 23, loss = 0.00404892\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01805333\n",
      "Iteration 2, loss = 0.01528724\n",
      "Iteration 3, loss = 0.01340821\n",
      "Iteration 4, loss = 0.01022613\n",
      "Iteration 5, loss = 0.00891666\n",
      "Iteration 6, loss = 0.00833462\n",
      "Iteration 7, loss = 0.00690608\n",
      "Iteration 8, loss = 0.00543471\n",
      "Iteration 9, loss = 0.00489795\n",
      "Iteration 10, loss = 0.00491033\n",
      "Iteration 11, loss = 0.00461507\n",
      "Iteration 12, loss = 0.00422463\n",
      "Iteration 13, loss = 0.00435774\n",
      "Iteration 14, loss = 0.00477461\n",
      "Iteration 15, loss = 0.00488426\n",
      "Iteration 16, loss = 0.00474807\n",
      "Iteration 17, loss = 0.00472221\n",
      "Iteration 18, loss = 0.00477551\n",
      "Iteration 19, loss = 0.00466312\n",
      "Iteration 20, loss = 0.00439059\n",
      "Iteration 21, loss = 0.00419193\n",
      "Iteration 22, loss = 0.00411968\n",
      "Iteration 23, loss = 0.00405254\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01804630\n",
      "Iteration 2, loss = 0.01528251\n",
      "Iteration 3, loss = 0.01340262\n",
      "Iteration 4, loss = 0.01022221\n",
      "Iteration 5, loss = 0.00891379\n",
      "Iteration 6, loss = 0.00833168\n",
      "Iteration 7, loss = 0.00690410\n",
      "Iteration 8, loss = 0.00543358\n",
      "Iteration 9, loss = 0.00489715\n",
      "Iteration 10, loss = 0.00490929\n",
      "Iteration 11, loss = 0.00461413\n",
      "Iteration 12, loss = 0.00422404\n",
      "Iteration 13, loss = 0.00435741\n",
      "Iteration 14, loss = 0.00477428\n",
      "Iteration 15, loss = 0.00488440\n",
      "Iteration 16, loss = 0.00474891\n",
      "Iteration 17, loss = 0.00472307\n",
      "Iteration 18, loss = 0.00477488\n",
      "Iteration 19, loss = 0.00466159\n",
      "Iteration 20, loss = 0.00438900\n",
      "Iteration 21, loss = 0.00419060\n",
      "Iteration 22, loss = 0.00411809\n",
      "Iteration 23, loss = 0.00405067\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01802902\n",
      "Iteration 2, loss = 0.01527047\n",
      "Iteration 3, loss = 0.01338915\n",
      "Iteration 4, loss = 0.01021127\n",
      "Iteration 5, loss = 0.00890745\n",
      "Iteration 6, loss = 0.00832593\n",
      "Iteration 7, loss = 0.00689781\n",
      "Iteration 8, loss = 0.00543120\n",
      "Iteration 9, loss = 0.00489716\n",
      "Iteration 10, loss = 0.00490998\n",
      "Iteration 11, loss = 0.00461426\n",
      "Iteration 12, loss = 0.00422438\n",
      "Iteration 13, loss = 0.00435755\n",
      "Iteration 14, loss = 0.00477415\n",
      "Iteration 15, loss = 0.00488095\n",
      "Iteration 16, loss = 0.00474047\n",
      "Iteration 17, loss = 0.00471310\n",
      "Iteration 18, loss = 0.00476899\n",
      "Iteration 19, loss = 0.00465714\n",
      "Iteration 20, loss = 0.00438853\n",
      "Iteration 21, loss = 0.00418963\n",
      "Iteration 22, loss = 0.00411846\n",
      "Iteration 23, loss = 0.00405348\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01807590\n",
      "Iteration 2, loss = 0.01530196\n",
      "Iteration 3, loss = 0.01342598\n",
      "Iteration 4, loss = 0.01024153\n",
      "Iteration 5, loss = 0.00892435\n",
      "Iteration 6, loss = 0.00833992\n",
      "Iteration 7, loss = 0.00691067\n",
      "Iteration 8, loss = 0.00543833\n",
      "Iteration 9, loss = 0.00489877\n",
      "Iteration 10, loss = 0.00491065\n",
      "Iteration 11, loss = 0.00461671\n",
      "Iteration 12, loss = 0.00422583\n",
      "Iteration 13, loss = 0.00435682\n",
      "Iteration 14, loss = 0.00477384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.00488261\n",
      "Iteration 16, loss = 0.00474186\n",
      "Iteration 17, loss = 0.00471356\n",
      "Iteration 18, loss = 0.00477219\n",
      "Iteration 19, loss = 0.00466305\n",
      "Iteration 20, loss = 0.00439682\n",
      "Iteration 21, loss = 0.00419687\n",
      "Iteration 22, loss = 0.00412495\n",
      "Iteration 23, loss = 0.00406054\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01818214\n",
      "Iteration 2, loss = 0.01536141\n",
      "Iteration 3, loss = 0.01350251\n",
      "Iteration 4, loss = 0.01030190\n",
      "Iteration 5, loss = 0.00895367\n",
      "Iteration 6, loss = 0.00836857\n",
      "Iteration 7, loss = 0.00694797\n",
      "Iteration 8, loss = 0.00546280\n",
      "Iteration 9, loss = 0.00490293\n",
      "Iteration 10, loss = 0.00491122\n",
      "Iteration 11, loss = 0.00462522\n",
      "Iteration 12, loss = 0.00422999\n",
      "Iteration 13, loss = 0.00434825\n",
      "Iteration 14, loss = 0.00476799\n",
      "Iteration 15, loss = 0.00488837\n",
      "Iteration 16, loss = 0.00475091\n",
      "Iteration 17, loss = 0.00472025\n",
      "Iteration 18, loss = 0.00478224\n",
      "Iteration 19, loss = 0.00468083\n",
      "Iteration 20, loss = 0.00441787\n",
      "Iteration 21, loss = 0.00421274\n",
      "Iteration 22, loss = 0.00413681\n",
      "Iteration 23, loss = 0.00407303\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01803885\n",
      "Iteration 2, loss = 0.01527737\n",
      "Iteration 3, loss = 0.01339651\n",
      "Iteration 4, loss = 0.01021779\n",
      "Iteration 5, loss = 0.00891097\n",
      "Iteration 6, loss = 0.00832880\n",
      "Iteration 7, loss = 0.00690093\n",
      "Iteration 8, loss = 0.00543222\n",
      "Iteration 9, loss = 0.00489665\n",
      "Iteration 10, loss = 0.00490900\n",
      "Iteration 11, loss = 0.00461357\n",
      "Iteration 12, loss = 0.00422374\n",
      "Iteration 13, loss = 0.00435751\n",
      "Iteration 14, loss = 0.00477428\n",
      "Iteration 15, loss = 0.00488410\n",
      "Iteration 16, loss = 0.00474867\n",
      "Iteration 17, loss = 0.00472306\n",
      "Iteration 18, loss = 0.00477453\n",
      "Iteration 19, loss = 0.00466074\n",
      "Iteration 20, loss = 0.00438768\n",
      "Iteration 21, loss = 0.00418940\n",
      "Iteration 22, loss = 0.00411700\n",
      "Iteration 23, loss = 0.00404936\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01804649\n",
      "Iteration 2, loss = 0.01528262\n",
      "Iteration 3, loss = 0.01340278\n",
      "Iteration 4, loss = 0.01022234\n",
      "Iteration 5, loss = 0.00891388\n",
      "Iteration 6, loss = 0.00833178\n",
      "Iteration 7, loss = 0.00690421\n",
      "Iteration 8, loss = 0.00543366\n",
      "Iteration 9, loss = 0.00489720\n",
      "Iteration 10, loss = 0.00490934\n",
      "Iteration 11, loss = 0.00461419\n",
      "Iteration 12, loss = 0.00422408\n",
      "Iteration 13, loss = 0.00435744\n",
      "Iteration 14, loss = 0.00477431\n",
      "Iteration 15, loss = 0.00488445\n",
      "Iteration 16, loss = 0.00474896\n",
      "Iteration 17, loss = 0.00472310\n",
      "Iteration 18, loss = 0.00477491\n",
      "Iteration 19, loss = 0.00466164\n",
      "Iteration 20, loss = 0.00438904\n",
      "Iteration 21, loss = 0.00419063\n",
      "Iteration 22, loss = 0.00411812\n",
      "Iteration 23, loss = 0.00405069\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01802666\n",
      "Iteration 2, loss = 0.01526874\n",
      "Iteration 3, loss = 0.01338754\n",
      "Iteration 4, loss = 0.01020994\n",
      "Iteration 5, loss = 0.00890651\n",
      "Iteration 6, loss = 0.00832500\n",
      "Iteration 7, loss = 0.00689685\n",
      "Iteration 8, loss = 0.00543051\n",
      "Iteration 9, loss = 0.00489707\n",
      "Iteration 10, loss = 0.00490984\n",
      "Iteration 11, loss = 0.00461426\n",
      "Iteration 12, loss = 0.00422455\n",
      "Iteration 13, loss = 0.00435757\n",
      "Iteration 14, loss = 0.00477410\n",
      "Iteration 15, loss = 0.00488092\n",
      "Iteration 16, loss = 0.00474053\n",
      "Iteration 17, loss = 0.00471315\n",
      "Iteration 18, loss = 0.00476889\n",
      "Iteration 19, loss = 0.00465688\n",
      "Iteration 20, loss = 0.00438809\n",
      "Iteration 21, loss = 0.00418928\n",
      "Iteration 22, loss = 0.00411813\n",
      "Iteration 23, loss = 0.00405304\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01805279\n",
      "Iteration 2, loss = 0.01528678\n",
      "Iteration 3, loss = 0.01340772\n",
      "Iteration 4, loss = 0.01022579\n",
      "Iteration 5, loss = 0.00891581\n",
      "Iteration 6, loss = 0.00833365\n",
      "Iteration 7, loss = 0.00690560\n",
      "Iteration 8, loss = 0.00543424\n",
      "Iteration 9, loss = 0.00489752\n",
      "Iteration 10, loss = 0.00491000\n",
      "Iteration 11, loss = 0.00461484\n",
      "Iteration 12, loss = 0.00422446\n",
      "Iteration 13, loss = 0.00435768\n",
      "Iteration 14, loss = 0.00477461\n",
      "Iteration 15, loss = 0.00488443\n",
      "Iteration 16, loss = 0.00474818\n",
      "Iteration 17, loss = 0.00472192\n",
      "Iteration 18, loss = 0.00477482\n",
      "Iteration 19, loss = 0.00466230\n",
      "Iteration 20, loss = 0.00439019\n",
      "Iteration 21, loss = 0.00419150\n",
      "Iteration 22, loss = 0.00411914\n",
      "Iteration 23, loss = 0.00405212\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01805255\n",
      "Iteration 2, loss = 0.01528664\n",
      "Iteration 3, loss = 0.01340750\n",
      "Iteration 4, loss = 0.01022557\n",
      "Iteration 5, loss = 0.00891565\n",
      "Iteration 6, loss = 0.00833347\n",
      "Iteration 7, loss = 0.00690541\n",
      "Iteration 8, loss = 0.00543407\n",
      "Iteration 9, loss = 0.00489739\n",
      "Iteration 10, loss = 0.00490988\n",
      "Iteration 11, loss = 0.00461472\n",
      "Iteration 12, loss = 0.00422437\n",
      "Iteration 13, loss = 0.00435752\n",
      "Iteration 14, loss = 0.00477446\n",
      "Iteration 15, loss = 0.00488439\n",
      "Iteration 16, loss = 0.00474814\n",
      "Iteration 17, loss = 0.00472187\n",
      "Iteration 18, loss = 0.00477486\n",
      "Iteration 19, loss = 0.00466235\n",
      "Iteration 20, loss = 0.00439029\n",
      "Iteration 21, loss = 0.00419155\n",
      "Iteration 22, loss = 0.00411921\n",
      "Iteration 23, loss = 0.00405218\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01812853\n",
      "Iteration 2, loss = 0.01533475\n",
      "Iteration 3, loss = 0.01346643\n",
      "Iteration 4, loss = 0.01027143\n",
      "Iteration 5, loss = 0.00893686\n",
      "Iteration 6, loss = 0.00835058\n",
      "Iteration 7, loss = 0.00692499\n",
      "Iteration 8, loss = 0.00544742\n",
      "Iteration 9, loss = 0.00489964\n",
      "Iteration 10, loss = 0.00490948\n",
      "Iteration 11, loss = 0.00461964\n",
      "Iteration 12, loss = 0.00422753\n",
      "Iteration 13, loss = 0.00435089\n",
      "Iteration 14, loss = 0.00476794\n",
      "Iteration 15, loss = 0.00488132\n",
      "Iteration 16, loss = 0.00474450\n",
      "Iteration 17, loss = 0.00471736\n",
      "Iteration 18, loss = 0.00477748\n",
      "Iteration 19, loss = 0.00467133\n",
      "Iteration 20, loss = 0.00440652\n",
      "Iteration 21, loss = 0.00420505\n",
      "Iteration 22, loss = 0.00413134\n",
      "Iteration 23, loss = 0.00406595\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01819597\n",
      "Iteration 2, loss = 0.01536840\n",
      "Iteration 3, loss = 0.01351324\n",
      "Iteration 4, loss = 0.01031097\n",
      "Iteration 5, loss = 0.00895827\n",
      "Iteration 6, loss = 0.00837323\n",
      "Iteration 7, loss = 0.00695389\n",
      "Iteration 8, loss = 0.00546694\n",
      "Iteration 9, loss = 0.00490336\n",
      "Iteration 10, loss = 0.00491265\n",
      "Iteration 11, loss = 0.00462867\n",
      "Iteration 12, loss = 0.00423193\n",
      "Iteration 13, loss = 0.00434914\n",
      "Iteration 14, loss = 0.00477080\n",
      "Iteration 15, loss = 0.00489297\n",
      "Iteration 16, loss = 0.00475332\n",
      "Iteration 17, loss = 0.00472015\n",
      "Iteration 18, loss = 0.00478297\n",
      "Iteration 19, loss = 0.00468378\n",
      "Iteration 20, loss = 0.00442132\n",
      "Iteration 21, loss = 0.00421458\n",
      "Iteration 22, loss = 0.00413842\n",
      "Iteration 23, loss = 0.00407602\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01808088\n",
      "Iteration 2, loss = 0.01530509\n",
      "Iteration 3, loss = 0.01342962\n",
      "Iteration 4, loss = 0.01024446\n",
      "Iteration 5, loss = 0.00892598\n",
      "Iteration 6, loss = 0.00833993\n",
      "Iteration 7, loss = 0.00691128\n",
      "Iteration 8, loss = 0.00543857\n",
      "Iteration 9, loss = 0.00489817\n",
      "Iteration 10, loss = 0.00490997\n",
      "Iteration 11, loss = 0.00461657\n",
      "Iteration 12, loss = 0.00422570\n",
      "Iteration 13, loss = 0.00435548\n",
      "Iteration 14, loss = 0.00477270\n",
      "Iteration 15, loss = 0.00488194\n",
      "Iteration 16, loss = 0.00474199\n",
      "Iteration 17, loss = 0.00471378\n",
      "Iteration 18, loss = 0.00477232\n",
      "Iteration 19, loss = 0.00466308\n",
      "Iteration 20, loss = 0.00439726\n",
      "Iteration 21, loss = 0.00419714\n",
      "Iteration 22, loss = 0.00412502\n",
      "Iteration 23, loss = 0.00406038\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01809178\n",
      "Iteration 2, loss = 0.01531204\n",
      "Iteration 3, loss = 0.01343839\n",
      "Iteration 4, loss = 0.01025205\n",
      "Iteration 5, loss = 0.00892997\n",
      "Iteration 6, loss = 0.00834166\n",
      "Iteration 7, loss = 0.00691438\n",
      "Iteration 8, loss = 0.00544108\n",
      "Iteration 9, loss = 0.00489895\n",
      "Iteration 10, loss = 0.00491043\n",
      "Iteration 11, loss = 0.00461779\n",
      "Iteration 12, loss = 0.00422657\n",
      "Iteration 13, loss = 0.00435583\n",
      "Iteration 14, loss = 0.00477310\n",
      "Iteration 15, loss = 0.00488299\n",
      "Iteration 16, loss = 0.00474229\n",
      "Iteration 17, loss = 0.00471335\n",
      "Iteration 18, loss = 0.00477252\n",
      "Iteration 19, loss = 0.00466458\n",
      "Iteration 20, loss = 0.00439967\n",
      "Iteration 21, loss = 0.00419912\n",
      "Iteration 22, loss = 0.00412681\n",
      "Iteration 23, loss = 0.00406277\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01813639\n",
      "Iteration 2, loss = 0.01533893\n",
      "Iteration 3, loss = 0.01347198\n",
      "Iteration 4, loss = 0.01027576\n",
      "Iteration 5, loss = 0.00893903\n",
      "Iteration 6, loss = 0.00835323\n",
      "Iteration 7, loss = 0.00692848\n",
      "Iteration 8, loss = 0.00544945\n",
      "Iteration 9, loss = 0.00490008\n",
      "Iteration 10, loss = 0.00490986\n",
      "Iteration 11, loss = 0.00462076\n",
      "Iteration 12, loss = 0.00422818\n",
      "Iteration 13, loss = 0.00435033\n",
      "Iteration 14, loss = 0.00476753\n",
      "Iteration 15, loss = 0.00488186\n",
      "Iteration 16, loss = 0.00474516\n",
      "Iteration 17, loss = 0.00471753\n",
      "Iteration 18, loss = 0.00477778\n",
      "Iteration 19, loss = 0.00467224\n",
      "Iteration 20, loss = 0.00440783\n",
      "Iteration 21, loss = 0.00420595\n",
      "Iteration 22, loss = 0.00413197\n",
      "Iteration 23, loss = 0.00406679\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01817462\n",
      "Iteration 2, loss = 0.01535656\n",
      "Iteration 3, loss = 0.01349620\n",
      "Iteration 4, loss = 0.01029601\n",
      "Iteration 5, loss = 0.00894963\n",
      "Iteration 6, loss = 0.00836545\n",
      "Iteration 7, loss = 0.00694431\n",
      "Iteration 8, loss = 0.00545990\n",
      "Iteration 9, loss = 0.00490163\n",
      "Iteration 10, loss = 0.00491209\n",
      "Iteration 11, loss = 0.00462629\n",
      "Iteration 12, loss = 0.00423104\n",
      "Iteration 13, loss = 0.00435005\n",
      "Iteration 14, loss = 0.00476959\n",
      "Iteration 15, loss = 0.00488878\n",
      "Iteration 16, loss = 0.00474966\n",
      "Iteration 17, loss = 0.00471810\n",
      "Iteration 18, loss = 0.00478031\n",
      "Iteration 19, loss = 0.00467904\n",
      "Iteration 20, loss = 0.00441552\n",
      "Iteration 21, loss = 0.00421058\n",
      "Iteration 22, loss = 0.00413545\n",
      "Iteration 23, loss = 0.00407229\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01818632\n",
      "Iteration 2, loss = 0.01536267\n",
      "Iteration 3, loss = 0.01350528\n",
      "Iteration 4, loss = 0.01030367\n",
      "Iteration 5, loss = 0.00895363\n",
      "Iteration 6, loss = 0.00836911\n",
      "Iteration 7, loss = 0.00694926\n",
      "Iteration 8, loss = 0.00546338\n",
      "Iteration 9, loss = 0.00490243\n",
      "Iteration 10, loss = 0.00491237\n",
      "Iteration 11, loss = 0.00462762\n",
      "Iteration 12, loss = 0.00423164\n",
      "Iteration 13, loss = 0.00434954\n",
      "Iteration 14, loss = 0.00476993\n",
      "Iteration 15, loss = 0.00489077\n",
      "Iteration 16, loss = 0.00475165\n",
      "Iteration 17, loss = 0.00471919\n",
      "Iteration 18, loss = 0.00478161\n",
      "Iteration 19, loss = 0.00468136\n",
      "Iteration 20, loss = 0.00441812\n",
      "Iteration 21, loss = 0.00421231\n",
      "Iteration 22, loss = 0.00413663\n",
      "Iteration 23, loss = 0.00407389\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01826122\n",
      "Iteration 2, loss = 0.01539320\n",
      "Iteration 3, loss = 0.01356043\n",
      "Iteration 4, loss = 0.01034711\n",
      "Iteration 5, loss = 0.00896602\n",
      "Iteration 6, loss = 0.00838334\n",
      "Iteration 7, loss = 0.00697533\n",
      "Iteration 8, loss = 0.00547763\n",
      "Iteration 9, loss = 0.00489460\n",
      "Iteration 10, loss = 0.00490370\n",
      "Iteration 11, loss = 0.00463241\n",
      "Iteration 12, loss = 0.00423588\n",
      "Iteration 13, loss = 0.00434342\n",
      "Iteration 14, loss = 0.00476737\n",
      "Iteration 15, loss = 0.00490085\n",
      "Iteration 16, loss = 0.00476555\n",
      "Iteration 17, loss = 0.00472863\n",
      "Iteration 18, loss = 0.00478835\n",
      "Iteration 19, loss = 0.00469167\n",
      "Iteration 20, loss = 0.00442909\n",
      "Iteration 21, loss = 0.00421860\n",
      "Iteration 22, loss = 0.00413816\n",
      "Iteration 23, loss = 0.00407399\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01829884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.01537838\n",
      "Iteration 3, loss = 0.01354362\n",
      "Iteration 4, loss = 0.01031746\n",
      "Iteration 5, loss = 0.00892500\n",
      "Iteration 6, loss = 0.00834801\n",
      "Iteration 7, loss = 0.00693727\n",
      "Iteration 8, loss = 0.00544185\n",
      "Iteration 9, loss = 0.00487853\n",
      "Iteration 10, loss = 0.00490650\n",
      "Iteration 11, loss = 0.00462983\n",
      "Iteration 12, loss = 0.00422412\n",
      "Iteration 13, loss = 0.00433368\n",
      "Iteration 14, loss = 0.00476965\n",
      "Iteration 15, loss = 0.00490719\n",
      "Iteration 16, loss = 0.00476758\n",
      "Iteration 17, loss = 0.00473228\n",
      "Iteration 18, loss = 0.00480151\n",
      "Iteration 19, loss = 0.00470974\n",
      "Iteration 20, loss = 0.00444577\n",
      "Iteration 21, loss = 0.00423207\n",
      "Iteration 22, loss = 0.00415317\n",
      "Iteration 23, loss = 0.00409320\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01826546\n",
      "Iteration 2, loss = 0.01539394\n",
      "Iteration 3, loss = 0.01356301\n",
      "Iteration 4, loss = 0.01034897\n",
      "Iteration 5, loss = 0.00896591\n",
      "Iteration 6, loss = 0.00838369\n",
      "Iteration 7, loss = 0.00697675\n",
      "Iteration 8, loss = 0.00547847\n",
      "Iteration 9, loss = 0.00489421\n",
      "Iteration 10, loss = 0.00490347\n",
      "Iteration 11, loss = 0.00463314\n",
      "Iteration 12, loss = 0.00423651\n",
      "Iteration 13, loss = 0.00434324\n",
      "Iteration 14, loss = 0.00476729\n",
      "Iteration 15, loss = 0.00490149\n",
      "Iteration 16, loss = 0.00476613\n",
      "Iteration 17, loss = 0.00472862\n",
      "Iteration 18, loss = 0.00478831\n",
      "Iteration 19, loss = 0.00469209\n",
      "Iteration 20, loss = 0.00442971\n",
      "Iteration 21, loss = 0.00421885\n",
      "Iteration 22, loss = 0.00413816\n",
      "Iteration 23, loss = 0.00407421\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01827291\n",
      "Iteration 2, loss = 0.01539514\n",
      "Iteration 3, loss = 0.01356734\n",
      "Iteration 4, loss = 0.01035244\n",
      "Iteration 5, loss = 0.00896614\n",
      "Iteration 6, loss = 0.00838468\n",
      "Iteration 7, loss = 0.00697975\n",
      "Iteration 8, loss = 0.00548043\n",
      "Iteration 9, loss = 0.00489365\n",
      "Iteration 10, loss = 0.00490303\n",
      "Iteration 11, loss = 0.00463435\n",
      "Iteration 12, loss = 0.00423744\n",
      "Iteration 13, loss = 0.00434266\n",
      "Iteration 14, loss = 0.00476696\n",
      "Iteration 15, loss = 0.00490232\n",
      "Iteration 16, loss = 0.00476686\n",
      "Iteration 17, loss = 0.00472855\n",
      "Iteration 18, loss = 0.00478851\n",
      "Iteration 19, loss = 0.00469322\n",
      "Iteration 20, loss = 0.00443153\n",
      "Iteration 21, loss = 0.00422009\n",
      "Iteration 22, loss = 0.00413894\n",
      "Iteration 23, loss = 0.00407526\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01827110\n",
      "Iteration 2, loss = 0.01539485\n",
      "Iteration 3, loss = 0.01356606\n",
      "Iteration 4, loss = 0.01035150\n",
      "Iteration 5, loss = 0.00896607\n",
      "Iteration 6, loss = 0.00838438\n",
      "Iteration 7, loss = 0.00697899\n",
      "Iteration 8, loss = 0.00547995\n",
      "Iteration 9, loss = 0.00489373\n",
      "Iteration 10, loss = 0.00490306\n",
      "Iteration 11, loss = 0.00463401\n",
      "Iteration 12, loss = 0.00423721\n",
      "Iteration 13, loss = 0.00434283\n",
      "Iteration 14, loss = 0.00476711\n",
      "Iteration 15, loss = 0.00490213\n",
      "Iteration 16, loss = 0.00476669\n",
      "Iteration 17, loss = 0.00472866\n",
      "Iteration 18, loss = 0.00478862\n",
      "Iteration 19, loss = 0.00469311\n",
      "Iteration 20, loss = 0.00443133\n",
      "Iteration 21, loss = 0.00422006\n",
      "Iteration 22, loss = 0.00413902\n",
      "Iteration 23, loss = 0.00407525\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01827394\n",
      "Iteration 2, loss = 0.01539538\n",
      "Iteration 3, loss = 0.01356893\n",
      "Iteration 4, loss = 0.01035377\n",
      "Iteration 5, loss = 0.00896696\n",
      "Iteration 6, loss = 0.00838568\n",
      "Iteration 7, loss = 0.00698077\n",
      "Iteration 8, loss = 0.00548118\n",
      "Iteration 9, loss = 0.00489422\n",
      "Iteration 10, loss = 0.00490340\n",
      "Iteration 11, loss = 0.00463450\n",
      "Iteration 12, loss = 0.00423724\n",
      "Iteration 13, loss = 0.00434216\n",
      "Iteration 14, loss = 0.00476637\n",
      "Iteration 15, loss = 0.00490199\n",
      "Iteration 16, loss = 0.00476651\n",
      "Iteration 17, loss = 0.00472796\n",
      "Iteration 18, loss = 0.00478786\n",
      "Iteration 19, loss = 0.00469277\n",
      "Iteration 20, loss = 0.00443120\n",
      "Iteration 21, loss = 0.00421967\n",
      "Iteration 22, loss = 0.00413844\n",
      "Iteration 23, loss = 0.00407484\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01826345\n",
      "Iteration 2, loss = 0.01539352\n",
      "Iteration 3, loss = 0.01356210\n",
      "Iteration 4, loss = 0.01034860\n",
      "Iteration 5, loss = 0.00896646\n",
      "Iteration 6, loss = 0.00838361\n",
      "Iteration 7, loss = 0.00697601\n",
      "Iteration 8, loss = 0.00547819\n",
      "Iteration 9, loss = 0.00489481\n",
      "Iteration 10, loss = 0.00490371\n",
      "Iteration 11, loss = 0.00463271\n",
      "Iteration 12, loss = 0.00423625\n",
      "Iteration 13, loss = 0.00434349\n",
      "Iteration 14, loss = 0.00476729\n",
      "Iteration 15, loss = 0.00490103\n",
      "Iteration 16, loss = 0.00476590\n",
      "Iteration 17, loss = 0.00472799\n",
      "Iteration 18, loss = 0.00478705\n",
      "Iteration 19, loss = 0.00469059\n",
      "Iteration 20, loss = 0.00442892\n",
      "Iteration 21, loss = 0.00421842\n",
      "Iteration 22, loss = 0.00413772\n",
      "Iteration 23, loss = 0.00407382\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01827713\n",
      "Iteration 2, loss = 0.01539573\n",
      "Iteration 3, loss = 0.01357011\n",
      "Iteration 4, loss = 0.01035437\n",
      "Iteration 5, loss = 0.00896601\n",
      "Iteration 6, loss = 0.00838508\n",
      "Iteration 7, loss = 0.00698129\n",
      "Iteration 8, loss = 0.00548134\n",
      "Iteration 9, loss = 0.00489327\n",
      "Iteration 10, loss = 0.00490285\n",
      "Iteration 11, loss = 0.00463516\n",
      "Iteration 12, loss = 0.00423807\n",
      "Iteration 13, loss = 0.00434232\n",
      "Iteration 14, loss = 0.00476662\n",
      "Iteration 15, loss = 0.00490274\n",
      "Iteration 16, loss = 0.00476730\n",
      "Iteration 17, loss = 0.00472834\n",
      "Iteration 18, loss = 0.00478825\n",
      "Iteration 19, loss = 0.00469346\n",
      "Iteration 20, loss = 0.00443196\n",
      "Iteration 21, loss = 0.00422013\n",
      "Iteration 22, loss = 0.00413873\n",
      "Iteration 23, loss = 0.00407533\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01829497\n",
      "Iteration 2, loss = 0.01537782\n",
      "Iteration 3, loss = 0.01354072\n",
      "Iteration 4, loss = 0.01031463\n",
      "Iteration 5, loss = 0.00892423\n",
      "Iteration 6, loss = 0.00834727\n",
      "Iteration 7, loss = 0.00693541\n",
      "Iteration 8, loss = 0.00544061\n",
      "Iteration 9, loss = 0.00487852\n",
      "Iteration 10, loss = 0.00490634\n",
      "Iteration 11, loss = 0.00462957\n",
      "Iteration 12, loss = 0.00422390\n",
      "Iteration 13, loss = 0.00433322\n",
      "Iteration 14, loss = 0.00476896\n",
      "Iteration 15, loss = 0.00490677\n",
      "Iteration 16, loss = 0.00476723\n",
      "Iteration 17, loss = 0.00473180\n",
      "Iteration 18, loss = 0.00480103\n",
      "Iteration 19, loss = 0.00470933\n",
      "Iteration 20, loss = 0.00444520\n",
      "Iteration 21, loss = 0.00423155\n",
      "Iteration 22, loss = 0.00415279\n",
      "Iteration 23, loss = 0.00409284\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01824597\n",
      "Iteration 2, loss = 0.01538878\n",
      "Iteration 3, loss = 0.01355000\n",
      "Iteration 4, loss = 0.01034039\n",
      "Iteration 5, loss = 0.00896635\n",
      "Iteration 6, loss = 0.00838092\n",
      "Iteration 7, loss = 0.00696870\n",
      "Iteration 8, loss = 0.00547415\n",
      "Iteration 9, loss = 0.00489560\n",
      "Iteration 10, loss = 0.00490270\n",
      "Iteration 11, loss = 0.00462830\n",
      "Iteration 12, loss = 0.00423367\n",
      "Iteration 13, loss = 0.00434480\n",
      "Iteration 14, loss = 0.00476754\n",
      "Iteration 15, loss = 0.00489777\n",
      "Iteration 16, loss = 0.00476087\n",
      "Iteration 17, loss = 0.00472285\n",
      "Iteration 18, loss = 0.00478414\n",
      "Iteration 19, loss = 0.00468796\n",
      "Iteration 20, loss = 0.00442630\n",
      "Iteration 21, loss = 0.00421675\n",
      "Iteration 22, loss = 0.00413767\n",
      "Iteration 23, loss = 0.00407408\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01826421\n",
      "Iteration 2, loss = 0.01539280\n",
      "Iteration 3, loss = 0.01356347\n",
      "Iteration 4, loss = 0.01035096\n",
      "Iteration 5, loss = 0.00896903\n",
      "Iteration 6, loss = 0.00838592\n",
      "Iteration 7, loss = 0.00697755\n",
      "Iteration 8, loss = 0.00548001\n",
      "Iteration 9, loss = 0.00489602\n",
      "Iteration 10, loss = 0.00490330\n",
      "Iteration 11, loss = 0.00463184\n",
      "Iteration 12, loss = 0.00423575\n",
      "Iteration 13, loss = 0.00434274\n",
      "Iteration 14, loss = 0.00476587\n",
      "Iteration 15, loss = 0.00489924\n",
      "Iteration 16, loss = 0.00476208\n",
      "Iteration 17, loss = 0.00472195\n",
      "Iteration 18, loss = 0.00478363\n",
      "Iteration 19, loss = 0.00469016\n",
      "Iteration 20, loss = 0.00442913\n",
      "Iteration 21, loss = 0.00421781\n",
      "Iteration 22, loss = 0.00413774\n",
      "Iteration 23, loss = 0.00407491\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01824407\n",
      "Iteration 2, loss = 0.01538670\n",
      "Iteration 3, loss = 0.01354757\n",
      "Iteration 4, loss = 0.01033738\n",
      "Iteration 5, loss = 0.00896513\n",
      "Iteration 6, loss = 0.00838187\n",
      "Iteration 7, loss = 0.00696850\n",
      "Iteration 8, loss = 0.00547438\n",
      "Iteration 9, loss = 0.00489786\n",
      "Iteration 10, loss = 0.00490505\n",
      "Iteration 11, loss = 0.00462943\n",
      "Iteration 12, loss = 0.00423432\n",
      "Iteration 13, loss = 0.00434553\n",
      "Iteration 14, loss = 0.00476793\n",
      "Iteration 15, loss = 0.00489727\n",
      "Iteration 16, loss = 0.00475982\n",
      "Iteration 17, loss = 0.00472174\n",
      "Iteration 18, loss = 0.00478152\n",
      "Iteration 19, loss = 0.00468489\n",
      "Iteration 20, loss = 0.00442458\n",
      "Iteration 21, loss = 0.00421497\n",
      "Iteration 22, loss = 0.00413560\n",
      "Iteration 23, loss = 0.00407248\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01825823\n",
      "Iteration 2, loss = 0.01539136\n",
      "Iteration 3, loss = 0.01355794\n",
      "Iteration 4, loss = 0.01034565\n",
      "Iteration 5, loss = 0.00896576\n",
      "Iteration 6, loss = 0.00838184\n",
      "Iteration 7, loss = 0.00697250\n",
      "Iteration 8, loss = 0.00547615\n",
      "Iteration 9, loss = 0.00489428\n",
      "Iteration 10, loss = 0.00490219\n",
      "Iteration 11, loss = 0.00463064\n",
      "Iteration 12, loss = 0.00423561\n",
      "Iteration 13, loss = 0.00434429\n",
      "Iteration 14, loss = 0.00476728\n",
      "Iteration 15, loss = 0.00489974\n",
      "Iteration 16, loss = 0.00476266\n",
      "Iteration 17, loss = 0.00472267\n",
      "Iteration 18, loss = 0.00478359\n",
      "Iteration 19, loss = 0.00468878\n",
      "Iteration 20, loss = 0.00442728\n",
      "Iteration 21, loss = 0.00421651\n",
      "Iteration 22, loss = 0.00413670\n",
      "Iteration 23, loss = 0.00407376\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01825383\n",
      "Iteration 2, loss = 0.01538991\n",
      "Iteration 3, loss = 0.01355486\n",
      "Iteration 4, loss = 0.01034339\n",
      "Iteration 5, loss = 0.00896651\n",
      "Iteration 6, loss = 0.00838259\n",
      "Iteration 7, loss = 0.00697163\n",
      "Iteration 8, loss = 0.00547570\n",
      "Iteration 9, loss = 0.00489541\n",
      "Iteration 10, loss = 0.00490295\n",
      "Iteration 11, loss = 0.00463008\n",
      "Iteration 12, loss = 0.00423493\n",
      "Iteration 13, loss = 0.00434415\n",
      "Iteration 14, loss = 0.00476703\n",
      "Iteration 15, loss = 0.00489853\n",
      "Iteration 16, loss = 0.00476139\n",
      "Iteration 17, loss = 0.00472214\n",
      "Iteration 18, loss = 0.00478343\n",
      "Iteration 19, loss = 0.00468834\n",
      "Iteration 20, loss = 0.00442692\n",
      "Iteration 21, loss = 0.00421631\n",
      "Iteration 22, loss = 0.00413684\n",
      "Iteration 23, loss = 0.00407383\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.01827127\n",
      "Iteration 2, loss = 0.01539450\n",
      "Iteration 3, loss = 0.01356720\n",
      "Iteration 4, loss = 0.01035205\n",
      "Iteration 5, loss = 0.00896606\n",
      "Iteration 6, loss = 0.00838395\n",
      "Iteration 7, loss = 0.00697826\n",
      "Iteration 8, loss = 0.00547943\n",
      "Iteration 9, loss = 0.00489405\n",
      "Iteration 10, loss = 0.00490344\n",
      "Iteration 11, loss = 0.00463428\n",
      "Iteration 12, loss = 0.00423760\n",
      "Iteration 13, loss = 0.00434324\n",
      "Iteration 14, loss = 0.00476707\n",
      "Iteration 15, loss = 0.00490214\n",
      "Iteration 16, loss = 0.00476674\n",
      "Iteration 17, loss = 0.00472740\n",
      "Iteration 18, loss = 0.00478697\n",
      "Iteration 19, loss = 0.00469135\n",
      "Iteration 20, loss = 0.00442884\n",
      "Iteration 21, loss = 0.00421749\n",
      "Iteration 22, loss = 0.00413661\n",
      "Iteration 23, loss = 0.00407319\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01829314\n",
      "Iteration 2, loss = 0.01537778\n",
      "Iteration 3, loss = 0.01353913\n",
      "Iteration 4, loss = 0.01031347\n",
      "Iteration 5, loss = 0.00892369\n",
      "Iteration 6, loss = 0.00834632\n",
      "Iteration 7, loss = 0.00693413\n",
      "Iteration 8, loss = 0.00543990\n",
      "Iteration 9, loss = 0.00487862\n",
      "Iteration 10, loss = 0.00490687\n",
      "Iteration 11, loss = 0.00462973\n",
      "Iteration 12, loss = 0.00422419\n",
      "Iteration 13, loss = 0.00433417\n",
      "Iteration 14, loss = 0.00476993\n",
      "Iteration 15, loss = 0.00490691\n",
      "Iteration 16, loss = 0.00476732\n",
      "Iteration 17, loss = 0.00473207\n",
      "Iteration 18, loss = 0.00480097\n",
      "Iteration 19, loss = 0.00470877\n",
      "Iteration 20, loss = 0.00444395\n",
      "Iteration 21, loss = 0.00423048\n",
      "Iteration 22, loss = 0.00415188\n",
      "Iteration 23, loss = 0.00409201\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01824896\n",
      "Iteration 2, loss = 0.01538871\n",
      "Iteration 3, loss = 0.01355159\n",
      "Iteration 4, loss = 0.01034114\n",
      "Iteration 5, loss = 0.00896666\n",
      "Iteration 6, loss = 0.00838225\n",
      "Iteration 7, loss = 0.00697009\n",
      "Iteration 8, loss = 0.00547485\n",
      "Iteration 9, loss = 0.00489592\n",
      "Iteration 10, loss = 0.00490320\n",
      "Iteration 11, loss = 0.00462924\n",
      "Iteration 12, loss = 0.00423421\n",
      "Iteration 13, loss = 0.00434437\n",
      "Iteration 14, loss = 0.00476716\n",
      "Iteration 15, loss = 0.00489777\n",
      "Iteration 16, loss = 0.00476066\n",
      "Iteration 17, loss = 0.00472219\n",
      "Iteration 18, loss = 0.00478365\n",
      "Iteration 19, loss = 0.00468803\n",
      "Iteration 20, loss = 0.00442643\n",
      "Iteration 21, loss = 0.00421625\n",
      "Iteration 22, loss = 0.00413708\n",
      "Iteration 23, loss = 0.00407380\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01825906\n",
      "Iteration 2, loss = 0.01539061\n",
      "Iteration 3, loss = 0.01355913\n",
      "Iteration 4, loss = 0.01034654\n",
      "Iteration 5, loss = 0.00896796\n",
      "Iteration 6, loss = 0.00838666\n",
      "Iteration 7, loss = 0.00697649\n",
      "Iteration 8, loss = 0.00547997\n",
      "Iteration 9, loss = 0.00489963\n",
      "Iteration 10, loss = 0.00490728\n",
      "Iteration 11, loss = 0.00463345\n",
      "Iteration 12, loss = 0.00423671\n",
      "Iteration 13, loss = 0.00434451\n",
      "Iteration 14, loss = 0.00476712\n",
      "Iteration 15, loss = 0.00489905\n",
      "Iteration 16, loss = 0.00476177\n",
      "Iteration 17, loss = 0.00472236\n",
      "Iteration 18, loss = 0.00478248\n",
      "Iteration 19, loss = 0.00468764\n",
      "Iteration 20, loss = 0.00442744\n",
      "Iteration 21, loss = 0.00421664\n",
      "Iteration 22, loss = 0.00413635\n",
      "Iteration 23, loss = 0.00407365\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01826506\n",
      "Iteration 2, loss = 0.01539162\n",
      "Iteration 3, loss = 0.01356253\n",
      "Iteration 4, loss = 0.01034815\n",
      "Iteration 5, loss = 0.00896636\n",
      "Iteration 6, loss = 0.00838612\n",
      "Iteration 7, loss = 0.00697798\n",
      "Iteration 8, loss = 0.00548059\n",
      "Iteration 9, loss = 0.00489863\n",
      "Iteration 10, loss = 0.00490713\n",
      "Iteration 11, loss = 0.00463511\n",
      "Iteration 12, loss = 0.00423817\n",
      "Iteration 13, loss = 0.00434464\n",
      "Iteration 14, loss = 0.00476750\n",
      "Iteration 15, loss = 0.00490059\n",
      "Iteration 16, loss = 0.00476300\n",
      "Iteration 17, loss = 0.00472219\n",
      "Iteration 18, loss = 0.00478206\n",
      "Iteration 19, loss = 0.00468787\n",
      "Iteration 20, loss = 0.00442772\n",
      "Iteration 21, loss = 0.00421623\n",
      "Iteration 22, loss = 0.00413552\n",
      "Iteration 23, loss = 0.00407337\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01825030\n",
      "Iteration 2, loss = 0.01538745\n",
      "Iteration 3, loss = 0.01355060\n",
      "Iteration 4, loss = 0.01033801\n",
      "Iteration 5, loss = 0.00896212\n",
      "Iteration 6, loss = 0.00838042\n",
      "Iteration 7, loss = 0.00696972\n",
      "Iteration 8, loss = 0.00547470\n",
      "Iteration 9, loss = 0.00489768\n",
      "Iteration 10, loss = 0.00490869\n",
      "Iteration 11, loss = 0.00463490\n",
      "Iteration 12, loss = 0.00423748\n",
      "Iteration 13, loss = 0.00434654\n",
      "Iteration 14, loss = 0.00477034\n",
      "Iteration 15, loss = 0.00490152\n",
      "Iteration 16, loss = 0.00476253\n",
      "Iteration 17, loss = 0.00472195\n",
      "Iteration 18, loss = 0.00478226\n",
      "Iteration 19, loss = 0.00468710\n",
      "Iteration 20, loss = 0.00442591\n",
      "Iteration 21, loss = 0.00421458\n",
      "Iteration 22, loss = 0.00413504\n",
      "Iteration 23, loss = 0.00407417\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01830175\n",
      "Iteration 2, loss = 0.01539785\n",
      "Iteration 3, loss = 0.01358608\n",
      "Iteration 4, loss = 0.01036505\n",
      "Iteration 5, loss = 0.00896360\n",
      "Iteration 6, loss = 0.00838625\n",
      "Iteration 7, loss = 0.00698995\n",
      "Iteration 8, loss = 0.00548628\n",
      "Iteration 9, loss = 0.00489030\n",
      "Iteration 10, loss = 0.00490144\n",
      "Iteration 11, loss = 0.00464037\n",
      "Iteration 12, loss = 0.00424239\n",
      "Iteration 13, loss = 0.00434080\n",
      "Iteration 14, loss = 0.00476535\n",
      "Iteration 15, loss = 0.00490665\n",
      "Iteration 16, loss = 0.00477141\n",
      "Iteration 17, loss = 0.00472833\n",
      "Iteration 18, loss = 0.00478747\n",
      "Iteration 19, loss = 0.00469562\n",
      "Iteration 20, loss = 0.00443491\n",
      "Iteration 21, loss = 0.00422044\n",
      "Iteration 22, loss = 0.00413713\n",
      "Iteration 23, loss = 0.00407498\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01825199\n",
      "Iteration 2, loss = 0.01538954\n",
      "Iteration 3, loss = 0.01355228\n",
      "Iteration 4, loss = 0.01034046\n",
      "Iteration 5, loss = 0.00896275\n",
      "Iteration 6, loss = 0.00837799\n",
      "Iteration 7, loss = 0.00696731\n",
      "Iteration 8, loss = 0.00547196\n",
      "Iteration 9, loss = 0.00489212\n",
      "Iteration 10, loss = 0.00490039\n",
      "Iteration 11, loss = 0.00462835\n",
      "Iteration 12, loss = 0.00423415\n",
      "Iteration 13, loss = 0.00434419\n",
      "Iteration 14, loss = 0.00476697\n",
      "Iteration 15, loss = 0.00489842\n",
      "Iteration 16, loss = 0.00476092\n",
      "Iteration 17, loss = 0.00472079\n",
      "Iteration 18, loss = 0.00478121\n",
      "Iteration 19, loss = 0.00468575\n",
      "Iteration 20, loss = 0.00442458\n",
      "Iteration 21, loss = 0.00421412\n",
      "Iteration 22, loss = 0.00413437\n",
      "Iteration 23, loss = 0.00407155\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01829749\n",
      "Iteration 2, loss = 0.01539772\n",
      "Iteration 3, loss = 0.01358408\n",
      "Iteration 4, loss = 0.01036434\n",
      "Iteration 5, loss = 0.00896549\n",
      "Iteration 6, loss = 0.00838738\n",
      "Iteration 7, loss = 0.00698940\n",
      "Iteration 8, loss = 0.00548623\n",
      "Iteration 9, loss = 0.00489155\n",
      "Iteration 10, loss = 0.00490193\n",
      "Iteration 11, loss = 0.00463884\n",
      "Iteration 12, loss = 0.00424066\n",
      "Iteration 13, loss = 0.00433997\n",
      "Iteration 14, loss = 0.00476441\n",
      "Iteration 15, loss = 0.00490484\n",
      "Iteration 16, loss = 0.00476952\n",
      "Iteration 17, loss = 0.00472723\n",
      "Iteration 18, loss = 0.00478668\n",
      "Iteration 19, loss = 0.00469453\n",
      "Iteration 20, loss = 0.00443424\n",
      "Iteration 21, loss = 0.00422032\n",
      "Iteration 22, loss = 0.00413728\n",
      "Iteration 23, loss = 0.00407488\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01824699\n",
      "Iteration 2, loss = 0.01538750\n",
      "Iteration 3, loss = 0.01354924\n",
      "Iteration 4, loss = 0.01033831\n",
      "Iteration 5, loss = 0.00896458\n",
      "Iteration 6, loss = 0.00838125\n",
      "Iteration 7, loss = 0.00696852\n",
      "Iteration 8, loss = 0.00547412\n",
      "Iteration 9, loss = 0.00489691\n",
      "Iteration 10, loss = 0.00490430\n",
      "Iteration 11, loss = 0.00462949\n",
      "Iteration 12, loss = 0.00423454\n",
      "Iteration 13, loss = 0.00434532\n",
      "Iteration 14, loss = 0.00476780\n",
      "Iteration 15, loss = 0.00489764\n",
      "Iteration 16, loss = 0.00476011\n",
      "Iteration 17, loss = 0.00472141\n",
      "Iteration 18, loss = 0.00478264\n",
      "Iteration 19, loss = 0.00468673\n",
      "Iteration 20, loss = 0.00442493\n",
      "Iteration 21, loss = 0.00421461\n",
      "Iteration 22, loss = 0.00413541\n",
      "Iteration 23, loss = 0.00407230\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01826301\n",
      "Iteration 2, loss = 0.01539156\n",
      "Iteration 3, loss = 0.01356152\n",
      "Iteration 4, loss = 0.01034801\n",
      "Iteration 5, loss = 0.00896743\n",
      "Iteration 6, loss = 0.00838609\n",
      "Iteration 7, loss = 0.00697695\n",
      "Iteration 8, loss = 0.00547986\n",
      "Iteration 9, loss = 0.00489767\n",
      "Iteration 10, loss = 0.00490532\n",
      "Iteration 11, loss = 0.00463319\n",
      "Iteration 12, loss = 0.00423702\n",
      "Iteration 13, loss = 0.00434414\n",
      "Iteration 14, loss = 0.00476688\n",
      "Iteration 15, loss = 0.00489970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.00476249\n",
      "Iteration 17, loss = 0.00472237\n",
      "Iteration 18, loss = 0.00478210\n",
      "Iteration 19, loss = 0.00468756\n",
      "Iteration 20, loss = 0.00442785\n",
      "Iteration 21, loss = 0.00421667\n",
      "Iteration 22, loss = 0.00413624\n",
      "Iteration 23, loss = 0.00407377\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01828916\n",
      "Iteration 2, loss = 0.01539626\n",
      "Iteration 3, loss = 0.01357978\n",
      "Iteration 4, loss = 0.01036111\n",
      "Iteration 5, loss = 0.00896789\n",
      "Iteration 6, loss = 0.00838974\n",
      "Iteration 7, loss = 0.00698719\n",
      "Iteration 8, loss = 0.00548576\n",
      "Iteration 9, loss = 0.00489533\n",
      "Iteration 10, loss = 0.00490419\n",
      "Iteration 11, loss = 0.00463778\n",
      "Iteration 12, loss = 0.00423997\n",
      "Iteration 13, loss = 0.00434119\n",
      "Iteration 14, loss = 0.00476463\n",
      "Iteration 15, loss = 0.00490276\n",
      "Iteration 16, loss = 0.00476587\n",
      "Iteration 17, loss = 0.00472249\n",
      "Iteration 18, loss = 0.00478359\n",
      "Iteration 19, loss = 0.00469260\n",
      "Iteration 20, loss = 0.00443212\n",
      "Iteration 21, loss = 0.00421794\n",
      "Iteration 22, loss = 0.00413579\n",
      "Iteration 23, loss = 0.00407388\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01824106\n",
      "Iteration 2, loss = 0.01538378\n",
      "Iteration 3, loss = 0.01354202\n",
      "Iteration 4, loss = 0.01032963\n",
      "Iteration 5, loss = 0.00895651\n",
      "Iteration 6, loss = 0.00837401\n",
      "Iteration 7, loss = 0.00696258\n",
      "Iteration 8, loss = 0.00546940\n",
      "Iteration 9, loss = 0.00489511\n",
      "Iteration 10, loss = 0.00490550\n",
      "Iteration 11, loss = 0.00463034\n",
      "Iteration 12, loss = 0.00423407\n",
      "Iteration 13, loss = 0.00434402\n",
      "Iteration 14, loss = 0.00476672\n",
      "Iteration 15, loss = 0.00489594\n",
      "Iteration 16, loss = 0.00475812\n",
      "Iteration 17, loss = 0.00472009\n",
      "Iteration 18, loss = 0.00478205\n",
      "Iteration 19, loss = 0.00468516\n",
      "Iteration 20, loss = 0.00442225\n",
      "Iteration 21, loss = 0.00421147\n",
      "Iteration 22, loss = 0.00413250\n",
      "Iteration 23, loss = 0.00407130\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01827827\n",
      "Iteration 2, loss = 0.01539377\n",
      "Iteration 3, loss = 0.01357185\n",
      "Iteration 4, loss = 0.01035446\n",
      "Iteration 5, loss = 0.00896647\n",
      "Iteration 6, loss = 0.00838823\n",
      "Iteration 7, loss = 0.00698449\n",
      "Iteration 8, loss = 0.00548477\n",
      "Iteration 9, loss = 0.00489844\n",
      "Iteration 10, loss = 0.00491009\n",
      "Iteration 11, loss = 0.00464164\n",
      "Iteration 12, loss = 0.00424157\n",
      "Iteration 13, loss = 0.00434466\n",
      "Iteration 14, loss = 0.00476867\n",
      "Iteration 15, loss = 0.00490506\n",
      "Iteration 16, loss = 0.00476578\n",
      "Iteration 17, loss = 0.00472309\n",
      "Iteration 18, loss = 0.00478459\n",
      "Iteration 19, loss = 0.00469352\n",
      "Iteration 20, loss = 0.00443315\n",
      "Iteration 21, loss = 0.00422018\n",
      "Iteration 22, loss = 0.00413937\n",
      "Iteration 23, loss = 0.00408022\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01828371\n",
      "Iteration 2, loss = 0.01539549\n",
      "Iteration 3, loss = 0.01357409\n",
      "Iteration 4, loss = 0.01035559\n",
      "Iteration 5, loss = 0.00896348\n",
      "Iteration 6, loss = 0.00838317\n",
      "Iteration 7, loss = 0.00698019\n",
      "Iteration 8, loss = 0.00547985\n",
      "Iteration 9, loss = 0.00489086\n",
      "Iteration 10, loss = 0.00490084\n",
      "Iteration 11, loss = 0.00463582\n",
      "Iteration 12, loss = 0.00423999\n",
      "Iteration 13, loss = 0.00434292\n",
      "Iteration 14, loss = 0.00476659\n",
      "Iteration 15, loss = 0.00490416\n",
      "Iteration 16, loss = 0.00476676\n",
      "Iteration 17, loss = 0.00472209\n",
      "Iteration 18, loss = 0.00478174\n",
      "Iteration 19, loss = 0.00469003\n",
      "Iteration 20, loss = 0.00442989\n",
      "Iteration 21, loss = 0.00421605\n",
      "Iteration 22, loss = 0.00413407\n",
      "Iteration 23, loss = 0.00407264\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01830388\n",
      "Iteration 2, loss = 0.01537902\n",
      "Iteration 3, loss = 0.01354711\n",
      "Iteration 4, loss = 0.01031883\n",
      "Iteration 5, loss = 0.00892185\n",
      "Iteration 6, loss = 0.00834679\n",
      "Iteration 7, loss = 0.00693919\n",
      "Iteration 8, loss = 0.00544269\n",
      "Iteration 9, loss = 0.00487676\n",
      "Iteration 10, loss = 0.00490626\n",
      "Iteration 11, loss = 0.00463399\n",
      "Iteration 12, loss = 0.00422855\n",
      "Iteration 13, loss = 0.00433451\n",
      "Iteration 14, loss = 0.00477015\n",
      "Iteration 15, loss = 0.00491093\n",
      "Iteration 16, loss = 0.00477130\n",
      "Iteration 17, loss = 0.00473197\n",
      "Iteration 18, loss = 0.00479948\n",
      "Iteration 19, loss = 0.00470988\n",
      "Iteration 20, loss = 0.00444463\n",
      "Iteration 21, loss = 0.00422852\n",
      "Iteration 22, loss = 0.00414821\n",
      "Iteration 23, loss = 0.00408988\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01825774\n",
      "Iteration 2, loss = 0.01538992\n",
      "Iteration 3, loss = 0.01355642\n",
      "Iteration 4, loss = 0.01034301\n",
      "Iteration 5, loss = 0.00896414\n",
      "Iteration 6, loss = 0.00838260\n",
      "Iteration 7, loss = 0.00697245\n",
      "Iteration 8, loss = 0.00547629\n",
      "Iteration 9, loss = 0.00489599\n",
      "Iteration 10, loss = 0.00490418\n",
      "Iteration 11, loss = 0.00463180\n",
      "Iteration 12, loss = 0.00423638\n",
      "Iteration 13, loss = 0.00434476\n",
      "Iteration 14, loss = 0.00476749\n",
      "Iteration 15, loss = 0.00489940\n",
      "Iteration 16, loss = 0.00476176\n",
      "Iteration 17, loss = 0.00472131\n",
      "Iteration 18, loss = 0.00478229\n",
      "Iteration 19, loss = 0.00468757\n",
      "Iteration 20, loss = 0.00442599\n",
      "Iteration 21, loss = 0.00421450\n",
      "Iteration 22, loss = 0.00413447\n",
      "Iteration 23, loss = 0.00407199\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01830862\n",
      "Iteration 2, loss = 0.01539798\n",
      "Iteration 3, loss = 0.01359220\n",
      "Iteration 4, loss = 0.01036967\n",
      "Iteration 5, loss = 0.00896426\n",
      "Iteration 6, loss = 0.00838798\n",
      "Iteration 7, loss = 0.00699350\n",
      "Iteration 8, loss = 0.00548853\n",
      "Iteration 9, loss = 0.00489035\n",
      "Iteration 10, loss = 0.00490128\n",
      "Iteration 11, loss = 0.00464136\n",
      "Iteration 12, loss = 0.00424288\n",
      "Iteration 13, loss = 0.00433909\n",
      "Iteration 14, loss = 0.00476364\n",
      "Iteration 15, loss = 0.00490677\n",
      "Iteration 16, loss = 0.00477119\n",
      "Iteration 17, loss = 0.00472643\n",
      "Iteration 18, loss = 0.00478537\n",
      "Iteration 19, loss = 0.00469477\n",
      "Iteration 20, loss = 0.00443464\n",
      "Iteration 21, loss = 0.00421933\n",
      "Iteration 22, loss = 0.00413542\n",
      "Iteration 23, loss = 0.00407383\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01826250\n",
      "Iteration 2, loss = 0.01539019\n",
      "Iteration 3, loss = 0.01355950\n",
      "Iteration 4, loss = 0.01034445\n",
      "Iteration 5, loss = 0.00896310\n",
      "Iteration 6, loss = 0.00838299\n",
      "Iteration 7, loss = 0.00697582\n",
      "Iteration 8, loss = 0.00547901\n",
      "Iteration 9, loss = 0.00489827\n",
      "Iteration 10, loss = 0.00491001\n",
      "Iteration 11, loss = 0.00463868\n",
      "Iteration 12, loss = 0.00424017\n",
      "Iteration 13, loss = 0.00434676\n",
      "Iteration 14, loss = 0.00477055\n",
      "Iteration 15, loss = 0.00490359\n",
      "Iteration 16, loss = 0.00476371\n",
      "Iteration 17, loss = 0.00472161\n",
      "Iteration 18, loss = 0.00478420\n",
      "Iteration 19, loss = 0.00469172\n",
      "Iteration 20, loss = 0.00443013\n",
      "Iteration 21, loss = 0.00421727\n",
      "Iteration 22, loss = 0.00413715\n",
      "Iteration 23, loss = 0.00407683\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01829326\n",
      "Iteration 2, loss = 0.01539646\n",
      "Iteration 3, loss = 0.01358196\n",
      "Iteration 4, loss = 0.01036171\n",
      "Iteration 5, loss = 0.00896610\n",
      "Iteration 6, loss = 0.00838934\n",
      "Iteration 7, loss = 0.00698881\n",
      "Iteration 8, loss = 0.00548667\n",
      "Iteration 9, loss = 0.00489560\n",
      "Iteration 10, loss = 0.00490563\n",
      "Iteration 11, loss = 0.00464035\n",
      "Iteration 12, loss = 0.00424196\n",
      "Iteration 13, loss = 0.00434188\n",
      "Iteration 14, loss = 0.00476554\n",
      "Iteration 15, loss = 0.00490444\n",
      "Iteration 16, loss = 0.00476702\n",
      "Iteration 17, loss = 0.00472236\n",
      "Iteration 18, loss = 0.00478172\n",
      "Iteration 19, loss = 0.00469086\n",
      "Iteration 20, loss = 0.00443186\n",
      "Iteration 21, loss = 0.00421772\n",
      "Iteration 22, loss = 0.00413487\n",
      "Iteration 23, loss = 0.00407382\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01827235\n",
      "Iteration 2, loss = 0.01539204\n",
      "Iteration 3, loss = 0.01356519\n",
      "Iteration 4, loss = 0.01034731\n",
      "Iteration 5, loss = 0.00896076\n",
      "Iteration 6, loss = 0.00838175\n",
      "Iteration 7, loss = 0.00697716\n",
      "Iteration 8, loss = 0.00547916\n",
      "Iteration 9, loss = 0.00489564\n",
      "Iteration 10, loss = 0.00490729\n",
      "Iteration 11, loss = 0.00463847\n",
      "Iteration 12, loss = 0.00424040\n",
      "Iteration 13, loss = 0.00434437\n",
      "Iteration 14, loss = 0.00476788\n",
      "Iteration 15, loss = 0.00490334\n",
      "Iteration 16, loss = 0.00476590\n",
      "Iteration 17, loss = 0.00472384\n",
      "Iteration 18, loss = 0.00478418\n",
      "Iteration 19, loss = 0.00469076\n",
      "Iteration 20, loss = 0.00442985\n",
      "Iteration 21, loss = 0.00421672\n",
      "Iteration 22, loss = 0.00413552\n",
      "Iteration 23, loss = 0.00407572\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01833361\n",
      "Iteration 2, loss = 0.01539635\n",
      "Iteration 3, loss = 0.01360890\n",
      "Iteration 4, loss = 0.01037909\n",
      "Iteration 5, loss = 0.00895699\n",
      "Iteration 6, loss = 0.00838682\n",
      "Iteration 7, loss = 0.00700125\n",
      "Iteration 8, loss = 0.00549141\n",
      "Iteration 9, loss = 0.00488275\n",
      "Iteration 10, loss = 0.00489583\n",
      "Iteration 11, loss = 0.00464363\n",
      "Iteration 12, loss = 0.00424326\n",
      "Iteration 13, loss = 0.00433169\n",
      "Iteration 14, loss = 0.00475651\n",
      "Iteration 15, loss = 0.00490614\n",
      "Iteration 16, loss = 0.00476960\n",
      "Iteration 17, loss = 0.00471948\n",
      "Iteration 18, loss = 0.00477902\n",
      "Iteration 19, loss = 0.00469413\n",
      "Iteration 20, loss = 0.00443410\n",
      "Iteration 21, loss = 0.00421475\n",
      "Iteration 22, loss = 0.00412957\n",
      "Iteration 23, loss = 0.00407039\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01831362\n",
      "Iteration 2, loss = 0.01539628\n",
      "Iteration 3, loss = 0.01358909\n",
      "Iteration 4, loss = 0.01035932\n",
      "Iteration 5, loss = 0.00894782\n",
      "Iteration 6, loss = 0.00837467\n",
      "Iteration 7, loss = 0.00698501\n",
      "Iteration 8, loss = 0.00547412\n",
      "Iteration 9, loss = 0.00487656\n",
      "Iteration 10, loss = 0.00489871\n",
      "Iteration 11, loss = 0.00464380\n",
      "Iteration 12, loss = 0.00424089\n",
      "Iteration 13, loss = 0.00433301\n",
      "Iteration 14, loss = 0.00476212\n",
      "Iteration 15, loss = 0.00491071\n",
      "Iteration 16, loss = 0.00477295\n",
      "Iteration 17, loss = 0.00472404\n",
      "Iteration 18, loss = 0.00478481\n",
      "Iteration 19, loss = 0.00469869\n",
      "Iteration 20, loss = 0.00443642\n",
      "Iteration 21, loss = 0.00421704\n",
      "Iteration 22, loss = 0.00413199\n",
      "Iteration 23, loss = 0.00407313\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01830896\n",
      "Iteration 2, loss = 0.01539783\n",
      "Iteration 3, loss = 0.01359251\n",
      "Iteration 4, loss = 0.01036889\n",
      "Iteration 5, loss = 0.00896475\n",
      "Iteration 6, loss = 0.00839020\n",
      "Iteration 7, loss = 0.00699430\n",
      "Iteration 8, loss = 0.00548954\n",
      "Iteration 9, loss = 0.00489317\n",
      "Iteration 10, loss = 0.00490415\n",
      "Iteration 11, loss = 0.00464303\n",
      "Iteration 12, loss = 0.00424397\n",
      "Iteration 13, loss = 0.00434005\n",
      "Iteration 14, loss = 0.00476409\n",
      "Iteration 15, loss = 0.00490628\n",
      "Iteration 16, loss = 0.00476894\n",
      "Iteration 17, loss = 0.00472177\n",
      "Iteration 18, loss = 0.00478261\n",
      "Iteration 19, loss = 0.00469423\n",
      "Iteration 20, loss = 0.00443418\n",
      "Iteration 21, loss = 0.00421790\n",
      "Iteration 22, loss = 0.00413434\n",
      "Iteration 23, loss = 0.00407396\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01831650\n",
      "Iteration 2, loss = 0.01539799\n",
      "Iteration 3, loss = 0.01359697\n",
      "Iteration 4, loss = 0.01037116\n",
      "Iteration 5, loss = 0.00896237\n",
      "Iteration 6, loss = 0.00838922\n",
      "Iteration 7, loss = 0.00699629\n",
      "Iteration 8, loss = 0.00549040\n",
      "Iteration 9, loss = 0.00489162\n",
      "Iteration 10, loss = 0.00490367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.00464533\n",
      "Iteration 12, loss = 0.00424629\n",
      "Iteration 13, loss = 0.00434059\n",
      "Iteration 14, loss = 0.00476485\n",
      "Iteration 15, loss = 0.00490891\n",
      "Iteration 16, loss = 0.00477334\n",
      "Iteration 17, loss = 0.00472650\n",
      "Iteration 18, loss = 0.00478467\n",
      "Iteration 19, loss = 0.00469514\n",
      "Iteration 20, loss = 0.00443347\n",
      "Iteration 21, loss = 0.00421670\n",
      "Iteration 22, loss = 0.00413238\n",
      "Iteration 23, loss = 0.00407183\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01832594\n",
      "Iteration 2, loss = 0.01539779\n",
      "Iteration 3, loss = 0.01360328\n",
      "Iteration 4, loss = 0.01037512\n",
      "Iteration 5, loss = 0.00896077\n",
      "Iteration 6, loss = 0.00838946\n",
      "Iteration 7, loss = 0.00700031\n",
      "Iteration 8, loss = 0.00549271\n",
      "Iteration 9, loss = 0.00489018\n",
      "Iteration 10, loss = 0.00490318\n",
      "Iteration 11, loss = 0.00464791\n",
      "Iteration 12, loss = 0.00424814\n",
      "Iteration 13, loss = 0.00434193\n",
      "Iteration 14, loss = 0.00476608\n",
      "Iteration 15, loss = 0.00491265\n",
      "Iteration 16, loss = 0.00477381\n",
      "Iteration 17, loss = 0.00472113\n",
      "Iteration 18, loss = 0.00477922\n",
      "Iteration 19, loss = 0.00469407\n",
      "Iteration 20, loss = 0.00443673\n",
      "Iteration 21, loss = 0.00421829\n",
      "Iteration 22, loss = 0.00413249\n",
      "Iteration 23, loss = 0.00407489\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01833514\n",
      "Iteration 2, loss = 0.01539706\n",
      "Iteration 3, loss = 0.01360943\n",
      "Iteration 4, loss = 0.01037893\n",
      "Iteration 5, loss = 0.00895887\n",
      "Iteration 6, loss = 0.00838952\n",
      "Iteration 7, loss = 0.00700401\n",
      "Iteration 8, loss = 0.00549498\n",
      "Iteration 9, loss = 0.00488881\n",
      "Iteration 10, loss = 0.00490163\n",
      "Iteration 11, loss = 0.00464899\n",
      "Iteration 12, loss = 0.00424881\n",
      "Iteration 13, loss = 0.00434015\n",
      "Iteration 14, loss = 0.00476363\n",
      "Iteration 15, loss = 0.00491354\n",
      "Iteration 16, loss = 0.00477817\n",
      "Iteration 17, loss = 0.00472541\n",
      "Iteration 18, loss = 0.00478210\n",
      "Iteration 19, loss = 0.00469691\n",
      "Iteration 20, loss = 0.00443722\n",
      "Iteration 21, loss = 0.00421820\n",
      "Iteration 22, loss = 0.00413126\n",
      "Iteration 23, loss = 0.00407267\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01834589\n",
      "Iteration 2, loss = 0.01539324\n",
      "Iteration 3, loss = 0.01361660\n",
      "Iteration 4, loss = 0.01038440\n",
      "Iteration 5, loss = 0.00895412\n",
      "Iteration 6, loss = 0.00838600\n",
      "Iteration 7, loss = 0.00700751\n",
      "Iteration 8, loss = 0.00549625\n",
      "Iteration 9, loss = 0.00488321\n",
      "Iteration 10, loss = 0.00489792\n",
      "Iteration 11, loss = 0.00465222\n",
      "Iteration 12, loss = 0.00425304\n",
      "Iteration 13, loss = 0.00433721\n",
      "Iteration 14, loss = 0.00476117\n",
      "Iteration 15, loss = 0.00491533\n",
      "Iteration 16, loss = 0.00478083\n",
      "Iteration 17, loss = 0.00472701\n",
      "Iteration 18, loss = 0.00478312\n",
      "Iteration 19, loss = 0.00469843\n",
      "Iteration 20, loss = 0.00443796\n",
      "Iteration 21, loss = 0.00421714\n",
      "Iteration 22, loss = 0.00412910\n",
      "Iteration 23, loss = 0.00407004\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01830401\n",
      "Iteration 2, loss = 0.01539736\n",
      "Iteration 3, loss = 0.01358632\n",
      "Iteration 4, loss = 0.01036174\n",
      "Iteration 5, loss = 0.00895839\n",
      "Iteration 6, loss = 0.00838227\n",
      "Iteration 7, loss = 0.00698468\n",
      "Iteration 8, loss = 0.00548137\n",
      "Iteration 9, loss = 0.00488681\n",
      "Iteration 10, loss = 0.00489834\n",
      "Iteration 11, loss = 0.00463835\n",
      "Iteration 12, loss = 0.00424178\n",
      "Iteration 13, loss = 0.00433965\n",
      "Iteration 14, loss = 0.00476274\n",
      "Iteration 15, loss = 0.00490464\n",
      "Iteration 16, loss = 0.00476835\n",
      "Iteration 17, loss = 0.00472171\n",
      "Iteration 18, loss = 0.00477953\n",
      "Iteration 19, loss = 0.00468865\n",
      "Iteration 20, loss = 0.00442694\n",
      "Iteration 21, loss = 0.00421095\n",
      "Iteration 22, loss = 0.00412714\n",
      "Iteration 23, loss = 0.00406618\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01832763\n",
      "Iteration 2, loss = 0.01539692\n",
      "Iteration 3, loss = 0.01360433\n",
      "Iteration 4, loss = 0.01037645\n",
      "Iteration 5, loss = 0.00895887\n",
      "Iteration 6, loss = 0.00838638\n",
      "Iteration 7, loss = 0.00699938\n",
      "Iteration 8, loss = 0.00549150\n",
      "Iteration 9, loss = 0.00488667\n",
      "Iteration 10, loss = 0.00489985\n",
      "Iteration 11, loss = 0.00464709\n",
      "Iteration 12, loss = 0.00424865\n",
      "Iteration 13, loss = 0.00433964\n",
      "Iteration 14, loss = 0.00476392\n",
      "Iteration 15, loss = 0.00491219\n",
      "Iteration 16, loss = 0.00477742\n",
      "Iteration 17, loss = 0.00472822\n",
      "Iteration 18, loss = 0.00478510\n",
      "Iteration 19, loss = 0.00469682\n",
      "Iteration 20, loss = 0.00443602\n",
      "Iteration 21, loss = 0.00421796\n",
      "Iteration 22, loss = 0.00413196\n",
      "Iteration 23, loss = 0.00407148\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01833801\n",
      "Iteration 2, loss = 0.01539614\n",
      "Iteration 3, loss = 0.01361176\n",
      "Iteration 4, loss = 0.01038025\n",
      "Iteration 5, loss = 0.00895587\n",
      "Iteration 6, loss = 0.00838807\n",
      "Iteration 7, loss = 0.00700424\n",
      "Iteration 8, loss = 0.00549313\n",
      "Iteration 9, loss = 0.00488311\n",
      "Iteration 10, loss = 0.00489732\n",
      "Iteration 11, loss = 0.00464667\n",
      "Iteration 12, loss = 0.00424558\n",
      "Iteration 13, loss = 0.00433163\n",
      "Iteration 14, loss = 0.00475677\n",
      "Iteration 15, loss = 0.00490742\n",
      "Iteration 16, loss = 0.00477190\n",
      "Iteration 17, loss = 0.00472106\n",
      "Iteration 18, loss = 0.00477947\n",
      "Iteration 19, loss = 0.00469407\n",
      "Iteration 20, loss = 0.00443364\n",
      "Iteration 21, loss = 0.00421459\n",
      "Iteration 22, loss = 0.00412857\n",
      "Iteration 23, loss = 0.00406980\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01833398\n",
      "Iteration 2, loss = 0.01539706\n",
      "Iteration 3, loss = 0.01360837\n",
      "Iteration 4, loss = 0.01037814\n",
      "Iteration 5, loss = 0.00895831\n",
      "Iteration 6, loss = 0.00838861\n",
      "Iteration 7, loss = 0.00700253\n",
      "Iteration 8, loss = 0.00549351\n",
      "Iteration 9, loss = 0.00488742\n",
      "Iteration 10, loss = 0.00490090\n",
      "Iteration 11, loss = 0.00464906\n",
      "Iteration 12, loss = 0.00424980\n",
      "Iteration 13, loss = 0.00433905\n",
      "Iteration 14, loss = 0.00476334\n",
      "Iteration 15, loss = 0.00491247\n",
      "Iteration 16, loss = 0.00477678\n",
      "Iteration 17, loss = 0.00472603\n",
      "Iteration 18, loss = 0.00478355\n",
      "Iteration 19, loss = 0.00469686\n",
      "Iteration 20, loss = 0.00443600\n",
      "Iteration 21, loss = 0.00421672\n",
      "Iteration 22, loss = 0.00413072\n",
      "Iteration 23, loss = 0.00407129\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01838192\n",
      "Iteration 2, loss = 0.01535149\n",
      "Iteration 3, loss = 0.01363480\n",
      "Iteration 4, loss = 0.01039078\n",
      "Iteration 5, loss = 0.00891829\n",
      "Iteration 6, loss = 0.00836743\n",
      "Iteration 7, loss = 0.00701846\n",
      "Iteration 8, loss = 0.00549606\n",
      "Iteration 9, loss = 0.00485434\n",
      "Iteration 10, loss = 0.00487222\n",
      "Iteration 11, loss = 0.00464735\n",
      "Iteration 12, loss = 0.00424338\n",
      "Iteration 13, loss = 0.00430722\n",
      "Iteration 14, loss = 0.00473259\n",
      "Iteration 15, loss = 0.00490527\n",
      "Iteration 16, loss = 0.00477010\n",
      "Iteration 17, loss = 0.00469973\n",
      "Iteration 18, loss = 0.00475384\n",
      "Iteration 19, loss = 0.00468273\n",
      "Iteration 20, loss = 0.00443133\n",
      "Iteration 21, loss = 0.00420180\n",
      "Iteration 22, loss = 0.00410730\n",
      "Iteration 23, loss = 0.00405410\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01836618\n",
      "Iteration 2, loss = 0.01538413\n",
      "Iteration 3, loss = 0.01363063\n",
      "Iteration 4, loss = 0.01039166\n",
      "Iteration 5, loss = 0.00894337\n",
      "Iteration 6, loss = 0.00838388\n",
      "Iteration 7, loss = 0.00701761\n",
      "Iteration 8, loss = 0.00550135\n",
      "Iteration 9, loss = 0.00487635\n",
      "Iteration 10, loss = 0.00489402\n",
      "Iteration 11, loss = 0.00465792\n",
      "Iteration 12, loss = 0.00425697\n",
      "Iteration 13, loss = 0.00433012\n",
      "Iteration 14, loss = 0.00475381\n",
      "Iteration 15, loss = 0.00491660\n",
      "Iteration 16, loss = 0.00478111\n",
      "Iteration 17, loss = 0.00471848\n",
      "Iteration 18, loss = 0.00477580\n",
      "Iteration 19, loss = 0.00469876\n",
      "Iteration 20, loss = 0.00444114\n",
      "Iteration 21, loss = 0.00421553\n",
      "Iteration 22, loss = 0.00412481\n",
      "Iteration 23, loss = 0.00406898\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01836521\n",
      "Iteration 2, loss = 0.01538605\n",
      "Iteration 3, loss = 0.01362991\n",
      "Iteration 4, loss = 0.01039087\n",
      "Iteration 5, loss = 0.00894481\n",
      "Iteration 6, loss = 0.00838568\n",
      "Iteration 7, loss = 0.00701765\n",
      "Iteration 8, loss = 0.00550145\n",
      "Iteration 9, loss = 0.00487857\n",
      "Iteration 10, loss = 0.00489667\n",
      "Iteration 11, loss = 0.00465865\n",
      "Iteration 12, loss = 0.00425700\n",
      "Iteration 13, loss = 0.00433214\n",
      "Iteration 14, loss = 0.00475693\n",
      "Iteration 15, loss = 0.00491803\n",
      "Iteration 16, loss = 0.00478130\n",
      "Iteration 17, loss = 0.00471988\n",
      "Iteration 18, loss = 0.00477807\n",
      "Iteration 19, loss = 0.00470068\n",
      "Iteration 20, loss = 0.00444288\n",
      "Iteration 21, loss = 0.00421728\n",
      "Iteration 22, loss = 0.00412719\n",
      "Iteration 23, loss = 0.00407156\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01837296\n",
      "Iteration 2, loss = 0.01537681\n",
      "Iteration 3, loss = 0.01363465\n",
      "Iteration 4, loss = 0.01039406\n",
      "Iteration 5, loss = 0.00893891\n",
      "Iteration 6, loss = 0.00838280\n",
      "Iteration 7, loss = 0.00702205\n",
      "Iteration 8, loss = 0.00550417\n",
      "Iteration 9, loss = 0.00487445\n",
      "Iteration 10, loss = 0.00489286\n",
      "Iteration 11, loss = 0.00466050\n",
      "Iteration 12, loss = 0.00425935\n",
      "Iteration 13, loss = 0.00432945\n",
      "Iteration 14, loss = 0.00475358\n",
      "Iteration 15, loss = 0.00491912\n",
      "Iteration 16, loss = 0.00478307\n",
      "Iteration 17, loss = 0.00471788\n",
      "Iteration 18, loss = 0.00477527\n",
      "Iteration 19, loss = 0.00470078\n",
      "Iteration 20, loss = 0.00444454\n",
      "Iteration 21, loss = 0.00421753\n",
      "Iteration 22, loss = 0.00412608\n",
      "Iteration 23, loss = 0.00407157\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01833429\n",
      "Iteration 2, loss = 0.01539682\n",
      "Iteration 3, loss = 0.01360670\n",
      "Iteration 4, loss = 0.01037389\n",
      "Iteration 5, loss = 0.00894879\n",
      "Iteration 6, loss = 0.00837644\n",
      "Iteration 7, loss = 0.00699142\n",
      "Iteration 8, loss = 0.00548033\n",
      "Iteration 9, loss = 0.00487041\n",
      "Iteration 10, loss = 0.00488458\n",
      "Iteration 11, loss = 0.00463540\n",
      "Iteration 12, loss = 0.00423470\n",
      "Iteration 13, loss = 0.00431875\n",
      "Iteration 14, loss = 0.00474226\n",
      "Iteration 15, loss = 0.00489439\n",
      "Iteration 16, loss = 0.00475852\n",
      "Iteration 17, loss = 0.00470395\n",
      "Iteration 18, loss = 0.00476078\n",
      "Iteration 19, loss = 0.00467664\n",
      "Iteration 20, loss = 0.00441637\n",
      "Iteration 21, loss = 0.00419631\n",
      "Iteration 22, loss = 0.00410953\n",
      "Iteration 23, loss = 0.00405041\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01831235\n",
      "Iteration 2, loss = 0.01539784\n",
      "Iteration 3, loss = 0.01359222\n",
      "Iteration 4, loss = 0.01036579\n",
      "Iteration 5, loss = 0.00895787\n",
      "Iteration 6, loss = 0.00838371\n",
      "Iteration 7, loss = 0.00698885\n",
      "Iteration 8, loss = 0.00548397\n",
      "Iteration 9, loss = 0.00488641\n",
      "Iteration 10, loss = 0.00489857\n",
      "Iteration 11, loss = 0.00464084\n",
      "Iteration 12, loss = 0.00424361\n",
      "Iteration 13, loss = 0.00433910\n",
      "Iteration 14, loss = 0.00476243\n",
      "Iteration 15, loss = 0.00490632\n",
      "Iteration 16, loss = 0.00476995\n",
      "Iteration 17, loss = 0.00472170\n",
      "Iteration 18, loss = 0.00477934\n",
      "Iteration 19, loss = 0.00468969\n",
      "Iteration 20, loss = 0.00442824\n",
      "Iteration 21, loss = 0.00421123\n",
      "Iteration 22, loss = 0.00412680\n",
      "Iteration 23, loss = 0.00406641\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01837039\n",
      "Iteration 2, loss = 0.01537833\n",
      "Iteration 3, loss = 0.01363015\n",
      "Iteration 4, loss = 0.01038995\n",
      "Iteration 5, loss = 0.00893729\n",
      "Iteration 6, loss = 0.00837801\n",
      "Iteration 7, loss = 0.00701225\n",
      "Iteration 8, loss = 0.00549365\n",
      "Iteration 9, loss = 0.00486507\n",
      "Iteration 10, loss = 0.00488137\n",
      "Iteration 11, loss = 0.00464439\n",
      "Iteration 12, loss = 0.00424102\n",
      "Iteration 13, loss = 0.00431370\n",
      "Iteration 14, loss = 0.00474014\n",
      "Iteration 15, loss = 0.00490323\n",
      "Iteration 16, loss = 0.00476867\n",
      "Iteration 17, loss = 0.00470955\n",
      "Iteration 18, loss = 0.00476661\n",
      "Iteration 19, loss = 0.00468891\n",
      "Iteration 20, loss = 0.00443143\n",
      "Iteration 21, loss = 0.00420602\n",
      "Iteration 22, loss = 0.00411577\n",
      "Iteration 23, loss = 0.00405949\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01837983\n",
      "Iteration 2, loss = 0.01536321\n",
      "Iteration 3, loss = 0.01363673\n",
      "Iteration 4, loss = 0.01039357\n",
      "Iteration 5, loss = 0.00892740\n",
      "Iteration 6, loss = 0.00837545\n",
      "Iteration 7, loss = 0.00702292\n",
      "Iteration 8, loss = 0.00550281\n",
      "Iteration 9, loss = 0.00486575\n",
      "Iteration 10, loss = 0.00488505\n",
      "Iteration 11, loss = 0.00465815\n",
      "Iteration 12, loss = 0.00425627\n",
      "Iteration 13, loss = 0.00432271\n",
      "Iteration 14, loss = 0.00474734\n",
      "Iteration 15, loss = 0.00491739\n",
      "Iteration 16, loss = 0.00478182\n",
      "Iteration 17, loss = 0.00471456\n",
      "Iteration 18, loss = 0.00477140\n",
      "Iteration 19, loss = 0.00469965\n",
      "Iteration 20, loss = 0.00444392\n",
      "Iteration 21, loss = 0.00421499\n",
      "Iteration 22, loss = 0.00412202\n",
      "Iteration 23, loss = 0.00406787\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01838431\n",
      "Iteration 2, loss = 0.01532970\n",
      "Iteration 3, loss = 0.01363707\n",
      "Iteration 4, loss = 0.01039059\n",
      "Iteration 5, loss = 0.00890618\n",
      "Iteration 6, loss = 0.00836471\n",
      "Iteration 7, loss = 0.00702928\n",
      "Iteration 8, loss = 0.00550511\n",
      "Iteration 9, loss = 0.00485506\n",
      "Iteration 10, loss = 0.00487689\n",
      "Iteration 11, loss = 0.00466261\n",
      "Iteration 12, loss = 0.00426022\n",
      "Iteration 13, loss = 0.00431431\n",
      "Iteration 14, loss = 0.00473826\n",
      "Iteration 15, loss = 0.00491728\n",
      "Iteration 16, loss = 0.00478186\n",
      "Iteration 17, loss = 0.00470586\n",
      "Iteration 18, loss = 0.00476114\n",
      "Iteration 19, loss = 0.00469579\n",
      "Iteration 20, loss = 0.00444437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.00421202\n",
      "Iteration 22, loss = 0.00411568\n",
      "Iteration 23, loss = 0.00406460\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01838295\n",
      "Iteration 2, loss = 0.01531632\n",
      "Iteration 3, loss = 0.01363885\n",
      "Iteration 4, loss = 0.01039348\n",
      "Iteration 5, loss = 0.00890310\n",
      "Iteration 6, loss = 0.00836711\n",
      "Iteration 7, loss = 0.00703919\n",
      "Iteration 8, loss = 0.00551383\n",
      "Iteration 9, loss = 0.00485892\n",
      "Iteration 10, loss = 0.00488271\n",
      "Iteration 11, loss = 0.00467436\n",
      "Iteration 12, loss = 0.00426941\n",
      "Iteration 13, loss = 0.00431712\n",
      "Iteration 14, loss = 0.00474006\n",
      "Iteration 15, loss = 0.00492224\n",
      "Iteration 16, loss = 0.00478693\n",
      "Iteration 17, loss = 0.00470761\n",
      "Iteration 18, loss = 0.00476246\n",
      "Iteration 19, loss = 0.00470015\n",
      "Iteration 20, loss = 0.00445019\n",
      "Iteration 21, loss = 0.00421655\n",
      "Iteration 22, loss = 0.00411916\n",
      "Iteration 23, loss = 0.00406913\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01838278\n",
      "Iteration 2, loss = 0.01531561\n",
      "Iteration 3, loss = 0.01364562\n",
      "Iteration 4, loss = 0.01040190\n",
      "Iteration 5, loss = 0.00891240\n",
      "Iteration 6, loss = 0.00837690\n",
      "Iteration 7, loss = 0.00704979\n",
      "Iteration 8, loss = 0.00552305\n",
      "Iteration 9, loss = 0.00486653\n",
      "Iteration 10, loss = 0.00488984\n",
      "Iteration 11, loss = 0.00468096\n",
      "Iteration 12, loss = 0.00427343\n",
      "Iteration 13, loss = 0.00431693\n",
      "Iteration 14, loss = 0.00473881\n",
      "Iteration 15, loss = 0.00492182\n",
      "Iteration 16, loss = 0.00478761\n",
      "Iteration 17, loss = 0.00470884\n",
      "Iteration 18, loss = 0.00476379\n",
      "Iteration 19, loss = 0.00470119\n",
      "Iteration 20, loss = 0.00444973\n",
      "Iteration 21, loss = 0.00421703\n",
      "Iteration 22, loss = 0.00411977\n",
      "Iteration 23, loss = 0.00406899\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01834889\n",
      "Iteration 2, loss = 0.01539227\n",
      "Iteration 3, loss = 0.01361720\n",
      "Iteration 4, loss = 0.01037745\n",
      "Iteration 5, loss = 0.00893507\n",
      "Iteration 6, loss = 0.00836681\n",
      "Iteration 7, loss = 0.00698957\n",
      "Iteration 8, loss = 0.00547019\n",
      "Iteration 9, loss = 0.00484887\n",
      "Iteration 10, loss = 0.00486413\n",
      "Iteration 11, loss = 0.00461804\n",
      "Iteration 12, loss = 0.00420830\n",
      "Iteration 13, loss = 0.00427982\n",
      "Iteration 14, loss = 0.00470552\n",
      "Iteration 15, loss = 0.00486657\n",
      "Iteration 16, loss = 0.00472968\n",
      "Iteration 17, loss = 0.00466854\n",
      "Iteration 18, loss = 0.00472713\n",
      "Iteration 19, loss = 0.00465244\n",
      "Iteration 20, loss = 0.00439551\n",
      "Iteration 21, loss = 0.00417263\n",
      "Iteration 22, loss = 0.00408317\n",
      "Iteration 23, loss = 0.00402582\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01838440\n",
      "Iteration 2, loss = 0.01533174\n",
      "Iteration 3, loss = 0.01363145\n",
      "Iteration 4, loss = 0.01038298\n",
      "Iteration 5, loss = 0.00889747\n",
      "Iteration 6, loss = 0.00835102\n",
      "Iteration 7, loss = 0.00701088\n",
      "Iteration 8, loss = 0.00548480\n",
      "Iteration 9, loss = 0.00483416\n",
      "Iteration 10, loss = 0.00485442\n",
      "Iteration 11, loss = 0.00463640\n",
      "Iteration 12, loss = 0.00423073\n",
      "Iteration 13, loss = 0.00428883\n",
      "Iteration 14, loss = 0.00471486\n",
      "Iteration 15, loss = 0.00489355\n",
      "Iteration 16, loss = 0.00476104\n",
      "Iteration 17, loss = 0.00469032\n",
      "Iteration 18, loss = 0.00474402\n",
      "Iteration 19, loss = 0.00467564\n",
      "Iteration 20, loss = 0.00442319\n",
      "Iteration 21, loss = 0.00419113\n",
      "Iteration 22, loss = 0.00409504\n",
      "Iteration 23, loss = 0.00404185\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01836914\n",
      "Iteration 2, loss = 0.01525984\n",
      "Iteration 3, loss = 0.01361768\n",
      "Iteration 4, loss = 0.01036851\n",
      "Iteration 5, loss = 0.00885861\n",
      "Iteration 6, loss = 0.00833294\n",
      "Iteration 7, loss = 0.00702329\n",
      "Iteration 8, loss = 0.00549467\n",
      "Iteration 9, loss = 0.00482588\n",
      "Iteration 10, loss = 0.00485085\n",
      "Iteration 11, loss = 0.00465705\n",
      "Iteration 12, loss = 0.00425347\n",
      "Iteration 13, loss = 0.00429139\n",
      "Iteration 14, loss = 0.00471433\n",
      "Iteration 15, loss = 0.00490570\n",
      "Iteration 16, loss = 0.00477031\n",
      "Iteration 17, loss = 0.00468366\n",
      "Iteration 18, loss = 0.00473713\n",
      "Iteration 19, loss = 0.00468122\n",
      "Iteration 20, loss = 0.00443612\n",
      "Iteration 21, loss = 0.00419849\n",
      "Iteration 22, loss = 0.00409699\n",
      "Iteration 23, loss = 0.00404984\n",
      "Iteration 24, loss = 0.00394101\n",
      "Iteration 25, loss = 0.00380746\n",
      "Iteration 26, loss = 0.00374920\n",
      "Iteration 27, loss = 0.00376424\n",
      "Iteration 28, loss = 0.00376565\n",
      "Iteration 29, loss = 0.00372322\n",
      "Iteration 30, loss = 0.00369352\n",
      "Iteration 31, loss = 0.00370827\n",
      "Iteration 32, loss = 0.00372672\n",
      "Iteration 33, loss = 0.00370367\n",
      "Iteration 34, loss = 0.00365719\n",
      "Iteration 35, loss = 0.00362734\n",
      "Iteration 36, loss = 0.00360943\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01835133\n",
      "Iteration 2, loss = 0.01521796\n",
      "Iteration 3, loss = 0.01361342\n",
      "Iteration 4, loss = 0.01036842\n",
      "Iteration 5, loss = 0.00885034\n",
      "Iteration 6, loss = 0.00833675\n",
      "Iteration 7, loss = 0.00704323\n",
      "Iteration 8, loss = 0.00551439\n",
      "Iteration 9, loss = 0.00483959\n",
      "Iteration 10, loss = 0.00486890\n",
      "Iteration 11, loss = 0.00468165\n",
      "Iteration 12, loss = 0.00427472\n",
      "Iteration 13, loss = 0.00430129\n",
      "Iteration 14, loss = 0.00472083\n",
      "Iteration 15, loss = 0.00491833\n",
      "Iteration 16, loss = 0.00478440\n",
      "Iteration 17, loss = 0.00469271\n",
      "Iteration 18, loss = 0.00474419\n",
      "Iteration 19, loss = 0.00469351\n",
      "Iteration 20, loss = 0.00445149\n",
      "Iteration 21, loss = 0.00421238\n",
      "Iteration 22, loss = 0.00410835\n",
      "Iteration 23, loss = 0.00406177\n",
      "Iteration 24, loss = 0.00395489\n",
      "Iteration 25, loss = 0.00382082\n",
      "Iteration 26, loss = 0.00375980\n",
      "Iteration 27, loss = 0.00377335\n",
      "Iteration 28, loss = 0.00377548\n",
      "Iteration 29, loss = 0.00373375\n",
      "Iteration 30, loss = 0.00370258\n",
      "Iteration 31, loss = 0.00371548\n",
      "Iteration 32, loss = 0.00373406\n",
      "Iteration 33, loss = 0.00371210\n",
      "Iteration 34, loss = 0.00366561\n",
      "Iteration 35, loss = 0.00363438\n",
      "Iteration 36, loss = 0.00361583\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01837193\n",
      "Iteration 2, loss = 0.01527691\n",
      "Iteration 3, loss = 0.01364421\n",
      "Iteration 4, loss = 0.01040209\n",
      "Iteration 5, loss = 0.00890121\n",
      "Iteration 6, loss = 0.00837735\n",
      "Iteration 7, loss = 0.00706590\n",
      "Iteration 8, loss = 0.00553594\n",
      "Iteration 9, loss = 0.00486814\n",
      "Iteration 10, loss = 0.00489005\n",
      "Iteration 11, loss = 0.00468639\n",
      "Iteration 12, loss = 0.00427326\n",
      "Iteration 13, loss = 0.00430363\n",
      "Iteration 14, loss = 0.00472312\n",
      "Iteration 15, loss = 0.00491337\n",
      "Iteration 16, loss = 0.00477998\n",
      "Iteration 17, loss = 0.00469548\n",
      "Iteration 18, loss = 0.00475194\n",
      "Iteration 19, loss = 0.00469750\n",
      "Iteration 20, loss = 0.00444852\n",
      "Iteration 21, loss = 0.00421447\n",
      "Iteration 22, loss = 0.00411610\n",
      "Iteration 23, loss = 0.00406701\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01834372\n",
      "Iteration 2, loss = 0.01520269\n",
      "Iteration 3, loss = 0.01360969\n",
      "Iteration 4, loss = 0.01036574\n",
      "Iteration 5, loss = 0.00884491\n",
      "Iteration 6, loss = 0.00833522\n",
      "Iteration 7, loss = 0.00704685\n",
      "Iteration 8, loss = 0.00551782\n",
      "Iteration 9, loss = 0.00484129\n",
      "Iteration 10, loss = 0.00487093\n",
      "Iteration 11, loss = 0.00468581\n",
      "Iteration 12, loss = 0.00427759\n",
      "Iteration 13, loss = 0.00430031\n",
      "Iteration 14, loss = 0.00471889\n",
      "Iteration 15, loss = 0.00491841\n",
      "Iteration 16, loss = 0.00478493\n",
      "Iteration 17, loss = 0.00469184\n",
      "Iteration 18, loss = 0.00474344\n",
      "Iteration 19, loss = 0.00469462\n",
      "Iteration 20, loss = 0.00445351\n",
      "Iteration 21, loss = 0.00421402\n",
      "Iteration 22, loss = 0.00410939\n",
      "Iteration 23, loss = 0.00406322\n",
      "Iteration 24, loss = 0.00395716\n",
      "Iteration 25, loss = 0.00382293\n",
      "Iteration 26, loss = 0.00376103\n",
      "Iteration 27, loss = 0.00377421\n",
      "Iteration 28, loss = 0.00377673\n",
      "Iteration 29, loss = 0.00373520\n",
      "Iteration 30, loss = 0.00370350\n",
      "Iteration 31, loss = 0.00371582\n",
      "Iteration 32, loss = 0.00373448\n",
      "Iteration 33, loss = 0.00371287\n",
      "Iteration 34, loss = 0.00366629\n",
      "Iteration 35, loss = 0.00363468\n",
      "Iteration 36, loss = 0.00361606\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01838202\n",
      "Iteration 2, loss = 0.01531554\n",
      "Iteration 3, loss = 0.01365577\n",
      "Iteration 4, loss = 0.01041381\n",
      "Iteration 5, loss = 0.00892124\n",
      "Iteration 6, loss = 0.00838734\n",
      "Iteration 7, loss = 0.00706198\n",
      "Iteration 8, loss = 0.00553027\n",
      "Iteration 9, loss = 0.00486555\n",
      "Iteration 10, loss = 0.00488188\n",
      "Iteration 11, loss = 0.00466713\n",
      "Iteration 12, loss = 0.00425134\n",
      "Iteration 13, loss = 0.00428667\n",
      "Iteration 14, loss = 0.00470661\n",
      "Iteration 15, loss = 0.00489245\n",
      "Iteration 16, loss = 0.00476066\n",
      "Iteration 17, loss = 0.00468357\n",
      "Iteration 18, loss = 0.00474110\n",
      "Iteration 19, loss = 0.00468370\n",
      "Iteration 20, loss = 0.00443146\n",
      "Iteration 21, loss = 0.00419945\n",
      "Iteration 22, loss = 0.00410206\n",
      "Iteration 23, loss = 0.00404961\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01837424\n",
      "Iteration 2, loss = 0.01526474\n",
      "Iteration 3, loss = 0.01363881\n",
      "Iteration 4, loss = 0.01039897\n",
      "Iteration 5, loss = 0.00888795\n",
      "Iteration 6, loss = 0.00836302\n",
      "Iteration 7, loss = 0.00706070\n",
      "Iteration 8, loss = 0.00553630\n",
      "Iteration 9, loss = 0.00486102\n",
      "Iteration 10, loss = 0.00487847\n",
      "Iteration 11, loss = 0.00468355\n",
      "Iteration 12, loss = 0.00428145\n",
      "Iteration 13, loss = 0.00431319\n",
      "Iteration 14, loss = 0.00472609\n",
      "Iteration 15, loss = 0.00491476\n",
      "Iteration 16, loss = 0.00478363\n",
      "Iteration 17, loss = 0.00469674\n",
      "Iteration 18, loss = 0.00474659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 0.00468746\n",
      "Iteration 20, loss = 0.00443796\n",
      "Iteration 21, loss = 0.00420495\n",
      "Iteration 22, loss = 0.00410640\n",
      "Iteration 23, loss = 0.00405719\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01837399\n",
      "Iteration 2, loss = 0.01537724\n",
      "Iteration 3, loss = 0.01364027\n",
      "Iteration 4, loss = 0.01039978\n",
      "Iteration 5, loss = 0.00894309\n",
      "Iteration 6, loss = 0.00838480\n",
      "Iteration 7, loss = 0.00702334\n",
      "Iteration 8, loss = 0.00550231\n",
      "Iteration 9, loss = 0.00486846\n",
      "Iteration 10, loss = 0.00488284\n",
      "Iteration 11, loss = 0.00464747\n",
      "Iteration 12, loss = 0.00423909\n",
      "Iteration 13, loss = 0.00430238\n",
      "Iteration 14, loss = 0.00472551\n",
      "Iteration 15, loss = 0.00489233\n",
      "Iteration 16, loss = 0.00475886\n",
      "Iteration 17, loss = 0.00469640\n",
      "Iteration 18, loss = 0.00475322\n",
      "Iteration 19, loss = 0.00467983\n",
      "Iteration 20, loss = 0.00441954\n",
      "Iteration 21, loss = 0.00419439\n",
      "Iteration 22, loss = 0.00410324\n",
      "Iteration 23, loss = 0.00404567\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01837689\n",
      "Iteration 2, loss = 0.01528618\n",
      "Iteration 3, loss = 0.01362424\n",
      "Iteration 4, loss = 0.01037430\n",
      "Iteration 5, loss = 0.00887069\n",
      "Iteration 6, loss = 0.00833753\n",
      "Iteration 7, loss = 0.00701834\n",
      "Iteration 8, loss = 0.00549084\n",
      "Iteration 9, loss = 0.00482814\n",
      "Iteration 10, loss = 0.00485211\n",
      "Iteration 11, loss = 0.00464972\n",
      "Iteration 12, loss = 0.00424720\n",
      "Iteration 13, loss = 0.00429258\n",
      "Iteration 14, loss = 0.00471630\n",
      "Iteration 15, loss = 0.00490432\n",
      "Iteration 16, loss = 0.00476947\n",
      "Iteration 17, loss = 0.00468746\n",
      "Iteration 18, loss = 0.00474076\n",
      "Iteration 19, loss = 0.00468193\n",
      "Iteration 20, loss = 0.00443433\n",
      "Iteration 21, loss = 0.00419814\n",
      "Iteration 22, loss = 0.00409807\n",
      "Iteration 23, loss = 0.00404865\n",
      "Iteration 24, loss = 0.00393740\n",
      "Iteration 25, loss = 0.00380416\n",
      "Iteration 26, loss = 0.00374786\n",
      "Iteration 27, loss = 0.00376348\n",
      "Iteration 28, loss = 0.00376381\n",
      "Iteration 29, loss = 0.00372123\n",
      "Iteration 30, loss = 0.00369293\n",
      "Iteration 31, loss = 0.00370880\n",
      "Iteration 32, loss = 0.00372688\n",
      "Iteration 33, loss = 0.00370323\n",
      "Iteration 34, loss = 0.00365737\n",
      "Iteration 35, loss = 0.00362857\n",
      "Iteration 36, loss = 0.00361076\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01834039\n",
      "Iteration 2, loss = 0.01519379\n",
      "Iteration 3, loss = 0.01359549\n",
      "Iteration 4, loss = 0.01034878\n",
      "Iteration 5, loss = 0.00882324\n",
      "Iteration 6, loss = 0.00831276\n",
      "Iteration 7, loss = 0.00702519\n",
      "Iteration 8, loss = 0.00549534\n",
      "Iteration 9, loss = 0.00481567\n",
      "Iteration 10, loss = 0.00484822\n",
      "Iteration 11, loss = 0.00466671\n",
      "Iteration 12, loss = 0.00426095\n",
      "Iteration 13, loss = 0.00428548\n",
      "Iteration 14, loss = 0.00470597\n",
      "Iteration 15, loss = 0.00490656\n",
      "Iteration 16, loss = 0.00477203\n",
      "Iteration 17, loss = 0.00467755\n",
      "Iteration 18, loss = 0.00472846\n",
      "Iteration 19, loss = 0.00467961\n",
      "Iteration 20, loss = 0.00443926\n",
      "Iteration 21, loss = 0.00419822\n",
      "Iteration 22, loss = 0.00409307\n",
      "Iteration 23, loss = 0.00404827\n",
      "Iteration 24, loss = 0.00394336\n",
      "Iteration 25, loss = 0.00380936\n",
      "Iteration 26, loss = 0.00374741\n",
      "Iteration 27, loss = 0.00376138\n",
      "Iteration 28, loss = 0.00376538\n",
      "Iteration 29, loss = 0.00372436\n",
      "Iteration 30, loss = 0.00369258\n",
      "Iteration 31, loss = 0.00370524\n",
      "Iteration 32, loss = 0.00372485\n",
      "Iteration 33, loss = 0.00370399\n",
      "Iteration 34, loss = 0.00365736\n",
      "Iteration 35, loss = 0.00362592\n",
      "Iteration 36, loss = 0.00360802\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01822484\n",
      "Iteration 2, loss = 0.01499521\n",
      "Iteration 3, loss = 0.01350402\n",
      "Iteration 4, loss = 0.01026870\n",
      "Iteration 5, loss = 0.00871675\n",
      "Iteration 6, loss = 0.00823909\n",
      "Iteration 7, loss = 0.00700040\n",
      "Iteration 8, loss = 0.00547387\n",
      "Iteration 9, loss = 0.00477422\n",
      "Iteration 10, loss = 0.00480988\n",
      "Iteration 11, loss = 0.00465642\n",
      "Iteration 12, loss = 0.00424990\n",
      "Iteration 13, loss = 0.00424374\n",
      "Iteration 14, loss = 0.00465431\n",
      "Iteration 15, loss = 0.00487322\n",
      "Iteration 16, loss = 0.00474287\n",
      "Iteration 17, loss = 0.00463246\n",
      "Iteration 18, loss = 0.00468036\n",
      "Iteration 19, loss = 0.00464510\n",
      "Iteration 20, loss = 0.00441390\n",
      "Iteration 21, loss = 0.00416892\n",
      "Iteration 22, loss = 0.00405785\n",
      "Iteration 23, loss = 0.00401852\n",
      "Iteration 24, loss = 0.00392373\n",
      "Iteration 25, loss = 0.00379062\n",
      "Iteration 26, loss = 0.00372089\n",
      "Iteration 27, loss = 0.00373127\n",
      "Iteration 28, loss = 0.00373949\n",
      "Iteration 29, loss = 0.00370155\n",
      "Iteration 30, loss = 0.00366638\n",
      "Iteration 31, loss = 0.00367476\n",
      "Iteration 32, loss = 0.00369560\n",
      "Iteration 33, loss = 0.00367884\n",
      "Iteration 34, loss = 0.00363284\n",
      "Iteration 35, loss = 0.00359873\n",
      "Iteration 36, loss = 0.00358071\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01824695\n",
      "Iteration 2, loss = 0.01505952\n",
      "Iteration 3, loss = 0.01356961\n",
      "Iteration 4, loss = 0.01034000\n",
      "Iteration 5, loss = 0.00880218\n",
      "Iteration 6, loss = 0.00832858\n",
      "Iteration 7, loss = 0.00708531\n",
      "Iteration 8, loss = 0.00555611\n",
      "Iteration 9, loss = 0.00485466\n",
      "Iteration 10, loss = 0.00488013\n",
      "Iteration 11, loss = 0.00471014\n",
      "Iteration 12, loss = 0.00429000\n",
      "Iteration 13, loss = 0.00427805\n",
      "Iteration 14, loss = 0.00468622\n",
      "Iteration 15, loss = 0.00490340\n",
      "Iteration 16, loss = 0.00477627\n",
      "Iteration 17, loss = 0.00467205\n",
      "Iteration 18, loss = 0.00472689\n",
      "Iteration 19, loss = 0.00469563\n",
      "Iteration 20, loss = 0.00446268\n",
      "Iteration 21, loss = 0.00422070\n",
      "Iteration 22, loss = 0.00411196\n",
      "Iteration 23, loss = 0.00406935\n",
      "Iteration 24, loss = 0.00397028\n",
      "Iteration 25, loss = 0.00383476\n",
      "Iteration 26, loss = 0.00376465\n",
      "Iteration 27, loss = 0.00377300\n",
      "Iteration 28, loss = 0.00377738\n",
      "Iteration 29, loss = 0.00373722\n",
      "Iteration 30, loss = 0.00370153\n",
      "Iteration 31, loss = 0.00370946\n",
      "Iteration 32, loss = 0.00372862\n",
      "Iteration 33, loss = 0.00370987\n",
      "Iteration 34, loss = 0.00366298\n",
      "Iteration 35, loss = 0.00362853\n",
      "Iteration 36, loss = 0.00360961\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01798765\n",
      "Iteration 2, loss = 0.01467981\n",
      "Iteration 3, loss = 0.01334693\n",
      "Iteration 4, loss = 0.01013785\n",
      "Iteration 5, loss = 0.00855417\n",
      "Iteration 6, loss = 0.00813463\n",
      "Iteration 7, loss = 0.00697781\n",
      "Iteration 8, loss = 0.00546342\n",
      "Iteration 9, loss = 0.00473665\n",
      "Iteration 10, loss = 0.00477971\n",
      "Iteration 11, loss = 0.00466092\n",
      "Iteration 12, loss = 0.00424548\n",
      "Iteration 13, loss = 0.00419078\n",
      "Iteration 14, loss = 0.00457859\n",
      "Iteration 15, loss = 0.00482112\n",
      "Iteration 16, loss = 0.00470005\n",
      "Iteration 17, loss = 0.00456557\n",
      "Iteration 18, loss = 0.00460496\n",
      "Iteration 19, loss = 0.00459344\n",
      "Iteration 20, loss = 0.00438126\n",
      "Iteration 21, loss = 0.00413345\n",
      "Iteration 22, loss = 0.00401294\n",
      "Iteration 23, loss = 0.00398044\n",
      "Iteration 24, loss = 0.00390297\n",
      "Iteration 25, loss = 0.00377377\n",
      "Iteration 26, loss = 0.00369220\n",
      "Iteration 27, loss = 0.00369559\n",
      "Iteration 28, loss = 0.00371020\n",
      "Iteration 29, loss = 0.00367722\n",
      "Iteration 30, loss = 0.00363297\n",
      "Iteration 31, loss = 0.00362959\n",
      "Iteration 32, loss = 0.00364794\n",
      "Iteration 33, loss = 0.00363462\n",
      "Iteration 34, loss = 0.00358621\n",
      "Iteration 35, loss = 0.00354361\n",
      "Iteration 36, loss = 0.00352117\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01838441\n",
      "Iteration 2, loss = 0.01535303\n",
      "Iteration 3, loss = 0.01368061\n",
      "Iteration 4, loss = 0.01042271\n",
      "Iteration 5, loss = 0.00890806\n",
      "Iteration 6, loss = 0.00834111\n",
      "Iteration 7, loss = 0.00697745\n",
      "Iteration 8, loss = 0.00539613\n",
      "Iteration 9, loss = 0.00466883\n",
      "Iteration 10, loss = 0.00463367\n",
      "Iteration 11, loss = 0.00438696\n",
      "Iteration 12, loss = 0.00394296\n",
      "Iteration 13, loss = 0.00396119\n",
      "Iteration 14, loss = 0.00439314\n",
      "Iteration 15, loss = 0.00460396\n",
      "Iteration 16, loss = 0.00449280\n",
      "Iteration 17, loss = 0.00444677\n",
      "Iteration 18, loss = 0.00453655\n",
      "Iteration 19, loss = 0.00448991\n",
      "Iteration 20, loss = 0.00424568\n",
      "Iteration 21, loss = 0.00402221\n",
      "Iteration 22, loss = 0.00393158\n",
      "Iteration 23, loss = 0.00387617\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01836568\n",
      "Iteration 2, loss = 0.01538712\n",
      "Iteration 3, loss = 0.01363206\n",
      "Iteration 4, loss = 0.01039054\n",
      "Iteration 5, loss = 0.00893963\n",
      "Iteration 6, loss = 0.00837751\n",
      "Iteration 7, loss = 0.00700819\n",
      "Iteration 8, loss = 0.00548749\n",
      "Iteration 9, loss = 0.00485730\n",
      "Iteration 10, loss = 0.00487047\n",
      "Iteration 11, loss = 0.00462959\n",
      "Iteration 12, loss = 0.00422057\n",
      "Iteration 13, loss = 0.00428690\n",
      "Iteration 14, loss = 0.00470869\n",
      "Iteration 15, loss = 0.00487354\n",
      "Iteration 16, loss = 0.00473840\n",
      "Iteration 17, loss = 0.00467266\n",
      "Iteration 18, loss = 0.00472792\n",
      "Iteration 19, loss = 0.00465473\n",
      "Iteration 20, loss = 0.00440252\n",
      "Iteration 21, loss = 0.00417829\n",
      "Iteration 22, loss = 0.00408683\n",
      "Iteration 23, loss = 0.00403088\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01807273\n",
      "Iteration 2, loss = 0.01475207\n",
      "Iteration 3, loss = 0.01328022\n",
      "Iteration 4, loss = 0.01001464\n",
      "Iteration 5, loss = 0.00839343\n",
      "Iteration 6, loss = 0.00790249\n",
      "Iteration 7, loss = 0.00668527\n",
      "Iteration 8, loss = 0.00513807\n",
      "Iteration 9, loss = 0.00439118\n",
      "Iteration 10, loss = 0.00441808\n",
      "Iteration 11, loss = 0.00428770\n",
      "Iteration 12, loss = 0.00389288\n",
      "Iteration 13, loss = 0.00387610\n",
      "Iteration 14, loss = 0.00429042\n",
      "Iteration 15, loss = 0.00453321\n",
      "Iteration 16, loss = 0.00441649\n",
      "Iteration 17, loss = 0.00430184\n",
      "Iteration 18, loss = 0.00434119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 0.00431150\n",
      "Iteration 20, loss = 0.00408491\n",
      "Iteration 21, loss = 0.00383200\n",
      "Iteration 22, loss = 0.00371105\n",
      "Iteration 23, loss = 0.00366775\n",
      "Iteration 24, loss = 0.00357671\n",
      "Iteration 25, loss = 0.00344511\n",
      "Iteration 26, loss = 0.00337375\n",
      "Iteration 27, loss = 0.00338622\n",
      "Iteration 28, loss = 0.00340251\n",
      "Iteration 29, loss = 0.00337180\n",
      "Iteration 30, loss = 0.00333875\n",
      "Iteration 31, loss = 0.00334833\n",
      "Iteration 32, loss = 0.00337366\n",
      "Iteration 33, loss = 0.00336314\n",
      "Iteration 34, loss = 0.00332016\n",
      "Iteration 35, loss = 0.00328637\n",
      "Iteration 36, loss = 0.00327047\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01820733\n",
      "Iteration 2, loss = 0.01500116\n",
      "Iteration 3, loss = 0.01355661\n",
      "Iteration 4, loss = 0.01033297\n",
      "Iteration 5, loss = 0.00879151\n",
      "Iteration 6, loss = 0.00833032\n",
      "Iteration 7, loss = 0.00710303\n",
      "Iteration 8, loss = 0.00557203\n",
      "Iteration 9, loss = 0.00485895\n",
      "Iteration 10, loss = 0.00488302\n",
      "Iteration 11, loss = 0.00471853\n",
      "Iteration 12, loss = 0.00429320\n",
      "Iteration 13, loss = 0.00426782\n",
      "Iteration 14, loss = 0.00467216\n",
      "Iteration 15, loss = 0.00489638\n",
      "Iteration 16, loss = 0.00477205\n",
      "Iteration 17, loss = 0.00466413\n",
      "Iteration 18, loss = 0.00471922\n",
      "Iteration 19, loss = 0.00469545\n",
      "Iteration 20, loss = 0.00446546\n",
      "Iteration 21, loss = 0.00422184\n",
      "Iteration 22, loss = 0.00411060\n",
      "Iteration 23, loss = 0.00406869\n",
      "Iteration 24, loss = 0.00397164\n",
      "Iteration 25, loss = 0.00383441\n",
      "Iteration 26, loss = 0.00376038\n",
      "Iteration 27, loss = 0.00376701\n",
      "Iteration 28, loss = 0.00377259\n",
      "Iteration 29, loss = 0.00373289\n",
      "Iteration 30, loss = 0.00369464\n",
      "Iteration 31, loss = 0.00370008\n",
      "Iteration 32, loss = 0.00371886\n",
      "Iteration 33, loss = 0.00370107\n",
      "Iteration 34, loss = 0.00365381\n",
      "Iteration 35, loss = 0.00361842\n",
      "Iteration 36, loss = 0.00359958\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01812709\n",
      "Iteration 2, loss = 0.01488550\n",
      "Iteration 3, loss = 0.01348608\n",
      "Iteration 4, loss = 0.01026807\n",
      "Iteration 5, loss = 0.00871583\n",
      "Iteration 6, loss = 0.00827250\n",
      "Iteration 7, loss = 0.00707200\n",
      "Iteration 8, loss = 0.00554732\n",
      "Iteration 9, loss = 0.00482996\n",
      "Iteration 10, loss = 0.00486025\n",
      "Iteration 11, loss = 0.00471184\n",
      "Iteration 12, loss = 0.00429047\n",
      "Iteration 13, loss = 0.00425503\n",
      "Iteration 14, loss = 0.00465378\n",
      "Iteration 15, loss = 0.00488368\n",
      "Iteration 16, loss = 0.00475930\n",
      "Iteration 17, loss = 0.00464235\n",
      "Iteration 18, loss = 0.00469272\n",
      "Iteration 19, loss = 0.00467229\n",
      "Iteration 20, loss = 0.00444964\n",
      "Iteration 21, loss = 0.00420624\n",
      "Iteration 22, loss = 0.00409294\n",
      "Iteration 23, loss = 0.00405504\n",
      "Iteration 24, loss = 0.00396537\n",
      "Iteration 25, loss = 0.00383187\n",
      "Iteration 26, loss = 0.00375571\n",
      "Iteration 27, loss = 0.00376140\n",
      "Iteration 28, loss = 0.00376976\n",
      "Iteration 29, loss = 0.00373250\n",
      "Iteration 30, loss = 0.00369264\n",
      "Iteration 31, loss = 0.00369544\n",
      "Iteration 32, loss = 0.00371447\n",
      "Iteration 33, loss = 0.00369842\n",
      "Iteration 34, loss = 0.00365118\n",
      "Iteration 35, loss = 0.00361377\n",
      "Iteration 36, loss = 0.00359441\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01814483\n",
      "Iteration 2, loss = 0.01491355\n",
      "Iteration 3, loss = 0.01351868\n",
      "Iteration 4, loss = 0.01030982\n",
      "Iteration 5, loss = 0.00875971\n",
      "Iteration 6, loss = 0.00831396\n",
      "Iteration 7, loss = 0.00710596\n",
      "Iteration 8, loss = 0.00557216\n",
      "Iteration 9, loss = 0.00484406\n",
      "Iteration 10, loss = 0.00487080\n",
      "Iteration 11, loss = 0.00471899\n",
      "Iteration 12, loss = 0.00429769\n",
      "Iteration 13, loss = 0.00426296\n",
      "Iteration 14, loss = 0.00466512\n",
      "Iteration 15, loss = 0.00489714\n",
      "Iteration 16, loss = 0.00477169\n",
      "Iteration 17, loss = 0.00465611\n",
      "Iteration 18, loss = 0.00471111\n",
      "Iteration 19, loss = 0.00469239\n",
      "Iteration 20, loss = 0.00446706\n",
      "Iteration 21, loss = 0.00422196\n",
      "Iteration 22, loss = 0.00410844\n",
      "Iteration 23, loss = 0.00406661\n",
      "Iteration 24, loss = 0.00396980\n",
      "Iteration 25, loss = 0.00383169\n",
      "Iteration 26, loss = 0.00375491\n",
      "Iteration 27, loss = 0.00375966\n",
      "Iteration 28, loss = 0.00376455\n",
      "Iteration 29, loss = 0.00372433\n",
      "Iteration 30, loss = 0.00368602\n",
      "Iteration 31, loss = 0.00369175\n",
      "Iteration 32, loss = 0.00371229\n",
      "Iteration 33, loss = 0.00369718\n",
      "Iteration 34, loss = 0.00365255\n",
      "Iteration 35, loss = 0.00361934\n",
      "Iteration 36, loss = 0.00360320\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01825788\n",
      "Iteration 2, loss = 0.01507850\n",
      "Iteration 3, loss = 0.01360858\n",
      "Iteration 4, loss = 0.01038707\n",
      "Iteration 5, loss = 0.00885022\n",
      "Iteration 6, loss = 0.00837488\n",
      "Iteration 7, loss = 0.00712563\n",
      "Iteration 8, loss = 0.00558291\n",
      "Iteration 9, loss = 0.00486110\n",
      "Iteration 10, loss = 0.00487448\n",
      "Iteration 11, loss = 0.00469711\n",
      "Iteration 12, loss = 0.00427158\n",
      "Iteration 13, loss = 0.00425399\n",
      "Iteration 14, loss = 0.00466322\n",
      "Iteration 15, loss = 0.00488453\n",
      "Iteration 16, loss = 0.00475943\n",
      "Iteration 17, loss = 0.00465929\n",
      "Iteration 18, loss = 0.00471857\n",
      "Iteration 19, loss = 0.00469098\n",
      "Iteration 20, loss = 0.00445709\n",
      "Iteration 21, loss = 0.00421500\n",
      "Iteration 22, loss = 0.00410476\n",
      "Iteration 23, loss = 0.00405676\n",
      "Iteration 24, loss = 0.00395222\n",
      "Iteration 25, loss = 0.00381258\n",
      "Iteration 26, loss = 0.00374140\n",
      "Iteration 27, loss = 0.00374900\n",
      "Iteration 28, loss = 0.00375080\n",
      "Iteration 29, loss = 0.00370905\n",
      "Iteration 30, loss = 0.00367390\n",
      "Iteration 31, loss = 0.00368401\n",
      "Iteration 32, loss = 0.00370451\n",
      "Iteration 33, loss = 0.00368688\n",
      "Iteration 34, loss = 0.00364252\n",
      "Iteration 35, loss = 0.00361226\n",
      "Iteration 36, loss = 0.00359681\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01837069\n",
      "Iteration 2, loss = 0.01527659\n",
      "Iteration 3, loss = 0.01367330\n",
      "Iteration 4, loss = 0.01043258\n",
      "Iteration 5, loss = 0.00891889\n",
      "Iteration 6, loss = 0.00839454\n",
      "Iteration 7, loss = 0.00708649\n",
      "Iteration 8, loss = 0.00554571\n",
      "Iteration 9, loss = 0.00484708\n",
      "Iteration 10, loss = 0.00484218\n",
      "Iteration 11, loss = 0.00462983\n",
      "Iteration 12, loss = 0.00420726\n",
      "Iteration 13, loss = 0.00422284\n",
      "Iteration 14, loss = 0.00463602\n",
      "Iteration 15, loss = 0.00483391\n",
      "Iteration 16, loss = 0.00471102\n",
      "Iteration 17, loss = 0.00463521\n",
      "Iteration 18, loss = 0.00469400\n",
      "Iteration 19, loss = 0.00464521\n",
      "Iteration 20, loss = 0.00439763\n",
      "Iteration 21, loss = 0.00416424\n",
      "Iteration 22, loss = 0.00406400\n",
      "Iteration 23, loss = 0.00401086\n",
      "Iteration 24, loss = 0.00389752\n",
      "Iteration 25, loss = 0.00376458\n",
      "Iteration 26, loss = 0.00370922\n",
      "Iteration 27, loss = 0.00372344\n",
      "Iteration 28, loss = 0.00372098\n",
      "Iteration 29, loss = 0.00367985\n",
      "Iteration 30, loss = 0.00365423\n",
      "Iteration 31, loss = 0.00367187\n",
      "Iteration 32, loss = 0.00369009\n",
      "Iteration 33, loss = 0.00366784\n",
      "Iteration 34, loss = 0.00362613\n",
      "Iteration 35, loss = 0.00360197\n",
      "Iteration 36, loss = 0.00358725\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01838390\n",
      "Iteration 2, loss = 0.01535377\n",
      "Iteration 3, loss = 0.01365006\n",
      "Iteration 4, loss = 0.01040566\n",
      "Iteration 5, loss = 0.00893100\n",
      "Iteration 6, loss = 0.00838595\n",
      "Iteration 7, loss = 0.00704047\n",
      "Iteration 8, loss = 0.00551380\n",
      "Iteration 9, loss = 0.00486579\n",
      "Iteration 10, loss = 0.00488282\n",
      "Iteration 11, loss = 0.00465645\n",
      "Iteration 12, loss = 0.00424403\n",
      "Iteration 13, loss = 0.00429483\n",
      "Iteration 14, loss = 0.00471735\n",
      "Iteration 15, loss = 0.00489281\n",
      "Iteration 16, loss = 0.00475772\n",
      "Iteration 17, loss = 0.00468447\n",
      "Iteration 18, loss = 0.00474274\n",
      "Iteration 19, loss = 0.00467758\n",
      "Iteration 20, loss = 0.00442748\n",
      "Iteration 21, loss = 0.00419794\n",
      "Iteration 22, loss = 0.00410348\n",
      "Iteration 23, loss = 0.00405072\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01799201\n",
      "Iteration 2, loss = 0.01465474\n",
      "Iteration 3, loss = 0.01322747\n",
      "Iteration 4, loss = 0.00997531\n",
      "Iteration 5, loss = 0.00835680\n",
      "Iteration 6, loss = 0.00788708\n",
      "Iteration 7, loss = 0.00669181\n",
      "Iteration 8, loss = 0.00515123\n",
      "Iteration 9, loss = 0.00440452\n",
      "Iteration 10, loss = 0.00445002\n",
      "Iteration 11, loss = 0.00433900\n",
      "Iteration 12, loss = 0.00394573\n",
      "Iteration 13, loss = 0.00391933\n",
      "Iteration 14, loss = 0.00433086\n",
      "Iteration 15, loss = 0.00457672\n",
      "Iteration 16, loss = 0.00445760\n",
      "Iteration 17, loss = 0.00433798\n",
      "Iteration 18, loss = 0.00437838\n",
      "Iteration 19, loss = 0.00435376\n",
      "Iteration 20, loss = 0.00413056\n",
      "Iteration 21, loss = 0.00387650\n",
      "Iteration 22, loss = 0.00375262\n",
      "Iteration 23, loss = 0.00371266\n",
      "Iteration 24, loss = 0.00362551\n",
      "Iteration 25, loss = 0.00349482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26, loss = 0.00342219\n",
      "Iteration 27, loss = 0.00343506\n",
      "Iteration 28, loss = 0.00345345\n",
      "Iteration 29, loss = 0.00342368\n",
      "Iteration 30, loss = 0.00338949\n",
      "Iteration 31, loss = 0.00339811\n",
      "Iteration 32, loss = 0.00342450\n",
      "Iteration 33, loss = 0.00341623\n",
      "Iteration 34, loss = 0.00337399\n",
      "Iteration 35, loss = 0.00333989\n",
      "Iteration 36, loss = 0.00332502\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01806866\n",
      "Iteration 2, loss = 0.01480942\n",
      "Iteration 3, loss = 0.01346315\n",
      "Iteration 4, loss = 0.01027070\n",
      "Iteration 5, loss = 0.00871166\n",
      "Iteration 6, loss = 0.00828323\n",
      "Iteration 7, loss = 0.00709941\n",
      "Iteration 8, loss = 0.00556851\n",
      "Iteration 9, loss = 0.00483201\n",
      "Iteration 10, loss = 0.00486196\n",
      "Iteration 11, loss = 0.00472239\n",
      "Iteration 12, loss = 0.00430104\n",
      "Iteration 13, loss = 0.00425291\n",
      "Iteration 14, loss = 0.00464815\n",
      "Iteration 15, loss = 0.00488731\n",
      "Iteration 16, loss = 0.00476390\n",
      "Iteration 17, loss = 0.00464087\n",
      "Iteration 18, loss = 0.00469454\n",
      "Iteration 19, loss = 0.00468358\n",
      "Iteration 20, loss = 0.00446440\n",
      "Iteration 21, loss = 0.00421765\n",
      "Iteration 22, loss = 0.00410102\n",
      "Iteration 23, loss = 0.00406221\n",
      "Iteration 24, loss = 0.00397021\n",
      "Iteration 25, loss = 0.00383173\n",
      "Iteration 26, loss = 0.00375091\n",
      "Iteration 27, loss = 0.00375397\n",
      "Iteration 28, loss = 0.00376063\n",
      "Iteration 29, loss = 0.00372107\n",
      "Iteration 30, loss = 0.00368043\n",
      "Iteration 31, loss = 0.00368377\n",
      "Iteration 32, loss = 0.00370483\n",
      "Iteration 33, loss = 0.00369151\n",
      "Iteration 34, loss = 0.00364669\n",
      "Iteration 35, loss = 0.00361178\n",
      "Iteration 36, loss = 0.00359542\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01814775\n",
      "Iteration 2, loss = 0.01490595\n",
      "Iteration 3, loss = 0.01353882\n",
      "Iteration 4, loss = 0.01034824\n",
      "Iteration 5, loss = 0.00878529\n",
      "Iteration 6, loss = 0.00834059\n",
      "Iteration 7, loss = 0.00714851\n",
      "Iteration 8, loss = 0.00561465\n",
      "Iteration 9, loss = 0.00486400\n",
      "Iteration 10, loss = 0.00487185\n",
      "Iteration 11, loss = 0.00472015\n",
      "Iteration 12, loss = 0.00429930\n",
      "Iteration 13, loss = 0.00425358\n",
      "Iteration 14, loss = 0.00464400\n",
      "Iteration 15, loss = 0.00487927\n",
      "Iteration 16, loss = 0.00476101\n",
      "Iteration 17, loss = 0.00464542\n",
      "Iteration 18, loss = 0.00469829\n",
      "Iteration 19, loss = 0.00468366\n",
      "Iteration 20, loss = 0.00446240\n",
      "Iteration 21, loss = 0.00422004\n",
      "Iteration 22, loss = 0.00410737\n",
      "Iteration 23, loss = 0.00406772\n",
      "Iteration 24, loss = 0.00397353\n",
      "Iteration 25, loss = 0.00383878\n",
      "Iteration 26, loss = 0.00376419\n",
      "Iteration 27, loss = 0.00376884\n",
      "Iteration 28, loss = 0.00377285\n",
      "Iteration 29, loss = 0.00373206\n",
      "Iteration 30, loss = 0.00369451\n",
      "Iteration 31, loss = 0.00370211\n",
      "Iteration 32, loss = 0.00372279\n",
      "Iteration 33, loss = 0.00370799\n",
      "Iteration 34, loss = 0.00366541\n",
      "Iteration 35, loss = 0.00363537\n",
      "Iteration 36, loss = 0.00362042\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01829686\n",
      "Iteration 2, loss = 0.01515242\n",
      "Iteration 3, loss = 0.01363585\n",
      "Iteration 4, loss = 0.01040545\n",
      "Iteration 5, loss = 0.00888118\n",
      "Iteration 6, loss = 0.00839000\n",
      "Iteration 7, loss = 0.00711739\n",
      "Iteration 8, loss = 0.00557287\n",
      "Iteration 9, loss = 0.00486142\n",
      "Iteration 10, loss = 0.00487200\n",
      "Iteration 11, loss = 0.00468052\n",
      "Iteration 12, loss = 0.00425006\n",
      "Iteration 13, loss = 0.00424110\n",
      "Iteration 14, loss = 0.00465300\n",
      "Iteration 15, loss = 0.00486614\n",
      "Iteration 16, loss = 0.00474143\n",
      "Iteration 17, loss = 0.00465032\n",
      "Iteration 18, loss = 0.00471187\n",
      "Iteration 19, loss = 0.00467884\n",
      "Iteration 20, loss = 0.00444048\n",
      "Iteration 21, loss = 0.00420088\n",
      "Iteration 22, loss = 0.00409428\n",
      "Iteration 23, loss = 0.00404560\n",
      "Iteration 24, loss = 0.00393777\n",
      "Iteration 25, loss = 0.00379748\n",
      "Iteration 26, loss = 0.00372737\n",
      "Iteration 27, loss = 0.00373403\n",
      "Iteration 28, loss = 0.00373337\n",
      "Iteration 29, loss = 0.00369190\n",
      "Iteration 30, loss = 0.00365704\n",
      "Iteration 31, loss = 0.00366624\n",
      "Iteration 32, loss = 0.00368371\n",
      "Iteration 33, loss = 0.00366293\n",
      "Iteration 34, loss = 0.00361645\n",
      "Iteration 35, loss = 0.00358448\n",
      "Iteration 36, loss = 0.00356611\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01835190\n",
      "Iteration 2, loss = 0.01524306\n",
      "Iteration 3, loss = 0.01365155\n",
      "Iteration 4, loss = 0.01041237\n",
      "Iteration 5, loss = 0.00890136\n",
      "Iteration 6, loss = 0.00838878\n",
      "Iteration 7, loss = 0.00709005\n",
      "Iteration 8, loss = 0.00555251\n",
      "Iteration 9, loss = 0.00486542\n",
      "Iteration 10, loss = 0.00487825\n",
      "Iteration 11, loss = 0.00467427\n",
      "Iteration 12, loss = 0.00425045\n",
      "Iteration 13, loss = 0.00426337\n",
      "Iteration 14, loss = 0.00467898\n",
      "Iteration 15, loss = 0.00487897\n",
      "Iteration 16, loss = 0.00475068\n",
      "Iteration 17, loss = 0.00466703\n",
      "Iteration 18, loss = 0.00472767\n",
      "Iteration 19, loss = 0.00468279\n",
      "Iteration 20, loss = 0.00443649\n",
      "Iteration 21, loss = 0.00420007\n",
      "Iteration 22, loss = 0.00409847\n",
      "Iteration 23, loss = 0.00404814\n",
      "Iteration 24, loss = 0.00393703\n",
      "Iteration 25, loss = 0.00379979\n",
      "Iteration 26, loss = 0.00373643\n",
      "Iteration 27, loss = 0.00374553\n",
      "Iteration 28, loss = 0.00374245\n",
      "Iteration 29, loss = 0.00369948\n",
      "Iteration 30, loss = 0.00366710\n",
      "Iteration 31, loss = 0.00367816\n",
      "Iteration 32, loss = 0.00369493\n",
      "Iteration 33, loss = 0.00367256\n",
      "Iteration 34, loss = 0.00362689\n",
      "Iteration 35, loss = 0.00359683\n",
      "Iteration 36, loss = 0.00357824\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01837697\n",
      "Iteration 2, loss = 0.01528999\n",
      "Iteration 3, loss = 0.01365693\n",
      "Iteration 4, loss = 0.01041732\n",
      "Iteration 5, loss = 0.00891334\n",
      "Iteration 6, loss = 0.00838579\n",
      "Iteration 7, loss = 0.00707482\n",
      "Iteration 8, loss = 0.00554576\n",
      "Iteration 9, loss = 0.00486915\n",
      "Iteration 10, loss = 0.00487817\n",
      "Iteration 11, loss = 0.00467052\n",
      "Iteration 12, loss = 0.00425890\n",
      "Iteration 13, loss = 0.00428777\n",
      "Iteration 14, loss = 0.00470153\n",
      "Iteration 15, loss = 0.00489054\n",
      "Iteration 16, loss = 0.00476142\n",
      "Iteration 17, loss = 0.00468096\n",
      "Iteration 18, loss = 0.00473613\n",
      "Iteration 19, loss = 0.00468028\n",
      "Iteration 20, loss = 0.00443033\n",
      "Iteration 21, loss = 0.00419890\n",
      "Iteration 22, loss = 0.00410087\n",
      "Iteration 23, loss = 0.00404890\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01834906\n",
      "Iteration 2, loss = 0.01521779\n",
      "Iteration 3, loss = 0.01360786\n",
      "Iteration 4, loss = 0.01036111\n",
      "Iteration 5, loss = 0.00884220\n",
      "Iteration 6, loss = 0.00832734\n",
      "Iteration 7, loss = 0.00703307\n",
      "Iteration 8, loss = 0.00550370\n",
      "Iteration 9, loss = 0.00482899\n",
      "Iteration 10, loss = 0.00486043\n",
      "Iteration 11, loss = 0.00467380\n",
      "Iteration 12, loss = 0.00426720\n",
      "Iteration 13, loss = 0.00429522\n",
      "Iteration 14, loss = 0.00471639\n",
      "Iteration 15, loss = 0.00491391\n",
      "Iteration 16, loss = 0.00477880\n",
      "Iteration 17, loss = 0.00468721\n",
      "Iteration 18, loss = 0.00473992\n",
      "Iteration 19, loss = 0.00469074\n",
      "Iteration 20, loss = 0.00444927\n",
      "Iteration 21, loss = 0.00420956\n",
      "Iteration 22, loss = 0.00410602\n",
      "Iteration 23, loss = 0.00406034\n",
      "Iteration 24, loss = 0.00395333\n",
      "Iteration 25, loss = 0.00381856\n",
      "Iteration 26, loss = 0.00375741\n",
      "Iteration 27, loss = 0.00377165\n",
      "Iteration 28, loss = 0.00377440\n",
      "Iteration 29, loss = 0.00373252\n",
      "Iteration 30, loss = 0.00370118\n",
      "Iteration 31, loss = 0.00371455\n",
      "Iteration 32, loss = 0.00373390\n",
      "Iteration 33, loss = 0.00371247\n",
      "Iteration 34, loss = 0.00366618\n",
      "Iteration 35, loss = 0.00363542\n",
      "Iteration 36, loss = 0.00361769\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01788723\n",
      "Iteration 2, loss = 0.01451729\n",
      "Iteration 3, loss = 0.01316322\n",
      "Iteration 4, loss = 0.00993489\n",
      "Iteration 5, loss = 0.00832072\n",
      "Iteration 6, loss = 0.00788699\n",
      "Iteration 7, loss = 0.00672949\n",
      "Iteration 8, loss = 0.00521428\n",
      "Iteration 9, loss = 0.00447951\n",
      "Iteration 10, loss = 0.00452969\n",
      "Iteration 11, loss = 0.00443186\n",
      "Iteration 12, loss = 0.00403829\n",
      "Iteration 13, loss = 0.00399964\n",
      "Iteration 14, loss = 0.00440088\n",
      "Iteration 15, loss = 0.00465032\n",
      "Iteration 16, loss = 0.00452558\n",
      "Iteration 17, loss = 0.00438278\n",
      "Iteration 18, loss = 0.00440921\n",
      "Iteration 19, loss = 0.00439751\n",
      "Iteration 20, loss = 0.00419909\n",
      "Iteration 21, loss = 0.00394380\n",
      "Iteration 22, loss = 0.00381173\n",
      "Iteration 23, loss = 0.00378027\n",
      "Iteration 24, loss = 0.00370532\n",
      "Iteration 25, loss = 0.00357403\n",
      "Iteration 26, loss = 0.00349071\n",
      "Iteration 27, loss = 0.00349862\n",
      "Iteration 28, loss = 0.00352175\n",
      "Iteration 29, loss = 0.00349393\n",
      "Iteration 30, loss = 0.00345200\n",
      "Iteration 31, loss = 0.00345283\n",
      "Iteration 32, loss = 0.00347931\n",
      "Iteration 33, loss = 0.00347465\n",
      "Iteration 34, loss = 0.00343084\n",
      "Iteration 35, loss = 0.00339114\n",
      "Iteration 36, loss = 0.00337393\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01812766\n",
      "Iteration 2, loss = 0.01490580\n",
      "Iteration 3, loss = 0.01353832\n",
      "Iteration 4, loss = 0.01034676\n",
      "Iteration 5, loss = 0.00879790\n",
      "Iteration 6, loss = 0.00835688\n",
      "Iteration 7, loss = 0.00715120\n",
      "Iteration 8, loss = 0.00561062\n",
      "Iteration 9, loss = 0.00487218\n",
      "Iteration 10, loss = 0.00488747\n",
      "Iteration 11, loss = 0.00472608\n",
      "Iteration 12, loss = 0.00428763\n",
      "Iteration 13, loss = 0.00423735\n",
      "Iteration 14, loss = 0.00463404\n",
      "Iteration 15, loss = 0.00487068\n",
      "Iteration 16, loss = 0.00475165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.00463869\n",
      "Iteration 18, loss = 0.00469457\n",
      "Iteration 19, loss = 0.00468371\n",
      "Iteration 20, loss = 0.00446522\n",
      "Iteration 21, loss = 0.00422300\n",
      "Iteration 22, loss = 0.00410947\n",
      "Iteration 23, loss = 0.00406955\n",
      "Iteration 24, loss = 0.00397735\n",
      "Iteration 25, loss = 0.00383992\n",
      "Iteration 26, loss = 0.00376073\n",
      "Iteration 27, loss = 0.00376389\n",
      "Iteration 28, loss = 0.00377021\n",
      "Iteration 29, loss = 0.00373091\n",
      "Iteration 30, loss = 0.00369011\n",
      "Iteration 31, loss = 0.00369300\n",
      "Iteration 32, loss = 0.00371304\n",
      "Iteration 33, loss = 0.00369872\n",
      "Iteration 34, loss = 0.00365344\n",
      "Iteration 35, loss = 0.00361803\n",
      "Iteration 36, loss = 0.00360071\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01835518\n",
      "Iteration 2, loss = 0.01525848\n",
      "Iteration 3, loss = 0.01367717\n",
      "Iteration 4, loss = 0.01044029\n",
      "Iteration 5, loss = 0.00892288\n",
      "Iteration 6, loss = 0.00840121\n",
      "Iteration 7, loss = 0.00708794\n",
      "Iteration 8, loss = 0.00552799\n",
      "Iteration 9, loss = 0.00481319\n",
      "Iteration 10, loss = 0.00480471\n",
      "Iteration 11, loss = 0.00458483\n",
      "Iteration 12, loss = 0.00414468\n",
      "Iteration 13, loss = 0.00414666\n",
      "Iteration 14, loss = 0.00456408\n",
      "Iteration 15, loss = 0.00477131\n",
      "Iteration 16, loss = 0.00464989\n",
      "Iteration 17, loss = 0.00457506\n",
      "Iteration 18, loss = 0.00464208\n",
      "Iteration 19, loss = 0.00460150\n",
      "Iteration 20, loss = 0.00435609\n",
      "Iteration 21, loss = 0.00411858\n",
      "Iteration 22, loss = 0.00401412\n",
      "Iteration 23, loss = 0.00395962\n",
      "Iteration 24, loss = 0.00384469\n",
      "Iteration 25, loss = 0.00370562\n",
      "Iteration 26, loss = 0.00364368\n",
      "Iteration 27, loss = 0.00365472\n",
      "Iteration 28, loss = 0.00365194\n",
      "Iteration 29, loss = 0.00360876\n",
      "Iteration 30, loss = 0.00357806\n",
      "Iteration 31, loss = 0.00359257\n",
      "Iteration 32, loss = 0.00361120\n",
      "Iteration 33, loss = 0.00358974\n",
      "Iteration 34, loss = 0.00354609\n",
      "Iteration 35, loss = 0.00351917\n",
      "Iteration 36, loss = 0.00350350\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01642290\n",
      "Iteration 2, loss = 0.01276114\n",
      "Iteration 3, loss = 0.01171286\n",
      "Iteration 4, loss = 0.00851720\n",
      "Iteration 5, loss = 0.00677997\n",
      "Iteration 6, loss = 0.00642925\n",
      "Iteration 7, loss = 0.00547825\n",
      "Iteration 8, loss = 0.00399222\n",
      "Iteration 9, loss = 0.00318039\n",
      "Iteration 10, loss = 0.00325361\n",
      "Iteration 11, loss = 0.00328178\n",
      "Iteration 12, loss = 0.00291837\n",
      "Iteration 13, loss = 0.00278172\n",
      "Iteration 14, loss = 0.00313634\n",
      "Iteration 15, loss = 0.00345788\n",
      "Iteration 16, loss = 0.00337980\n",
      "Iteration 17, loss = 0.00319666\n",
      "Iteration 18, loss = 0.00319792\n",
      "Iteration 19, loss = 0.00322563\n",
      "Iteration 20, loss = 0.00305966\n",
      "Iteration 21, loss = 0.00279325\n",
      "Iteration 22, loss = 0.00262958\n",
      "Iteration 23, loss = 0.00259472\n",
      "Iteration 24, loss = 0.00255507\n",
      "Iteration 25, loss = 0.00244503\n",
      "Iteration 26, loss = 0.00234966\n",
      "Iteration 27, loss = 0.00234472\n",
      "Iteration 28, loss = 0.00238451\n",
      "Iteration 29, loss = 0.00238412\n",
      "Iteration 30, loss = 0.00234481\n",
      "Iteration 31, loss = 0.00233333\n",
      "Iteration 32, loss = 0.00236253\n",
      "Iteration 33, loss = 0.00238031\n",
      "Iteration 34, loss = 0.00235422\n",
      "Iteration 35, loss = 0.00231427\n",
      "Iteration 36, loss = 0.00229618\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01805911\n",
      "Iteration 2, loss = 0.01488540\n",
      "Iteration 3, loss = 0.01365830\n",
      "Iteration 4, loss = 0.01053200\n",
      "Iteration 5, loss = 0.00898665\n",
      "Iteration 6, loss = 0.00855139\n",
      "Iteration 7, loss = 0.00736658\n",
      "Iteration 8, loss = 0.00579028\n",
      "Iteration 9, loss = 0.00493849\n",
      "Iteration 10, loss = 0.00483783\n",
      "Iteration 11, loss = 0.00457463\n",
      "Iteration 12, loss = 0.00397174\n",
      "Iteration 13, loss = 0.00368887\n",
      "Iteration 14, loss = 0.00393811\n",
      "Iteration 15, loss = 0.00420188\n",
      "Iteration 16, loss = 0.00415005\n",
      "Iteration 17, loss = 0.00408682\n",
      "Iteration 18, loss = 0.00423316\n",
      "Iteration 19, loss = 0.00435297\n",
      "Iteration 20, loss = 0.00425670\n",
      "Iteration 21, loss = 0.00407040\n",
      "Iteration 22, loss = 0.00399013\n",
      "Iteration 23, loss = 0.00397884\n",
      "Iteration 24, loss = 0.00389523\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Config of the regressors and cross val cross val leave one out\n",
    "####\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes = (50,), alpha = 0.001,\n",
    "    learning_rate_init = 0.01, max_iter = 1000,\n",
    "    random_state = 9, tol = 0.0001, verbose = True)\n",
    "svr = SVR(kernel = 'linear', C = 0.25, epsilon = 0.01, verbose = True, max_iter = 1000)\n",
    "lr = LinearRegression()\n",
    "\n",
    "full_predict_lr = cross_val_predict(lr, X, target, cv = 10)\n",
    "full_predict_mlp = cross_val_predict(mlp, X, target, cv = loo)\n",
    "full_predict_svr = cross_val_predict(svr, X, target, cv = loo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error in LR: 0.009448053373454285\n",
      "Mean Squared Error in MLP: 0.009074726708389266\n",
      "Mean Squared Error in SVR: 0.009087871364951056\n",
      "R² score in LR: 0.7777126920023809\n",
      "R² score in MLP: 0.7864960652646651\n",
      "R² score in SVR: 0.7861868068167944\n",
      "adjusted R² score in LR: 0.7737784033652549\n",
      "adjusted R² score in MLP: 0.7827172345613849\n",
      "adjusted R² score in SVR: 0.7824025025126669\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Printing some metrics of the regressors\n",
    "####\n",
    "\n",
    "print('Mean Squared Error in LR: %s' %(metrics.mean_squared_error(target, full_predict_lr)))\n",
    "print('Mean Squared Error in MLP: %s' %(metrics.mean_squared_error(target, full_predict_mlp)))\n",
    "print('Mean Squared Error in SVR: %s' %(metrics.mean_squared_error(target, full_predict_svr)))\n",
    "\n",
    "r_squared_lr = metrics.r2_score(target, full_predict_lr)\n",
    "r_squared_mlp = metrics.r2_score(target, full_predict_mlp)\n",
    "r_squared_svr = metrics.r2_score(target, full_predict_svr)\n",
    "\n",
    "print('R² score in LR: %s' %(r_squared_lr))\n",
    "print('R² score in MLP: %s' %(r_squared_mlp))\n",
    "print('R² score in SVR: %s' %(r_squared_svr))\n",
    "\n",
    "adjusted_r_squared_lr = 1 - (1 - r_squared_lr) * (len(target) - 1) / (len(target) - X.shape[1] - 1)\n",
    "adjusted_r_squared_mlp = 1 - (1 - r_squared_mlp) * (len(target) - 1) / (len(target) - X.shape[1] - 1)\n",
    "adjusted_r_squared_svr = 1 - (1 - r_squared_svr) * (len(target) - 1) / (len(target) - X.shape[1] - 1)\n",
    "\n",
    "print('adjusted R² score in LR: %s' %(adjusted_r_squared_lr))\n",
    "print('adjusted R² score in MLP: %s' %(adjusted_r_squared_mlp))\n",
    "print('adjusted R² score in SVR: %s' %(adjusted_r_squared_svr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "## Filling lists with NaN so the len is the same across all lists \n",
    "## so that a graph can be generated\n",
    "####\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "values_to_add = list()\n",
    "for i in range(0, window_size):\n",
    "    values_to_add.append(float('NaN'))\n",
    "    \n",
    "full_predict_svr = np.insert(full_predict_svr, 0, values_to_add)\n",
    "full_predict_svr.shape = (len(full_predict_svr), 1)\n",
    "    \n",
    "full_predict_mlp = np.insert(full_predict_mlp, 0, values_to_add)\n",
    "full_predict_mlp.shape = (len(full_predict_mlp), 1)\n",
    "\n",
    "full_predict_lr = np.insert(full_predict_lr, 0, values_to_add)\n",
    "full_predict_lr.shape = (len(full_predict_lr), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Casos</th>\n",
       "      <th>Taxa</th>\n",
       "      <th>CasosNormalizados</th>\n",
       "      <th>TaxaNormalizadas</th>\n",
       "      <th>Predict_lr</th>\n",
       "      <th>Predict_mlp</th>\n",
       "      <th>Predict_svr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26/2/20</th>\n",
       "      <td>1</td>\n",
       "      <td>24.7</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>27.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074667</td>\n",
       "      <td>0.100525</td>\n",
       "      <td>0.029645</td>\n",
       "      <td>0.010405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050667</td>\n",
       "      <td>0.090213</td>\n",
       "      <td>0.019866</td>\n",
       "      <td>0.010233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>31.4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178667</td>\n",
       "      <td>0.093523</td>\n",
       "      <td>0.022619</td>\n",
       "      <td>0.010268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/3/20</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.461333</td>\n",
       "      <td>0.075872</td>\n",
       "      <td>0.017681</td>\n",
       "      <td>0.010187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17/6/20</th>\n",
       "      <td>34918</td>\n",
       "      <td>37.3</td>\n",
       "      <td>0.637527</td>\n",
       "      <td>0.336000</td>\n",
       "      <td>0.376114</td>\n",
       "      <td>0.365810</td>\n",
       "      <td>0.340494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18/6/20</th>\n",
       "      <td>32188</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0.587683</td>\n",
       "      <td>0.368000</td>\n",
       "      <td>0.627571</td>\n",
       "      <td>0.570731</td>\n",
       "      <td>0.566618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19/6/20</th>\n",
       "      <td>22765</td>\n",
       "      <td>34.7</td>\n",
       "      <td>0.415640</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.578492</td>\n",
       "      <td>0.540920</td>\n",
       "      <td>0.533563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20/6/20</th>\n",
       "      <td>54771</td>\n",
       "      <td>39.1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>0.418148</td>\n",
       "      <td>0.397230</td>\n",
       "      <td>0.372794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21/6/20</th>\n",
       "      <td>34666</td>\n",
       "      <td>47.3</td>\n",
       "      <td>0.632926</td>\n",
       "      <td>0.602667</td>\n",
       "      <td>0.972473</td>\n",
       "      <td>0.949963</td>\n",
       "      <td>0.901518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Casos  Taxa  CasosNormalizados  TaxaNormalizadas  Predict_lr  \\\n",
       "Data                                                                    \n",
       "26/2/20      1  24.7           0.000018          0.000000         NaN   \n",
       "27/2/20      0  27.5           0.000000          0.074667    0.100525   \n",
       "28/2/20      0  26.6           0.000000          0.050667    0.090213   \n",
       "29/2/20      0  31.4           0.000000          0.178667    0.093523   \n",
       "1/3/20       1    42           0.000018          0.461333    0.075872   \n",
       "...        ...   ...                ...               ...         ...   \n",
       "17/6/20  34918  37.3           0.637527          0.336000    0.376114   \n",
       "18/6/20  32188  38.5           0.587683          0.368000    0.627571   \n",
       "19/6/20  22765  34.7           0.415640          0.266667    0.578492   \n",
       "20/6/20  54771  39.1           1.000000          0.384000    0.418148   \n",
       "21/6/20  34666  47.3           0.632926          0.602667    0.972473   \n",
       "\n",
       "         Predict_mlp  Predict_svr  \n",
       "Data                               \n",
       "26/2/20          NaN          NaN  \n",
       "27/2/20     0.029645     0.010405  \n",
       "28/2/20     0.019866     0.010233  \n",
       "29/2/20     0.022619     0.010268  \n",
       "1/3/20      0.017681     0.010187  \n",
       "...              ...          ...  \n",
       "17/6/20     0.365810     0.340494  \n",
       "18/6/20     0.570731     0.566618  \n",
       "19/6/20     0.540920     0.533563  \n",
       "20/6/20     0.397230     0.372794  \n",
       "21/6/20     0.949963     0.901518  \n",
       "\n",
       "[117 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####\n",
    "## Adding the data to plot \n",
    "####\n",
    "\n",
    "data['Predict_lr'] = full_predict_lr\n",
    "data['Predict_mlp'] = full_predict_mlp\n",
    "data['Predict_svr'] = full_predict_svr\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "####\n",
    "## Ploting the graph\n",
    "####\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "data.plot(y=['CasosNormalizados', 'Predict_mlp', 'Predict_svr', 'Predict_lr', 'TaxaNormalizadas'], style=['-s', '--o'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
