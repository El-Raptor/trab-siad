{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "## Celso Antonio Uliana Junior\n",
    "## July 2 2020\n",
    "####\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#####\n",
    "## Consuming and shaping the data to analysis\n",
    "## Covid-19 numbers in Brazil by date\n",
    "## Isolation percentage in Brazil by date\n",
    "#####\n",
    "\n",
    "data_raw_covid = pd.read_csv(\"C:/Users/PCDOMILHAO/Documents/GitHub/trab-siad/scripts/Python/Jupyter/dados/covidBrasil.csv\", sep = \";\", decimal = \",\")\n",
    "data_covid = data_raw_covid['Data'].values.copy()\n",
    "data_covid = data_raw_covid.dropna().set_index(\"Data\")\n",
    "\n",
    "####\n",
    "## Shaping a central pandas dataFrame for all our ML needs\n",
    "####\n",
    "\n",
    "data2 = data_covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Casos</th>\n",
       "      <th>CasosNormalizados</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26/2/20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/3/20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/3/20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6/3/20</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7/3/20</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9/3/20</th>\n",
       "      <td>12</td>\n",
       "      <td>0.000219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/3/20</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/3/20</th>\n",
       "      <td>18</td>\n",
       "      <td>0.000329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13/3/20</th>\n",
       "      <td>25</td>\n",
       "      <td>0.000456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14/3/20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.000383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15/3/20</th>\n",
       "      <td>23</td>\n",
       "      <td>0.000420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16/3/20</th>\n",
       "      <td>79</td>\n",
       "      <td>0.001442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Casos  CasosNormalizados\n",
       "Data                             \n",
       "26/2/20      1           0.000018\n",
       "27/2/20      0           0.000000\n",
       "28/2/20      0           0.000000\n",
       "29/2/20      0           0.000000\n",
       "1/3/20       1           0.000018\n",
       "2/3/20       0           0.000000\n",
       "3/3/20       0           0.000000\n",
       "4/3/20       0           0.000000\n",
       "5/3/20       1           0.000018\n",
       "6/3/20       5           0.000091\n",
       "7/3/20       5           0.000091\n",
       "8/3/20       0           0.000000\n",
       "9/3/20      12           0.000219\n",
       "10/3/20      0           0.000000\n",
       "11/3/20      9           0.000164\n",
       "12/3/20     18           0.000329\n",
       "13/3/20     25           0.000456\n",
       "14/3/20     21           0.000383\n",
       "15/3/20     23           0.000420\n",
       "16/3/20     79           0.001442"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "####\n",
    "## normalizing values for both covid and isolation percentage \n",
    "## between range [0,1] using sklearn MinMaxScaler\n",
    "####\n",
    "\n",
    "covid_norm = data_covid[\"Casos\"].values.copy()\n",
    "covid_norm.shape = (len(covid_norm), 1)\n",
    "\n",
    "####\n",
    "## Shaping the central dataFrame with normalized values\n",
    "####\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "covid_norm = min_max_scaler.fit_transform(covid_norm)\n",
    "\n",
    "data2[\"CasosNormalizados\"] = covid_norm\n",
    "data = data2.copy()\n",
    "#data = data.iloc[20:]\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               E0        E1        E2        E3        E4        E5        E6  \\\n",
      "Data                                                                            \n",
      "26/2/20  0.000018  0.000000  0.000000  0.000000  0.000018  0.000000  0.000000   \n",
      "27/2/20  0.000000  0.000000  0.000000  0.000018  0.000000  0.000000  0.000000   \n",
      "28/2/20  0.000000  0.000000  0.000018  0.000000  0.000000  0.000000  0.000018   \n",
      "29/2/20  0.000000  0.000018  0.000000  0.000000  0.000000  0.000018  0.000091   \n",
      "1/3/20   0.000018  0.000000  0.000000  0.000000  0.000018  0.000091  0.000091   \n",
      "...           ...       ...       ...       ...       ...       ...       ...   \n",
      "10/6/20  0.585912  0.600920  0.555257  0.474375  0.396268  0.312392  0.376970   \n",
      "11/6/20  0.600920  0.555257  0.474375  0.396268  0.312392  0.376970  0.637527   \n",
      "12/6/20  0.555257  0.474375  0.396268  0.312392  0.376970  0.637527  0.587683   \n",
      "13/6/20  0.474375  0.396268  0.312392  0.376970  0.637527  0.587683  0.415640   \n",
      "14/6/20  0.396268  0.312392  0.376970  0.637527  0.587683  0.415640  1.000000   \n",
      "\n",
      "               E7  \n",
      "Data               \n",
      "26/2/20  0.000000  \n",
      "27/2/20  0.000018  \n",
      "28/2/20  0.000091  \n",
      "29/2/20  0.000091  \n",
      "1/3/20   0.000000  \n",
      "...           ...  \n",
      "10/6/20  0.637527  \n",
      "11/6/20  0.587683  \n",
      "12/6/20  0.415640  \n",
      "13/6/20  1.000000  \n",
      "14/6/20  0.632926  \n",
      "\n",
      "[110 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Sliding window\n",
    "####\n",
    "df = pd.DataFrame()\n",
    "window_size = 7\n",
    "for i in range(0, window_size + 1):\n",
    "    df['E{}'.format(i)] = data['CasosNormalizados'].shift(-i)\n",
    "df = df.iloc[: -window_size]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.82578372e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.82578372e-05 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.82578372e-05\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.82578372e-05 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.82578372e-05]\n",
      " [0.00000000e+00 1.82578372e-05 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.82578372e-05 9.12891859e-05]\n",
      " [1.82578372e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.82578372e-05 9.12891859e-05 9.12891859e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.82578372e-05\n",
      "  9.12891859e-05 9.12891859e-05 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.82578372e-05 9.12891859e-05\n",
      "  9.12891859e-05 0.00000000e+00 2.19094046e-04]\n",
      " [0.00000000e+00 1.82578372e-05 9.12891859e-05 9.12891859e-05\n",
      "  0.00000000e+00 2.19094046e-04 0.00000000e+00]\n",
      " [1.82578372e-05 9.12891859e-05 9.12891859e-05 0.00000000e+00\n",
      "  2.19094046e-04 0.00000000e+00 1.64320535e-04]\n",
      " [9.12891859e-05 9.12891859e-05 0.00000000e+00 2.19094046e-04\n",
      "  0.00000000e+00 1.64320535e-04 3.28641069e-04]\n",
      " [9.12891859e-05 0.00000000e+00 2.19094046e-04 0.00000000e+00\n",
      "  1.64320535e-04 3.28641069e-04 4.56445929e-04]\n",
      " [0.00000000e+00 2.19094046e-04 0.00000000e+00 1.64320535e-04\n",
      "  3.28641069e-04 4.56445929e-04 3.83414581e-04]\n",
      " [2.19094046e-04 0.00000000e+00 1.64320535e-04 3.28641069e-04\n",
      "  4.56445929e-04 3.83414581e-04 4.19930255e-04]\n",
      " [0.00000000e+00 1.64320535e-04 3.28641069e-04 4.56445929e-04\n",
      "  3.83414581e-04 4.19930255e-04 1.44236914e-03]\n",
      " [1.64320535e-04 3.28641069e-04 4.56445929e-04 3.83414581e-04\n",
      "  4.19930255e-04 1.44236914e-03 6.20766464e-04]\n",
      " [3.28641069e-04 4.56445929e-04 3.83414581e-04 4.19930255e-04\n",
      "  1.44236914e-03 6.20766464e-04 1.04069672e-03]\n",
      " [4.56445929e-04 3.83414581e-04 4.19930255e-04 1.44236914e-03\n",
      "  6.20766464e-04 1.04069672e-03 2.50132369e-03]\n",
      " [3.83414581e-04 4.19930255e-04 1.44236914e-03 6.20766464e-04\n",
      "  1.04069672e-03 2.50132369e-03 3.52376258e-03]\n",
      " [4.19930255e-04 1.44236914e-03 6.20766464e-04 1.04069672e-03\n",
      "  2.50132369e-03 3.52376258e-03 5.16696792e-03]\n",
      " [1.44236914e-03 6.20766464e-04 1.04069672e-03 2.50132369e-03\n",
      "  3.52376258e-03 5.16696792e-03 4.08975553e-03]\n",
      " [6.20766464e-04 1.04069672e-03 2.50132369e-03 3.52376258e-03\n",
      "  5.16696792e-03 4.08975553e-03 7.63177594e-03]\n",
      " [1.04069672e-03 2.50132369e-03 3.52376258e-03 5.16696792e-03\n",
      "  4.08975553e-03 7.63177594e-03 6.29895383e-03]\n",
      " [2.50132369e-03 3.52376258e-03 5.16696792e-03 4.08975553e-03\n",
      "  7.63177594e-03 6.29895383e-03 5.65992952e-03]\n",
      " [3.52376258e-03 5.16696792e-03 4.08975553e-03 7.63177594e-03\n",
      "  6.29895383e-03 5.65992952e-03 4.23581822e-03]\n",
      " [5.16696792e-03 4.08975553e-03 7.63177594e-03 6.29895383e-03\n",
      "  5.65992952e-03 4.23581822e-03 8.80027752e-03]\n",
      " [4.08975553e-03 7.63177594e-03 6.29895383e-03 5.65992952e-03\n",
      "  4.23581822e-03 8.80027752e-03 9.16543426e-03]\n",
      " [7.63177594e-03 6.29895383e-03 5.65992952e-03 4.23581822e-03\n",
      "  8.80027752e-03 9.16543426e-03 8.89156671e-03]\n",
      " [6.29895383e-03 5.65992952e-03 4.23581822e-03 8.80027752e-03\n",
      "  9.16543426e-03 8.89156671e-03 6.42675869e-03]\n",
      " [5.65992952e-03 4.23581822e-03 8.80027752e-03 9.16543426e-03\n",
      "  8.89156671e-03 6.42675869e-03 5.89728141e-03]\n",
      " [4.23581822e-03 8.80027752e-03 9.16543426e-03 8.89156671e-03\n",
      "  6.42675869e-03 5.89728141e-03 2.07774187e-02]\n",
      " [8.80027752e-03 9.16543426e-03 8.89156671e-03 6.42675869e-03\n",
      "  5.89728141e-03 2.07774187e-02 2.04305198e-02]\n",
      " [9.16543426e-03 8.89156671e-03 6.42675869e-03 5.89728141e-03\n",
      "  2.07774187e-02 2.04305198e-02 1.96089171e-02]\n",
      " [8.89156671e-03 6.42675869e-03 5.89728141e-03 2.07774187e-02\n",
      "  2.04305198e-02 1.96089171e-02 2.09234814e-02]\n",
      " [6.42675869e-03 5.89728141e-03 2.07774187e-02 2.04305198e-02\n",
      "  1.96089171e-02 2.09234814e-02 2.23110770e-02]\n",
      " [5.89728141e-03 2.07774187e-02 2.04305198e-02 1.96089171e-02\n",
      "  2.09234814e-02 2.23110770e-02 1.55556773e-02]\n",
      " [2.07774187e-02 2.04305198e-02 1.96089171e-02 2.09234814e-02\n",
      "  2.23110770e-02 1.55556773e-02 1.69067572e-02]\n",
      " [2.04305198e-02 1.96089171e-02 2.09234814e-02 2.23110770e-02\n",
      "  1.55556773e-02 1.69067572e-02 3.03262676e-02]\n",
      " [1.96089171e-02 2.09234814e-02 2.23110770e-02 1.55556773e-02\n",
      "  1.69067572e-02 3.03262676e-02 4.03498202e-02]\n",
      " [2.09234814e-02 2.23110770e-02 1.55556773e-02 1.69067572e-02\n",
      "  3.03262676e-02 4.03498202e-02 3.52376258e-02]\n",
      " [2.23110770e-02 1.55556773e-02 1.69067572e-02 3.03262676e-02\n",
      "  4.03498202e-02 3.52376258e-02 3.25172080e-02]\n",
      " [1.55556773e-02 1.69067572e-02 3.03262676e-02 4.03498202e-02\n",
      "  3.52376258e-02 3.25172080e-02 1.98827847e-02]\n",
      " [1.69067572e-02 3.03262676e-02 4.03498202e-02 3.52376258e-02\n",
      "  3.25172080e-02 1.98827847e-02 2.63278012e-02]\n",
      " [3.03262676e-02 4.03498202e-02 3.52376258e-02 3.25172080e-02\n",
      "  1.98827847e-02 2.63278012e-02 2.30231327e-02]\n",
      " [4.03498202e-02 3.52376258e-02 3.25172080e-02 1.98827847e-02\n",
      "  2.63278012e-02 2.30231327e-02 3.34483577e-02]\n",
      " [3.52376258e-02 3.25172080e-02 1.98827847e-02 2.63278012e-02\n",
      "  2.30231327e-02 3.34483577e-02 5.58324661e-02]\n",
      " [3.25172080e-02 1.98827847e-02 2.63278012e-02 2.30231327e-02\n",
      "  3.34483577e-02 5.58324661e-02 3.84327473e-02]\n",
      " [1.98827847e-02 2.63278012e-02 2.30231327e-02 3.34483577e-02\n",
      "  5.58324661e-02 3.84327473e-02 5.94657757e-02]\n",
      " [2.63278012e-02 2.30231327e-02 3.34483577e-02 5.58324661e-02\n",
      "  3.84327473e-02 5.94657757e-02 5.32581110e-02]\n",
      " [2.30231327e-02 3.34483577e-02 5.58324661e-02 3.84327473e-02\n",
      "  5.94657757e-02 5.32581110e-02 3.75198554e-02]\n",
      " [3.34483577e-02 5.58324661e-02 3.84327473e-02 5.94657757e-02\n",
      "  5.32581110e-02 3.75198554e-02 3.51828522e-02]\n",
      " [5.58324661e-02 3.84327473e-02 5.94657757e-02 5.32581110e-02\n",
      "  3.75198554e-02 3.51828522e-02 4.56080773e-02]\n",
      " [3.84327473e-02 5.94657757e-02 5.32581110e-02 3.75198554e-02\n",
      "  3.51828522e-02 4.56080773e-02 4.88944880e-02]\n",
      " [5.94657757e-02 5.32581110e-02 3.75198554e-02 3.51828522e-02\n",
      "  4.56080773e-02 4.88944880e-02 6.81930219e-02]\n",
      " [5.32581110e-02 3.75198554e-02 3.51828522e-02 4.56080773e-02\n",
      "  4.88944880e-02 6.81930219e-02 6.39572036e-02]\n",
      " [3.75198554e-02 3.51828522e-02 4.56080773e-02 4.88944880e-02\n",
      "  6.81930219e-02 6.39572036e-02 1.00673714e-01]\n",
      " [3.51828522e-02 4.56080773e-02 4.88944880e-02 6.81930219e-02\n",
      "  6.39572036e-02 1.00673714e-01 6.16932318e-02]\n",
      " [4.56080773e-02 4.88944880e-02 6.81930219e-02 6.39572036e-02\n",
      "  1.00673714e-01 6.16932318e-02 8.42234029e-02]\n",
      " [4.88944880e-02 6.81930219e-02 6.39572036e-02 1.00673714e-01\n",
      "  6.16932318e-02 8.42234029e-02 9.83184532e-02]\n",
      " [6.81930219e-02 6.39572036e-02 1.00673714e-01 6.16932318e-02\n",
      "  8.42234029e-02 9.83184532e-02 1.14586186e-01]\n",
      " [6.39572036e-02 1.00673714e-01 6.16932318e-02 8.42234029e-02\n",
      "  9.83184532e-02 1.14586186e-01 1.31785069e-01]\n",
      " [1.00673714e-01 6.16932318e-02 8.42234029e-02 9.83184532e-02\n",
      "  1.14586186e-01 1.31785069e-01 1.13362911e-01]\n",
      " [6.16932318e-02 8.42234029e-02 9.83184532e-02 1.14586186e-01\n",
      "  1.31785069e-01 1.13362911e-01 9.07414508e-02]\n",
      " [8.42234029e-02 9.83184532e-02 1.14586186e-01 1.31785069e-01\n",
      "  1.13362911e-01 9.07414508e-02 8.37669570e-02]\n",
      " [9.83184532e-02 1.14586186e-01 1.31785069e-01 1.13362911e-01\n",
      "  9.07414508e-02 8.37669570e-02 1.21104234e-01]\n",
      " [1.14586186e-01 1.31785069e-01 1.13362911e-01 9.07414508e-02\n",
      "  8.37669570e-02 1.21104234e-01 1.26618101e-01]\n",
      " [1.31785069e-01 1.13362911e-01 9.07414508e-02 8.37669570e-02\n",
      "  1.21104234e-01 1.26618101e-01 1.91762064e-01]\n",
      " [1.13362911e-01 9.07414508e-02 8.37669570e-02 1.21104234e-01\n",
      "  1.26618101e-01 1.91762064e-01 1.80533494e-01]\n",
      " [9.07414508e-02 8.37669570e-02 1.21104234e-01 1.26618101e-01\n",
      "  1.91762064e-01 1.80533494e-01 1.86631612e-01]\n",
      " [8.37669570e-02 1.21104234e-01 1.26618101e-01 1.91762064e-01\n",
      "  1.80533494e-01 1.86631612e-01 1.93733910e-01]\n",
      " [1.21104234e-01 1.26618101e-01 1.91762064e-01 1.80533494e-01\n",
      "  1.86631612e-01 1.93733910e-01 1.23422979e-01]\n",
      " [1.26618101e-01 1.91762064e-01 1.80533494e-01 1.86631612e-01\n",
      "  1.93733910e-01 1.23422979e-01 1.02828139e-01]\n",
      " [1.91762064e-01 1.80533494e-01 1.86631612e-01 1.93733910e-01\n",
      "  1.23422979e-01 1.02828139e-01 1.69031057e-01]\n",
      " [1.80533494e-01 1.86631612e-01 1.93733910e-01 1.23422979e-01\n",
      "  1.02828139e-01 1.69031057e-01 2.07865476e-01]\n",
      " [1.86631612e-01 1.93733910e-01 1.23422979e-01 1.02828139e-01\n",
      "  1.69031057e-01 2.07865476e-01 2.54587282e-01]\n",
      " [1.93733910e-01 1.23422979e-01 1.02828139e-01 1.69031057e-01\n",
      "  2.07865476e-01 2.54587282e-01 2.79436198e-01]\n",
      " [1.23422979e-01 1.02828139e-01 1.69031057e-01 2.07865476e-01\n",
      "  2.54587282e-01 2.79436198e-01 2.72388673e-01]\n",
      " [1.02828139e-01 1.69031057e-01 2.07865476e-01 2.54587282e-01\n",
      "  2.79436198e-01 2.72388673e-01 1.44930712e-01]\n",
      " [1.69031057e-01 2.07865476e-01 2.54587282e-01 2.79436198e-01\n",
      "  2.72388673e-01 1.44930712e-01 2.39907981e-01]\n",
      " [2.07865476e-01 2.54587282e-01 2.79436198e-01 2.72388673e-01\n",
      "  1.44930712e-01 2.39907981e-01 3.17832430e-01]\n",
      " [2.54587282e-01 2.79436198e-01 2.72388673e-01 1.44930712e-01\n",
      "  2.39907981e-01 3.17832430e-01 3.64262110e-01]\n",
      " [2.79436198e-01 2.72388673e-01 1.44930712e-01 2.39907981e-01\n",
      "  3.17832430e-01 3.64262110e-01 3.37916050e-01]\n",
      " [2.72388673e-01 1.44930712e-01 2.39907981e-01 3.17832430e-01\n",
      "  3.64262110e-01 3.37916050e-01 3.79817787e-01]\n",
      " [1.44930712e-01 2.39907981e-01 3.17832430e-01 3.64262110e-01\n",
      "  3.37916050e-01 3.79817787e-01 3.01400376e-01]\n",
      " [2.39907981e-01 3.17832430e-01 3.64262110e-01 3.37916050e-01\n",
      "  3.79817787e-01 3.01400376e-01 2.88711179e-01]\n",
      " [3.17832430e-01 3.64262110e-01 3.37916050e-01 3.79817787e-01\n",
      "  3.01400376e-01 2.88711179e-01 2.13379343e-01]\n",
      " [3.64262110e-01 3.37916050e-01 3.79817787e-01 3.01400376e-01\n",
      "  2.88711179e-01 2.13379343e-01 2.98040934e-01]\n",
      " [3.37916050e-01 3.79817787e-01 3.01400376e-01 2.88711179e-01\n",
      "  2.13379343e-01 2.98040934e-01 3.76093188e-01]\n",
      " [3.79817787e-01 3.01400376e-01 2.88711179e-01 2.13379343e-01\n",
      "  2.98040934e-01 3.76093188e-01 4.82317285e-01]\n",
      " [3.01400376e-01 2.88711179e-01 2.13379343e-01 2.98040934e-01\n",
      "  3.76093188e-01 4.82317285e-01 4.91647039e-01]\n",
      " [2.88711179e-01 2.13379343e-01 2.98040934e-01 3.76093188e-01\n",
      "  4.82317285e-01 4.91647039e-01 6.07511274e-01]\n",
      " [2.13379343e-01 2.98040934e-01 3.76093188e-01 4.82317285e-01\n",
      "  4.91647039e-01 6.07511274e-01 2.99592850e-01]\n",
      " [2.98040934e-01 3.76093188e-01 4.82317285e-01 4.91647039e-01\n",
      "  6.07511274e-01 2.99592850e-01 2.11754396e-01]\n",
      " [3.76093188e-01 4.82317285e-01 4.91647039e-01 6.07511274e-01\n",
      "  2.99592850e-01 2.11754396e-01 5.28308777e-01]\n",
      " [4.82317285e-01 4.91647039e-01 6.07511274e-01 2.99592850e-01\n",
      "  2.11754396e-01 5.28308777e-01 5.22776652e-01]\n",
      " [4.91647039e-01 6.07511274e-01 2.99592850e-01 2.11754396e-01\n",
      "  5.28308777e-01 5.22776652e-01 5.64459294e-01]\n",
      " [6.07511274e-01 2.99592850e-01 2.11754396e-01 5.28308777e-01\n",
      "  5.22776652e-01 5.64459294e-01 5.62889120e-01]\n",
      " [2.99592850e-01 2.11754396e-01 5.28308777e-01 5.22776652e-01\n",
      "  5.64459294e-01 5.62889120e-01 4.94330942e-01]\n",
      " [2.11754396e-01 5.28308777e-01 5.22776652e-01 5.64459294e-01\n",
      "  5.62889120e-01 4.94330942e-01 3.45456537e-01]\n",
      " [5.28308777e-01 5.22776652e-01 5.64459294e-01 5.62889120e-01\n",
      "  4.94330942e-01 3.45456537e-01 2.85808183e-01]\n",
      " [5.22776652e-01 5.64459294e-01 5.62889120e-01 4.94330942e-01\n",
      "  3.45456537e-01 2.85808183e-01 5.85912253e-01]\n",
      " [5.64459294e-01 5.62889120e-01 4.94330942e-01 3.45456537e-01\n",
      "  2.85808183e-01 5.85912253e-01 6.00920195e-01]\n",
      " [5.62889120e-01 4.94330942e-01 3.45456537e-01 2.85808183e-01\n",
      "  5.85912253e-01 6.00920195e-01 5.55257344e-01]\n",
      " [4.94330942e-01 3.45456537e-01 2.85808183e-01 5.85912253e-01\n",
      "  6.00920195e-01 5.55257344e-01 4.74375126e-01]\n",
      " [3.45456537e-01 2.85808183e-01 5.85912253e-01 6.00920195e-01\n",
      "  5.55257344e-01 4.74375126e-01 3.96268098e-01]\n",
      " [2.85808183e-01 5.85912253e-01 6.00920195e-01 5.55257344e-01\n",
      "  4.74375126e-01 3.96268098e-01 3.12391594e-01]\n",
      " [5.85912253e-01 6.00920195e-01 5.55257344e-01 4.74375126e-01\n",
      "  3.96268098e-01 3.12391594e-01 3.76969564e-01]\n",
      " [6.00920195e-01 5.55257344e-01 4.74375126e-01 3.96268098e-01\n",
      "  3.12391594e-01 3.76969564e-01 6.37527159e-01]\n",
      " [5.55257344e-01 4.74375126e-01 3.96268098e-01 3.12391594e-01\n",
      "  3.76969564e-01 6.37527159e-01 5.87683263e-01]\n",
      " [4.74375126e-01 3.96268098e-01 3.12391594e-01 3.76969564e-01\n",
      "  6.37527159e-01 5.87683263e-01 4.15639663e-01]\n",
      " [3.96268098e-01 3.12391594e-01 3.76969564e-01 6.37527159e-01\n",
      "  5.87683263e-01 4.15639663e-01 1.00000000e+00]]\n",
      "[0.00000000e+00 1.82578372e-05 9.12891859e-05 9.12891859e-05\n",
      " 0.00000000e+00 2.19094046e-04 0.00000000e+00 1.64320535e-04\n",
      " 3.28641069e-04 4.56445929e-04 3.83414581e-04 4.19930255e-04\n",
      " 1.44236914e-03 6.20766464e-04 1.04069672e-03 2.50132369e-03\n",
      " 3.52376258e-03 5.16696792e-03 4.08975553e-03 7.63177594e-03\n",
      " 6.29895383e-03 5.65992952e-03 4.23581822e-03 8.80027752e-03\n",
      " 9.16543426e-03 8.89156671e-03 6.42675869e-03 5.89728141e-03\n",
      " 2.07774187e-02 2.04305198e-02 1.96089171e-02 2.09234814e-02\n",
      " 2.23110770e-02 1.55556773e-02 1.69067572e-02 3.03262676e-02\n",
      " 4.03498202e-02 3.52376258e-02 3.25172080e-02 1.98827847e-02\n",
      " 2.63278012e-02 2.30231327e-02 3.34483577e-02 5.58324661e-02\n",
      " 3.84327473e-02 5.94657757e-02 5.32581110e-02 3.75198554e-02\n",
      " 3.51828522e-02 4.56080773e-02 4.88944880e-02 6.81930219e-02\n",
      " 6.39572036e-02 1.00673714e-01 6.16932318e-02 8.42234029e-02\n",
      " 9.83184532e-02 1.14586186e-01 1.31785069e-01 1.13362911e-01\n",
      " 9.07414508e-02 8.37669570e-02 1.21104234e-01 1.26618101e-01\n",
      " 1.91762064e-01 1.80533494e-01 1.86631612e-01 1.93733910e-01\n",
      " 1.23422979e-01 1.02828139e-01 1.69031057e-01 2.07865476e-01\n",
      " 2.54587282e-01 2.79436198e-01 2.72388673e-01 1.44930712e-01\n",
      " 2.39907981e-01 3.17832430e-01 3.64262110e-01 3.37916050e-01\n",
      " 3.79817787e-01 3.01400376e-01 2.88711179e-01 2.13379343e-01\n",
      " 2.98040934e-01 3.76093188e-01 4.82317285e-01 4.91647039e-01\n",
      " 6.07511274e-01 2.99592850e-01 2.11754396e-01 5.28308777e-01\n",
      " 5.22776652e-01 5.64459294e-01 5.62889120e-01 4.94330942e-01\n",
      " 3.45456537e-01 2.85808183e-01 5.85912253e-01 6.00920195e-01\n",
      " 5.55257344e-01 4.74375126e-01 3.96268098e-01 3.12391594e-01\n",
      " 3.76969564e-01 6.37527159e-01 5.87683263e-01 4.15639663e-01\n",
      " 1.00000000e+00 6.32926184e-01]\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Manipulating the data to split into X(a window size of values)\n",
    "## and target, or Y, the value X \"produces\"\n",
    "####\n",
    "\n",
    "arr = df.values\n",
    "\n",
    "X = arr[:, : -1]\n",
    "target = arr[:, -1]\n",
    "print(X)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.02499385\n",
      "Iteration 2, loss = 0.05254608\n",
      "Iteration 3, loss = 0.01592508\n",
      "Iteration 4, loss = 0.00589378\n",
      "Iteration 5, loss = 0.01355393\n",
      "Iteration 6, loss = 0.01611312\n",
      "Iteration 7, loss = 0.01106270\n",
      "Iteration 8, loss = 0.00550324\n",
      "Iteration 9, loss = 0.00423437\n",
      "Iteration 10, loss = 0.00636684\n",
      "Iteration 11, loss = 0.00807723\n",
      "Iteration 12, loss = 0.00748324\n",
      "Iteration 13, loss = 0.00554845\n",
      "Iteration 14, loss = 0.00402702\n",
      "Iteration 15, loss = 0.00376395\n",
      "Iteration 16, loss = 0.00442603\n",
      "Iteration 17, loss = 0.00523676\n",
      "Iteration 18, loss = 0.00557726\n",
      "Iteration 19, loss = 0.00525340\n",
      "Iteration 20, loss = 0.00446769\n",
      "Iteration 21, loss = 0.00363869\n",
      "Iteration 22, loss = 0.00317070\n",
      "Iteration 23, loss = 0.00322508\n",
      "Iteration 24, loss = 0.00361537\n",
      "Iteration 25, loss = 0.00394995\n",
      "Iteration 26, loss = 0.00394085\n",
      "Iteration 27, loss = 0.00361370\n",
      "Iteration 28, loss = 0.00323024\n",
      "Iteration 29, loss = 0.00303098\n",
      "Iteration 30, loss = 0.00307045\n",
      "Iteration 31, loss = 0.00322442\n",
      "Iteration 32, loss = 0.00332466\n",
      "Iteration 33, loss = 0.00327674\n",
      "Iteration 34, loss = 0.00310292\n",
      "Iteration 35, loss = 0.00290576\n",
      "Iteration 36, loss = 0.00280807\n",
      "Iteration 37, loss = 0.00285901\n",
      "Iteration 38, loss = 0.00293980\n",
      "Iteration 39, loss = 0.00294925\n",
      "Iteration 40, loss = 0.00287939\n",
      "Iteration 41, loss = 0.00278019\n",
      "Iteration 42, loss = 0.00269603\n",
      "Iteration 43, loss = 0.00265977\n",
      "Iteration 44, loss = 0.00266797\n",
      "Iteration 45, loss = 0.00268692\n",
      "Iteration 46, loss = 0.00268255\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499385\n",
      "Iteration 2, loss = 0.05254612\n",
      "Iteration 3, loss = 0.01592508\n",
      "Iteration 4, loss = 0.00589379\n",
      "Iteration 5, loss = 0.01355394\n",
      "Iteration 6, loss = 0.01611312\n",
      "Iteration 7, loss = 0.01106270\n",
      "Iteration 8, loss = 0.00550324\n",
      "Iteration 9, loss = 0.00423437\n",
      "Iteration 10, loss = 0.00636684\n",
      "Iteration 11, loss = 0.00807723\n",
      "Iteration 12, loss = 0.00748324\n",
      "Iteration 13, loss = 0.00554844\n",
      "Iteration 14, loss = 0.00402702\n",
      "Iteration 15, loss = 0.00376395\n",
      "Iteration 16, loss = 0.00442603\n",
      "Iteration 17, loss = 0.00523677\n",
      "Iteration 18, loss = 0.00557726\n",
      "Iteration 19, loss = 0.00525340\n",
      "Iteration 20, loss = 0.00446769\n",
      "Iteration 21, loss = 0.00363869\n",
      "Iteration 22, loss = 0.00317070\n",
      "Iteration 23, loss = 0.00322508\n",
      "Iteration 24, loss = 0.00361538\n",
      "Iteration 25, loss = 0.00394995\n",
      "Iteration 26, loss = 0.00394085\n",
      "Iteration 27, loss = 0.00361370\n",
      "Iteration 28, loss = 0.00323024\n",
      "Iteration 29, loss = 0.00303098\n",
      "Iteration 30, loss = 0.00307045\n",
      "Iteration 31, loss = 0.00322442\n",
      "Iteration 32, loss = 0.00332467\n",
      "Iteration 33, loss = 0.00327674\n",
      "Iteration 34, loss = 0.00310292\n",
      "Iteration 35, loss = 0.00290576\n",
      "Iteration 36, loss = 0.00280807\n",
      "Iteration 37, loss = 0.00285901\n",
      "Iteration 38, loss = 0.00293980\n",
      "Iteration 39, loss = 0.00294925\n",
      "Iteration 40, loss = 0.00287939\n",
      "Iteration 41, loss = 0.00278019\n",
      "Iteration 42, loss = 0.00269603\n",
      "Iteration 43, loss = 0.00265977\n",
      "Iteration 44, loss = 0.00266797\n",
      "Iteration 45, loss = 0.00268692\n",
      "Iteration 46, loss = 0.00268255\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499385\n",
      "Iteration 2, loss = 0.05254631\n",
      "Iteration 3, loss = 0.01592511\n",
      "Iteration 4, loss = 0.00589382\n",
      "Iteration 5, loss = 0.01355399\n",
      "Iteration 6, loss = 0.01611316\n",
      "Iteration 7, loss = 0.01106271\n",
      "Iteration 8, loss = 0.00550324\n",
      "Iteration 9, loss = 0.00423438\n",
      "Iteration 10, loss = 0.00636686\n",
      "Iteration 11, loss = 0.00807725\n",
      "Iteration 12, loss = 0.00748325\n",
      "Iteration 13, loss = 0.00554844\n",
      "Iteration 14, loss = 0.00402702\n",
      "Iteration 15, loss = 0.00376395\n",
      "Iteration 16, loss = 0.00442603\n",
      "Iteration 17, loss = 0.00523677\n",
      "Iteration 18, loss = 0.00557727\n",
      "Iteration 19, loss = 0.00525340\n",
      "Iteration 20, loss = 0.00446770\n",
      "Iteration 21, loss = 0.00363869\n",
      "Iteration 22, loss = 0.00317070\n",
      "Iteration 23, loss = 0.00322508\n",
      "Iteration 24, loss = 0.00361538\n",
      "Iteration 25, loss = 0.00394995\n",
      "Iteration 26, loss = 0.00394085\n",
      "Iteration 27, loss = 0.00361371\n",
      "Iteration 28, loss = 0.00323023\n",
      "Iteration 29, loss = 0.00303101\n",
      "Iteration 30, loss = 0.00307053\n",
      "Iteration 31, loss = 0.00322452\n",
      "Iteration 32, loss = 0.00332475\n",
      "Iteration 33, loss = 0.00327678\n",
      "Iteration 34, loss = 0.00310292\n",
      "Iteration 35, loss = 0.00290574\n",
      "Iteration 36, loss = 0.00280805\n",
      "Iteration 37, loss = 0.00285899\n",
      "Iteration 38, loss = 0.00293980\n",
      "Iteration 39, loss = 0.00294925\n",
      "Iteration 40, loss = 0.00287930\n",
      "Iteration 41, loss = 0.00278006\n",
      "Iteration 42, loss = 0.00269594\n",
      "Iteration 43, loss = 0.00265974\n",
      "Iteration 44, loss = 0.00266797\n",
      "Iteration 45, loss = 0.00268693\n",
      "Iteration 46, loss = 0.00268254\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499385\n",
      "Iteration 2, loss = 0.05254628\n",
      "Iteration 3, loss = 0.01592509\n",
      "Iteration 4, loss = 0.00589381\n",
      "Iteration 5, loss = 0.01355398\n",
      "Iteration 6, loss = 0.01611315\n",
      "Iteration 7, loss = 0.01106270\n",
      "Iteration 8, loss = 0.00550324\n",
      "Iteration 9, loss = 0.00423438\n",
      "Iteration 10, loss = 0.00636685\n",
      "Iteration 11, loss = 0.00807724\n",
      "Iteration 12, loss = 0.00748324\n",
      "Iteration 13, loss = 0.00554844\n",
      "Iteration 14, loss = 0.00402702\n",
      "Iteration 15, loss = 0.00376395\n",
      "Iteration 16, loss = 0.00442603\n",
      "Iteration 17, loss = 0.00523677\n",
      "Iteration 18, loss = 0.00557727\n",
      "Iteration 19, loss = 0.00525340\n",
      "Iteration 20, loss = 0.00446769\n",
      "Iteration 21, loss = 0.00363869\n",
      "Iteration 22, loss = 0.00317070\n",
      "Iteration 23, loss = 0.00322508\n",
      "Iteration 24, loss = 0.00361538\n",
      "Iteration 25, loss = 0.00394995\n",
      "Iteration 26, loss = 0.00394085\n",
      "Iteration 27, loss = 0.00361370\n",
      "Iteration 28, loss = 0.00323024\n",
      "Iteration 29, loss = 0.00303098\n",
      "Iteration 30, loss = 0.00307045\n",
      "Iteration 31, loss = 0.00322442\n",
      "Iteration 32, loss = 0.00332467\n",
      "Iteration 33, loss = 0.00327674\n",
      "Iteration 34, loss = 0.00310292\n",
      "Iteration 35, loss = 0.00290576\n",
      "Iteration 36, loss = 0.00280807\n",
      "Iteration 37, loss = 0.00285901\n",
      "Iteration 38, loss = 0.00293980\n",
      "Iteration 39, loss = 0.00294925\n",
      "Iteration 40, loss = 0.00287939\n",
      "Iteration 41, loss = 0.00278019\n",
      "Iteration 42, loss = 0.00269603\n",
      "Iteration 43, loss = 0.00265977\n",
      "Iteration 44, loss = 0.00266797\n",
      "Iteration 45, loss = 0.00268693\n",
      "Iteration 46, loss = 0.00268255\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499385\n",
      "Iteration 2, loss = 0.05254604\n",
      "Iteration 3, loss = 0.01592506\n",
      "Iteration 4, loss = 0.00589377\n",
      "Iteration 5, loss = 0.01355391\n",
      "Iteration 6, loss = 0.01611311\n",
      "Iteration 7, loss = 0.01106269\n",
      "Iteration 8, loss = 0.00550324\n",
      "Iteration 9, loss = 0.00423437\n",
      "Iteration 10, loss = 0.00636683\n",
      "Iteration 11, loss = 0.00807722\n",
      "Iteration 12, loss = 0.00748323\n",
      "Iteration 13, loss = 0.00554845\n",
      "Iteration 14, loss = 0.00402702\n",
      "Iteration 15, loss = 0.00376395\n",
      "Iteration 16, loss = 0.00442603\n",
      "Iteration 17, loss = 0.00523676\n",
      "Iteration 18, loss = 0.00557726\n",
      "Iteration 19, loss = 0.00525340\n",
      "Iteration 20, loss = 0.00446770\n",
      "Iteration 21, loss = 0.00363869\n",
      "Iteration 22, loss = 0.00317070\n",
      "Iteration 23, loss = 0.00322508\n",
      "Iteration 24, loss = 0.00361537\n",
      "Iteration 25, loss = 0.00394995\n",
      "Iteration 26, loss = 0.00394085\n",
      "Iteration 27, loss = 0.00361370\n",
      "Iteration 28, loss = 0.00323024\n",
      "Iteration 29, loss = 0.00303098\n",
      "Iteration 30, loss = 0.00307045\n",
      "Iteration 31, loss = 0.00322442\n",
      "Iteration 32, loss = 0.00332467\n",
      "Iteration 33, loss = 0.00327674\n",
      "Iteration 34, loss = 0.00310292\n",
      "Iteration 35, loss = 0.00290576\n",
      "Iteration 36, loss = 0.00280807\n",
      "Iteration 37, loss = 0.00285900\n",
      "Iteration 38, loss = 0.00293980\n",
      "Iteration 39, loss = 0.00294925\n",
      "Iteration 40, loss = 0.00287939\n",
      "Iteration 41, loss = 0.00278019\n",
      "Iteration 42, loss = 0.00269603\n",
      "Iteration 43, loss = 0.00265977\n",
      "Iteration 44, loss = 0.00266796\n",
      "Iteration 45, loss = 0.00268692\n",
      "Iteration 46, loss = 0.00268255\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499386\n",
      "Iteration 2, loss = 0.05254659\n",
      "Iteration 3, loss = 0.01592515\n",
      "Iteration 4, loss = 0.00589386\n",
      "Iteration 5, loss = 0.01355408\n",
      "Iteration 6, loss = 0.01611322\n",
      "Iteration 7, loss = 0.01106273\n",
      "Iteration 8, loss = 0.00550325\n",
      "Iteration 9, loss = 0.00423440\n",
      "Iteration 10, loss = 0.00636689\n",
      "Iteration 11, loss = 0.00807727\n",
      "Iteration 12, loss = 0.00748325\n",
      "Iteration 13, loss = 0.00554844\n",
      "Iteration 14, loss = 0.00402702\n",
      "Iteration 15, loss = 0.00376396\n",
      "Iteration 16, loss = 0.00442605\n",
      "Iteration 17, loss = 0.00523679\n",
      "Iteration 18, loss = 0.00557728\n",
      "Iteration 19, loss = 0.00525342\n",
      "Iteration 20, loss = 0.00446770\n",
      "Iteration 21, loss = 0.00363869\n",
      "Iteration 22, loss = 0.00317071\n",
      "Iteration 23, loss = 0.00322509\n",
      "Iteration 24, loss = 0.00361538\n",
      "Iteration 25, loss = 0.00394996\n",
      "Iteration 26, loss = 0.00394086\n",
      "Iteration 27, loss = 0.00361371\n",
      "Iteration 28, loss = 0.00323024\n",
      "Iteration 29, loss = 0.00303101\n",
      "Iteration 30, loss = 0.00307053\n",
      "Iteration 31, loss = 0.00322452\n",
      "Iteration 32, loss = 0.00332475\n",
      "Iteration 33, loss = 0.00327678\n",
      "Iteration 34, loss = 0.00310292\n",
      "Iteration 35, loss = 0.00290575\n",
      "Iteration 36, loss = 0.00280806\n",
      "Iteration 37, loss = 0.00285900\n",
      "Iteration 38, loss = 0.00293980\n",
      "Iteration 39, loss = 0.00294926\n",
      "Iteration 40, loss = 0.00287931\n",
      "Iteration 41, loss = 0.00278006\n",
      "Iteration 42, loss = 0.00269595\n",
      "Iteration 43, loss = 0.00265974\n",
      "Iteration 44, loss = 0.00266797\n",
      "Iteration 45, loss = 0.00268694\n",
      "Iteration 46, loss = 0.00268254\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499385\n",
      "Iteration 2, loss = 0.05254594\n",
      "Iteration 3, loss = 0.01592506\n",
      "Iteration 4, loss = 0.00589375\n",
      "Iteration 5, loss = 0.01355387\n",
      "Iteration 6, loss = 0.01611309\n",
      "Iteration 7, loss = 0.01106269\n",
      "Iteration 8, loss = 0.00550325\n",
      "Iteration 9, loss = 0.00423437\n",
      "Iteration 10, loss = 0.00636682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.00807721\n",
      "Iteration 12, loss = 0.00748322\n",
      "Iteration 13, loss = 0.00554844\n",
      "Iteration 14, loss = 0.00402702\n",
      "Iteration 15, loss = 0.00376395\n",
      "Iteration 16, loss = 0.00442603\n",
      "Iteration 17, loss = 0.00523676\n",
      "Iteration 18, loss = 0.00557726\n",
      "Iteration 19, loss = 0.00525340\n",
      "Iteration 20, loss = 0.00446769\n",
      "Iteration 21, loss = 0.00363869\n",
      "Iteration 22, loss = 0.00317070\n",
      "Iteration 23, loss = 0.00322508\n",
      "Iteration 24, loss = 0.00361537\n",
      "Iteration 25, loss = 0.00394994\n",
      "Iteration 26, loss = 0.00394084\n",
      "Iteration 27, loss = 0.00361370\n",
      "Iteration 28, loss = 0.00323024\n",
      "Iteration 29, loss = 0.00303098\n",
      "Iteration 30, loss = 0.00307045\n",
      "Iteration 31, loss = 0.00322442\n",
      "Iteration 32, loss = 0.00332467\n",
      "Iteration 33, loss = 0.00327675\n",
      "Iteration 34, loss = 0.00310293\n",
      "Iteration 35, loss = 0.00290576\n",
      "Iteration 36, loss = 0.00280807\n",
      "Iteration 37, loss = 0.00285900\n",
      "Iteration 38, loss = 0.00293980\n",
      "Iteration 39, loss = 0.00294925\n",
      "Iteration 40, loss = 0.00287939\n",
      "Iteration 41, loss = 0.00278019\n",
      "Iteration 42, loss = 0.00269603\n",
      "Iteration 43, loss = 0.00265977\n",
      "Iteration 44, loss = 0.00266796\n",
      "Iteration 45, loss = 0.00268692\n",
      "Iteration 46, loss = 0.00268255\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499385\n",
      "Iteration 2, loss = 0.05254643\n",
      "Iteration 3, loss = 0.01592512\n",
      "Iteration 4, loss = 0.00589383\n",
      "Iteration 5, loss = 0.01355402\n",
      "Iteration 6, loss = 0.01611319\n",
      "Iteration 7, loss = 0.01106272\n",
      "Iteration 8, loss = 0.00550325\n",
      "Iteration 9, loss = 0.00423440\n",
      "Iteration 10, loss = 0.00636687\n",
      "Iteration 11, loss = 0.00807725\n",
      "Iteration 12, loss = 0.00748324\n",
      "Iteration 13, loss = 0.00554845\n",
      "Iteration 14, loss = 0.00402702\n",
      "Iteration 15, loss = 0.00376396\n",
      "Iteration 16, loss = 0.00442605\n",
      "Iteration 17, loss = 0.00523679\n",
      "Iteration 18, loss = 0.00557728\n",
      "Iteration 19, loss = 0.00525342\n",
      "Iteration 20, loss = 0.00446771\n",
      "Iteration 21, loss = 0.00363870\n",
      "Iteration 22, loss = 0.00317071\n",
      "Iteration 23, loss = 0.00322508\n",
      "Iteration 24, loss = 0.00361538\n",
      "Iteration 25, loss = 0.00394996\n",
      "Iteration 26, loss = 0.00394086\n",
      "Iteration 27, loss = 0.00361371\n",
      "Iteration 28, loss = 0.00323025\n",
      "Iteration 29, loss = 0.00303098\n",
      "Iteration 30, loss = 0.00307045\n",
      "Iteration 31, loss = 0.00322442\n",
      "Iteration 32, loss = 0.00332467\n",
      "Iteration 33, loss = 0.00327674\n",
      "Iteration 34, loss = 0.00310292\n",
      "Iteration 35, loss = 0.00290576\n",
      "Iteration 36, loss = 0.00280808\n",
      "Iteration 37, loss = 0.00285901\n",
      "Iteration 38, loss = 0.00293980\n",
      "Iteration 39, loss = 0.00294925\n",
      "Iteration 40, loss = 0.00287940\n",
      "Iteration 41, loss = 0.00278019\n",
      "Iteration 42, loss = 0.00269603\n",
      "Iteration 43, loss = 0.00265977\n",
      "Iteration 44, loss = 0.00266797\n",
      "Iteration 45, loss = 0.00268693\n",
      "Iteration 46, loss = 0.00268255\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499386\n",
      "Iteration 2, loss = 0.05254673\n",
      "Iteration 3, loss = 0.01592518\n",
      "Iteration 4, loss = 0.00589388\n",
      "Iteration 5, loss = 0.01355434\n",
      "Iteration 6, loss = 0.01611364\n",
      "Iteration 7, loss = 0.01106284\n",
      "Iteration 8, loss = 0.00550303\n",
      "Iteration 9, loss = 0.00423460\n",
      "Iteration 10, loss = 0.00636746\n",
      "Iteration 11, loss = 0.00807769\n",
      "Iteration 12, loss = 0.00748324\n",
      "Iteration 13, loss = 0.00554809\n",
      "Iteration 14, loss = 0.00402677\n",
      "Iteration 15, loss = 0.00376407\n",
      "Iteration 16, loss = 0.00442632\n",
      "Iteration 17, loss = 0.00523737\n",
      "Iteration 18, loss = 0.00557791\n",
      "Iteration 19, loss = 0.00525369\n",
      "Iteration 20, loss = 0.00446754\n",
      "Iteration 21, loss = 0.00363830\n",
      "Iteration 22, loss = 0.00317018\n",
      "Iteration 23, loss = 0.00322456\n",
      "Iteration 24, loss = 0.00361478\n",
      "Iteration 25, loss = 0.00394909\n",
      "Iteration 26, loss = 0.00393971\n",
      "Iteration 27, loss = 0.00361281\n",
      "Iteration 28, loss = 0.00322957\n",
      "Iteration 29, loss = 0.00303033\n",
      "Iteration 30, loss = 0.00306976\n",
      "Iteration 31, loss = 0.00322274\n",
      "Iteration 32, loss = 0.00332314\n",
      "Iteration 33, loss = 0.00327524\n",
      "Iteration 34, loss = 0.00310121\n",
      "Iteration 35, loss = 0.00290347\n",
      "Iteration 36, loss = 0.00280640\n",
      "Iteration 37, loss = 0.00285882\n",
      "Iteration 38, loss = 0.00293992\n",
      "Iteration 39, loss = 0.00294849\n",
      "Iteration 40, loss = 0.00287854\n",
      "Iteration 41, loss = 0.00277906\n",
      "Iteration 42, loss = 0.00269581\n",
      "Iteration 43, loss = 0.00266085\n",
      "Iteration 44, loss = 0.00266934\n",
      "Iteration 45, loss = 0.00268830\n",
      "Iteration 46, loss = 0.00268340\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499386\n",
      "Iteration 2, loss = 0.05254704\n",
      "Iteration 3, loss = 0.01592518\n",
      "Iteration 4, loss = 0.00589391\n",
      "Iteration 5, loss = 0.01355441\n",
      "Iteration 6, loss = 0.01611367\n",
      "Iteration 7, loss = 0.01106284\n",
      "Iteration 8, loss = 0.00550303\n",
      "Iteration 9, loss = 0.00423462\n",
      "Iteration 10, loss = 0.00636749\n",
      "Iteration 11, loss = 0.00807770\n",
      "Iteration 12, loss = 0.00748323\n",
      "Iteration 13, loss = 0.00554808\n",
      "Iteration 14, loss = 0.00402677\n",
      "Iteration 15, loss = 0.00376408\n",
      "Iteration 16, loss = 0.00442634\n",
      "Iteration 17, loss = 0.00523738\n",
      "Iteration 18, loss = 0.00557792\n",
      "Iteration 19, loss = 0.00525369\n",
      "Iteration 20, loss = 0.00446754\n",
      "Iteration 21, loss = 0.00363830\n",
      "Iteration 22, loss = 0.00317018\n",
      "Iteration 23, loss = 0.00322456\n",
      "Iteration 24, loss = 0.00361478\n",
      "Iteration 25, loss = 0.00394909\n",
      "Iteration 26, loss = 0.00393972\n",
      "Iteration 27, loss = 0.00361281\n",
      "Iteration 28, loss = 0.00322958\n",
      "Iteration 29, loss = 0.00303034\n",
      "Iteration 30, loss = 0.00306976\n",
      "Iteration 31, loss = 0.00322274\n",
      "Iteration 32, loss = 0.00332314\n",
      "Iteration 33, loss = 0.00327524\n",
      "Iteration 34, loss = 0.00310121\n",
      "Iteration 35, loss = 0.00290347\n",
      "Iteration 36, loss = 0.00280640\n",
      "Iteration 37, loss = 0.00285882\n",
      "Iteration 38, loss = 0.00293993\n",
      "Iteration 39, loss = 0.00294849\n",
      "Iteration 40, loss = 0.00287854\n",
      "Iteration 41, loss = 0.00277906\n",
      "Iteration 42, loss = 0.00269582\n",
      "Iteration 43, loss = 0.00266085\n",
      "Iteration 44, loss = 0.00266934\n",
      "Iteration 45, loss = 0.00268831\n",
      "Iteration 46, loss = 0.00268340\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499386\n",
      "Iteration 2, loss = 0.05254685\n",
      "Iteration 3, loss = 0.01592517\n",
      "Iteration 4, loss = 0.00589387\n",
      "Iteration 5, loss = 0.01355435\n",
      "Iteration 6, loss = 0.01611366\n",
      "Iteration 7, loss = 0.01106285\n",
      "Iteration 8, loss = 0.00550305\n",
      "Iteration 9, loss = 0.00423462\n",
      "Iteration 10, loss = 0.00636746\n",
      "Iteration 11, loss = 0.00807768\n",
      "Iteration 12, loss = 0.00748323\n",
      "Iteration 13, loss = 0.00554810\n",
      "Iteration 14, loss = 0.00402679\n",
      "Iteration 15, loss = 0.00376409\n",
      "Iteration 16, loss = 0.00442634\n",
      "Iteration 17, loss = 0.00523738\n",
      "Iteration 18, loss = 0.00557792\n",
      "Iteration 19, loss = 0.00525370\n",
      "Iteration 20, loss = 0.00446756\n",
      "Iteration 21, loss = 0.00363831\n",
      "Iteration 22, loss = 0.00317019\n",
      "Iteration 23, loss = 0.00322455\n",
      "Iteration 24, loss = 0.00361477\n",
      "Iteration 25, loss = 0.00394908\n",
      "Iteration 26, loss = 0.00393972\n",
      "Iteration 27, loss = 0.00361282\n",
      "Iteration 28, loss = 0.00322958\n",
      "Iteration 29, loss = 0.00303034\n",
      "Iteration 30, loss = 0.00306976\n",
      "Iteration 31, loss = 0.00322274\n",
      "Iteration 32, loss = 0.00332314\n",
      "Iteration 33, loss = 0.00327524\n",
      "Iteration 34, loss = 0.00310122\n",
      "Iteration 35, loss = 0.00290347\n",
      "Iteration 36, loss = 0.00280640\n",
      "Iteration 37, loss = 0.00285882\n",
      "Iteration 38, loss = 0.00293992\n",
      "Iteration 39, loss = 0.00294849\n",
      "Iteration 40, loss = 0.00287855\n",
      "Iteration 41, loss = 0.00277906\n",
      "Iteration 42, loss = 0.00269582\n",
      "Iteration 43, loss = 0.00266085\n",
      "Iteration 44, loss = 0.00266934\n",
      "Iteration 45, loss = 0.00268830\n",
      "Iteration 46, loss = 0.00268340\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499386\n",
      "Iteration 2, loss = 0.05254665\n",
      "Iteration 3, loss = 0.01592514\n",
      "Iteration 4, loss = 0.00589384\n",
      "Iteration 5, loss = 0.01355429\n",
      "Iteration 6, loss = 0.01611362\n",
      "Iteration 7, loss = 0.01106283\n",
      "Iteration 8, loss = 0.00550305\n",
      "Iteration 9, loss = 0.00423462\n",
      "Iteration 10, loss = 0.00636746\n",
      "Iteration 11, loss = 0.00807766\n",
      "Iteration 12, loss = 0.00748321\n",
      "Iteration 13, loss = 0.00554809\n",
      "Iteration 14, loss = 0.00402679\n",
      "Iteration 15, loss = 0.00376410\n",
      "Iteration 16, loss = 0.00442635\n",
      "Iteration 17, loss = 0.00523739\n",
      "Iteration 18, loss = 0.00557792\n",
      "Iteration 19, loss = 0.00525369\n",
      "Iteration 20, loss = 0.00446755\n",
      "Iteration 21, loss = 0.00363831\n",
      "Iteration 22, loss = 0.00317019\n",
      "Iteration 23, loss = 0.00322456\n",
      "Iteration 24, loss = 0.00361477\n",
      "Iteration 25, loss = 0.00394909\n",
      "Iteration 26, loss = 0.00393971\n",
      "Iteration 27, loss = 0.00361281\n",
      "Iteration 28, loss = 0.00322958\n",
      "Iteration 29, loss = 0.00303034\n",
      "Iteration 30, loss = 0.00306976\n",
      "Iteration 31, loss = 0.00322274\n",
      "Iteration 32, loss = 0.00332314\n",
      "Iteration 33, loss = 0.00327524\n",
      "Iteration 34, loss = 0.00310121\n",
      "Iteration 35, loss = 0.00290347\n",
      "Iteration 36, loss = 0.00280641\n",
      "Iteration 37, loss = 0.00285882\n",
      "Iteration 38, loss = 0.00293993\n",
      "Iteration 39, loss = 0.00294849\n",
      "Iteration 40, loss = 0.00287864\n",
      "Iteration 41, loss = 0.00277919\n",
      "Iteration 42, loss = 0.00269591\n",
      "Iteration 43, loss = 0.00266088\n",
      "Iteration 44, loss = 0.00266933\n",
      "Iteration 45, loss = 0.00268829\n",
      "Iteration 46, loss = 0.00268341\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499387\n",
      "Iteration 2, loss = 0.05254940\n",
      "Iteration 3, loss = 0.01592553\n",
      "Iteration 4, loss = 0.00589426\n",
      "Iteration 5, loss = 0.01355509\n",
      "Iteration 6, loss = 0.01611416\n",
      "Iteration 7, loss = 0.01106300\n",
      "Iteration 8, loss = 0.00550308\n",
      "Iteration 9, loss = 0.00423477\n",
      "Iteration 10, loss = 0.00636771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.00807787\n",
      "Iteration 12, loss = 0.00748329\n",
      "Iteration 13, loss = 0.00554807\n",
      "Iteration 14, loss = 0.00402676\n",
      "Iteration 15, loss = 0.00376413\n",
      "Iteration 16, loss = 0.00442643\n",
      "Iteration 17, loss = 0.00523750\n",
      "Iteration 18, loss = 0.00557803\n",
      "Iteration 19, loss = 0.00525377\n",
      "Iteration 20, loss = 0.00446758\n",
      "Iteration 21, loss = 0.00363832\n",
      "Iteration 22, loss = 0.00317016\n",
      "Iteration 23, loss = 0.00322470\n",
      "Iteration 24, loss = 0.00361518\n",
      "Iteration 25, loss = 0.00394962\n",
      "Iteration 26, loss = 0.00394016\n",
      "Iteration 27, loss = 0.00361305\n",
      "Iteration 28, loss = 0.00322964\n",
      "Iteration 29, loss = 0.00303040\n",
      "Iteration 30, loss = 0.00306988\n",
      "Iteration 31, loss = 0.00322295\n",
      "Iteration 32, loss = 0.00332335\n",
      "Iteration 33, loss = 0.00327538\n",
      "Iteration 34, loss = 0.00310129\n",
      "Iteration 35, loss = 0.00290351\n",
      "Iteration 36, loss = 0.00280643\n",
      "Iteration 37, loss = 0.00285887\n",
      "Iteration 38, loss = 0.00294001\n",
      "Iteration 39, loss = 0.00294861\n",
      "Iteration 40, loss = 0.00287874\n",
      "Iteration 41, loss = 0.00277926\n",
      "Iteration 42, loss = 0.00269593\n",
      "Iteration 43, loss = 0.00266087\n",
      "Iteration 44, loss = 0.00266932\n",
      "Iteration 45, loss = 0.00268822\n",
      "Iteration 46, loss = 0.00268331\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499386\n",
      "Iteration 2, loss = 0.05254675\n",
      "Iteration 3, loss = 0.01592515\n",
      "Iteration 4, loss = 0.00589382\n",
      "Iteration 5, loss = 0.01355425\n",
      "Iteration 6, loss = 0.01611360\n",
      "Iteration 7, loss = 0.01106284\n",
      "Iteration 8, loss = 0.00550309\n",
      "Iteration 9, loss = 0.00423465\n",
      "Iteration 10, loss = 0.00636745\n",
      "Iteration 11, loss = 0.00807763\n",
      "Iteration 12, loss = 0.00748317\n",
      "Iteration 13, loss = 0.00554808\n",
      "Iteration 14, loss = 0.00402679\n",
      "Iteration 15, loss = 0.00376411\n",
      "Iteration 16, loss = 0.00442636\n",
      "Iteration 17, loss = 0.00523739\n",
      "Iteration 18, loss = 0.00557792\n",
      "Iteration 19, loss = 0.00525369\n",
      "Iteration 20, loss = 0.00446755\n",
      "Iteration 21, loss = 0.00363831\n",
      "Iteration 22, loss = 0.00317019\n",
      "Iteration 23, loss = 0.00322456\n",
      "Iteration 24, loss = 0.00361476\n",
      "Iteration 25, loss = 0.00394908\n",
      "Iteration 26, loss = 0.00393971\n",
      "Iteration 27, loss = 0.00361281\n",
      "Iteration 28, loss = 0.00322958\n",
      "Iteration 29, loss = 0.00303035\n",
      "Iteration 30, loss = 0.00306976\n",
      "Iteration 31, loss = 0.00322273\n",
      "Iteration 32, loss = 0.00332312\n",
      "Iteration 33, loss = 0.00327523\n",
      "Iteration 34, loss = 0.00310121\n",
      "Iteration 35, loss = 0.00290347\n",
      "Iteration 36, loss = 0.00280641\n",
      "Iteration 37, loss = 0.00285882\n",
      "Iteration 38, loss = 0.00293992\n",
      "Iteration 39, loss = 0.00294848\n",
      "Iteration 40, loss = 0.00287864\n",
      "Iteration 41, loss = 0.00277919\n",
      "Iteration 42, loss = 0.00269591\n",
      "Iteration 43, loss = 0.00266089\n",
      "Iteration 44, loss = 0.00266933\n",
      "Iteration 45, loss = 0.00268829\n",
      "Iteration 46, loss = 0.00268341\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499387\n",
      "Iteration 2, loss = 0.05254792\n",
      "Iteration 3, loss = 0.01592530\n",
      "Iteration 4, loss = 0.00589397\n",
      "Iteration 5, loss = 0.01355461\n",
      "Iteration 6, loss = 0.01611388\n",
      "Iteration 7, loss = 0.01106295\n",
      "Iteration 8, loss = 0.00550313\n",
      "Iteration 9, loss = 0.00423475\n",
      "Iteration 10, loss = 0.00636759\n",
      "Iteration 11, loss = 0.00807772\n",
      "Iteration 12, loss = 0.00748321\n",
      "Iteration 13, loss = 0.00554810\n",
      "Iteration 14, loss = 0.00402681\n",
      "Iteration 15, loss = 0.00376417\n",
      "Iteration 16, loss = 0.00442644\n",
      "Iteration 17, loss = 0.00523748\n",
      "Iteration 18, loss = 0.00557800\n",
      "Iteration 19, loss = 0.00525376\n",
      "Iteration 20, loss = 0.00446760\n",
      "Iteration 21, loss = 0.00363834\n",
      "Iteration 22, loss = 0.00317020\n",
      "Iteration 23, loss = 0.00322456\n",
      "Iteration 24, loss = 0.00361478\n",
      "Iteration 25, loss = 0.00394910\n",
      "Iteration 26, loss = 0.00393975\n",
      "Iteration 27, loss = 0.00361286\n",
      "Iteration 28, loss = 0.00322962\n",
      "Iteration 29, loss = 0.00303037\n",
      "Iteration 30, loss = 0.00306977\n",
      "Iteration 31, loss = 0.00322274\n",
      "Iteration 32, loss = 0.00332313\n",
      "Iteration 33, loss = 0.00327524\n",
      "Iteration 34, loss = 0.00310123\n",
      "Iteration 35, loss = 0.00290350\n",
      "Iteration 36, loss = 0.00280642\n",
      "Iteration 37, loss = 0.00285881\n",
      "Iteration 38, loss = 0.00293991\n",
      "Iteration 39, loss = 0.00294848\n",
      "Iteration 40, loss = 0.00287854\n",
      "Iteration 41, loss = 0.00277906\n",
      "Iteration 42, loss = 0.00269583\n",
      "Iteration 43, loss = 0.00266086\n",
      "Iteration 44, loss = 0.00266933\n",
      "Iteration 45, loss = 0.00268829\n",
      "Iteration 46, loss = 0.00268340\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499387\n",
      "Iteration 2, loss = 0.05255122\n",
      "Iteration 3, loss = 0.01592583\n",
      "Iteration 4, loss = 0.00589450\n",
      "Iteration 5, loss = 0.01355557\n",
      "Iteration 6, loss = 0.01611452\n",
      "Iteration 7, loss = 0.01106313\n",
      "Iteration 8, loss = 0.00550316\n",
      "Iteration 9, loss = 0.00423492\n",
      "Iteration 10, loss = 0.00636789\n",
      "Iteration 11, loss = 0.00807796\n",
      "Iteration 12, loss = 0.00748330\n",
      "Iteration 13, loss = 0.00554805\n",
      "Iteration 14, loss = 0.00402676\n",
      "Iteration 15, loss = 0.00376418\n",
      "Iteration 16, loss = 0.00442653\n",
      "Iteration 17, loss = 0.00523761\n",
      "Iteration 18, loss = 0.00557812\n",
      "Iteration 19, loss = 0.00525383\n",
      "Iteration 20, loss = 0.00446761\n",
      "Iteration 21, loss = 0.00363832\n",
      "Iteration 22, loss = 0.00317016\n",
      "Iteration 23, loss = 0.00322471\n",
      "Iteration 24, loss = 0.00361519\n",
      "Iteration 25, loss = 0.00394964\n",
      "Iteration 26, loss = 0.00394019\n",
      "Iteration 27, loss = 0.00361307\n",
      "Iteration 28, loss = 0.00322966\n",
      "Iteration 29, loss = 0.00303042\n",
      "Iteration 30, loss = 0.00306989\n",
      "Iteration 31, loss = 0.00322295\n",
      "Iteration 32, loss = 0.00332334\n",
      "Iteration 33, loss = 0.00327536\n",
      "Iteration 34, loss = 0.00310128\n",
      "Iteration 35, loss = 0.00290351\n",
      "Iteration 36, loss = 0.00280644\n",
      "Iteration 37, loss = 0.00285889\n",
      "Iteration 38, loss = 0.00294003\n",
      "Iteration 39, loss = 0.00294862\n",
      "Iteration 40, loss = 0.00287875\n",
      "Iteration 41, loss = 0.00277926\n",
      "Iteration 42, loss = 0.00269593\n",
      "Iteration 43, loss = 0.00266087\n",
      "Iteration 44, loss = 0.00266932\n",
      "Iteration 45, loss = 0.00268823\n",
      "Iteration 46, loss = 0.00268332\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499386\n",
      "Iteration 2, loss = 0.05255347\n",
      "Iteration 3, loss = 0.01592596\n",
      "Iteration 4, loss = 0.00589477\n",
      "Iteration 5, loss = 0.01355485\n",
      "Iteration 6, loss = 0.01611579\n",
      "Iteration 7, loss = 0.01106503\n",
      "Iteration 8, loss = 0.00550466\n",
      "Iteration 9, loss = 0.00423572\n",
      "Iteration 10, loss = 0.00636779\n",
      "Iteration 11, loss = 0.00807704\n",
      "Iteration 12, loss = 0.00748276\n",
      "Iteration 13, loss = 0.00554832\n",
      "Iteration 14, loss = 0.00402709\n",
      "Iteration 15, loss = 0.00376423\n",
      "Iteration 16, loss = 0.00442620\n",
      "Iteration 17, loss = 0.00523717\n",
      "Iteration 18, loss = 0.00557775\n",
      "Iteration 19, loss = 0.00525395\n",
      "Iteration 20, loss = 0.00446824\n",
      "Iteration 21, loss = 0.00363909\n",
      "Iteration 22, loss = 0.00317049\n",
      "Iteration 23, loss = 0.00322425\n",
      "Iteration 24, loss = 0.00361403\n",
      "Iteration 25, loss = 0.00394844\n",
      "Iteration 26, loss = 0.00393967\n",
      "Iteration 27, loss = 0.00361318\n",
      "Iteration 28, loss = 0.00323005\n",
      "Iteration 29, loss = 0.00303052\n",
      "Iteration 30, loss = 0.00306957\n",
      "Iteration 31, loss = 0.00322224\n",
      "Iteration 32, loss = 0.00332211\n",
      "Iteration 33, loss = 0.00327403\n",
      "Iteration 34, loss = 0.00310045\n",
      "Iteration 35, loss = 0.00290322\n",
      "Iteration 36, loss = 0.00280654\n",
      "Iteration 37, loss = 0.00285882\n",
      "Iteration 38, loss = 0.00293965\n",
      "Iteration 39, loss = 0.00294817\n",
      "Iteration 40, loss = 0.00287867\n",
      "Iteration 41, loss = 0.00277931\n",
      "Iteration 42, loss = 0.00269574\n",
      "Iteration 43, loss = 0.00266054\n",
      "Iteration 44, loss = 0.00266910\n",
      "Iteration 45, loss = 0.00268809\n",
      "Iteration 46, loss = 0.00268327\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499384\n",
      "Iteration 2, loss = 0.05255733\n",
      "Iteration 3, loss = 0.01592643\n",
      "Iteration 4, loss = 0.00589525\n",
      "Iteration 5, loss = 0.01355584\n",
      "Iteration 6, loss = 0.01611653\n",
      "Iteration 7, loss = 0.01106527\n",
      "Iteration 8, loss = 0.00550475\n",
      "Iteration 9, loss = 0.00423593\n",
      "Iteration 10, loss = 0.00636810\n",
      "Iteration 11, loss = 0.00807725\n",
      "Iteration 12, loss = 0.00748282\n",
      "Iteration 13, loss = 0.00554833\n",
      "Iteration 14, loss = 0.00402709\n",
      "Iteration 15, loss = 0.00376429\n",
      "Iteration 16, loss = 0.00442632\n",
      "Iteration 17, loss = 0.00523733\n",
      "Iteration 18, loss = 0.00557790\n",
      "Iteration 19, loss = 0.00525407\n",
      "Iteration 20, loss = 0.00446831\n",
      "Iteration 21, loss = 0.00363911\n",
      "Iteration 22, loss = 0.00317047\n",
      "Iteration 23, loss = 0.00322417\n",
      "Iteration 24, loss = 0.00361390\n",
      "Iteration 25, loss = 0.00394833\n",
      "Iteration 26, loss = 0.00393968\n",
      "Iteration 27, loss = 0.00361330\n",
      "Iteration 28, loss = 0.00323018\n",
      "Iteration 29, loss = 0.00303063\n",
      "Iteration 30, loss = 0.00306966\n",
      "Iteration 31, loss = 0.00322229\n",
      "Iteration 32, loss = 0.00332215\n",
      "Iteration 33, loss = 0.00327411\n",
      "Iteration 34, loss = 0.00310059\n",
      "Iteration 35, loss = 0.00290348\n",
      "Iteration 36, loss = 0.00280675\n",
      "Iteration 37, loss = 0.00285905\n",
      "Iteration 38, loss = 0.00293992\n",
      "Iteration 39, loss = 0.00294847\n",
      "Iteration 40, loss = 0.00287895\n",
      "Iteration 41, loss = 0.00277958\n",
      "Iteration 42, loss = 0.00269594\n",
      "Iteration 43, loss = 0.00266069\n",
      "Iteration 44, loss = 0.00266919\n",
      "Iteration 45, loss = 0.00268826\n",
      "Iteration 46, loss = 0.00268353\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499386\n",
      "Iteration 2, loss = 0.05255256\n",
      "Iteration 3, loss = 0.01592577\n",
      "Iteration 4, loss = 0.00589443\n",
      "Iteration 5, loss = 0.01355562\n",
      "Iteration 6, loss = 0.01611465\n",
      "Iteration 7, loss = 0.01106320\n",
      "Iteration 8, loss = 0.00550339\n",
      "Iteration 9, loss = 0.00423521\n",
      "Iteration 10, loss = 0.00636800\n",
      "Iteration 11, loss = 0.00807783\n",
      "Iteration 12, loss = 0.00748308\n",
      "Iteration 13, loss = 0.00554803\n",
      "Iteration 14, loss = 0.00402684\n",
      "Iteration 15, loss = 0.00376437\n",
      "Iteration 16, loss = 0.00442673\n",
      "Iteration 17, loss = 0.00523778\n",
      "Iteration 18, loss = 0.00557823\n",
      "Iteration 19, loss = 0.00525391\n",
      "Iteration 20, loss = 0.00446768\n",
      "Iteration 21, loss = 0.00363837\n",
      "Iteration 22, loss = 0.00317020\n",
      "Iteration 23, loss = 0.00322481\n",
      "Iteration 24, loss = 0.00361539\n",
      "Iteration 25, loss = 0.00394987\n",
      "Iteration 26, loss = 0.00394038\n",
      "Iteration 27, loss = 0.00361318\n",
      "Iteration 28, loss = 0.00322969\n",
      "Iteration 29, loss = 0.00303043\n",
      "Iteration 30, loss = 0.00306993\n",
      "Iteration 31, loss = 0.00322319\n",
      "Iteration 32, loss = 0.00332375\n",
      "Iteration 33, loss = 0.00327582\n",
      "Iteration 34, loss = 0.00310168\n",
      "Iteration 35, loss = 0.00290372\n",
      "Iteration 36, loss = 0.00280646\n",
      "Iteration 37, loss = 0.00285888\n",
      "Iteration 38, loss = 0.00294012\n",
      "Iteration 39, loss = 0.00294880\n",
      "Iteration 40, loss = 0.00287877\n",
      "Iteration 41, loss = 0.00277919\n",
      "Iteration 42, loss = 0.00269586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43, loss = 0.00266080\n",
      "Iteration 44, loss = 0.00266919\n",
      "Iteration 45, loss = 0.00268784\n",
      "Iteration 46, loss = 0.00268284\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499374\n",
      "Iteration 2, loss = 0.05256180\n",
      "Iteration 3, loss = 0.01592702\n",
      "Iteration 4, loss = 0.00589568\n",
      "Iteration 5, loss = 0.01355704\n",
      "Iteration 6, loss = 0.01611753\n",
      "Iteration 7, loss = 0.01106567\n",
      "Iteration 8, loss = 0.00550504\n",
      "Iteration 9, loss = 0.00423643\n",
      "Iteration 10, loss = 0.00636862\n",
      "Iteration 11, loss = 0.00807749\n",
      "Iteration 12, loss = 0.00748279\n",
      "Iteration 13, loss = 0.00554830\n",
      "Iteration 14, loss = 0.00402719\n",
      "Iteration 15, loss = 0.00376457\n",
      "Iteration 16, loss = 0.00442666\n",
      "Iteration 17, loss = 0.00523767\n",
      "Iteration 18, loss = 0.00557817\n",
      "Iteration 19, loss = 0.00525429\n",
      "Iteration 20, loss = 0.00446848\n",
      "Iteration 21, loss = 0.00363921\n",
      "Iteration 22, loss = 0.00317048\n",
      "Iteration 23, loss = 0.00322418\n",
      "Iteration 24, loss = 0.00361396\n",
      "Iteration 25, loss = 0.00394846\n",
      "Iteration 26, loss = 0.00393985\n",
      "Iteration 27, loss = 0.00361345\n",
      "Iteration 28, loss = 0.00323027\n",
      "Iteration 29, loss = 0.00303066\n",
      "Iteration 30, loss = 0.00306964\n",
      "Iteration 31, loss = 0.00322226\n",
      "Iteration 32, loss = 0.00332214\n",
      "Iteration 33, loss = 0.00327413\n",
      "Iteration 34, loss = 0.00310062\n",
      "Iteration 35, loss = 0.00290349\n",
      "Iteration 36, loss = 0.00280672\n",
      "Iteration 37, loss = 0.00285888\n",
      "Iteration 38, loss = 0.00293973\n",
      "Iteration 39, loss = 0.00294829\n",
      "Iteration 40, loss = 0.00287880\n",
      "Iteration 41, loss = 0.00277947\n",
      "Iteration 42, loss = 0.00269587\n",
      "Iteration 43, loss = 0.00266060\n",
      "Iteration 44, loss = 0.00266908\n",
      "Iteration 45, loss = 0.00268804\n",
      "Iteration 46, loss = 0.00268326\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499381\n",
      "Iteration 2, loss = 0.05255581\n",
      "Iteration 3, loss = 0.01592657\n",
      "Iteration 4, loss = 0.00589475\n",
      "Iteration 5, loss = 0.01355639\n",
      "Iteration 6, loss = 0.01611543\n",
      "Iteration 7, loss = 0.01106365\n",
      "Iteration 8, loss = 0.00550377\n",
      "Iteration 9, loss = 0.00423565\n",
      "Iteration 10, loss = 0.00636832\n",
      "Iteration 11, loss = 0.00807790\n",
      "Iteration 12, loss = 0.00748299\n",
      "Iteration 13, loss = 0.00554795\n",
      "Iteration 14, loss = 0.00402695\n",
      "Iteration 15, loss = 0.00376461\n",
      "Iteration 16, loss = 0.00442696\n",
      "Iteration 17, loss = 0.00523798\n",
      "Iteration 18, loss = 0.00557836\n",
      "Iteration 19, loss = 0.00525404\n",
      "Iteration 20, loss = 0.00446782\n",
      "Iteration 21, loss = 0.00363848\n",
      "Iteration 22, loss = 0.00317023\n",
      "Iteration 23, loss = 0.00322476\n",
      "Iteration 24, loss = 0.00361528\n",
      "Iteration 25, loss = 0.00394979\n",
      "Iteration 26, loss = 0.00394040\n",
      "Iteration 27, loss = 0.00361327\n",
      "Iteration 28, loss = 0.00322981\n",
      "Iteration 29, loss = 0.00303050\n",
      "Iteration 30, loss = 0.00306994\n",
      "Iteration 31, loss = 0.00322311\n",
      "Iteration 32, loss = 0.00332366\n",
      "Iteration 33, loss = 0.00327578\n",
      "Iteration 34, loss = 0.00310173\n",
      "Iteration 35, loss = 0.00290383\n",
      "Iteration 36, loss = 0.00280655\n",
      "Iteration 37, loss = 0.00285895\n",
      "Iteration 38, loss = 0.00294020\n",
      "Iteration 39, loss = 0.00294888\n",
      "Iteration 40, loss = 0.00287895\n",
      "Iteration 41, loss = 0.00277940\n",
      "Iteration 42, loss = 0.00269600\n",
      "Iteration 43, loss = 0.00266085\n",
      "Iteration 44, loss = 0.00266918\n",
      "Iteration 45, loss = 0.00268783\n",
      "Iteration 46, loss = 0.00268287\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499383\n",
      "Iteration 2, loss = 0.05255321\n",
      "Iteration 3, loss = 0.01592614\n",
      "Iteration 4, loss = 0.00589419\n",
      "Iteration 5, loss = 0.01355553\n",
      "Iteration 6, loss = 0.01611502\n",
      "Iteration 7, loss = 0.01106374\n",
      "Iteration 8, loss = 0.00550404\n",
      "Iteration 9, loss = 0.00423578\n",
      "Iteration 10, loss = 0.00636814\n",
      "Iteration 11, loss = 0.00807756\n",
      "Iteration 12, loss = 0.00748271\n",
      "Iteration 13, loss = 0.00554801\n",
      "Iteration 14, loss = 0.00402713\n",
      "Iteration 15, loss = 0.00376477\n",
      "Iteration 16, loss = 0.00442704\n",
      "Iteration 17, loss = 0.00523798\n",
      "Iteration 18, loss = 0.00557834\n",
      "Iteration 19, loss = 0.00525408\n",
      "Iteration 20, loss = 0.00446791\n",
      "Iteration 21, loss = 0.00363857\n",
      "Iteration 22, loss = 0.00317027\n",
      "Iteration 23, loss = 0.00322471\n",
      "Iteration 24, loss = 0.00361517\n",
      "Iteration 25, loss = 0.00394971\n",
      "Iteration 26, loss = 0.00394038\n",
      "Iteration 27, loss = 0.00361332\n",
      "Iteration 28, loss = 0.00322987\n",
      "Iteration 29, loss = 0.00303053\n",
      "Iteration 30, loss = 0.00306987\n",
      "Iteration 31, loss = 0.00322300\n",
      "Iteration 32, loss = 0.00332360\n",
      "Iteration 33, loss = 0.00327583\n",
      "Iteration 34, loss = 0.00310186\n",
      "Iteration 35, loss = 0.00290394\n",
      "Iteration 36, loss = 0.00280651\n",
      "Iteration 37, loss = 0.00285879\n",
      "Iteration 38, loss = 0.00294005\n",
      "Iteration 39, loss = 0.00294880\n",
      "Iteration 40, loss = 0.00287891\n",
      "Iteration 41, loss = 0.00277939\n",
      "Iteration 42, loss = 0.00269610\n",
      "Iteration 43, loss = 0.00266089\n",
      "Iteration 44, loss = 0.00266912\n",
      "Iteration 45, loss = 0.00268773\n",
      "Iteration 46, loss = 0.00268280\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499386\n",
      "Iteration 2, loss = 0.05254848\n",
      "Iteration 3, loss = 0.01592609\n",
      "Iteration 4, loss = 0.00589342\n",
      "Iteration 5, loss = 0.01355420\n",
      "Iteration 6, loss = 0.01611442\n",
      "Iteration 7, loss = 0.01106383\n",
      "Iteration 8, loss = 0.00550424\n",
      "Iteration 9, loss = 0.00423567\n",
      "Iteration 10, loss = 0.00636770\n",
      "Iteration 11, loss = 0.00807712\n",
      "Iteration 12, loss = 0.00748251\n",
      "Iteration 13, loss = 0.00554804\n",
      "Iteration 14, loss = 0.00402724\n",
      "Iteration 15, loss = 0.00376480\n",
      "Iteration 16, loss = 0.00442694\n",
      "Iteration 17, loss = 0.00523779\n",
      "Iteration 18, loss = 0.00557816\n",
      "Iteration 19, loss = 0.00525405\n",
      "Iteration 20, loss = 0.00446799\n",
      "Iteration 21, loss = 0.00363868\n",
      "Iteration 22, loss = 0.00317033\n",
      "Iteration 23, loss = 0.00322447\n",
      "Iteration 24, loss = 0.00361465\n",
      "Iteration 25, loss = 0.00394907\n",
      "Iteration 26, loss = 0.00393992\n",
      "Iteration 27, loss = 0.00361314\n",
      "Iteration 28, loss = 0.00322988\n",
      "Iteration 29, loss = 0.00303049\n",
      "Iteration 30, loss = 0.00306971\n",
      "Iteration 31, loss = 0.00322274\n",
      "Iteration 32, loss = 0.00332334\n",
      "Iteration 33, loss = 0.00327567\n",
      "Iteration 34, loss = 0.00310179\n",
      "Iteration 35, loss = 0.00290391\n",
      "Iteration 36, loss = 0.00280644\n",
      "Iteration 37, loss = 0.00285861\n",
      "Iteration 38, loss = 0.00293976\n",
      "Iteration 39, loss = 0.00294846\n",
      "Iteration 40, loss = 0.00287861\n",
      "Iteration 41, loss = 0.00277918\n",
      "Iteration 42, loss = 0.00269605\n",
      "Iteration 43, loss = 0.00266088\n",
      "Iteration 44, loss = 0.00266906\n",
      "Iteration 45, loss = 0.00268760\n",
      "Iteration 46, loss = 0.00268264\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499369\n",
      "Iteration 2, loss = 0.05255995\n",
      "Iteration 3, loss = 0.01592736\n",
      "Iteration 4, loss = 0.00589512\n",
      "Iteration 5, loss = 0.01355616\n",
      "Iteration 6, loss = 0.01611744\n",
      "Iteration 7, loss = 0.01106618\n",
      "Iteration 8, loss = 0.00550575\n",
      "Iteration 9, loss = 0.00423696\n",
      "Iteration 10, loss = 0.00636856\n",
      "Iteration 11, loss = 0.00807728\n",
      "Iteration 12, loss = 0.00748278\n",
      "Iteration 13, loss = 0.00554861\n",
      "Iteration 14, loss = 0.00402773\n",
      "Iteration 15, loss = 0.00376521\n",
      "Iteration 16, loss = 0.00442731\n",
      "Iteration 17, loss = 0.00523824\n",
      "Iteration 18, loss = 0.00557855\n",
      "Iteration 19, loss = 0.00525459\n",
      "Iteration 20, loss = 0.00446865\n",
      "Iteration 21, loss = 0.00363925\n",
      "Iteration 22, loss = 0.00317049\n",
      "Iteration 23, loss = 0.00322420\n",
      "Iteration 24, loss = 0.00361406\n",
      "Iteration 25, loss = 0.00394861\n",
      "Iteration 26, loss = 0.00393998\n",
      "Iteration 27, loss = 0.00361354\n",
      "Iteration 28, loss = 0.00323034\n",
      "Iteration 29, loss = 0.00303077\n",
      "Iteration 30, loss = 0.00306968\n",
      "Iteration 31, loss = 0.00322238\n",
      "Iteration 32, loss = 0.00332241\n",
      "Iteration 33, loss = 0.00327455\n",
      "Iteration 34, loss = 0.00310110\n",
      "Iteration 35, loss = 0.00290378\n",
      "Iteration 36, loss = 0.00280663\n",
      "Iteration 37, loss = 0.00285867\n",
      "Iteration 38, loss = 0.00293959\n",
      "Iteration 39, loss = 0.00294828\n",
      "Iteration 40, loss = 0.00287866\n",
      "Iteration 41, loss = 0.00277933\n",
      "Iteration 42, loss = 0.00269609\n",
      "Iteration 43, loss = 0.00266085\n",
      "Iteration 44, loss = 0.00266911\n",
      "Iteration 45, loss = 0.00268787\n",
      "Iteration 46, loss = 0.00268303\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499367\n",
      "Iteration 2, loss = 0.05256178\n",
      "Iteration 3, loss = 0.01592764\n",
      "Iteration 4, loss = 0.00589516\n",
      "Iteration 5, loss = 0.01355635\n",
      "Iteration 6, loss = 0.01611775\n",
      "Iteration 7, loss = 0.01106646\n",
      "Iteration 8, loss = 0.00550594\n",
      "Iteration 9, loss = 0.00423703\n",
      "Iteration 10, loss = 0.00636850\n",
      "Iteration 11, loss = 0.00807721\n",
      "Iteration 12, loss = 0.00748278\n",
      "Iteration 13, loss = 0.00554867\n",
      "Iteration 14, loss = 0.00402779\n",
      "Iteration 15, loss = 0.00376522\n",
      "Iteration 16, loss = 0.00442726\n",
      "Iteration 17, loss = 0.00523820\n",
      "Iteration 18, loss = 0.00557855\n",
      "Iteration 19, loss = 0.00525466\n",
      "Iteration 20, loss = 0.00446876\n",
      "Iteration 21, loss = 0.00363934\n",
      "Iteration 22, loss = 0.00317049\n",
      "Iteration 23, loss = 0.00322407\n",
      "Iteration 24, loss = 0.00361383\n",
      "Iteration 25, loss = 0.00394839\n",
      "Iteration 26, loss = 0.00393989\n",
      "Iteration 27, loss = 0.00361358\n",
      "Iteration 28, loss = 0.00323043\n",
      "Iteration 29, loss = 0.00303084\n",
      "Iteration 30, loss = 0.00306972\n",
      "Iteration 31, loss = 0.00322235\n",
      "Iteration 32, loss = 0.00332237\n",
      "Iteration 33, loss = 0.00327455\n",
      "Iteration 34, loss = 0.00310118\n",
      "Iteration 35, loss = 0.00290398\n",
      "Iteration 36, loss = 0.00280676\n",
      "Iteration 37, loss = 0.00285869\n",
      "Iteration 38, loss = 0.00293957\n",
      "Iteration 39, loss = 0.00294829\n",
      "Iteration 40, loss = 0.00287884\n",
      "Iteration 41, loss = 0.00277953\n",
      "Iteration 42, loss = 0.00269618\n",
      "Iteration 43, loss = 0.00266083\n",
      "Iteration 44, loss = 0.00266898\n",
      "Iteration 45, loss = 0.00268772\n",
      "Iteration 46, loss = 0.00268296\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499370\n",
      "Iteration 2, loss = 0.05255880\n",
      "Iteration 3, loss = 0.01592670\n",
      "Iteration 4, loss = 0.00589461\n",
      "Iteration 5, loss = 0.01355652\n",
      "Iteration 6, loss = 0.01611596\n",
      "Iteration 7, loss = 0.01106423\n",
      "Iteration 8, loss = 0.00550440\n",
      "Iteration 9, loss = 0.00423635\n",
      "Iteration 10, loss = 0.00636846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.00807738\n",
      "Iteration 12, loss = 0.00748239\n",
      "Iteration 13, loss = 0.00554787\n",
      "Iteration 14, loss = 0.00402728\n",
      "Iteration 15, loss = 0.00376504\n",
      "Iteration 16, loss = 0.00442741\n",
      "Iteration 17, loss = 0.00523842\n",
      "Iteration 18, loss = 0.00557883\n",
      "Iteration 19, loss = 0.00525458\n",
      "Iteration 20, loss = 0.00446824\n",
      "Iteration 21, loss = 0.00363870\n",
      "Iteration 22, loss = 0.00317032\n",
      "Iteration 23, loss = 0.00322475\n",
      "Iteration 24, loss = 0.00361535\n",
      "Iteration 25, loss = 0.00394990\n",
      "Iteration 26, loss = 0.00394058\n",
      "Iteration 27, loss = 0.00361349\n",
      "Iteration 28, loss = 0.00322997\n",
      "Iteration 29, loss = 0.00303069\n",
      "Iteration 30, loss = 0.00307011\n",
      "Iteration 31, loss = 0.00322322\n",
      "Iteration 32, loss = 0.00332433\n",
      "Iteration 33, loss = 0.00327698\n",
      "Iteration 34, loss = 0.00310304\n",
      "Iteration 35, loss = 0.00290480\n",
      "Iteration 36, loss = 0.00280669\n",
      "Iteration 37, loss = 0.00285917\n",
      "Iteration 38, loss = 0.00294093\n",
      "Iteration 39, loss = 0.00294995\n",
      "Iteration 40, loss = 0.00287997\n",
      "Iteration 41, loss = 0.00278007\n",
      "Iteration 42, loss = 0.00269635\n",
      "Iteration 43, loss = 0.00266095\n",
      "Iteration 44, loss = 0.00266932\n",
      "Iteration 45, loss = 0.00268826\n",
      "Iteration 46, loss = 0.00268356\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499380\n",
      "Iteration 2, loss = 0.05255295\n",
      "Iteration 3, loss = 0.01592635\n",
      "Iteration 4, loss = 0.00589350\n",
      "Iteration 5, loss = 0.01355502\n",
      "Iteration 6, loss = 0.01611523\n",
      "Iteration 7, loss = 0.01106433\n",
      "Iteration 8, loss = 0.00550463\n",
      "Iteration 9, loss = 0.00423624\n",
      "Iteration 10, loss = 0.00636793\n",
      "Iteration 11, loss = 0.00807678\n",
      "Iteration 12, loss = 0.00748209\n",
      "Iteration 13, loss = 0.00554789\n",
      "Iteration 14, loss = 0.00402752\n",
      "Iteration 15, loss = 0.00376518\n",
      "Iteration 16, loss = 0.00442736\n",
      "Iteration 17, loss = 0.00523824\n",
      "Iteration 18, loss = 0.00557866\n",
      "Iteration 19, loss = 0.00525453\n",
      "Iteration 20, loss = 0.00446830\n",
      "Iteration 21, loss = 0.00363877\n",
      "Iteration 22, loss = 0.00317029\n",
      "Iteration 23, loss = 0.00322451\n",
      "Iteration 24, loss = 0.00361496\n",
      "Iteration 25, loss = 0.00394955\n",
      "Iteration 26, loss = 0.00394043\n",
      "Iteration 27, loss = 0.00361354\n",
      "Iteration 28, loss = 0.00323009\n",
      "Iteration 29, loss = 0.00303073\n",
      "Iteration 30, loss = 0.00306997\n",
      "Iteration 31, loss = 0.00322296\n",
      "Iteration 32, loss = 0.00332405\n",
      "Iteration 33, loss = 0.00327676\n",
      "Iteration 34, loss = 0.00310284\n",
      "Iteration 35, loss = 0.00290453\n",
      "Iteration 36, loss = 0.00280645\n",
      "Iteration 37, loss = 0.00285896\n",
      "Iteration 38, loss = 0.00294074\n",
      "Iteration 39, loss = 0.00294976\n",
      "Iteration 40, loss = 0.00287958\n",
      "Iteration 41, loss = 0.00277962\n",
      "Iteration 42, loss = 0.00269608\n",
      "Iteration 43, loss = 0.00266086\n",
      "Iteration 44, loss = 0.00266932\n",
      "Iteration 45, loss = 0.00268822\n",
      "Iteration 46, loss = 0.00268338\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499382\n",
      "Iteration 2, loss = 0.05255028\n",
      "Iteration 3, loss = 0.01592626\n",
      "Iteration 4, loss = 0.00589326\n",
      "Iteration 5, loss = 0.01355451\n",
      "Iteration 6, loss = 0.01611479\n",
      "Iteration 7, loss = 0.01106416\n",
      "Iteration 8, loss = 0.00550463\n",
      "Iteration 9, loss = 0.00423624\n",
      "Iteration 10, loss = 0.00636790\n",
      "Iteration 11, loss = 0.00807673\n",
      "Iteration 12, loss = 0.00748203\n",
      "Iteration 13, loss = 0.00554785\n",
      "Iteration 14, loss = 0.00402750\n",
      "Iteration 15, loss = 0.00376521\n",
      "Iteration 16, loss = 0.00442743\n",
      "Iteration 17, loss = 0.00523827\n",
      "Iteration 18, loss = 0.00557862\n",
      "Iteration 19, loss = 0.00525445\n",
      "Iteration 20, loss = 0.00446822\n",
      "Iteration 21, loss = 0.00363873\n",
      "Iteration 22, loss = 0.00317031\n",
      "Iteration 23, loss = 0.00322458\n",
      "Iteration 24, loss = 0.00361502\n",
      "Iteration 25, loss = 0.00394958\n",
      "Iteration 26, loss = 0.00394040\n",
      "Iteration 27, loss = 0.00361350\n",
      "Iteration 28, loss = 0.00323008\n",
      "Iteration 29, loss = 0.00303076\n",
      "Iteration 30, loss = 0.00307003\n",
      "Iteration 31, loss = 0.00322302\n",
      "Iteration 32, loss = 0.00332407\n",
      "Iteration 33, loss = 0.00327673\n",
      "Iteration 34, loss = 0.00310282\n",
      "Iteration 35, loss = 0.00290456\n",
      "Iteration 36, loss = 0.00280653\n",
      "Iteration 37, loss = 0.00285904\n",
      "Iteration 38, loss = 0.00294080\n",
      "Iteration 39, loss = 0.00294981\n",
      "Iteration 40, loss = 0.00287959\n",
      "Iteration 41, loss = 0.00277963\n",
      "Iteration 42, loss = 0.00269608\n",
      "Iteration 43, loss = 0.00266086\n",
      "Iteration 44, loss = 0.00266934\n",
      "Iteration 45, loss = 0.00268835\n",
      "Iteration 46, loss = 0.00268358\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499234\n",
      "Iteration 2, loss = 0.05259074\n",
      "Iteration 3, loss = 0.01593058\n",
      "Iteration 4, loss = 0.00589881\n",
      "Iteration 5, loss = 0.01356494\n",
      "Iteration 6, loss = 0.01612019\n",
      "Iteration 7, loss = 0.01106456\n",
      "Iteration 8, loss = 0.00550357\n",
      "Iteration 9, loss = 0.00423739\n",
      "Iteration 10, loss = 0.00637149\n",
      "Iteration 11, loss = 0.00808004\n",
      "Iteration 12, loss = 0.00748298\n",
      "Iteration 13, loss = 0.00554671\n",
      "Iteration 14, loss = 0.00402591\n",
      "Iteration 15, loss = 0.00376468\n",
      "Iteration 16, loss = 0.00442809\n",
      "Iteration 17, loss = 0.00523973\n",
      "Iteration 18, loss = 0.00557995\n",
      "Iteration 19, loss = 0.00525509\n",
      "Iteration 20, loss = 0.00446808\n",
      "Iteration 21, loss = 0.00363812\n",
      "Iteration 22, loss = 0.00316976\n",
      "Iteration 23, loss = 0.00322453\n",
      "Iteration 24, loss = 0.00361543\n",
      "Iteration 25, loss = 0.00395015\n",
      "Iteration 26, loss = 0.00394070\n",
      "Iteration 27, loss = 0.00361337\n",
      "Iteration 28, loss = 0.00322963\n",
      "Iteration 29, loss = 0.00303031\n",
      "Iteration 30, loss = 0.00306977\n",
      "Iteration 31, loss = 0.00322295\n",
      "Iteration 32, loss = 0.00332348\n",
      "Iteration 33, loss = 0.00327562\n",
      "Iteration 34, loss = 0.00310159\n",
      "Iteration 35, loss = 0.00290362\n",
      "Iteration 36, loss = 0.00280606\n",
      "Iteration 37, loss = 0.00285841\n",
      "Iteration 38, loss = 0.00293975\n",
      "Iteration 39, loss = 0.00294852\n",
      "Iteration 40, loss = 0.00287836\n",
      "Iteration 41, loss = 0.00277872\n",
      "Iteration 42, loss = 0.00269555\n",
      "Iteration 43, loss = 0.00266037\n",
      "Iteration 44, loss = 0.00266865\n",
      "Iteration 45, loss = 0.00268728\n",
      "Iteration 46, loss = 0.00268249\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499253\n",
      "Iteration 2, loss = 0.05258414\n",
      "Iteration 3, loss = 0.01592934\n",
      "Iteration 4, loss = 0.00589752\n",
      "Iteration 5, loss = 0.01356133\n",
      "Iteration 6, loss = 0.01612086\n",
      "Iteration 7, loss = 0.01106737\n",
      "Iteration 8, loss = 0.00550606\n",
      "Iteration 9, loss = 0.00423784\n",
      "Iteration 10, loss = 0.00636992\n",
      "Iteration 11, loss = 0.00807818\n",
      "Iteration 12, loss = 0.00748317\n",
      "Iteration 13, loss = 0.00554852\n",
      "Iteration 14, loss = 0.00402744\n",
      "Iteration 15, loss = 0.00376528\n",
      "Iteration 16, loss = 0.00442783\n",
      "Iteration 17, loss = 0.00523924\n",
      "Iteration 18, loss = 0.00557964\n",
      "Iteration 19, loss = 0.00525549\n",
      "Iteration 20, loss = 0.00446909\n",
      "Iteration 21, loss = 0.00363922\n",
      "Iteration 22, loss = 0.00317028\n",
      "Iteration 23, loss = 0.00322431\n",
      "Iteration 24, loss = 0.00361478\n",
      "Iteration 25, loss = 0.00394966\n",
      "Iteration 26, loss = 0.00394092\n",
      "Iteration 27, loss = 0.00361380\n",
      "Iteration 28, loss = 0.00323008\n",
      "Iteration 29, loss = 0.00303052\n",
      "Iteration 30, loss = 0.00307012\n",
      "Iteration 31, loss = 0.00322334\n",
      "Iteration 32, loss = 0.00332320\n",
      "Iteration 33, loss = 0.00327482\n",
      "Iteration 34, loss = 0.00310077\n",
      "Iteration 35, loss = 0.00290318\n",
      "Iteration 36, loss = 0.00280598\n",
      "Iteration 37, loss = 0.00285826\n",
      "Iteration 38, loss = 0.00293941\n",
      "Iteration 39, loss = 0.00294808\n",
      "Iteration 40, loss = 0.00287817\n",
      "Iteration 41, loss = 0.00277853\n",
      "Iteration 42, loss = 0.00269522\n",
      "Iteration 43, loss = 0.00265996\n",
      "Iteration 44, loss = 0.00266819\n",
      "Iteration 45, loss = 0.00268682\n",
      "Iteration 46, loss = 0.00268162\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499259\n",
      "Iteration 2, loss = 0.05258331\n",
      "Iteration 3, loss = 0.01592845\n",
      "Iteration 4, loss = 0.00589668\n",
      "Iteration 5, loss = 0.01356108\n",
      "Iteration 6, loss = 0.01612136\n",
      "Iteration 7, loss = 0.01106807\n",
      "Iteration 8, loss = 0.00550686\n",
      "Iteration 9, loss = 0.00423865\n",
      "Iteration 10, loss = 0.00636991\n",
      "Iteration 11, loss = 0.00807737\n",
      "Iteration 12, loss = 0.00748245\n",
      "Iteration 13, loss = 0.00554868\n",
      "Iteration 14, loss = 0.00402846\n",
      "Iteration 15, loss = 0.00376619\n",
      "Iteration 16, loss = 0.00442835\n",
      "Iteration 17, loss = 0.00523951\n",
      "Iteration 18, loss = 0.00557996\n",
      "Iteration 19, loss = 0.00525617\n",
      "Iteration 20, loss = 0.00447062\n",
      "Iteration 21, loss = 0.00364080\n",
      "Iteration 22, loss = 0.00317100\n",
      "Iteration 23, loss = 0.00322387\n",
      "Iteration 24, loss = 0.00361358\n",
      "Iteration 25, loss = 0.00394876\n",
      "Iteration 26, loss = 0.00394093\n",
      "Iteration 27, loss = 0.00361458\n",
      "Iteration 28, loss = 0.00323098\n",
      "Iteration 29, loss = 0.00303103\n",
      "Iteration 30, loss = 0.00306998\n",
      "Iteration 31, loss = 0.00322301\n",
      "Iteration 32, loss = 0.00332304\n",
      "Iteration 33, loss = 0.00327505\n",
      "Iteration 34, loss = 0.00310139\n",
      "Iteration 35, loss = 0.00290382\n",
      "Iteration 36, loss = 0.00280629\n",
      "Iteration 37, loss = 0.00285831\n",
      "Iteration 38, loss = 0.00293956\n",
      "Iteration 39, loss = 0.00294834\n",
      "Iteration 40, loss = 0.00287876\n",
      "Iteration 41, loss = 0.00277921\n",
      "Iteration 42, loss = 0.00269582\n",
      "Iteration 43, loss = 0.00266031\n",
      "Iteration 44, loss = 0.00266847\n",
      "Iteration 45, loss = 0.00268736\n",
      "Iteration 46, loss = 0.00268255\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499250\n",
      "Iteration 2, loss = 0.05258274\n",
      "Iteration 3, loss = 0.01592981\n",
      "Iteration 4, loss = 0.00589646\n",
      "Iteration 5, loss = 0.01356148\n",
      "Iteration 6, loss = 0.01612223\n",
      "Iteration 7, loss = 0.01106875\n",
      "Iteration 8, loss = 0.00550745\n",
      "Iteration 9, loss = 0.00423921\n",
      "Iteration 10, loss = 0.00637018\n",
      "Iteration 11, loss = 0.00807740\n",
      "Iteration 12, loss = 0.00748257\n",
      "Iteration 13, loss = 0.00554890\n",
      "Iteration 14, loss = 0.00402892\n",
      "Iteration 15, loss = 0.00376667\n",
      "Iteration 16, loss = 0.00442874\n",
      "Iteration 17, loss = 0.00523982\n",
      "Iteration 18, loss = 0.00558020\n",
      "Iteration 19, loss = 0.00525642\n",
      "Iteration 20, loss = 0.00447086\n",
      "Iteration 21, loss = 0.00364096\n",
      "Iteration 22, loss = 0.00317107\n",
      "Iteration 23, loss = 0.00322386\n",
      "Iteration 24, loss = 0.00361356\n",
      "Iteration 25, loss = 0.00394878\n",
      "Iteration 26, loss = 0.00394104\n",
      "Iteration 27, loss = 0.00361474\n",
      "Iteration 28, loss = 0.00323115\n",
      "Iteration 29, loss = 0.00303114\n",
      "Iteration 30, loss = 0.00307003\n",
      "Iteration 31, loss = 0.00322296\n",
      "Iteration 32, loss = 0.00332291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33, loss = 0.00327489\n",
      "Iteration 34, loss = 0.00310126\n",
      "Iteration 35, loss = 0.00290375\n",
      "Iteration 36, loss = 0.00280622\n",
      "Iteration 37, loss = 0.00285813\n",
      "Iteration 38, loss = 0.00293928\n",
      "Iteration 39, loss = 0.00294802\n",
      "Iteration 40, loss = 0.00287830\n",
      "Iteration 41, loss = 0.00277879\n",
      "Iteration 42, loss = 0.00269557\n",
      "Iteration 43, loss = 0.00266022\n",
      "Iteration 44, loss = 0.00266846\n",
      "Iteration 45, loss = 0.00268729\n",
      "Iteration 46, loss = 0.00268240\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499217\n",
      "Iteration 2, loss = 0.05258108\n",
      "Iteration 3, loss = 0.01593147\n",
      "Iteration 4, loss = 0.00589601\n",
      "Iteration 5, loss = 0.01356133\n",
      "Iteration 6, loss = 0.01612209\n",
      "Iteration 7, loss = 0.01106920\n",
      "Iteration 8, loss = 0.00550826\n",
      "Iteration 9, loss = 0.00423981\n",
      "Iteration 10, loss = 0.00637055\n",
      "Iteration 11, loss = 0.00807782\n",
      "Iteration 12, loss = 0.00748280\n",
      "Iteration 13, loss = 0.00554889\n",
      "Iteration 14, loss = 0.00402893\n",
      "Iteration 15, loss = 0.00376716\n",
      "Iteration 16, loss = 0.00442953\n",
      "Iteration 17, loss = 0.00524061\n",
      "Iteration 18, loss = 0.00558068\n",
      "Iteration 19, loss = 0.00525655\n",
      "Iteration 20, loss = 0.00447079\n",
      "Iteration 21, loss = 0.00364090\n",
      "Iteration 22, loss = 0.00317117\n",
      "Iteration 23, loss = 0.00322417\n",
      "Iteration 24, loss = 0.00361390\n",
      "Iteration 25, loss = 0.00394896\n",
      "Iteration 26, loss = 0.00394099\n",
      "Iteration 27, loss = 0.00361455\n",
      "Iteration 28, loss = 0.00323104\n",
      "Iteration 29, loss = 0.00303121\n",
      "Iteration 30, loss = 0.00307016\n",
      "Iteration 31, loss = 0.00322316\n",
      "Iteration 32, loss = 0.00332308\n",
      "Iteration 33, loss = 0.00327497\n",
      "Iteration 34, loss = 0.00310134\n",
      "Iteration 35, loss = 0.00290392\n",
      "Iteration 36, loss = 0.00280646\n",
      "Iteration 37, loss = 0.00285835\n",
      "Iteration 38, loss = 0.00293943\n",
      "Iteration 39, loss = 0.00294815\n",
      "Iteration 40, loss = 0.00287838\n",
      "Iteration 41, loss = 0.00277895\n",
      "Iteration 42, loss = 0.00269573\n",
      "Iteration 43, loss = 0.00266034\n",
      "Iteration 44, loss = 0.00266842\n",
      "Iteration 45, loss = 0.00268716\n",
      "Iteration 46, loss = 0.00268227\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499326\n",
      "Iteration 2, loss = 0.05255771\n",
      "Iteration 3, loss = 0.01593076\n",
      "Iteration 4, loss = 0.00589334\n",
      "Iteration 5, loss = 0.01355786\n",
      "Iteration 6, loss = 0.01611687\n",
      "Iteration 7, loss = 0.01106403\n",
      "Iteration 8, loss = 0.00550574\n",
      "Iteration 9, loss = 0.00423912\n",
      "Iteration 10, loss = 0.00637002\n",
      "Iteration 11, loss = 0.00807645\n",
      "Iteration 12, loss = 0.00748027\n",
      "Iteration 13, loss = 0.00554644\n",
      "Iteration 14, loss = 0.00402797\n",
      "Iteration 15, loss = 0.00376695\n",
      "Iteration 16, loss = 0.00442934\n",
      "Iteration 17, loss = 0.00523988\n",
      "Iteration 18, loss = 0.00557965\n",
      "Iteration 19, loss = 0.00525528\n",
      "Iteration 20, loss = 0.00446952\n",
      "Iteration 21, loss = 0.00364005\n",
      "Iteration 22, loss = 0.00317100\n",
      "Iteration 23, loss = 0.00322445\n",
      "Iteration 24, loss = 0.00361427\n",
      "Iteration 25, loss = 0.00394894\n",
      "Iteration 26, loss = 0.00394041\n",
      "Iteration 27, loss = 0.00361409\n",
      "Iteration 28, loss = 0.00323083\n",
      "Iteration 29, loss = 0.00303140\n",
      "Iteration 30, loss = 0.00307024\n",
      "Iteration 31, loss = 0.00322303\n",
      "Iteration 32, loss = 0.00332411\n",
      "Iteration 33, loss = 0.00327717\n",
      "Iteration 34, loss = 0.00310379\n",
      "Iteration 35, loss = 0.00290557\n",
      "Iteration 36, loss = 0.00280691\n",
      "Iteration 37, loss = 0.00285886\n",
      "Iteration 38, loss = 0.00294063\n",
      "Iteration 39, loss = 0.00294981\n",
      "Iteration 40, loss = 0.00287994\n",
      "Iteration 41, loss = 0.00278020\n",
      "Iteration 42, loss = 0.00269660\n",
      "Iteration 43, loss = 0.00266111\n",
      "Iteration 44, loss = 0.00266932\n",
      "Iteration 45, loss = 0.00268845\n",
      "Iteration 46, loss = 0.00268409\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499320\n",
      "Iteration 2, loss = 0.05255347\n",
      "Iteration 3, loss = 0.01593037\n",
      "Iteration 4, loss = 0.00589254\n",
      "Iteration 5, loss = 0.01355578\n",
      "Iteration 6, loss = 0.01611686\n",
      "Iteration 7, loss = 0.01106528\n",
      "Iteration 8, loss = 0.00550685\n",
      "Iteration 9, loss = 0.00423945\n",
      "Iteration 10, loss = 0.00636850\n",
      "Iteration 11, loss = 0.00807457\n",
      "Iteration 12, loss = 0.00747980\n",
      "Iteration 13, loss = 0.00554729\n",
      "Iteration 14, loss = 0.00402893\n",
      "Iteration 15, loss = 0.00376713\n",
      "Iteration 16, loss = 0.00442892\n",
      "Iteration 17, loss = 0.00523921\n",
      "Iteration 18, loss = 0.00557906\n",
      "Iteration 19, loss = 0.00525508\n",
      "Iteration 20, loss = 0.00446952\n",
      "Iteration 21, loss = 0.00364000\n",
      "Iteration 22, loss = 0.00317088\n",
      "Iteration 23, loss = 0.00322427\n",
      "Iteration 24, loss = 0.00361415\n",
      "Iteration 25, loss = 0.00394894\n",
      "Iteration 26, loss = 0.00394045\n",
      "Iteration 27, loss = 0.00361410\n",
      "Iteration 28, loss = 0.00323078\n",
      "Iteration 29, loss = 0.00303135\n",
      "Iteration 30, loss = 0.00307015\n",
      "Iteration 31, loss = 0.00322299\n",
      "Iteration 32, loss = 0.00332413\n",
      "Iteration 33, loss = 0.00327717\n",
      "Iteration 34, loss = 0.00310374\n",
      "Iteration 35, loss = 0.00290550\n",
      "Iteration 36, loss = 0.00280688\n",
      "Iteration 37, loss = 0.00285892\n",
      "Iteration 38, loss = 0.00294070\n",
      "Iteration 39, loss = 0.00294983\n",
      "Iteration 40, loss = 0.00287978\n",
      "Iteration 41, loss = 0.00277997\n",
      "Iteration 42, loss = 0.00269640\n",
      "Iteration 43, loss = 0.00266098\n",
      "Iteration 44, loss = 0.00266925\n",
      "Iteration 45, loss = 0.00268832\n",
      "Iteration 46, loss = 0.00268388\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499038\n",
      "Iteration 2, loss = 0.05260355\n",
      "Iteration 3, loss = 0.01593399\n",
      "Iteration 4, loss = 0.00589775\n",
      "Iteration 5, loss = 0.01356744\n",
      "Iteration 6, loss = 0.01612572\n",
      "Iteration 7, loss = 0.01106899\n",
      "Iteration 8, loss = 0.00550805\n",
      "Iteration 9, loss = 0.00424223\n",
      "Iteration 10, loss = 0.00637327\n",
      "Iteration 11, loss = 0.00807820\n",
      "Iteration 12, loss = 0.00748179\n",
      "Iteration 13, loss = 0.00554756\n",
      "Iteration 14, loss = 0.00402839\n",
      "Iteration 15, loss = 0.00376766\n",
      "Iteration 16, loss = 0.00443060\n",
      "Iteration 17, loss = 0.00524166\n",
      "Iteration 18, loss = 0.00558118\n",
      "Iteration 19, loss = 0.00525668\n",
      "Iteration 20, loss = 0.00447077\n",
      "Iteration 21, loss = 0.00364095\n",
      "Iteration 22, loss = 0.00317120\n",
      "Iteration 23, loss = 0.00322408\n",
      "Iteration 24, loss = 0.00361362\n",
      "Iteration 25, loss = 0.00394864\n",
      "Iteration 26, loss = 0.00394092\n",
      "Iteration 27, loss = 0.00361480\n",
      "Iteration 28, loss = 0.00323143\n",
      "Iteration 29, loss = 0.00303160\n",
      "Iteration 30, loss = 0.00307030\n",
      "Iteration 31, loss = 0.00322277\n",
      "Iteration 32, loss = 0.00332230\n",
      "Iteration 33, loss = 0.00327417\n",
      "Iteration 34, loss = 0.00310082\n",
      "Iteration 35, loss = 0.00290374\n",
      "Iteration 36, loss = 0.00280635\n",
      "Iteration 37, loss = 0.00285809\n",
      "Iteration 38, loss = 0.00293901\n",
      "Iteration 39, loss = 0.00294764\n",
      "Iteration 40, loss = 0.00287800\n",
      "Iteration 41, loss = 0.00277867\n",
      "Iteration 42, loss = 0.00269570\n",
      "Iteration 43, loss = 0.00266026\n",
      "Iteration 44, loss = 0.00266829\n",
      "Iteration 45, loss = 0.00268692\n",
      "Iteration 46, loss = 0.00268198\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02498740\n",
      "Iteration 2, loss = 0.05262987\n",
      "Iteration 3, loss = 0.01593481\n",
      "Iteration 4, loss = 0.00589966\n",
      "Iteration 5, loss = 0.01357401\n",
      "Iteration 6, loss = 0.01613210\n",
      "Iteration 7, loss = 0.01107274\n",
      "Iteration 8, loss = 0.00550821\n",
      "Iteration 9, loss = 0.00423954\n",
      "Iteration 10, loss = 0.00637119\n",
      "Iteration 11, loss = 0.00807862\n",
      "Iteration 12, loss = 0.00748308\n",
      "Iteration 13, loss = 0.00554923\n",
      "Iteration 14, loss = 0.00402902\n",
      "Iteration 15, loss = 0.00376713\n",
      "Iteration 16, loss = 0.00442977\n",
      "Iteration 17, loss = 0.00524148\n",
      "Iteration 18, loss = 0.00558190\n",
      "Iteration 19, loss = 0.00525781\n",
      "Iteration 20, loss = 0.00447079\n",
      "Iteration 21, loss = 0.00363985\n",
      "Iteration 22, loss = 0.00316999\n",
      "Iteration 23, loss = 0.00322361\n",
      "Iteration 24, loss = 0.00361452\n",
      "Iteration 25, loss = 0.00395036\n",
      "Iteration 26, loss = 0.00394236\n",
      "Iteration 27, loss = 0.00361544\n",
      "Iteration 28, loss = 0.00323135\n",
      "Iteration 29, loss = 0.00303124\n",
      "Iteration 30, loss = 0.00306994\n",
      "Iteration 31, loss = 0.00322290\n",
      "Iteration 32, loss = 0.00332328\n",
      "Iteration 33, loss = 0.00327564\n",
      "Iteration 34, loss = 0.00310189\n",
      "Iteration 35, loss = 0.00290414\n",
      "Iteration 36, loss = 0.00280613\n",
      "Iteration 37, loss = 0.00285805\n",
      "Iteration 38, loss = 0.00293953\n",
      "Iteration 39, loss = 0.00294850\n",
      "Iteration 40, loss = 0.00287839\n",
      "Iteration 41, loss = 0.00277857\n",
      "Iteration 42, loss = 0.00269543\n",
      "Iteration 43, loss = 0.00265978\n",
      "Iteration 44, loss = 0.00266778\n",
      "Iteration 45, loss = 0.00268630\n",
      "Iteration 46, loss = 0.00268129\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02498934\n",
      "Iteration 2, loss = 0.05261067\n",
      "Iteration 3, loss = 0.01593372\n",
      "Iteration 4, loss = 0.00589733\n",
      "Iteration 5, loss = 0.01356748\n",
      "Iteration 6, loss = 0.01612972\n",
      "Iteration 7, loss = 0.01107448\n",
      "Iteration 8, loss = 0.00551131\n",
      "Iteration 9, loss = 0.00424126\n",
      "Iteration 10, loss = 0.00637030\n",
      "Iteration 11, loss = 0.00807702\n",
      "Iteration 12, loss = 0.00748407\n",
      "Iteration 13, loss = 0.00555209\n",
      "Iteration 14, loss = 0.00403132\n",
      "Iteration 15, loss = 0.00376809\n",
      "Iteration 16, loss = 0.00442927\n",
      "Iteration 17, loss = 0.00524052\n",
      "Iteration 18, loss = 0.00558161\n",
      "Iteration 19, loss = 0.00525882\n",
      "Iteration 20, loss = 0.00447369\n",
      "Iteration 21, loss = 0.00364318\n",
      "Iteration 22, loss = 0.00317178\n",
      "Iteration 23, loss = 0.00322307\n",
      "Iteration 24, loss = 0.00361219\n",
      "Iteration 25, loss = 0.00394817\n",
      "Iteration 26, loss = 0.00394189\n",
      "Iteration 27, loss = 0.00361654\n",
      "Iteration 28, loss = 0.00323297\n",
      "Iteration 29, loss = 0.00303222\n",
      "Iteration 30, loss = 0.00307007\n",
      "Iteration 31, loss = 0.00322265\n",
      "Iteration 32, loss = 0.00332286\n",
      "Iteration 33, loss = 0.00327539\n",
      "Iteration 34, loss = 0.00310222\n",
      "Iteration 35, loss = 0.00290478\n",
      "Iteration 36, loss = 0.00280675\n",
      "Iteration 37, loss = 0.00285818\n",
      "Iteration 38, loss = 0.00293936\n",
      "Iteration 39, loss = 0.00294838\n",
      "Iteration 40, loss = 0.00287893\n",
      "Iteration 41, loss = 0.00277950\n",
      "Iteration 42, loss = 0.00269623\n",
      "Iteration 43, loss = 0.00266060\n",
      "Iteration 44, loss = 0.00266856\n",
      "Iteration 45, loss = 0.00268735\n",
      "Iteration 46, loss = 0.00268262\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499023\n",
      "Iteration 2, loss = 0.05259556\n",
      "Iteration 3, loss = 0.01593360\n",
      "Iteration 4, loss = 0.00589524\n",
      "Iteration 5, loss = 0.01356442\n",
      "Iteration 6, loss = 0.01612699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.01107346\n",
      "Iteration 8, loss = 0.00551206\n",
      "Iteration 9, loss = 0.00424190\n",
      "Iteration 10, loss = 0.00636908\n",
      "Iteration 11, loss = 0.00807499\n",
      "Iteration 12, loss = 0.00748309\n",
      "Iteration 13, loss = 0.00555205\n",
      "Iteration 14, loss = 0.00403202\n",
      "Iteration 15, loss = 0.00376875\n",
      "Iteration 16, loss = 0.00442956\n",
      "Iteration 17, loss = 0.00524053\n",
      "Iteration 18, loss = 0.00558150\n",
      "Iteration 19, loss = 0.00525877\n",
      "Iteration 20, loss = 0.00447376\n",
      "Iteration 21, loss = 0.00364326\n",
      "Iteration 22, loss = 0.00317170\n",
      "Iteration 23, loss = 0.00322285\n",
      "Iteration 24, loss = 0.00361187\n",
      "Iteration 25, loss = 0.00394789\n",
      "Iteration 26, loss = 0.00394176\n",
      "Iteration 27, loss = 0.00361661\n",
      "Iteration 28, loss = 0.00323321\n",
      "Iteration 29, loss = 0.00303262\n",
      "Iteration 30, loss = 0.00307043\n",
      "Iteration 31, loss = 0.00322292\n",
      "Iteration 32, loss = 0.00332336\n",
      "Iteration 33, loss = 0.00327609\n",
      "Iteration 34, loss = 0.00310294\n",
      "Iteration 35, loss = 0.00290538\n",
      "Iteration 36, loss = 0.00280709\n",
      "Iteration 37, loss = 0.00285846\n",
      "Iteration 38, loss = 0.00293979\n",
      "Iteration 39, loss = 0.00294898\n",
      "Iteration 40, loss = 0.00287932\n",
      "Iteration 41, loss = 0.00277976\n",
      "Iteration 42, loss = 0.00269631\n",
      "Iteration 43, loss = 0.00266051\n",
      "Iteration 44, loss = 0.00266841\n",
      "Iteration 45, loss = 0.00268726\n",
      "Iteration 46, loss = 0.00268271\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499273\n",
      "Iteration 2, loss = 0.05255207\n",
      "Iteration 3, loss = 0.01593132\n",
      "Iteration 4, loss = 0.00588947\n",
      "Iteration 5, loss = 0.01355438\n",
      "Iteration 6, loss = 0.01611714\n",
      "Iteration 7, loss = 0.01106695\n",
      "Iteration 8, loss = 0.00550936\n",
      "Iteration 9, loss = 0.00424445\n",
      "Iteration 10, loss = 0.00638094\n",
      "Iteration 11, loss = 0.00808059\n",
      "Iteration 12, loss = 0.00746330\n",
      "Iteration 13, loss = 0.00553688\n",
      "Iteration 14, loss = 0.00402713\n",
      "Iteration 15, loss = 0.00377031\n",
      "Iteration 16, loss = 0.00443108\n",
      "Iteration 17, loss = 0.00523788\n",
      "Iteration 18, loss = 0.00557566\n",
      "Iteration 19, loss = 0.00525119\n",
      "Iteration 20, loss = 0.00446628\n",
      "Iteration 21, loss = 0.00363816\n",
      "Iteration 22, loss = 0.00316999\n",
      "Iteration 23, loss = 0.00322344\n",
      "Iteration 24, loss = 0.00361253\n",
      "Iteration 25, loss = 0.00394645\n",
      "Iteration 26, loss = 0.00393778\n",
      "Iteration 27, loss = 0.00361227\n",
      "Iteration 28, loss = 0.00323027\n",
      "Iteration 29, loss = 0.00303174\n",
      "Iteration 30, loss = 0.00307051\n",
      "Iteration 31, loss = 0.00322250\n",
      "Iteration 32, loss = 0.00332237\n",
      "Iteration 33, loss = 0.00327464\n",
      "Iteration 34, loss = 0.00310123\n",
      "Iteration 35, loss = 0.00290381\n",
      "Iteration 36, loss = 0.00280623\n",
      "Iteration 37, loss = 0.00285858\n",
      "Iteration 38, loss = 0.00293995\n",
      "Iteration 39, loss = 0.00294878\n",
      "Iteration 40, loss = 0.00287908\n",
      "Iteration 41, loss = 0.00277950\n",
      "Iteration 42, loss = 0.00269609\n",
      "Iteration 43, loss = 0.00266060\n",
      "Iteration 44, loss = 0.00266867\n",
      "Iteration 45, loss = 0.00268764\n",
      "Iteration 46, loss = 0.00268336\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499176\n",
      "Iteration 2, loss = 0.05256450\n",
      "Iteration 3, loss = 0.01593241\n",
      "Iteration 4, loss = 0.00589294\n",
      "Iteration 5, loss = 0.01356048\n",
      "Iteration 6, loss = 0.01612021\n",
      "Iteration 7, loss = 0.01106817\n",
      "Iteration 8, loss = 0.00550985\n",
      "Iteration 9, loss = 0.00424226\n",
      "Iteration 10, loss = 0.00636991\n",
      "Iteration 11, loss = 0.00807490\n",
      "Iteration 12, loss = 0.00748096\n",
      "Iteration 13, loss = 0.00554919\n",
      "Iteration 14, loss = 0.00403051\n",
      "Iteration 15, loss = 0.00376845\n",
      "Iteration 16, loss = 0.00443001\n",
      "Iteration 17, loss = 0.00524072\n",
      "Iteration 18, loss = 0.00558114\n",
      "Iteration 19, loss = 0.00525789\n",
      "Iteration 20, loss = 0.00447271\n",
      "Iteration 21, loss = 0.00364260\n",
      "Iteration 22, loss = 0.00317192\n",
      "Iteration 23, loss = 0.00322355\n",
      "Iteration 24, loss = 0.00361229\n",
      "Iteration 25, loss = 0.00394759\n",
      "Iteration 26, loss = 0.00394064\n",
      "Iteration 27, loss = 0.00361559\n",
      "Iteration 28, loss = 0.00323259\n",
      "Iteration 29, loss = 0.00303245\n",
      "Iteration 30, loss = 0.00307028\n",
      "Iteration 31, loss = 0.00322226\n",
      "Iteration 32, loss = 0.00332235\n",
      "Iteration 33, loss = 0.00327521\n",
      "Iteration 34, loss = 0.00310241\n",
      "Iteration 35, loss = 0.00290516\n",
      "Iteration 36, loss = 0.00280720\n",
      "Iteration 37, loss = 0.00285865\n",
      "Iteration 38, loss = 0.00293972\n",
      "Iteration 39, loss = 0.00294861\n",
      "Iteration 40, loss = 0.00287927\n",
      "Iteration 41, loss = 0.00278008\n",
      "Iteration 42, loss = 0.00269696\n",
      "Iteration 43, loss = 0.00266136\n",
      "Iteration 44, loss = 0.00266916\n",
      "Iteration 45, loss = 0.00268789\n",
      "Iteration 46, loss = 0.00268353\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499254\n",
      "Iteration 2, loss = 0.05255161\n",
      "Iteration 3, loss = 0.01592579\n",
      "Iteration 4, loss = 0.00589126\n",
      "Iteration 5, loss = 0.01355984\n",
      "Iteration 6, loss = 0.01611706\n",
      "Iteration 7, loss = 0.01106328\n",
      "Iteration 8, loss = 0.00550712\n",
      "Iteration 9, loss = 0.00424655\n",
      "Iteration 10, loss = 0.00638516\n",
      "Iteration 11, loss = 0.00808311\n",
      "Iteration 12, loss = 0.00746363\n",
      "Iteration 13, loss = 0.00553554\n",
      "Iteration 14, loss = 0.00402630\n",
      "Iteration 15, loss = 0.00377138\n",
      "Iteration 16, loss = 0.00443393\n",
      "Iteration 17, loss = 0.00524109\n",
      "Iteration 18, loss = 0.00557779\n",
      "Iteration 19, loss = 0.00525175\n",
      "Iteration 20, loss = 0.00446590\n",
      "Iteration 21, loss = 0.00363788\n",
      "Iteration 22, loss = 0.00317053\n",
      "Iteration 23, loss = 0.00322496\n",
      "Iteration 24, loss = 0.00361427\n",
      "Iteration 25, loss = 0.00394757\n",
      "Iteration 26, loss = 0.00393798\n",
      "Iteration 27, loss = 0.00361214\n",
      "Iteration 28, loss = 0.00323050\n",
      "Iteration 29, loss = 0.00303254\n",
      "Iteration 30, loss = 0.00307165\n",
      "Iteration 31, loss = 0.00322299\n",
      "Iteration 32, loss = 0.00332169\n",
      "Iteration 33, loss = 0.00327319\n",
      "Iteration 34, loss = 0.00310000\n",
      "Iteration 35, loss = 0.00290356\n",
      "Iteration 36, loss = 0.00280696\n",
      "Iteration 37, loss = 0.00285944\n",
      "Iteration 38, loss = 0.00294023\n",
      "Iteration 39, loss = 0.00294848\n",
      "Iteration 40, loss = 0.00287874\n",
      "Iteration 41, loss = 0.00277942\n",
      "Iteration 42, loss = 0.00269641\n",
      "Iteration 43, loss = 0.00266106\n",
      "Iteration 44, loss = 0.00266901\n",
      "Iteration 45, loss = 0.00268769\n",
      "Iteration 46, loss = 0.00268320\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02498971\n",
      "Iteration 2, loss = 0.05259267\n",
      "Iteration 3, loss = 0.01593342\n",
      "Iteration 4, loss = 0.00589506\n",
      "Iteration 5, loss = 0.01356456\n",
      "Iteration 6, loss = 0.01612537\n",
      "Iteration 7, loss = 0.01107099\n",
      "Iteration 8, loss = 0.00551164\n",
      "Iteration 9, loss = 0.00424371\n",
      "Iteration 10, loss = 0.00636991\n",
      "Iteration 11, loss = 0.00807308\n",
      "Iteration 12, loss = 0.00748052\n",
      "Iteration 13, loss = 0.00555084\n",
      "Iteration 14, loss = 0.00403166\n",
      "Iteration 15, loss = 0.00376847\n",
      "Iteration 16, loss = 0.00442898\n",
      "Iteration 17, loss = 0.00523951\n",
      "Iteration 18, loss = 0.00558019\n",
      "Iteration 19, loss = 0.00525766\n",
      "Iteration 20, loss = 0.00447313\n",
      "Iteration 21, loss = 0.00364314\n",
      "Iteration 22, loss = 0.00317180\n",
      "Iteration 23, loss = 0.00322271\n",
      "Iteration 24, loss = 0.00361121\n",
      "Iteration 25, loss = 0.00394686\n",
      "Iteration 26, loss = 0.00394079\n",
      "Iteration 27, loss = 0.00361596\n",
      "Iteration 28, loss = 0.00323290\n",
      "Iteration 29, loss = 0.00303240\n",
      "Iteration 30, loss = 0.00307005\n",
      "Iteration 31, loss = 0.00322194\n",
      "Iteration 32, loss = 0.00332142\n",
      "Iteration 33, loss = 0.00327368\n",
      "Iteration 34, loss = 0.00310075\n",
      "Iteration 35, loss = 0.00290404\n",
      "Iteration 36, loss = 0.00280657\n",
      "Iteration 37, loss = 0.00285806\n",
      "Iteration 38, loss = 0.00293887\n",
      "Iteration 39, loss = 0.00294754\n",
      "Iteration 40, loss = 0.00287811\n",
      "Iteration 41, loss = 0.00277886\n",
      "Iteration 42, loss = 0.00269594\n",
      "Iteration 43, loss = 0.00266032\n",
      "Iteration 44, loss = 0.00266813\n",
      "Iteration 45, loss = 0.00268662\n",
      "Iteration 46, loss = 0.00268182\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02498086\n",
      "Iteration 2, loss = 0.05266834\n",
      "Iteration 3, loss = 0.01593679\n",
      "Iteration 4, loss = 0.00590054\n",
      "Iteration 5, loss = 0.01358680\n",
      "Iteration 6, loss = 0.01614147\n",
      "Iteration 7, loss = 0.01107480\n",
      "Iteration 8, loss = 0.00550889\n",
      "Iteration 9, loss = 0.00424009\n",
      "Iteration 10, loss = 0.00637225\n",
      "Iteration 11, loss = 0.00807805\n",
      "Iteration 12, loss = 0.00748336\n",
      "Iteration 13, loss = 0.00555079\n",
      "Iteration 14, loss = 0.00403150\n",
      "Iteration 15, loss = 0.00376896\n",
      "Iteration 16, loss = 0.00443203\n",
      "Iteration 17, loss = 0.00524477\n",
      "Iteration 18, loss = 0.00558590\n",
      "Iteration 19, loss = 0.00526230\n",
      "Iteration 20, loss = 0.00447415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.00364186\n",
      "Iteration 22, loss = 0.00317005\n",
      "Iteration 23, loss = 0.00322223\n",
      "Iteration 24, loss = 0.00361233\n",
      "Iteration 25, loss = 0.00394878\n",
      "Iteration 26, loss = 0.00394249\n",
      "Iteration 27, loss = 0.00361682\n",
      "Iteration 28, loss = 0.00323289\n",
      "Iteration 29, loss = 0.00303186\n",
      "Iteration 30, loss = 0.00306908\n",
      "Iteration 31, loss = 0.00322161\n",
      "Iteration 32, loss = 0.00332251\n",
      "Iteration 33, loss = 0.00327598\n",
      "Iteration 34, loss = 0.00310309\n",
      "Iteration 35, loss = 0.00290533\n",
      "Iteration 36, loss = 0.00280637\n",
      "Iteration 37, loss = 0.00285733\n",
      "Iteration 38, loss = 0.00293886\n",
      "Iteration 39, loss = 0.00294849\n",
      "Iteration 40, loss = 0.00287906\n",
      "Iteration 41, loss = 0.00277949\n",
      "Iteration 42, loss = 0.00269595\n",
      "Iteration 43, loss = 0.00265993\n",
      "Iteration 44, loss = 0.00266765\n",
      "Iteration 45, loss = 0.00268635\n",
      "Iteration 46, loss = 0.00268206\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02498813\n",
      "Iteration 2, loss = 0.05261044\n",
      "Iteration 3, loss = 0.01593304\n",
      "Iteration 4, loss = 0.00589558\n",
      "Iteration 5, loss = 0.01356759\n",
      "Iteration 6, loss = 0.01612938\n",
      "Iteration 7, loss = 0.01107477\n",
      "Iteration 8, loss = 0.00551313\n",
      "Iteration 9, loss = 0.00424322\n",
      "Iteration 10, loss = 0.00636861\n",
      "Iteration 11, loss = 0.00807337\n",
      "Iteration 12, loss = 0.00748229\n",
      "Iteration 13, loss = 0.00555329\n",
      "Iteration 14, loss = 0.00403311\n",
      "Iteration 15, loss = 0.00376908\n",
      "Iteration 16, loss = 0.00442886\n",
      "Iteration 17, loss = 0.00523957\n",
      "Iteration 18, loss = 0.00558101\n",
      "Iteration 19, loss = 0.00525921\n",
      "Iteration 20, loss = 0.00447485\n",
      "Iteration 21, loss = 0.00364424\n",
      "Iteration 22, loss = 0.00317174\n",
      "Iteration 23, loss = 0.00322161\n",
      "Iteration 24, loss = 0.00360991\n",
      "Iteration 25, loss = 0.00394636\n",
      "Iteration 26, loss = 0.00394151\n",
      "Iteration 27, loss = 0.00361730\n",
      "Iteration 28, loss = 0.00323399\n",
      "Iteration 29, loss = 0.00303260\n",
      "Iteration 30, loss = 0.00306953\n",
      "Iteration 31, loss = 0.00322147\n",
      "Iteration 32, loss = 0.00332145\n",
      "Iteration 33, loss = 0.00327426\n",
      "Iteration 34, loss = 0.00310149\n",
      "Iteration 35, loss = 0.00290451\n",
      "Iteration 36, loss = 0.00280657\n",
      "Iteration 37, loss = 0.00285784\n",
      "Iteration 38, loss = 0.00293895\n",
      "Iteration 39, loss = 0.00294792\n",
      "Iteration 40, loss = 0.00287854\n",
      "Iteration 41, loss = 0.00277919\n",
      "Iteration 42, loss = 0.00269598\n",
      "Iteration 43, loss = 0.00266032\n",
      "Iteration 44, loss = 0.00266811\n",
      "Iteration 45, loss = 0.00268679\n",
      "Iteration 46, loss = 0.00268214\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02497962\n",
      "Iteration 2, loss = 0.05266776\n",
      "Iteration 3, loss = 0.01593629\n",
      "Iteration 4, loss = 0.00590064\n",
      "Iteration 5, loss = 0.01358962\n",
      "Iteration 6, loss = 0.01614156\n",
      "Iteration 7, loss = 0.01107418\n",
      "Iteration 8, loss = 0.00550925\n",
      "Iteration 9, loss = 0.00424219\n",
      "Iteration 10, loss = 0.00637353\n",
      "Iteration 11, loss = 0.00807810\n",
      "Iteration 12, loss = 0.00748284\n",
      "Iteration 13, loss = 0.00555100\n",
      "Iteration 14, loss = 0.00403248\n",
      "Iteration 15, loss = 0.00377057\n",
      "Iteration 16, loss = 0.00443350\n",
      "Iteration 17, loss = 0.00524560\n",
      "Iteration 18, loss = 0.00558618\n",
      "Iteration 19, loss = 0.00526232\n",
      "Iteration 20, loss = 0.00447487\n",
      "Iteration 21, loss = 0.00364290\n",
      "Iteration 22, loss = 0.00317074\n",
      "Iteration 23, loss = 0.00322215\n",
      "Iteration 24, loss = 0.00361142\n",
      "Iteration 25, loss = 0.00394772\n",
      "Iteration 26, loss = 0.00394187\n",
      "Iteration 27, loss = 0.00361685\n",
      "Iteration 28, loss = 0.00323323\n",
      "Iteration 29, loss = 0.00303209\n",
      "Iteration 30, loss = 0.00306907\n",
      "Iteration 31, loss = 0.00322118\n",
      "Iteration 32, loss = 0.00332189\n",
      "Iteration 33, loss = 0.00327543\n",
      "Iteration 34, loss = 0.00310280\n",
      "Iteration 35, loss = 0.00290527\n",
      "Iteration 36, loss = 0.00280660\n",
      "Iteration 37, loss = 0.00285762\n",
      "Iteration 38, loss = 0.00293902\n",
      "Iteration 39, loss = 0.00294842\n",
      "Iteration 40, loss = 0.00287897\n",
      "Iteration 41, loss = 0.00277949\n",
      "Iteration 42, loss = 0.00269593\n",
      "Iteration 43, loss = 0.00266005\n",
      "Iteration 44, loss = 0.00266776\n",
      "Iteration 45, loss = 0.00268653\n",
      "Iteration 46, loss = 0.00268240\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02498458\n",
      "Iteration 2, loss = 0.05262424\n",
      "Iteration 3, loss = 0.01593036\n",
      "Iteration 4, loss = 0.00589747\n",
      "Iteration 5, loss = 0.01357811\n",
      "Iteration 6, loss = 0.01613225\n",
      "Iteration 7, loss = 0.01107054\n",
      "Iteration 8, loss = 0.00550981\n",
      "Iteration 9, loss = 0.00424512\n",
      "Iteration 10, loss = 0.00637288\n",
      "Iteration 11, loss = 0.00807599\n",
      "Iteration 12, loss = 0.00748143\n",
      "Iteration 13, loss = 0.00555102\n",
      "Iteration 14, loss = 0.00403221\n",
      "Iteration 15, loss = 0.00377093\n",
      "Iteration 16, loss = 0.00443265\n",
      "Iteration 17, loss = 0.00524336\n",
      "Iteration 18, loss = 0.00558359\n",
      "Iteration 19, loss = 0.00525966\n",
      "Iteration 20, loss = 0.00447373\n",
      "Iteration 21, loss = 0.00364295\n",
      "Iteration 22, loss = 0.00317183\n",
      "Iteration 23, loss = 0.00322378\n",
      "Iteration 24, loss = 0.00361331\n",
      "Iteration 25, loss = 0.00394924\n",
      "Iteration 26, loss = 0.00394254\n",
      "Iteration 27, loss = 0.00361706\n",
      "Iteration 28, loss = 0.00323356\n",
      "Iteration 29, loss = 0.00303309\n",
      "Iteration 30, loss = 0.00307063\n",
      "Iteration 31, loss = 0.00322243\n",
      "Iteration 32, loss = 0.00332296\n",
      "Iteration 33, loss = 0.00327624\n",
      "Iteration 34, loss = 0.00310374\n",
      "Iteration 35, loss = 0.00290628\n",
      "Iteration 36, loss = 0.00280760\n",
      "Iteration 37, loss = 0.00285898\n",
      "Iteration 38, loss = 0.00294045\n",
      "Iteration 39, loss = 0.00294956\n",
      "Iteration 40, loss = 0.00287980\n",
      "Iteration 41, loss = 0.00278023\n",
      "Iteration 42, loss = 0.00269720\n",
      "Iteration 43, loss = 0.00266153\n",
      "Iteration 44, loss = 0.00266941\n",
      "Iteration 45, loss = 0.00268817\n",
      "Iteration 46, loss = 0.00268402\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02498980\n",
      "Iteration 2, loss = 0.05257156\n",
      "Iteration 3, loss = 0.01593009\n",
      "Iteration 4, loss = 0.00589046\n",
      "Iteration 5, loss = 0.01355647\n",
      "Iteration 6, loss = 0.01611439\n",
      "Iteration 7, loss = 0.01106735\n",
      "Iteration 8, loss = 0.00551353\n",
      "Iteration 9, loss = 0.00424367\n",
      "Iteration 10, loss = 0.00636226\n",
      "Iteration 11, loss = 0.00806416\n",
      "Iteration 12, loss = 0.00747527\n",
      "Iteration 13, loss = 0.00554996\n",
      "Iteration 14, loss = 0.00403179\n",
      "Iteration 15, loss = 0.00376781\n",
      "Iteration 16, loss = 0.00442696\n",
      "Iteration 17, loss = 0.00523639\n",
      "Iteration 18, loss = 0.00557707\n",
      "Iteration 19, loss = 0.00525496\n",
      "Iteration 20, loss = 0.00447117\n",
      "Iteration 21, loss = 0.00364173\n",
      "Iteration 22, loss = 0.00317079\n",
      "Iteration 23, loss = 0.00322203\n",
      "Iteration 24, loss = 0.00361057\n",
      "Iteration 25, loss = 0.00394601\n",
      "Iteration 26, loss = 0.00393942\n",
      "Iteration 27, loss = 0.00361465\n",
      "Iteration 28, loss = 0.00323171\n",
      "Iteration 29, loss = 0.00303126\n",
      "Iteration 30, loss = 0.00306854\n",
      "Iteration 31, loss = 0.00322006\n",
      "Iteration 32, loss = 0.00332044\n",
      "Iteration 33, loss = 0.00327400\n",
      "Iteration 34, loss = 0.00310177\n",
      "Iteration 35, loss = 0.00290467\n",
      "Iteration 36, loss = 0.00280644\n",
      "Iteration 37, loss = 0.00285795\n",
      "Iteration 38, loss = 0.00293927\n",
      "Iteration 39, loss = 0.00294826\n",
      "Iteration 40, loss = 0.00287844\n",
      "Iteration 41, loss = 0.00277892\n",
      "Iteration 42, loss = 0.00269580\n",
      "Iteration 43, loss = 0.00266037\n",
      "Iteration 44, loss = 0.00266834\n",
      "Iteration 45, loss = 0.00268721\n",
      "Iteration 46, loss = 0.00268300\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02499129\n",
      "Iteration 2, loss = 0.05254859\n",
      "Iteration 3, loss = 0.01592765\n",
      "Iteration 4, loss = 0.00588616\n",
      "Iteration 5, loss = 0.01354892\n",
      "Iteration 6, loss = 0.01610992\n",
      "Iteration 7, loss = 0.01106340\n",
      "Iteration 8, loss = 0.00551205\n",
      "Iteration 9, loss = 0.00425061\n",
      "Iteration 10, loss = 0.00638063\n",
      "Iteration 11, loss = 0.00807437\n",
      "Iteration 12, loss = 0.00745831\n",
      "Iteration 13, loss = 0.00553546\n",
      "Iteration 14, loss = 0.00402679\n",
      "Iteration 15, loss = 0.00377060\n",
      "Iteration 16, loss = 0.00443209\n",
      "Iteration 17, loss = 0.00523865\n",
      "Iteration 18, loss = 0.00557525\n",
      "Iteration 19, loss = 0.00525018\n",
      "Iteration 20, loss = 0.00446545\n",
      "Iteration 21, loss = 0.00363787\n",
      "Iteration 22, loss = 0.00317020\n",
      "Iteration 23, loss = 0.00322383\n",
      "Iteration 24, loss = 0.00361261\n",
      "Iteration 25, loss = 0.00394557\n",
      "Iteration 26, loss = 0.00393661\n",
      "Iteration 27, loss = 0.00361107\n",
      "Iteration 28, loss = 0.00322961\n",
      "Iteration 29, loss = 0.00303175\n",
      "Iteration 30, loss = 0.00307029\n",
      "Iteration 31, loss = 0.00322199\n",
      "Iteration 32, loss = 0.00332064\n",
      "Iteration 33, loss = 0.00327262\n",
      "Iteration 34, loss = 0.00310020\n",
      "Iteration 35, loss = 0.00290479\n",
      "Iteration 36, loss = 0.00280768\n",
      "Iteration 37, loss = 0.00285864\n",
      "Iteration 38, loss = 0.00293905\n",
      "Iteration 39, loss = 0.00294805\n",
      "Iteration 40, loss = 0.00287787\n",
      "Iteration 41, loss = 0.00277826\n",
      "Iteration 42, loss = 0.00269496\n",
      "Iteration 43, loss = 0.00265914\n",
      "Iteration 44, loss = 0.00266741\n",
      "Iteration 45, loss = 0.00268641\n",
      "Iteration 46, loss = 0.00268209\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02498747\n",
      "Iteration 2, loss = 0.05257712\n",
      "Iteration 3, loss = 0.01592304\n",
      "Iteration 4, loss = 0.00589050\n",
      "Iteration 5, loss = 0.01356248\n",
      "Iteration 6, loss = 0.01611499\n",
      "Iteration 7, loss = 0.01106060\n",
      "Iteration 8, loss = 0.00550983\n",
      "Iteration 9, loss = 0.00424858\n",
      "Iteration 10, loss = 0.00636962\n",
      "Iteration 11, loss = 0.00806777\n",
      "Iteration 12, loss = 0.00747333\n",
      "Iteration 13, loss = 0.00554599\n",
      "Iteration 14, loss = 0.00403080\n",
      "Iteration 15, loss = 0.00377010\n",
      "Iteration 16, loss = 0.00443123\n",
      "Iteration 17, loss = 0.00524034\n",
      "Iteration 18, loss = 0.00557889\n",
      "Iteration 19, loss = 0.00525461\n",
      "Iteration 20, loss = 0.00446950\n",
      "Iteration 21, loss = 0.00364047\n",
      "Iteration 22, loss = 0.00317088\n",
      "Iteration 23, loss = 0.00322369\n",
      "Iteration 24, loss = 0.00361292\n",
      "Iteration 25, loss = 0.00394755\n",
      "Iteration 26, loss = 0.00393954\n",
      "Iteration 27, loss = 0.00361397\n",
      "Iteration 28, loss = 0.00323137\n",
      "Iteration 29, loss = 0.00303210\n",
      "Iteration 30, loss = 0.00307022\n",
      "Iteration 31, loss = 0.00322207\n",
      "Iteration 32, loss = 0.00332199\n",
      "Iteration 33, loss = 0.00327440\n",
      "Iteration 34, loss = 0.00310136\n",
      "Iteration 35, loss = 0.00290448\n",
      "Iteration 36, loss = 0.00280661\n",
      "Iteration 37, loss = 0.00285869\n",
      "Iteration 38, loss = 0.00294000\n",
      "Iteration 39, loss = 0.00294865\n",
      "Iteration 40, loss = 0.00287836\n",
      "Iteration 41, loss = 0.00277868\n",
      "Iteration 42, loss = 0.00269587\n",
      "Iteration 43, loss = 0.00266018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 44, loss = 0.00266807\n",
      "Iteration 45, loss = 0.00268670\n",
      "Iteration 46, loss = 0.00268200\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02498408\n",
      "Iteration 2, loss = 0.05261954\n",
      "Iteration 3, loss = 0.01592830\n",
      "Iteration 4, loss = 0.00589614\n",
      "Iteration 5, loss = 0.01357567\n",
      "Iteration 6, loss = 0.01612982\n",
      "Iteration 7, loss = 0.01106983\n",
      "Iteration 8, loss = 0.00551234\n",
      "Iteration 9, loss = 0.00424756\n",
      "Iteration 10, loss = 0.00637071\n",
      "Iteration 11, loss = 0.00807188\n",
      "Iteration 12, loss = 0.00747814\n",
      "Iteration 13, loss = 0.00554930\n",
      "Iteration 14, loss = 0.00403178\n",
      "Iteration 15, loss = 0.00377018\n",
      "Iteration 16, loss = 0.00443110\n",
      "Iteration 17, loss = 0.00524099\n",
      "Iteration 18, loss = 0.00558056\n",
      "Iteration 19, loss = 0.00525671\n",
      "Iteration 20, loss = 0.00447135\n",
      "Iteration 21, loss = 0.00364141\n",
      "Iteration 22, loss = 0.00317076\n",
      "Iteration 23, loss = 0.00322295\n",
      "Iteration 24, loss = 0.00361221\n",
      "Iteration 25, loss = 0.00394773\n",
      "Iteration 26, loss = 0.00394072\n",
      "Iteration 27, loss = 0.00361562\n",
      "Iteration 28, loss = 0.00323253\n",
      "Iteration 29, loss = 0.00303210\n",
      "Iteration 30, loss = 0.00306906\n",
      "Iteration 31, loss = 0.00322067\n",
      "Iteration 32, loss = 0.00332062\n",
      "Iteration 33, loss = 0.00327387\n",
      "Iteration 34, loss = 0.00310193\n",
      "Iteration 35, loss = 0.00290532\n",
      "Iteration 36, loss = 0.00280697\n",
      "Iteration 37, loss = 0.00285785\n",
      "Iteration 38, loss = 0.00293891\n",
      "Iteration 39, loss = 0.00294788\n",
      "Iteration 40, loss = 0.00287808\n",
      "Iteration 41, loss = 0.00277899\n",
      "Iteration 42, loss = 0.00269661\n",
      "Iteration 43, loss = 0.00266091\n",
      "Iteration 44, loss = 0.00266835\n",
      "Iteration 45, loss = 0.00268689\n",
      "Iteration 46, loss = 0.00268278\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02497690\n",
      "Iteration 2, loss = 0.05265451\n",
      "Iteration 3, loss = 0.01592729\n",
      "Iteration 4, loss = 0.00589905\n",
      "Iteration 5, loss = 0.01359102\n",
      "Iteration 6, loss = 0.01613689\n",
      "Iteration 7, loss = 0.01106608\n",
      "Iteration 8, loss = 0.00550758\n",
      "Iteration 9, loss = 0.00424759\n",
      "Iteration 10, loss = 0.00637553\n",
      "Iteration 11, loss = 0.00807520\n",
      "Iteration 12, loss = 0.00747830\n",
      "Iteration 13, loss = 0.00554800\n",
      "Iteration 14, loss = 0.00403188\n",
      "Iteration 15, loss = 0.00377198\n",
      "Iteration 16, loss = 0.00443535\n",
      "Iteration 17, loss = 0.00524610\n",
      "Iteration 18, loss = 0.00558504\n",
      "Iteration 19, loss = 0.00525949\n",
      "Iteration 20, loss = 0.00447207\n",
      "Iteration 21, loss = 0.00364090\n",
      "Iteration 22, loss = 0.00317030\n",
      "Iteration 23, loss = 0.00322314\n",
      "Iteration 24, loss = 0.00361311\n",
      "Iteration 25, loss = 0.00394955\n",
      "Iteration 26, loss = 0.00394308\n",
      "Iteration 27, loss = 0.00361721\n",
      "Iteration 28, loss = 0.00323352\n",
      "Iteration 29, loss = 0.00303291\n",
      "Iteration 30, loss = 0.00306995\n",
      "Iteration 31, loss = 0.00322197\n",
      "Iteration 32, loss = 0.00332213\n",
      "Iteration 33, loss = 0.00327500\n",
      "Iteration 34, loss = 0.00310248\n",
      "Iteration 35, loss = 0.00290579\n",
      "Iteration 36, loss = 0.00280759\n",
      "Iteration 37, loss = 0.00285866\n",
      "Iteration 38, loss = 0.00293973\n",
      "Iteration 39, loss = 0.00294877\n",
      "Iteration 40, loss = 0.00287909\n",
      "Iteration 41, loss = 0.00277981\n",
      "Iteration 42, loss = 0.00269706\n",
      "Iteration 43, loss = 0.00266104\n",
      "Iteration 44, loss = 0.00266852\n",
      "Iteration 45, loss = 0.00268702\n",
      "Iteration 46, loss = 0.00268312\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02497801\n",
      "Iteration 2, loss = 0.05265431\n",
      "Iteration 3, loss = 0.01593473\n",
      "Iteration 4, loss = 0.00589735\n",
      "Iteration 5, loss = 0.01358328\n",
      "Iteration 6, loss = 0.01613837\n",
      "Iteration 7, loss = 0.01107641\n",
      "Iteration 8, loss = 0.00551568\n",
      "Iteration 9, loss = 0.00424793\n",
      "Iteration 10, loss = 0.00636901\n",
      "Iteration 11, loss = 0.00807167\n",
      "Iteration 12, loss = 0.00747989\n",
      "Iteration 13, loss = 0.00555221\n",
      "Iteration 14, loss = 0.00403454\n",
      "Iteration 15, loss = 0.00377059\n",
      "Iteration 16, loss = 0.00443017\n",
      "Iteration 17, loss = 0.00524073\n",
      "Iteration 18, loss = 0.00558218\n",
      "Iteration 19, loss = 0.00526075\n",
      "Iteration 20, loss = 0.00447625\n",
      "Iteration 21, loss = 0.00364502\n",
      "Iteration 22, loss = 0.00317165\n",
      "Iteration 23, loss = 0.00322133\n",
      "Iteration 24, loss = 0.00361001\n",
      "Iteration 25, loss = 0.00394716\n",
      "Iteration 26, loss = 0.00394259\n",
      "Iteration 27, loss = 0.00361852\n",
      "Iteration 28, loss = 0.00323497\n",
      "Iteration 29, loss = 0.00303289\n",
      "Iteration 30, loss = 0.00306844\n",
      "Iteration 31, loss = 0.00321974\n",
      "Iteration 32, loss = 0.00332097\n",
      "Iteration 33, loss = 0.00327561\n",
      "Iteration 34, loss = 0.00310392\n",
      "Iteration 35, loss = 0.00290667\n",
      "Iteration 36, loss = 0.00280701\n",
      "Iteration 37, loss = 0.00285779\n",
      "Iteration 38, loss = 0.00293954\n",
      "Iteration 39, loss = 0.00294900\n",
      "Iteration 40, loss = 0.00287942\n",
      "Iteration 41, loss = 0.00277976\n",
      "Iteration 42, loss = 0.00269676\n",
      "Iteration 43, loss = 0.00266063\n",
      "Iteration 44, loss = 0.00266835\n",
      "Iteration 45, loss = 0.00268719\n",
      "Iteration 46, loss = 0.00268326\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02495402\n",
      "Iteration 2, loss = 0.05274216\n",
      "Iteration 3, loss = 0.01592881\n",
      "Iteration 4, loss = 0.00589986\n",
      "Iteration 5, loss = 0.01360845\n",
      "Iteration 6, loss = 0.01615290\n",
      "Iteration 7, loss = 0.01107337\n",
      "Iteration 8, loss = 0.00550714\n",
      "Iteration 9, loss = 0.00424850\n",
      "Iteration 10, loss = 0.00638960\n",
      "Iteration 11, loss = 0.00808679\n",
      "Iteration 12, loss = 0.00746792\n",
      "Iteration 13, loss = 0.00553895\n",
      "Iteration 14, loss = 0.00402844\n",
      "Iteration 15, loss = 0.00377206\n",
      "Iteration 16, loss = 0.00443616\n",
      "Iteration 17, loss = 0.00524547\n",
      "Iteration 18, loss = 0.00558410\n",
      "Iteration 19, loss = 0.00525991\n",
      "Iteration 20, loss = 0.00447289\n",
      "Iteration 21, loss = 0.00364096\n",
      "Iteration 22, loss = 0.00316796\n",
      "Iteration 23, loss = 0.00321736\n",
      "Iteration 24, loss = 0.00360569\n",
      "Iteration 25, loss = 0.00394282\n",
      "Iteration 26, loss = 0.00393919\n",
      "Iteration 27, loss = 0.00361595\n",
      "Iteration 28, loss = 0.00323244\n",
      "Iteration 29, loss = 0.00303029\n",
      "Iteration 30, loss = 0.00306511\n",
      "Iteration 31, loss = 0.00321685\n",
      "Iteration 32, loss = 0.00331771\n",
      "Iteration 33, loss = 0.00327227\n",
      "Iteration 34, loss = 0.00310113\n",
      "Iteration 35, loss = 0.00290465\n",
      "Iteration 36, loss = 0.00280471\n",
      "Iteration 37, loss = 0.00285411\n",
      "Iteration 38, loss = 0.00293541\n",
      "Iteration 39, loss = 0.00294606\n",
      "Iteration 40, loss = 0.00287641\n",
      "Iteration 41, loss = 0.00277695\n",
      "Iteration 42, loss = 0.00269305\n",
      "Iteration 43, loss = 0.00265626\n",
      "Iteration 44, loss = 0.00266377\n",
      "Iteration 45, loss = 0.00268240\n",
      "Iteration 46, loss = 0.00267900\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02498382\n",
      "Iteration 2, loss = 0.05260889\n",
      "Iteration 3, loss = 0.01592534\n",
      "Iteration 4, loss = 0.00589032\n",
      "Iteration 5, loss = 0.01356691\n",
      "Iteration 6, loss = 0.01612249\n",
      "Iteration 7, loss = 0.01107088\n",
      "Iteration 8, loss = 0.00551787\n",
      "Iteration 9, loss = 0.00424668\n",
      "Iteration 10, loss = 0.00636180\n",
      "Iteration 11, loss = 0.00806418\n",
      "Iteration 12, loss = 0.00747515\n",
      "Iteration 13, loss = 0.00555113\n",
      "Iteration 14, loss = 0.00403653\n",
      "Iteration 15, loss = 0.00377204\n",
      "Iteration 16, loss = 0.00442985\n",
      "Iteration 17, loss = 0.00523782\n",
      "Iteration 18, loss = 0.00557789\n",
      "Iteration 19, loss = 0.00525672\n",
      "Iteration 20, loss = 0.00447389\n",
      "Iteration 21, loss = 0.00364377\n",
      "Iteration 22, loss = 0.00317126\n",
      "Iteration 23, loss = 0.00322075\n",
      "Iteration 24, loss = 0.00360929\n",
      "Iteration 25, loss = 0.00394560\n",
      "Iteration 26, loss = 0.00394022\n",
      "Iteration 27, loss = 0.00361643\n",
      "Iteration 28, loss = 0.00323352\n",
      "Iteration 29, loss = 0.00303292\n",
      "Iteration 30, loss = 0.00306841\n",
      "Iteration 31, loss = 0.00321983\n",
      "Iteration 32, loss = 0.00331941\n",
      "Iteration 33, loss = 0.00327294\n",
      "Iteration 34, loss = 0.00310207\n",
      "Iteration 35, loss = 0.00290698\n",
      "Iteration 36, loss = 0.00280867\n",
      "Iteration 37, loss = 0.00285818\n",
      "Iteration 38, loss = 0.00293828\n",
      "Iteration 39, loss = 0.00294734\n",
      "Iteration 40, loss = 0.00287719\n",
      "Iteration 41, loss = 0.00277826\n",
      "Iteration 42, loss = 0.00269545\n",
      "Iteration 43, loss = 0.00265907\n",
      "Iteration 44, loss = 0.00266679\n",
      "Iteration 45, loss = 0.00268544\n",
      "Iteration 46, loss = 0.00268188\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02497378\n",
      "Iteration 2, loss = 0.05265405\n",
      "Iteration 3, loss = 0.01592930\n",
      "Iteration 4, loss = 0.00589580\n",
      "Iteration 5, loss = 0.01357806\n",
      "Iteration 6, loss = 0.01612856\n",
      "Iteration 7, loss = 0.01107304\n",
      "Iteration 8, loss = 0.00552006\n",
      "Iteration 9, loss = 0.00425129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.00636740\n",
      "Iteration 11, loss = 0.00806681\n",
      "Iteration 12, loss = 0.00747583\n",
      "Iteration 13, loss = 0.00555161\n",
      "Iteration 14, loss = 0.00403765\n",
      "Iteration 15, loss = 0.00377457\n",
      "Iteration 16, loss = 0.00443277\n",
      "Iteration 17, loss = 0.00524050\n",
      "Iteration 18, loss = 0.00558057\n",
      "Iteration 19, loss = 0.00525987\n",
      "Iteration 20, loss = 0.00447709\n",
      "Iteration 21, loss = 0.00364626\n",
      "Iteration 22, loss = 0.00317287\n",
      "Iteration 23, loss = 0.00322149\n",
      "Iteration 24, loss = 0.00360974\n",
      "Iteration 25, loss = 0.00394641\n",
      "Iteration 26, loss = 0.00394206\n",
      "Iteration 27, loss = 0.00361873\n",
      "Iteration 28, loss = 0.00323567\n",
      "Iteration 29, loss = 0.00303466\n",
      "Iteration 30, loss = 0.00306995\n",
      "Iteration 31, loss = 0.00322129\n",
      "Iteration 32, loss = 0.00332119\n",
      "Iteration 33, loss = 0.00327509\n",
      "Iteration 34, loss = 0.00310424\n",
      "Iteration 35, loss = 0.00290881\n",
      "Iteration 36, loss = 0.00281000\n",
      "Iteration 37, loss = 0.00285936\n",
      "Iteration 38, loss = 0.00293974\n",
      "Iteration 39, loss = 0.00294940\n",
      "Iteration 40, loss = 0.00287956\n",
      "Iteration 41, loss = 0.00278048\n",
      "Iteration 42, loss = 0.00269739\n",
      "Iteration 43, loss = 0.00266117\n",
      "Iteration 44, loss = 0.00266882\n",
      "Iteration 45, loss = 0.00268776\n",
      "Iteration 46, loss = 0.00268383\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02496567\n",
      "Iteration 2, loss = 0.05268500\n",
      "Iteration 3, loss = 0.01592483\n",
      "Iteration 4, loss = 0.00589633\n",
      "Iteration 5, loss = 0.01359089\n",
      "Iteration 6, loss = 0.01614117\n",
      "Iteration 7, loss = 0.01107819\n",
      "Iteration 8, loss = 0.00552009\n",
      "Iteration 9, loss = 0.00425124\n",
      "Iteration 10, loss = 0.00637057\n",
      "Iteration 11, loss = 0.00807056\n",
      "Iteration 12, loss = 0.00747851\n",
      "Iteration 13, loss = 0.00555341\n",
      "Iteration 14, loss = 0.00403899\n",
      "Iteration 15, loss = 0.00377541\n",
      "Iteration 16, loss = 0.00443325\n",
      "Iteration 17, loss = 0.00524095\n",
      "Iteration 18, loss = 0.00558067\n",
      "Iteration 19, loss = 0.00525912\n",
      "Iteration 20, loss = 0.00447561\n",
      "Iteration 21, loss = 0.00364461\n",
      "Iteration 22, loss = 0.00317141\n",
      "Iteration 23, loss = 0.00322064\n",
      "Iteration 24, loss = 0.00360942\n",
      "Iteration 25, loss = 0.00394621\n",
      "Iteration 26, loss = 0.00394168\n",
      "Iteration 27, loss = 0.00361810\n",
      "Iteration 28, loss = 0.00323483\n",
      "Iteration 29, loss = 0.00303347\n",
      "Iteration 30, loss = 0.00306871\n",
      "Iteration 31, loss = 0.00321995\n",
      "Iteration 32, loss = 0.00332009\n",
      "Iteration 33, loss = 0.00327440\n",
      "Iteration 34, loss = 0.00310385\n",
      "Iteration 35, loss = 0.00290842\n",
      "Iteration 36, loss = 0.00280962\n",
      "Iteration 37, loss = 0.00285926\n",
      "Iteration 38, loss = 0.00293972\n",
      "Iteration 39, loss = 0.00294929\n",
      "Iteration 40, loss = 0.00287934\n",
      "Iteration 41, loss = 0.00278013\n",
      "Iteration 42, loss = 0.00269638\n",
      "Iteration 43, loss = 0.00265977\n",
      "Iteration 44, loss = 0.00266747\n",
      "Iteration 45, loss = 0.00268660\n",
      "Iteration 46, loss = 0.00268317\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02495256\n",
      "Iteration 2, loss = 0.05271329\n",
      "Iteration 3, loss = 0.01592092\n",
      "Iteration 4, loss = 0.00589749\n",
      "Iteration 5, loss = 0.01360360\n",
      "Iteration 6, loss = 0.01614532\n",
      "Iteration 7, loss = 0.01107479\n",
      "Iteration 8, loss = 0.00551944\n",
      "Iteration 9, loss = 0.00425378\n",
      "Iteration 10, loss = 0.00637388\n",
      "Iteration 11, loss = 0.00807031\n",
      "Iteration 12, loss = 0.00747762\n",
      "Iteration 13, loss = 0.00555195\n",
      "Iteration 14, loss = 0.00403913\n",
      "Iteration 15, loss = 0.00377708\n",
      "Iteration 16, loss = 0.00443570\n",
      "Iteration 17, loss = 0.00524272\n",
      "Iteration 18, loss = 0.00558170\n",
      "Iteration 19, loss = 0.00525906\n",
      "Iteration 20, loss = 0.00447393\n",
      "Iteration 21, loss = 0.00364236\n",
      "Iteration 22, loss = 0.00316983\n",
      "Iteration 23, loss = 0.00321983\n",
      "Iteration 24, loss = 0.00360918\n",
      "Iteration 25, loss = 0.00394681\n",
      "Iteration 26, loss = 0.00394294\n",
      "Iteration 27, loss = 0.00361894\n",
      "Iteration 28, loss = 0.00323499\n",
      "Iteration 29, loss = 0.00303283\n",
      "Iteration 30, loss = 0.00306726\n",
      "Iteration 31, loss = 0.00321859\n",
      "Iteration 32, loss = 0.00331846\n",
      "Iteration 33, loss = 0.00327259\n",
      "Iteration 34, loss = 0.00310210\n",
      "Iteration 35, loss = 0.00290706\n",
      "Iteration 36, loss = 0.00280857\n",
      "Iteration 37, loss = 0.00285785\n",
      "Iteration 38, loss = 0.00293777\n",
      "Iteration 39, loss = 0.00294744\n",
      "Iteration 40, loss = 0.00287765\n",
      "Iteration 41, loss = 0.00277870\n",
      "Iteration 42, loss = 0.00269532\n",
      "Iteration 43, loss = 0.00265871\n",
      "Iteration 44, loss = 0.00266635\n",
      "Iteration 45, loss = 0.00268527\n",
      "Iteration 46, loss = 0.00268158\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02493914\n",
      "Iteration 2, loss = 0.05276081\n",
      "Iteration 3, loss = 0.01592372\n",
      "Iteration 4, loss = 0.00589664\n",
      "Iteration 5, loss = 0.01361511\n",
      "Iteration 6, loss = 0.01616055\n",
      "Iteration 7, loss = 0.01108434\n",
      "Iteration 8, loss = 0.00552165\n",
      "Iteration 9, loss = 0.00425466\n",
      "Iteration 10, loss = 0.00638945\n",
      "Iteration 11, loss = 0.00808585\n",
      "Iteration 12, loss = 0.00747199\n",
      "Iteration 13, loss = 0.00554734\n",
      "Iteration 14, loss = 0.00403777\n",
      "Iteration 15, loss = 0.00377824\n",
      "Iteration 16, loss = 0.00443779\n",
      "Iteration 17, loss = 0.00524400\n",
      "Iteration 18, loss = 0.00558264\n",
      "Iteration 19, loss = 0.00525924\n",
      "Iteration 20, loss = 0.00447333\n",
      "Iteration 21, loss = 0.00364104\n",
      "Iteration 22, loss = 0.00316814\n",
      "Iteration 23, loss = 0.00321819\n",
      "Iteration 24, loss = 0.00360737\n",
      "Iteration 25, loss = 0.00394463\n",
      "Iteration 26, loss = 0.00394089\n",
      "Iteration 27, loss = 0.00361725\n",
      "Iteration 28, loss = 0.00323359\n",
      "Iteration 29, loss = 0.00303156\n",
      "Iteration 30, loss = 0.00306630\n",
      "Iteration 31, loss = 0.00321756\n",
      "Iteration 32, loss = 0.00331777\n",
      "Iteration 33, loss = 0.00327151\n",
      "Iteration 34, loss = 0.00310069\n",
      "Iteration 35, loss = 0.00290549\n",
      "Iteration 36, loss = 0.00280692\n",
      "Iteration 37, loss = 0.00285664\n",
      "Iteration 38, loss = 0.00293696\n",
      "Iteration 39, loss = 0.00294679\n",
      "Iteration 40, loss = 0.00287703\n",
      "Iteration 41, loss = 0.00277746\n",
      "Iteration 42, loss = 0.00269392\n",
      "Iteration 43, loss = 0.00265761\n",
      "Iteration 44, loss = 0.00266536\n",
      "Iteration 45, loss = 0.00268420\n",
      "Iteration 46, loss = 0.00267990\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02496060\n",
      "Iteration 2, loss = 0.05266392\n",
      "Iteration 3, loss = 0.01591004\n",
      "Iteration 4, loss = 0.00589215\n",
      "Iteration 5, loss = 0.01360116\n",
      "Iteration 6, loss = 0.01614755\n",
      "Iteration 7, loss = 0.01108219\n",
      "Iteration 8, loss = 0.00552586\n",
      "Iteration 9, loss = 0.00425309\n",
      "Iteration 10, loss = 0.00636618\n",
      "Iteration 11, loss = 0.00806577\n",
      "Iteration 12, loss = 0.00748028\n",
      "Iteration 13, loss = 0.00556079\n",
      "Iteration 14, loss = 0.00404624\n",
      "Iteration 15, loss = 0.00377920\n",
      "Iteration 16, loss = 0.00443666\n",
      "Iteration 17, loss = 0.00524428\n",
      "Iteration 18, loss = 0.00558432\n",
      "Iteration 19, loss = 0.00526199\n",
      "Iteration 20, loss = 0.00447587\n",
      "Iteration 21, loss = 0.00364309\n",
      "Iteration 22, loss = 0.00316968\n",
      "Iteration 23, loss = 0.00322005\n",
      "Iteration 24, loss = 0.00361002\n",
      "Iteration 25, loss = 0.00394719\n",
      "Iteration 26, loss = 0.00394292\n",
      "Iteration 27, loss = 0.00361875\n",
      "Iteration 28, loss = 0.00323408\n",
      "Iteration 29, loss = 0.00303129\n",
      "Iteration 30, loss = 0.00306531\n",
      "Iteration 31, loss = 0.00321600\n",
      "Iteration 32, loss = 0.00331673\n",
      "Iteration 33, loss = 0.00327132\n",
      "Iteration 34, loss = 0.00310107\n",
      "Iteration 35, loss = 0.00290587\n",
      "Iteration 36, loss = 0.00280678\n",
      "Iteration 37, loss = 0.00285674\n",
      "Iteration 38, loss = 0.00293726\n",
      "Iteration 39, loss = 0.00294643\n",
      "Iteration 40, loss = 0.00287607\n",
      "Iteration 41, loss = 0.00277709\n",
      "Iteration 42, loss = 0.00269406\n",
      "Iteration 43, loss = 0.00265793\n",
      "Iteration 44, loss = 0.00266605\n",
      "Iteration 45, loss = 0.00268508\n",
      "Iteration 46, loss = 0.00268151\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02497494\n",
      "Iteration 2, loss = 0.05262164\n",
      "Iteration 3, loss = 0.01590668\n",
      "Iteration 4, loss = 0.00588387\n",
      "Iteration 5, loss = 0.01357947\n",
      "Iteration 6, loss = 0.01613578\n",
      "Iteration 7, loss = 0.01107850\n",
      "Iteration 8, loss = 0.00552141\n",
      "Iteration 9, loss = 0.00425229\n",
      "Iteration 10, loss = 0.00636991\n",
      "Iteration 11, loss = 0.00806244\n",
      "Iteration 12, loss = 0.00745469\n",
      "Iteration 13, loss = 0.00554165\n",
      "Iteration 14, loss = 0.00403558\n",
      "Iteration 15, loss = 0.00377035\n",
      "Iteration 16, loss = 0.00442304\n",
      "Iteration 17, loss = 0.00522644\n",
      "Iteration 18, loss = 0.00556666\n",
      "Iteration 19, loss = 0.00524712\n",
      "Iteration 20, loss = 0.00446511\n",
      "Iteration 21, loss = 0.00363580\n",
      "Iteration 22, loss = 0.00316341\n",
      "Iteration 23, loss = 0.00321257\n",
      "Iteration 24, loss = 0.00360009\n",
      "Iteration 25, loss = 0.00393609\n",
      "Iteration 26, loss = 0.00393333\n",
      "Iteration 27, loss = 0.00361214\n",
      "Iteration 28, loss = 0.00322892\n",
      "Iteration 29, loss = 0.00302617\n",
      "Iteration 30, loss = 0.00305956\n",
      "Iteration 31, loss = 0.00320937\n",
      "Iteration 32, loss = 0.00330937\n",
      "Iteration 33, loss = 0.00326394\n",
      "Iteration 34, loss = 0.00309427\n",
      "Iteration 35, loss = 0.00289988\n",
      "Iteration 36, loss = 0.00280149\n",
      "Iteration 37, loss = 0.00285123\n",
      "Iteration 38, loss = 0.00293157\n",
      "Iteration 39, loss = 0.00294140\n",
      "Iteration 40, loss = 0.00287192\n",
      "Iteration 41, loss = 0.00277322\n",
      "Iteration 42, loss = 0.00268965\n",
      "Iteration 43, loss = 0.00265283\n",
      "Iteration 44, loss = 0.00266049\n",
      "Iteration 45, loss = 0.00267984\n",
      "Iteration 46, loss = 0.00267621\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02498284\n",
      "Iteration 2, loss = 0.05254942\n",
      "Iteration 3, loss = 0.01589544\n",
      "Iteration 4, loss = 0.00587534\n",
      "Iteration 5, loss = 0.01355249\n",
      "Iteration 6, loss = 0.01610728\n",
      "Iteration 7, loss = 0.01106178\n",
      "Iteration 8, loss = 0.00552088\n",
      "Iteration 9, loss = 0.00424965\n",
      "Iteration 10, loss = 0.00635915\n",
      "Iteration 11, loss = 0.00804937\n",
      "Iteration 12, loss = 0.00744461\n",
      "Iteration 13, loss = 0.00553448\n",
      "Iteration 14, loss = 0.00403064\n",
      "Iteration 15, loss = 0.00376740\n",
      "Iteration 16, loss = 0.00442075\n",
      "Iteration 17, loss = 0.00522434\n",
      "Iteration 18, loss = 0.00556451\n",
      "Iteration 19, loss = 0.00524471\n",
      "Iteration 20, loss = 0.00446279\n",
      "Iteration 21, loss = 0.00363469\n",
      "Iteration 22, loss = 0.00316367\n",
      "Iteration 23, loss = 0.00321323\n",
      "Iteration 24, loss = 0.00360014\n",
      "Iteration 25, loss = 0.00393578\n",
      "Iteration 26, loss = 0.00393235\n",
      "Iteration 27, loss = 0.00361099\n",
      "Iteration 28, loss = 0.00322826\n",
      "Iteration 29, loss = 0.00302562\n",
      "Iteration 30, loss = 0.00305937\n",
      "Iteration 31, loss = 0.00320892\n",
      "Iteration 32, loss = 0.00330890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33, loss = 0.00326320\n",
      "Iteration 34, loss = 0.00309261\n",
      "Iteration 35, loss = 0.00289777\n",
      "Iteration 36, loss = 0.00280053\n",
      "Iteration 37, loss = 0.00285200\n",
      "Iteration 38, loss = 0.00293253\n",
      "Iteration 39, loss = 0.00294119\n",
      "Iteration 40, loss = 0.00287157\n",
      "Iteration 41, loss = 0.00277231\n",
      "Iteration 42, loss = 0.00268952\n",
      "Iteration 43, loss = 0.00265412\n",
      "Iteration 44, loss = 0.00266245\n",
      "Iteration 45, loss = 0.00268177\n",
      "Iteration 46, loss = 0.00267738\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02495421\n",
      "Iteration 2, loss = 0.05267922\n",
      "Iteration 3, loss = 0.01590163\n",
      "Iteration 4, loss = 0.00589316\n",
      "Iteration 5, loss = 0.01360436\n",
      "Iteration 6, loss = 0.01614701\n",
      "Iteration 7, loss = 0.01107964\n",
      "Iteration 8, loss = 0.00552636\n",
      "Iteration 9, loss = 0.00425584\n",
      "Iteration 10, loss = 0.00636701\n",
      "Iteration 11, loss = 0.00806260\n",
      "Iteration 12, loss = 0.00747669\n",
      "Iteration 13, loss = 0.00555971\n",
      "Iteration 14, loss = 0.00404825\n",
      "Iteration 15, loss = 0.00378210\n",
      "Iteration 16, loss = 0.00443814\n",
      "Iteration 17, loss = 0.00524456\n",
      "Iteration 18, loss = 0.00558420\n",
      "Iteration 19, loss = 0.00526260\n",
      "Iteration 20, loss = 0.00447682\n",
      "Iteration 21, loss = 0.00364414\n",
      "Iteration 22, loss = 0.00317062\n",
      "Iteration 23, loss = 0.00322045\n",
      "Iteration 24, loss = 0.00361027\n",
      "Iteration 25, loss = 0.00394825\n",
      "Iteration 26, loss = 0.00394541\n",
      "Iteration 27, loss = 0.00362259\n",
      "Iteration 28, loss = 0.00323797\n",
      "Iteration 29, loss = 0.00303387\n",
      "Iteration 30, loss = 0.00306600\n",
      "Iteration 31, loss = 0.00321661\n",
      "Iteration 32, loss = 0.00331770\n",
      "Iteration 33, loss = 0.00327323\n",
      "Iteration 34, loss = 0.00310400\n",
      "Iteration 35, loss = 0.00290928\n",
      "Iteration 36, loss = 0.00280981\n",
      "Iteration 37, loss = 0.00285822\n",
      "Iteration 38, loss = 0.00293817\n",
      "Iteration 39, loss = 0.00294783\n",
      "Iteration 40, loss = 0.00287763\n",
      "Iteration 41, loss = 0.00277826\n",
      "Iteration 42, loss = 0.00269519\n",
      "Iteration 43, loss = 0.00265925\n",
      "Iteration 44, loss = 0.00266738\n",
      "Iteration 45, loss = 0.00268645\n",
      "Iteration 46, loss = 0.00268326\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02494425\n",
      "Iteration 2, loss = 0.05271070\n",
      "Iteration 3, loss = 0.01590609\n",
      "Iteration 4, loss = 0.00589511\n",
      "Iteration 5, loss = 0.01361534\n",
      "Iteration 6, loss = 0.01615834\n",
      "Iteration 7, loss = 0.01108644\n",
      "Iteration 8, loss = 0.00552799\n",
      "Iteration 9, loss = 0.00425801\n",
      "Iteration 10, loss = 0.00637196\n",
      "Iteration 11, loss = 0.00806720\n",
      "Iteration 12, loss = 0.00747872\n",
      "Iteration 13, loss = 0.00555982\n",
      "Iteration 14, loss = 0.00404782\n",
      "Iteration 15, loss = 0.00378172\n",
      "Iteration 16, loss = 0.00443847\n",
      "Iteration 17, loss = 0.00524533\n",
      "Iteration 18, loss = 0.00558516\n",
      "Iteration 19, loss = 0.00526373\n",
      "Iteration 20, loss = 0.00447785\n",
      "Iteration 21, loss = 0.00364463\n",
      "Iteration 22, loss = 0.00317015\n",
      "Iteration 23, loss = 0.00321884\n",
      "Iteration 24, loss = 0.00360812\n",
      "Iteration 25, loss = 0.00394657\n",
      "Iteration 26, loss = 0.00394493\n",
      "Iteration 27, loss = 0.00362301\n",
      "Iteration 28, loss = 0.00323863\n",
      "Iteration 29, loss = 0.00303408\n",
      "Iteration 30, loss = 0.00306542\n",
      "Iteration 31, loss = 0.00321531\n",
      "Iteration 32, loss = 0.00331642\n",
      "Iteration 33, loss = 0.00327261\n",
      "Iteration 34, loss = 0.00310429\n",
      "Iteration 35, loss = 0.00290995\n",
      "Iteration 36, loss = 0.00280966\n",
      "Iteration 37, loss = 0.00285672\n",
      "Iteration 38, loss = 0.00293642\n",
      "Iteration 39, loss = 0.00294698\n",
      "Iteration 40, loss = 0.00287769\n",
      "Iteration 41, loss = 0.00277871\n",
      "Iteration 42, loss = 0.00269526\n",
      "Iteration 43, loss = 0.00265834\n",
      "Iteration 44, loss = 0.00266566\n",
      "Iteration 45, loss = 0.00268482\n",
      "Iteration 46, loss = 0.00268231\n",
      "Iteration 47, loss = 0.00264916\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02486375\n",
      "Iteration 2, loss = 0.05286738\n",
      "Iteration 3, loss = 0.01588019\n",
      "Iteration 4, loss = 0.00588672\n",
      "Iteration 5, loss = 0.01365303\n",
      "Iteration 6, loss = 0.01618232\n",
      "Iteration 7, loss = 0.01108227\n",
      "Iteration 8, loss = 0.00551167\n",
      "Iteration 9, loss = 0.00425443\n",
      "Iteration 10, loss = 0.00638988\n",
      "Iteration 11, loss = 0.00807977\n",
      "Iteration 12, loss = 0.00746452\n",
      "Iteration 13, loss = 0.00554474\n",
      "Iteration 14, loss = 0.00403900\n",
      "Iteration 15, loss = 0.00377995\n",
      "Iteration 16, loss = 0.00444014\n",
      "Iteration 17, loss = 0.00524438\n",
      "Iteration 18, loss = 0.00558097\n",
      "Iteration 19, loss = 0.00525737\n",
      "Iteration 20, loss = 0.00447070\n",
      "Iteration 21, loss = 0.00363641\n",
      "Iteration 22, loss = 0.00316103\n",
      "Iteration 23, loss = 0.00320948\n",
      "Iteration 24, loss = 0.00359774\n",
      "Iteration 25, loss = 0.00393650\n",
      "Iteration 26, loss = 0.00393634\n",
      "Iteration 27, loss = 0.00361485\n",
      "Iteration 28, loss = 0.00322973\n",
      "Iteration 29, loss = 0.00302389\n",
      "Iteration 30, loss = 0.00305500\n",
      "Iteration 31, loss = 0.00320539\n",
      "Iteration 32, loss = 0.00330694\n",
      "Iteration 33, loss = 0.00326322\n",
      "Iteration 34, loss = 0.00309515\n",
      "Iteration 35, loss = 0.00290051\n",
      "Iteration 36, loss = 0.00280033\n",
      "Iteration 37, loss = 0.00284703\n",
      "Iteration 38, loss = 0.00292738\n",
      "Iteration 39, loss = 0.00293875\n",
      "Iteration 40, loss = 0.00287048\n",
      "Iteration 41, loss = 0.00277050\n",
      "Iteration 42, loss = 0.00268596\n",
      "Iteration 43, loss = 0.00264858\n",
      "Iteration 44, loss = 0.00265649\n",
      "Iteration 45, loss = 0.00267578\n",
      "Iteration 46, loss = 0.00267249\n",
      "Iteration 47, loss = 0.00263856\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02488117\n",
      "Iteration 2, loss = 0.05282582\n",
      "Iteration 3, loss = 0.01589342\n",
      "Iteration 4, loss = 0.00588846\n",
      "Iteration 5, loss = 0.01365044\n",
      "Iteration 6, loss = 0.01618004\n",
      "Iteration 7, loss = 0.01107394\n",
      "Iteration 8, loss = 0.00551191\n",
      "Iteration 9, loss = 0.00426383\n",
      "Iteration 10, loss = 0.00639952\n",
      "Iteration 11, loss = 0.00808122\n",
      "Iteration 12, loss = 0.00746166\n",
      "Iteration 13, loss = 0.00554453\n",
      "Iteration 14, loss = 0.00404414\n",
      "Iteration 15, loss = 0.00378818\n",
      "Iteration 16, loss = 0.00444646\n",
      "Iteration 17, loss = 0.00524855\n",
      "Iteration 18, loss = 0.00558353\n",
      "Iteration 19, loss = 0.00525803\n",
      "Iteration 20, loss = 0.00447189\n",
      "Iteration 21, loss = 0.00364003\n",
      "Iteration 22, loss = 0.00316808\n",
      "Iteration 23, loss = 0.00321760\n",
      "Iteration 24, loss = 0.00360507\n",
      "Iteration 25, loss = 0.00394266\n",
      "Iteration 26, loss = 0.00394269\n",
      "Iteration 27, loss = 0.00362311\n",
      "Iteration 28, loss = 0.00324016\n",
      "Iteration 29, loss = 0.00303553\n",
      "Iteration 30, loss = 0.00306577\n",
      "Iteration 31, loss = 0.00321440\n",
      "Iteration 32, loss = 0.00331584\n",
      "Iteration 33, loss = 0.00327309\n",
      "Iteration 34, loss = 0.00310612\n",
      "Iteration 35, loss = 0.00291268\n",
      "Iteration 36, loss = 0.00281226\n",
      "Iteration 37, loss = 0.00285796\n",
      "Iteration 38, loss = 0.00293755\n",
      "Iteration 39, loss = 0.00294893\n",
      "Iteration 40, loss = 0.00288090\n",
      "Iteration 41, loss = 0.00278161\n",
      "Iteration 42, loss = 0.00269728\n",
      "Iteration 43, loss = 0.00266001\n",
      "Iteration 44, loss = 0.00266731\n",
      "Iteration 45, loss = 0.00268678\n",
      "Iteration 46, loss = 0.00268469\n",
      "Iteration 47, loss = 0.00265195\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02489281\n",
      "Iteration 2, loss = 0.05280883\n",
      "Iteration 3, loss = 0.01589825\n",
      "Iteration 4, loss = 0.00589197\n",
      "Iteration 5, loss = 0.01364045\n",
      "Iteration 6, loss = 0.01616506\n",
      "Iteration 7, loss = 0.01106765\n",
      "Iteration 8, loss = 0.00551399\n",
      "Iteration 9, loss = 0.00427107\n",
      "Iteration 10, loss = 0.00640367\n",
      "Iteration 11, loss = 0.00807854\n",
      "Iteration 12, loss = 0.00745492\n",
      "Iteration 13, loss = 0.00553871\n",
      "Iteration 14, loss = 0.00404180\n",
      "Iteration 15, loss = 0.00379105\n",
      "Iteration 16, loss = 0.00445307\n",
      "Iteration 17, loss = 0.00525469\n",
      "Iteration 18, loss = 0.00558699\n",
      "Iteration 19, loss = 0.00525702\n",
      "Iteration 20, loss = 0.00446715\n",
      "Iteration 21, loss = 0.00363527\n",
      "Iteration 22, loss = 0.00316592\n",
      "Iteration 23, loss = 0.00321871\n",
      "Iteration 24, loss = 0.00360709\n",
      "Iteration 25, loss = 0.00394300\n",
      "Iteration 26, loss = 0.00393969\n",
      "Iteration 27, loss = 0.00361864\n",
      "Iteration 28, loss = 0.00323700\n",
      "Iteration 29, loss = 0.00303380\n",
      "Iteration 30, loss = 0.00306548\n",
      "Iteration 31, loss = 0.00321424\n",
      "Iteration 32, loss = 0.00331363\n",
      "Iteration 33, loss = 0.00326901\n",
      "Iteration 34, loss = 0.00310034\n",
      "Iteration 35, loss = 0.00290710\n",
      "Iteration 36, loss = 0.00280865\n",
      "Iteration 37, loss = 0.00285622\n",
      "Iteration 38, loss = 0.00293547\n",
      "Iteration 39, loss = 0.00294538\n",
      "Iteration 40, loss = 0.00287641\n",
      "Iteration 41, loss = 0.00277702\n",
      "Iteration 42, loss = 0.00269286\n",
      "Iteration 43, loss = 0.00265582\n",
      "Iteration 44, loss = 0.00266354\n",
      "Iteration 45, loss = 0.00268318\n",
      "Iteration 46, loss = 0.00267987\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02489250\n",
      "Iteration 2, loss = 0.05276501\n",
      "Iteration 3, loss = 0.01589784\n",
      "Iteration 4, loss = 0.00589120\n",
      "Iteration 5, loss = 0.01363287\n",
      "Iteration 6, loss = 0.01615839\n",
      "Iteration 7, loss = 0.01107009\n",
      "Iteration 8, loss = 0.00551950\n",
      "Iteration 9, loss = 0.00426767\n",
      "Iteration 10, loss = 0.00639307\n",
      "Iteration 11, loss = 0.00806929\n",
      "Iteration 12, loss = 0.00745186\n",
      "Iteration 13, loss = 0.00554148\n",
      "Iteration 14, loss = 0.00404448\n",
      "Iteration 15, loss = 0.00379096\n",
      "Iteration 16, loss = 0.00444898\n",
      "Iteration 17, loss = 0.00525015\n",
      "Iteration 18, loss = 0.00558383\n",
      "Iteration 19, loss = 0.00525645\n",
      "Iteration 20, loss = 0.00446838\n",
      "Iteration 21, loss = 0.00363736\n",
      "Iteration 22, loss = 0.00316701\n",
      "Iteration 23, loss = 0.00321885\n",
      "Iteration 24, loss = 0.00360674\n",
      "Iteration 25, loss = 0.00394389\n",
      "Iteration 26, loss = 0.00394177\n",
      "Iteration 27, loss = 0.00362095\n",
      "Iteration 28, loss = 0.00323907\n",
      "Iteration 29, loss = 0.00303472\n",
      "Iteration 30, loss = 0.00306589\n",
      "Iteration 31, loss = 0.00321436\n",
      "Iteration 32, loss = 0.00331350\n",
      "Iteration 33, loss = 0.00326928\n",
      "Iteration 34, loss = 0.00310125\n",
      "Iteration 35, loss = 0.00290749\n",
      "Iteration 36, loss = 0.00280856\n",
      "Iteration 37, loss = 0.00285537\n",
      "Iteration 38, loss = 0.00293481\n",
      "Iteration 39, loss = 0.00294490\n",
      "Iteration 40, loss = 0.00287585\n",
      "Iteration 41, loss = 0.00277651\n",
      "Iteration 42, loss = 0.00269170\n",
      "Iteration 43, loss = 0.00265440\n",
      "Iteration 44, loss = 0.00266192\n",
      "Iteration 45, loss = 0.00268130\n",
      "Iteration 46, loss = 0.00267829\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02496863\n",
      "Iteration 2, loss = 0.05249418\n",
      "Iteration 3, loss = 0.01588301\n",
      "Iteration 4, loss = 0.00585844\n",
      "Iteration 5, loss = 0.01356616\n",
      "Iteration 6, loss = 0.01611921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.01105832\n",
      "Iteration 8, loss = 0.00549545\n",
      "Iteration 9, loss = 0.00425165\n",
      "Iteration 10, loss = 0.00638499\n",
      "Iteration 11, loss = 0.00804558\n",
      "Iteration 12, loss = 0.00739964\n",
      "Iteration 13, loss = 0.00547910\n",
      "Iteration 14, loss = 0.00398717\n",
      "Iteration 15, loss = 0.00374543\n",
      "Iteration 16, loss = 0.00441299\n",
      "Iteration 17, loss = 0.00522042\n",
      "Iteration 18, loss = 0.00555586\n",
      "Iteration 19, loss = 0.00522858\n",
      "Iteration 20, loss = 0.00444291\n",
      "Iteration 21, loss = 0.00361706\n",
      "Iteration 22, loss = 0.00314980\n",
      "Iteration 23, loss = 0.00320235\n",
      "Iteration 24, loss = 0.00359060\n",
      "Iteration 25, loss = 0.00392657\n",
      "Iteration 26, loss = 0.00392326\n",
      "Iteration 27, loss = 0.00360278\n",
      "Iteration 28, loss = 0.00322181\n",
      "Iteration 29, loss = 0.00302145\n",
      "Iteration 30, loss = 0.00305586\n",
      "Iteration 31, loss = 0.00320810\n",
      "Iteration 32, loss = 0.00330875\n",
      "Iteration 33, loss = 0.00326303\n",
      "Iteration 34, loss = 0.00309280\n",
      "Iteration 35, loss = 0.00289764\n",
      "Iteration 36, loss = 0.00279837\n",
      "Iteration 37, loss = 0.00284695\n",
      "Iteration 38, loss = 0.00292757\n",
      "Iteration 39, loss = 0.00293832\n",
      "Iteration 40, loss = 0.00287010\n",
      "Iteration 41, loss = 0.00277141\n",
      "Iteration 42, loss = 0.00268766\n",
      "Iteration 43, loss = 0.00265070\n",
      "Iteration 44, loss = 0.00265819\n",
      "Iteration 45, loss = 0.00267771\n",
      "Iteration 46, loss = 0.00267462\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02498298\n",
      "Iteration 2, loss = 0.05246443\n",
      "Iteration 3, loss = 0.01587628\n",
      "Iteration 4, loss = 0.00584621\n",
      "Iteration 5, loss = 0.01353853\n",
      "Iteration 6, loss = 0.01609919\n",
      "Iteration 7, loss = 0.01105046\n",
      "Iteration 8, loss = 0.00548646\n",
      "Iteration 9, loss = 0.00422789\n",
      "Iteration 10, loss = 0.00635339\n",
      "Iteration 11, loss = 0.00802022\n",
      "Iteration 12, loss = 0.00738873\n",
      "Iteration 13, loss = 0.00547281\n",
      "Iteration 14, loss = 0.00397818\n",
      "Iteration 15, loss = 0.00372821\n",
      "Iteration 16, loss = 0.00438992\n",
      "Iteration 17, loss = 0.00519946\n",
      "Iteration 18, loss = 0.00554198\n",
      "Iteration 19, loss = 0.00522113\n",
      "Iteration 20, loss = 0.00443936\n",
      "Iteration 21, loss = 0.00361090\n",
      "Iteration 22, loss = 0.00314028\n",
      "Iteration 23, loss = 0.00318511\n",
      "Iteration 24, loss = 0.00356707\n",
      "Iteration 25, loss = 0.00390451\n",
      "Iteration 26, loss = 0.00390861\n",
      "Iteration 27, loss = 0.00359544\n",
      "Iteration 28, loss = 0.00321646\n",
      "Iteration 29, loss = 0.00301333\n",
      "Iteration 30, loss = 0.00304178\n",
      "Iteration 31, loss = 0.00318955\n",
      "Iteration 32, loss = 0.00329043\n",
      "Iteration 33, loss = 0.00324887\n",
      "Iteration 34, loss = 0.00308270\n",
      "Iteration 35, loss = 0.00288940\n",
      "Iteration 36, loss = 0.00278935\n",
      "Iteration 37, loss = 0.00283618\n",
      "Iteration 38, loss = 0.00291607\n",
      "Iteration 39, loss = 0.00292639\n",
      "Iteration 40, loss = 0.00285723\n",
      "Iteration 41, loss = 0.00275820\n",
      "Iteration 42, loss = 0.00267588\n",
      "Iteration 43, loss = 0.00263923\n",
      "Iteration 44, loss = 0.00264700\n",
      "Iteration 45, loss = 0.00266532\n",
      "Iteration 46, loss = 0.00266200\n",
      "Iteration 47, loss = 0.00262868\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02492618\n",
      "Iteration 2, loss = 0.05265833\n",
      "Iteration 3, loss = 0.01589066\n",
      "Iteration 4, loss = 0.00588455\n",
      "Iteration 5, loss = 0.01359540\n",
      "Iteration 6, loss = 0.01611581\n",
      "Iteration 7, loss = 0.01104254\n",
      "Iteration 8, loss = 0.00551070\n",
      "Iteration 9, loss = 0.00426242\n",
      "Iteration 10, loss = 0.00636769\n",
      "Iteration 11, loss = 0.00804387\n",
      "Iteration 12, loss = 0.00744250\n",
      "Iteration 13, loss = 0.00552861\n",
      "Iteration 14, loss = 0.00402990\n",
      "Iteration 15, loss = 0.00378055\n",
      "Iteration 16, loss = 0.00444371\n",
      "Iteration 17, loss = 0.00524721\n",
      "Iteration 18, loss = 0.00557490\n",
      "Iteration 19, loss = 0.00524086\n",
      "Iteration 20, loss = 0.00444903\n",
      "Iteration 21, loss = 0.00362084\n",
      "Iteration 22, loss = 0.00315955\n",
      "Iteration 23, loss = 0.00322231\n",
      "Iteration 24, loss = 0.00361524\n",
      "Iteration 25, loss = 0.00394989\n",
      "Iteration 26, loss = 0.00393988\n",
      "Iteration 27, loss = 0.00361368\n",
      "Iteration 28, loss = 0.00323138\n",
      "Iteration 29, loss = 0.00303200\n",
      "Iteration 30, loss = 0.00306822\n",
      "Iteration 31, loss = 0.00321868\n",
      "Iteration 32, loss = 0.00331580\n",
      "Iteration 33, loss = 0.00326780\n",
      "Iteration 34, loss = 0.00309959\n",
      "Iteration 35, loss = 0.00291199\n",
      "Iteration 36, loss = 0.00280953\n",
      "Iteration 37, loss = 0.00284356\n",
      "Iteration 38, loss = 0.00292184\n",
      "Iteration 39, loss = 0.00295273\n",
      "Iteration 40, loss = 0.00290662\n",
      "Iteration 41, loss = 0.00280471\n",
      "Iteration 42, loss = 0.00270339\n",
      "Iteration 43, loss = 0.00265433\n",
      "Iteration 44, loss = 0.00266219\n",
      "Iteration 45, loss = 0.00269094\n",
      "Iteration 46, loss = 0.00269670\n",
      "Iteration 47, loss = 0.00266353\n",
      "Iteration 48, loss = 0.00261146\n",
      "Iteration 49, loss = 0.00257314\n",
      "Iteration 50, loss = 0.00256421\n",
      "Iteration 51, loss = 0.00257117\n",
      "Iteration 52, loss = 0.00256734\n",
      "Iteration 53, loss = 0.00254007\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02484505\n",
      "Iteration 2, loss = 0.05283085\n",
      "Iteration 3, loss = 0.01588040\n",
      "Iteration 4, loss = 0.00588829\n",
      "Iteration 5, loss = 0.01365066\n",
      "Iteration 6, loss = 0.01616476\n",
      "Iteration 7, loss = 0.01106383\n",
      "Iteration 8, loss = 0.00551611\n",
      "Iteration 9, loss = 0.00427596\n",
      "Iteration 10, loss = 0.00640043\n",
      "Iteration 11, loss = 0.00807059\n",
      "Iteration 12, loss = 0.00744848\n",
      "Iteration 13, loss = 0.00553609\n",
      "Iteration 14, loss = 0.00404028\n",
      "Iteration 15, loss = 0.00379092\n",
      "Iteration 16, loss = 0.00445178\n",
      "Iteration 17, loss = 0.00525243\n",
      "Iteration 18, loss = 0.00558459\n",
      "Iteration 19, loss = 0.00525629\n",
      "Iteration 20, loss = 0.00446812\n",
      "Iteration 21, loss = 0.00363619\n",
      "Iteration 22, loss = 0.00316600\n",
      "Iteration 23, loss = 0.00321758\n",
      "Iteration 24, loss = 0.00360517\n",
      "Iteration 25, loss = 0.00394410\n",
      "Iteration 26, loss = 0.00394385\n",
      "Iteration 27, loss = 0.00362438\n",
      "Iteration 28, loss = 0.00324185\n",
      "Iteration 29, loss = 0.00303637\n",
      "Iteration 30, loss = 0.00306640\n",
      "Iteration 31, loss = 0.00321447\n",
      "Iteration 32, loss = 0.00331435\n",
      "Iteration 33, loss = 0.00327117\n",
      "Iteration 34, loss = 0.00310443\n",
      "Iteration 35, loss = 0.00291096\n",
      "Iteration 36, loss = 0.00281096\n",
      "Iteration 37, loss = 0.00285663\n",
      "Iteration 38, loss = 0.00293668\n",
      "Iteration 39, loss = 0.00294795\n",
      "Iteration 40, loss = 0.00287954\n",
      "Iteration 41, loss = 0.00277998\n",
      "Iteration 42, loss = 0.00269459\n",
      "Iteration 43, loss = 0.00265659\n",
      "Iteration 44, loss = 0.00266433\n",
      "Iteration 45, loss = 0.00268403\n",
      "Iteration 46, loss = 0.00268217\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02476643\n",
      "Iteration 2, loss = 0.05294090\n",
      "Iteration 3, loss = 0.01583633\n",
      "Iteration 4, loss = 0.00587691\n",
      "Iteration 5, loss = 0.01368786\n",
      "Iteration 6, loss = 0.01620126\n",
      "Iteration 7, loss = 0.01107280\n",
      "Iteration 8, loss = 0.00550103\n",
      "Iteration 9, loss = 0.00425294\n",
      "Iteration 10, loss = 0.00638941\n",
      "Iteration 11, loss = 0.00807812\n",
      "Iteration 12, loss = 0.00746938\n",
      "Iteration 13, loss = 0.00555430\n",
      "Iteration 14, loss = 0.00404496\n",
      "Iteration 15, loss = 0.00377901\n",
      "Iteration 16, loss = 0.00443302\n",
      "Iteration 17, loss = 0.00523608\n",
      "Iteration 18, loss = 0.00557805\n",
      "Iteration 19, loss = 0.00526191\n",
      "Iteration 20, loss = 0.00448142\n",
      "Iteration 21, loss = 0.00364566\n",
      "Iteration 22, loss = 0.00316270\n",
      "Iteration 23, loss = 0.00320186\n",
      "Iteration 24, loss = 0.00358664\n",
      "Iteration 25, loss = 0.00393202\n",
      "Iteration 26, loss = 0.00394239\n",
      "Iteration 27, loss = 0.00362815\n",
      "Iteration 28, loss = 0.00324213\n",
      "Iteration 29, loss = 0.00302856\n",
      "Iteration 30, loss = 0.00305237\n",
      "Iteration 31, loss = 0.00320069\n",
      "Iteration 32, loss = 0.00330701\n",
      "Iteration 33, loss = 0.00327064\n",
      "Iteration 34, loss = 0.00310705\n",
      "Iteration 35, loss = 0.00291151\n",
      "Iteration 36, loss = 0.00280603\n",
      "Iteration 37, loss = 0.00284777\n",
      "Iteration 38, loss = 0.00292903\n",
      "Iteration 39, loss = 0.00294315\n",
      "Iteration 40, loss = 0.00287672\n",
      "Iteration 41, loss = 0.00277825\n",
      "Iteration 42, loss = 0.00269353\n",
      "Iteration 43, loss = 0.00265451\n",
      "Iteration 44, loss = 0.00266072\n",
      "Iteration 45, loss = 0.00268056\n",
      "Iteration 46, loss = 0.00267950\n",
      "Iteration 47, loss = 0.00264796\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02472404\n",
      "Iteration 2, loss = 0.05292516\n",
      "Iteration 3, loss = 0.01583441\n",
      "Iteration 4, loss = 0.00586290\n",
      "Iteration 5, loss = 0.01366065\n",
      "Iteration 6, loss = 0.01617933\n",
      "Iteration 7, loss = 0.01106462\n",
      "Iteration 8, loss = 0.00550512\n",
      "Iteration 9, loss = 0.00425960\n",
      "Iteration 10, loss = 0.00638964\n",
      "Iteration 11, loss = 0.00806921\n",
      "Iteration 12, loss = 0.00745227\n",
      "Iteration 13, loss = 0.00554179\n",
      "Iteration 14, loss = 0.00404164\n",
      "Iteration 15, loss = 0.00378487\n",
      "Iteration 16, loss = 0.00444078\n",
      "Iteration 17, loss = 0.00524283\n",
      "Iteration 18, loss = 0.00558200\n",
      "Iteration 19, loss = 0.00526168\n",
      "Iteration 20, loss = 0.00447810\n",
      "Iteration 21, loss = 0.00364329\n",
      "Iteration 22, loss = 0.00316362\n",
      "Iteration 23, loss = 0.00320430\n",
      "Iteration 24, loss = 0.00358802\n",
      "Iteration 25, loss = 0.00393223\n",
      "Iteration 26, loss = 0.00394333\n",
      "Iteration 27, loss = 0.00363083\n",
      "Iteration 28, loss = 0.00324658\n",
      "Iteration 29, loss = 0.00303329\n",
      "Iteration 30, loss = 0.00305699\n",
      "Iteration 31, loss = 0.00320448\n",
      "Iteration 32, loss = 0.00330932\n",
      "Iteration 33, loss = 0.00327299\n",
      "Iteration 34, loss = 0.00310930\n",
      "Iteration 35, loss = 0.00291344\n",
      "Iteration 36, loss = 0.00280852\n",
      "Iteration 37, loss = 0.00285112\n",
      "Iteration 38, loss = 0.00293331\n",
      "Iteration 39, loss = 0.00294772\n",
      "Iteration 40, loss = 0.00288107\n",
      "Iteration 41, loss = 0.00278287\n",
      "Iteration 42, loss = 0.00269744\n",
      "Iteration 43, loss = 0.00265783\n",
      "Iteration 44, loss = 0.00266373\n",
      "Iteration 45, loss = 0.00268317\n",
      "Iteration 46, loss = 0.00268303\n",
      "Iteration 47, loss = 0.00265272\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02475680\n",
      "Iteration 2, loss = 0.05288019\n",
      "Iteration 3, loss = 0.01585260\n",
      "Iteration 4, loss = 0.00587959\n",
      "Iteration 5, loss = 0.01365701\n",
      "Iteration 6, loss = 0.01617905\n",
      "Iteration 7, loss = 0.01107874\n",
      "Iteration 8, loss = 0.00551973\n",
      "Iteration 9, loss = 0.00426697\n",
      "Iteration 10, loss = 0.00639408\n",
      "Iteration 11, loss = 0.00807142\n",
      "Iteration 12, loss = 0.00745581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.00554541\n",
      "Iteration 14, loss = 0.00404591\n",
      "Iteration 15, loss = 0.00378959\n",
      "Iteration 16, loss = 0.00444464\n",
      "Iteration 17, loss = 0.00524504\n",
      "Iteration 18, loss = 0.00558366\n",
      "Iteration 19, loss = 0.00526417\n",
      "Iteration 20, loss = 0.00448126\n",
      "Iteration 21, loss = 0.00364715\n",
      "Iteration 22, loss = 0.00316711\n",
      "Iteration 23, loss = 0.00320723\n",
      "Iteration 24, loss = 0.00359079\n",
      "Iteration 25, loss = 0.00393475\n",
      "Iteration 26, loss = 0.00394399\n",
      "Iteration 27, loss = 0.00362988\n",
      "Iteration 28, loss = 0.00324603\n",
      "Iteration 29, loss = 0.00303421\n",
      "Iteration 30, loss = 0.00305860\n",
      "Iteration 31, loss = 0.00320549\n",
      "Iteration 32, loss = 0.00330923\n",
      "Iteration 33, loss = 0.00327163\n",
      "Iteration 34, loss = 0.00310743\n",
      "Iteration 35, loss = 0.00291189\n",
      "Iteration 36, loss = 0.00280783\n",
      "Iteration 37, loss = 0.00285104\n",
      "Iteration 38, loss = 0.00293260\n",
      "Iteration 39, loss = 0.00294602\n",
      "Iteration 40, loss = 0.00287845\n",
      "Iteration 41, loss = 0.00277953\n",
      "Iteration 42, loss = 0.00269480\n",
      "Iteration 43, loss = 0.00265586\n",
      "Iteration 44, loss = 0.00266178\n",
      "Iteration 45, loss = 0.00268071\n",
      "Iteration 46, loss = 0.00267944\n",
      "Iteration 47, loss = 0.00264815\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02496893\n",
      "Iteration 2, loss = 0.05241868\n",
      "Iteration 3, loss = 0.01587493\n",
      "Iteration 4, loss = 0.00583374\n",
      "Iteration 5, loss = 0.01352380\n",
      "Iteration 6, loss = 0.01607938\n",
      "Iteration 7, loss = 0.01102621\n",
      "Iteration 8, loss = 0.00547439\n",
      "Iteration 9, loss = 0.00421694\n",
      "Iteration 10, loss = 0.00633245\n",
      "Iteration 11, loss = 0.00802175\n",
      "Iteration 12, loss = 0.00740916\n",
      "Iteration 13, loss = 0.00548729\n",
      "Iteration 14, loss = 0.00398631\n",
      "Iteration 15, loss = 0.00373785\n",
      "Iteration 16, loss = 0.00440476\n",
      "Iteration 17, loss = 0.00521098\n",
      "Iteration 18, loss = 0.00554044\n",
      "Iteration 19, loss = 0.00520959\n",
      "Iteration 20, loss = 0.00441707\n",
      "Iteration 21, loss = 0.00358415\n",
      "Iteration 22, loss = 0.00311904\n",
      "Iteration 23, loss = 0.00318075\n",
      "Iteration 24, loss = 0.00357843\n",
      "Iteration 25, loss = 0.00391750\n",
      "Iteration 26, loss = 0.00390881\n",
      "Iteration 27, loss = 0.00358086\n",
      "Iteration 28, loss = 0.00319679\n",
      "Iteration 29, loss = 0.00299896\n",
      "Iteration 30, loss = 0.00303937\n",
      "Iteration 31, loss = 0.00319231\n",
      "Iteration 32, loss = 0.00328933\n",
      "Iteration 33, loss = 0.00324041\n",
      "Iteration 34, loss = 0.00307190\n",
      "Iteration 35, loss = 0.00288528\n",
      "Iteration 36, loss = 0.00278595\n",
      "Iteration 37, loss = 0.00282360\n",
      "Iteration 38, loss = 0.00290282\n",
      "Iteration 39, loss = 0.00293313\n",
      "Iteration 40, loss = 0.00288509\n",
      "Iteration 41, loss = 0.00277948\n",
      "Iteration 42, loss = 0.00267843\n",
      "Iteration 43, loss = 0.00263142\n",
      "Iteration 44, loss = 0.00264189\n",
      "Iteration 45, loss = 0.00267192\n",
      "Iteration 46, loss = 0.00267778\n",
      "Iteration 47, loss = 0.00264374\n",
      "Iteration 48, loss = 0.00259141\n",
      "Iteration 49, loss = 0.00255382\n",
      "Iteration 50, loss = 0.00254648\n",
      "Iteration 51, loss = 0.00255517\n",
      "Iteration 52, loss = 0.00255274\n",
      "Iteration 53, loss = 0.00252696\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02488946\n",
      "Iteration 2, loss = 0.05263550\n",
      "Iteration 3, loss = 0.01586826\n",
      "Iteration 4, loss = 0.00588645\n",
      "Iteration 5, loss = 0.01360725\n",
      "Iteration 6, loss = 0.01613697\n",
      "Iteration 7, loss = 0.01106158\n",
      "Iteration 8, loss = 0.00551989\n",
      "Iteration 9, loss = 0.00426312\n",
      "Iteration 10, loss = 0.00636776\n",
      "Iteration 11, loss = 0.00805883\n",
      "Iteration 12, loss = 0.00746328\n",
      "Iteration 13, loss = 0.00554389\n",
      "Iteration 14, loss = 0.00404186\n",
      "Iteration 15, loss = 0.00379123\n",
      "Iteration 16, loss = 0.00445561\n",
      "Iteration 17, loss = 0.00526169\n",
      "Iteration 18, loss = 0.00559402\n",
      "Iteration 19, loss = 0.00526350\n",
      "Iteration 20, loss = 0.00447219\n",
      "Iteration 21, loss = 0.00363858\n",
      "Iteration 22, loss = 0.00317006\n",
      "Iteration 23, loss = 0.00322487\n",
      "Iteration 24, loss = 0.00361413\n",
      "Iteration 25, loss = 0.00395175\n",
      "Iteration 26, loss = 0.00394850\n",
      "Iteration 27, loss = 0.00362675\n",
      "Iteration 28, loss = 0.00324445\n",
      "Iteration 29, loss = 0.00304010\n",
      "Iteration 30, loss = 0.00307149\n",
      "Iteration 31, loss = 0.00321993\n",
      "Iteration 32, loss = 0.00331949\n",
      "Iteration 33, loss = 0.00327506\n",
      "Iteration 34, loss = 0.00310647\n",
      "Iteration 35, loss = 0.00291216\n",
      "Iteration 36, loss = 0.00281302\n",
      "Iteration 37, loss = 0.00286021\n",
      "Iteration 38, loss = 0.00293946\n",
      "Iteration 39, loss = 0.00294882\n",
      "Iteration 40, loss = 0.00287884\n",
      "Iteration 41, loss = 0.00277863\n",
      "Iteration 42, loss = 0.00269318\n",
      "Iteration 43, loss = 0.00265533\n",
      "Iteration 44, loss = 0.00266246\n",
      "Iteration 45, loss = 0.00268122\n",
      "Iteration 46, loss = 0.00267824\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02469479\n",
      "Iteration 2, loss = 0.05284878\n",
      "Iteration 3, loss = 0.01579857\n",
      "Iteration 4, loss = 0.00589673\n",
      "Iteration 5, loss = 0.01370645\n",
      "Iteration 6, loss = 0.01618570\n",
      "Iteration 7, loss = 0.01105006\n",
      "Iteration 8, loss = 0.00549558\n",
      "Iteration 9, loss = 0.00426594\n",
      "Iteration 10, loss = 0.00640389\n",
      "Iteration 11, loss = 0.00807821\n",
      "Iteration 12, loss = 0.00744766\n",
      "Iteration 13, loss = 0.00553028\n",
      "Iteration 14, loss = 0.00403751\n",
      "Iteration 15, loss = 0.00378882\n",
      "Iteration 16, loss = 0.00444558\n",
      "Iteration 17, loss = 0.00524188\n",
      "Iteration 18, loss = 0.00557444\n",
      "Iteration 19, loss = 0.00525131\n",
      "Iteration 20, loss = 0.00446889\n",
      "Iteration 21, loss = 0.00363543\n",
      "Iteration 22, loss = 0.00315770\n",
      "Iteration 23, loss = 0.00319997\n",
      "Iteration 24, loss = 0.00358492\n",
      "Iteration 25, loss = 0.00392786\n",
      "Iteration 26, loss = 0.00393588\n",
      "Iteration 27, loss = 0.00362135\n",
      "Iteration 28, loss = 0.00323684\n",
      "Iteration 29, loss = 0.00302486\n",
      "Iteration 30, loss = 0.00304817\n",
      "Iteration 31, loss = 0.00319395\n",
      "Iteration 32, loss = 0.00329737\n",
      "Iteration 33, loss = 0.00326031\n",
      "Iteration 34, loss = 0.00309739\n",
      "Iteration 35, loss = 0.00290307\n",
      "Iteration 36, loss = 0.00279875\n",
      "Iteration 37, loss = 0.00283978\n",
      "Iteration 38, loss = 0.00291960\n",
      "Iteration 39, loss = 0.00293184\n",
      "Iteration 40, loss = 0.00286444\n",
      "Iteration 41, loss = 0.00276692\n",
      "Iteration 42, loss = 0.00268304\n",
      "Iteration 43, loss = 0.00264363\n",
      "Iteration 44, loss = 0.00264875\n",
      "Iteration 45, loss = 0.00266729\n",
      "Iteration 46, loss = 0.00266697\n",
      "Iteration 47, loss = 0.00263706\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02454394\n",
      "Iteration 2, loss = 0.05297263\n",
      "Iteration 3, loss = 0.01575806\n",
      "Iteration 4, loss = 0.00586391\n",
      "Iteration 5, loss = 0.01374013\n",
      "Iteration 6, loss = 0.01621749\n",
      "Iteration 7, loss = 0.01104710\n",
      "Iteration 8, loss = 0.00548420\n",
      "Iteration 9, loss = 0.00424969\n",
      "Iteration 10, loss = 0.00638791\n",
      "Iteration 11, loss = 0.00806872\n",
      "Iteration 12, loss = 0.00744564\n",
      "Iteration 13, loss = 0.00553223\n",
      "Iteration 14, loss = 0.00403143\n",
      "Iteration 15, loss = 0.00377173\n",
      "Iteration 16, loss = 0.00442348\n",
      "Iteration 17, loss = 0.00522308\n",
      "Iteration 18, loss = 0.00556461\n",
      "Iteration 19, loss = 0.00524932\n",
      "Iteration 20, loss = 0.00447119\n",
      "Iteration 21, loss = 0.00363576\n",
      "Iteration 22, loss = 0.00314821\n",
      "Iteration 23, loss = 0.00317996\n",
      "Iteration 24, loss = 0.00356114\n",
      "Iteration 25, loss = 0.00391110\n",
      "Iteration 26, loss = 0.00392943\n",
      "Iteration 27, loss = 0.00362092\n",
      "Iteration 28, loss = 0.00323460\n",
      "Iteration 29, loss = 0.00301582\n",
      "Iteration 30, loss = 0.00303284\n",
      "Iteration 31, loss = 0.00317923\n",
      "Iteration 32, loss = 0.00328787\n",
      "Iteration 33, loss = 0.00325720\n",
      "Iteration 34, loss = 0.00309746\n",
      "Iteration 35, loss = 0.00290103\n",
      "Iteration 36, loss = 0.00279003\n",
      "Iteration 37, loss = 0.00282761\n",
      "Iteration 38, loss = 0.00291044\n",
      "Iteration 39, loss = 0.00292726\n",
      "Iteration 40, loss = 0.00286204\n",
      "Iteration 41, loss = 0.00276378\n",
      "Iteration 42, loss = 0.00267920\n",
      "Iteration 43, loss = 0.00263782\n",
      "Iteration 44, loss = 0.00264161\n",
      "Iteration 45, loss = 0.00266036\n",
      "Iteration 46, loss = 0.00266058\n",
      "Iteration 47, loss = 0.00263080\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02462604\n",
      "Iteration 2, loss = 0.05291055\n",
      "Iteration 3, loss = 0.01579977\n",
      "Iteration 4, loss = 0.00588089\n",
      "Iteration 5, loss = 0.01372459\n",
      "Iteration 6, loss = 0.01621599\n",
      "Iteration 7, loss = 0.01107263\n",
      "Iteration 8, loss = 0.00551085\n",
      "Iteration 9, loss = 0.00426862\n",
      "Iteration 10, loss = 0.00640218\n",
      "Iteration 11, loss = 0.00808279\n",
      "Iteration 12, loss = 0.00746275\n",
      "Iteration 13, loss = 0.00554940\n",
      "Iteration 14, loss = 0.00404986\n",
      "Iteration 15, loss = 0.00379376\n",
      "Iteration 16, loss = 0.00444821\n",
      "Iteration 17, loss = 0.00524807\n",
      "Iteration 18, loss = 0.00558686\n",
      "Iteration 19, loss = 0.00526773\n",
      "Iteration 20, loss = 0.00448596\n",
      "Iteration 21, loss = 0.00365107\n",
      "Iteration 22, loss = 0.00316912\n",
      "Iteration 23, loss = 0.00320829\n",
      "Iteration 24, loss = 0.00359364\n",
      "Iteration 25, loss = 0.00394057\n",
      "Iteration 26, loss = 0.00395209\n",
      "Iteration 27, loss = 0.00363790\n",
      "Iteration 28, loss = 0.00325175\n",
      "Iteration 29, loss = 0.00303855\n",
      "Iteration 30, loss = 0.00306273\n",
      "Iteration 31, loss = 0.00321091\n",
      "Iteration 32, loss = 0.00331632\n",
      "Iteration 33, loss = 0.00327987\n",
      "Iteration 34, loss = 0.00311807\n",
      "Iteration 35, loss = 0.00292748\n",
      "Iteration 36, loss = 0.00281543\n",
      "Iteration 37, loss = 0.00284017\n",
      "Iteration 38, loss = 0.00291914\n",
      "Iteration 39, loss = 0.00295448\n",
      "Iteration 40, loss = 0.00291464\n",
      "Iteration 41, loss = 0.00281580\n",
      "Iteration 42, loss = 0.00271255\n",
      "Iteration 43, loss = 0.00265811\n",
      "Iteration 44, loss = 0.00266143\n",
      "Iteration 45, loss = 0.00268935\n",
      "Iteration 46, loss = 0.00269882\n",
      "Iteration 47, loss = 0.00267046\n",
      "Iteration 48, loss = 0.00261956\n",
      "Iteration 49, loss = 0.00257853\n",
      "Iteration 50, loss = 0.00256594\n",
      "Iteration 51, loss = 0.00257250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, loss = 0.00257222\n",
      "Iteration 53, loss = 0.00254942\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02453584\n",
      "Iteration 2, loss = 0.05288833\n",
      "Iteration 3, loss = 0.01574508\n",
      "Iteration 4, loss = 0.00592009\n",
      "Iteration 5, loss = 0.01377523\n",
      "Iteration 6, loss = 0.01624272\n",
      "Iteration 7, loss = 0.01108332\n",
      "Iteration 8, loss = 0.00549651\n",
      "Iteration 9, loss = 0.00424548\n",
      "Iteration 10, loss = 0.00638227\n",
      "Iteration 11, loss = 0.00808066\n",
      "Iteration 12, loss = 0.00747118\n",
      "Iteration 13, loss = 0.00556721\n",
      "Iteration 14, loss = 0.00405955\n",
      "Iteration 15, loss = 0.00378734\n",
      "Iteration 16, loss = 0.00442747\n",
      "Iteration 17, loss = 0.00522462\n",
      "Iteration 18, loss = 0.00557137\n",
      "Iteration 19, loss = 0.00526468\n",
      "Iteration 20, loss = 0.00449263\n",
      "Iteration 21, loss = 0.00365763\n",
      "Iteration 22, loss = 0.00316555\n",
      "Iteration 23, loss = 0.00319377\n",
      "Iteration 24, loss = 0.00357489\n",
      "Iteration 25, loss = 0.00392827\n",
      "Iteration 26, loss = 0.00394931\n",
      "Iteration 27, loss = 0.00364102\n",
      "Iteration 28, loss = 0.00325293\n",
      "Iteration 29, loss = 0.00303263\n",
      "Iteration 30, loss = 0.00305040\n",
      "Iteration 31, loss = 0.00319673\n",
      "Iteration 32, loss = 0.00330468\n",
      "Iteration 33, loss = 0.00327423\n",
      "Iteration 34, loss = 0.00311573\n",
      "Iteration 35, loss = 0.00292030\n",
      "Iteration 36, loss = 0.00281029\n",
      "Iteration 37, loss = 0.00284716\n",
      "Iteration 38, loss = 0.00292935\n",
      "Iteration 39, loss = 0.00294650\n",
      "Iteration 40, loss = 0.00288267\n",
      "Iteration 41, loss = 0.00278501\n",
      "Iteration 42, loss = 0.00269868\n",
      "Iteration 43, loss = 0.00265667\n",
      "Iteration 44, loss = 0.00266146\n",
      "Iteration 45, loss = 0.00268160\n",
      "Iteration 46, loss = 0.00268285\n",
      "Iteration 47, loss = 0.00265318\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02477108\n",
      "Iteration 2, loss = 0.05263292\n",
      "Iteration 3, loss = 0.01581684\n",
      "Iteration 4, loss = 0.00589784\n",
      "Iteration 5, loss = 0.01364746\n",
      "Iteration 6, loss = 0.01610955\n",
      "Iteration 7, loss = 0.01099693\n",
      "Iteration 8, loss = 0.00547781\n",
      "Iteration 9, loss = 0.00425532\n",
      "Iteration 10, loss = 0.00638169\n",
      "Iteration 11, loss = 0.00804656\n",
      "Iteration 12, loss = 0.00742230\n",
      "Iteration 13, loss = 0.00550572\n",
      "Iteration 14, loss = 0.00401789\n",
      "Iteration 15, loss = 0.00377877\n",
      "Iteration 16, loss = 0.00444458\n",
      "Iteration 17, loss = 0.00524216\n",
      "Iteration 18, loss = 0.00556592\n",
      "Iteration 19, loss = 0.00523045\n",
      "Iteration 20, loss = 0.00444038\n",
      "Iteration 21, loss = 0.00361282\n",
      "Iteration 22, loss = 0.00315016\n",
      "Iteration 23, loss = 0.00320986\n",
      "Iteration 24, loss = 0.00360119\n",
      "Iteration 25, loss = 0.00393573\n",
      "Iteration 26, loss = 0.00392701\n",
      "Iteration 27, loss = 0.00360184\n",
      "Iteration 28, loss = 0.00322030\n",
      "Iteration 29, loss = 0.00302100\n",
      "Iteration 30, loss = 0.00305756\n",
      "Iteration 31, loss = 0.00320682\n",
      "Iteration 32, loss = 0.00330477\n",
      "Iteration 33, loss = 0.00325794\n",
      "Iteration 34, loss = 0.00308804\n",
      "Iteration 35, loss = 0.00289509\n",
      "Iteration 36, loss = 0.00280078\n",
      "Iteration 37, loss = 0.00285188\n",
      "Iteration 38, loss = 0.00293125\n",
      "Iteration 39, loss = 0.00293908\n",
      "Iteration 40, loss = 0.00286967\n",
      "Iteration 41, loss = 0.00276987\n",
      "Iteration 42, loss = 0.00268636\n",
      "Iteration 43, loss = 0.00265089\n",
      "Iteration 44, loss = 0.00266005\n",
      "Iteration 45, loss = 0.00267977\n",
      "Iteration 46, loss = 0.00267660\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02484604\n",
      "Iteration 2, loss = 0.05243386\n",
      "Iteration 3, loss = 0.01581561\n",
      "Iteration 4, loss = 0.00586600\n",
      "Iteration 5, loss = 0.01358983\n",
      "Iteration 6, loss = 0.01608069\n",
      "Iteration 7, loss = 0.01098649\n",
      "Iteration 8, loss = 0.00546723\n",
      "Iteration 9, loss = 0.00424498\n",
      "Iteration 10, loss = 0.00636089\n",
      "Iteration 11, loss = 0.00804131\n",
      "Iteration 12, loss = 0.00743284\n",
      "Iteration 13, loss = 0.00550810\n",
      "Iteration 14, loss = 0.00401653\n",
      "Iteration 15, loss = 0.00377663\n",
      "Iteration 16, loss = 0.00444506\n",
      "Iteration 17, loss = 0.00524739\n",
      "Iteration 18, loss = 0.00557312\n",
      "Iteration 19, loss = 0.00523941\n",
      "Iteration 20, loss = 0.00445140\n",
      "Iteration 21, loss = 0.00362315\n",
      "Iteration 22, loss = 0.00315947\n",
      "Iteration 23, loss = 0.00321686\n",
      "Iteration 24, loss = 0.00360861\n",
      "Iteration 25, loss = 0.00394624\n",
      "Iteration 26, loss = 0.00394091\n",
      "Iteration 27, loss = 0.00361703\n",
      "Iteration 28, loss = 0.00323570\n",
      "Iteration 29, loss = 0.00303633\n",
      "Iteration 30, loss = 0.00307265\n",
      "Iteration 31, loss = 0.00322303\n",
      "Iteration 32, loss = 0.00332310\n",
      "Iteration 33, loss = 0.00327886\n",
      "Iteration 34, loss = 0.00311251\n",
      "Iteration 35, loss = 0.00292595\n",
      "Iteration 36, loss = 0.00282456\n",
      "Iteration 37, loss = 0.00285811\n",
      "Iteration 38, loss = 0.00293666\n",
      "Iteration 39, loss = 0.00296837\n",
      "Iteration 40, loss = 0.00292328\n",
      "Iteration 41, loss = 0.00282021\n",
      "Iteration 42, loss = 0.00271539\n",
      "Iteration 43, loss = 0.00266513\n",
      "Iteration 44, loss = 0.00267240\n",
      "Iteration 45, loss = 0.00270142\n",
      "Iteration 46, loss = 0.00270796\n",
      "Iteration 47, loss = 0.00267552\n",
      "Iteration 48, loss = 0.00262307\n",
      "Iteration 49, loss = 0.00258372\n",
      "Iteration 50, loss = 0.00257410\n",
      "Iteration 51, loss = 0.00258105\n",
      "Iteration 52, loss = 0.00257863\n",
      "Iteration 53, loss = 0.00255318\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02493650\n",
      "Iteration 2, loss = 0.05216143\n",
      "Iteration 3, loss = 0.01578980\n",
      "Iteration 4, loss = 0.00575258\n",
      "Iteration 5, loss = 0.01339643\n",
      "Iteration 6, loss = 0.01590913\n",
      "Iteration 7, loss = 0.01084869\n",
      "Iteration 8, loss = 0.00535793\n",
      "Iteration 9, loss = 0.00415709\n",
      "Iteration 10, loss = 0.00626918\n",
      "Iteration 11, loss = 0.00794688\n",
      "Iteration 12, loss = 0.00730386\n",
      "Iteration 13, loss = 0.00536083\n",
      "Iteration 14, loss = 0.00387998\n",
      "Iteration 15, loss = 0.00366961\n",
      "Iteration 16, loss = 0.00435943\n",
      "Iteration 17, loss = 0.00516388\n",
      "Iteration 18, loss = 0.00547825\n",
      "Iteration 19, loss = 0.00512835\n",
      "Iteration 20, loss = 0.00433094\n",
      "Iteration 21, loss = 0.00350418\n",
      "Iteration 22, loss = 0.00305183\n",
      "Iteration 23, loss = 0.00312014\n",
      "Iteration 24, loss = 0.00351290\n",
      "Iteration 25, loss = 0.00384212\n",
      "Iteration 26, loss = 0.00382602\n",
      "Iteration 27, loss = 0.00349880\n",
      "Iteration 28, loss = 0.00312381\n",
      "Iteration 29, loss = 0.00293522\n",
      "Iteration 30, loss = 0.00297882\n",
      "Iteration 31, loss = 0.00313008\n",
      "Iteration 32, loss = 0.00322418\n",
      "Iteration 33, loss = 0.00317324\n",
      "Iteration 34, loss = 0.00300419\n",
      "Iteration 35, loss = 0.00282008\n",
      "Iteration 36, loss = 0.00272501\n",
      "Iteration 37, loss = 0.00276421\n",
      "Iteration 38, loss = 0.00284439\n",
      "Iteration 39, loss = 0.00287760\n",
      "Iteration 40, loss = 0.00283412\n",
      "Iteration 41, loss = 0.00273182\n",
      "Iteration 42, loss = 0.00262763\n",
      "Iteration 43, loss = 0.00257535\n",
      "Iteration 44, loss = 0.00258210\n",
      "Iteration 45, loss = 0.00261113\n",
      "Iteration 46, loss = 0.00262001\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02478585\n",
      "Iteration 2, loss = 0.05258559\n",
      "Iteration 3, loss = 0.01581450\n",
      "Iteration 4, loss = 0.00589692\n",
      "Iteration 5, loss = 0.01367584\n",
      "Iteration 6, loss = 0.01615559\n",
      "Iteration 7, loss = 0.01102279\n",
      "Iteration 8, loss = 0.00545797\n",
      "Iteration 9, loss = 0.00423699\n",
      "Iteration 10, loss = 0.00639692\n",
      "Iteration 11, loss = 0.00808306\n",
      "Iteration 12, loss = 0.00744026\n",
      "Iteration 13, loss = 0.00549787\n",
      "Iteration 14, loss = 0.00399876\n",
      "Iteration 15, loss = 0.00375741\n",
      "Iteration 16, loss = 0.00442511\n",
      "Iteration 17, loss = 0.00522976\n",
      "Iteration 18, loss = 0.00556191\n",
      "Iteration 19, loss = 0.00523406\n",
      "Iteration 20, loss = 0.00444750\n",
      "Iteration 21, loss = 0.00361398\n",
      "Iteration 22, loss = 0.00314001\n",
      "Iteration 23, loss = 0.00318709\n",
      "Iteration 24, loss = 0.00357679\n",
      "Iteration 25, loss = 0.00391964\n",
      "Iteration 26, loss = 0.00392212\n",
      "Iteration 27, loss = 0.00360169\n",
      "Iteration 28, loss = 0.00321714\n",
      "Iteration 29, loss = 0.00301053\n",
      "Iteration 30, loss = 0.00304048\n",
      "Iteration 31, loss = 0.00319067\n",
      "Iteration 32, loss = 0.00329486\n",
      "Iteration 33, loss = 0.00325500\n",
      "Iteration 34, loss = 0.00308963\n",
      "Iteration 35, loss = 0.00290039\n",
      "Iteration 36, loss = 0.00279421\n",
      "Iteration 37, loss = 0.00282449\n",
      "Iteration 38, loss = 0.00290375\n",
      "Iteration 39, loss = 0.00293757\n",
      "Iteration 40, loss = 0.00289548\n",
      "Iteration 41, loss = 0.00279401\n",
      "Iteration 42, loss = 0.00268932\n",
      "Iteration 43, loss = 0.00263613\n",
      "Iteration 44, loss = 0.00264189\n",
      "Iteration 45, loss = 0.00267159\n",
      "Iteration 46, loss = 0.00268045\n",
      "Iteration 47, loss = 0.00264990\n",
      "Iteration 48, loss = 0.00259798\n",
      "Iteration 49, loss = 0.00255772\n",
      "Iteration 50, loss = 0.00254651\n",
      "Iteration 51, loss = 0.00255392\n",
      "Iteration 52, loss = 0.00255329\n",
      "Iteration 53, loss = 0.00252956\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02456069\n",
      "Iteration 2, loss = 0.05287579\n",
      "Iteration 3, loss = 0.01575124\n",
      "Iteration 4, loss = 0.00591842\n",
      "Iteration 5, loss = 0.01376425\n",
      "Iteration 6, loss = 0.01621710\n",
      "Iteration 7, loss = 0.01105162\n",
      "Iteration 8, loss = 0.00548946\n",
      "Iteration 9, loss = 0.00426196\n",
      "Iteration 10, loss = 0.00640578\n",
      "Iteration 11, loss = 0.00809167\n",
      "Iteration 12, loss = 0.00746290\n",
      "Iteration 13, loss = 0.00554288\n",
      "Iteration 14, loss = 0.00404369\n",
      "Iteration 15, loss = 0.00379153\n",
      "Iteration 16, loss = 0.00444806\n",
      "Iteration 17, loss = 0.00524651\n",
      "Iteration 18, loss = 0.00558188\n",
      "Iteration 19, loss = 0.00526000\n",
      "Iteration 20, loss = 0.00447724\n",
      "Iteration 21, loss = 0.00364329\n",
      "Iteration 22, loss = 0.00316419\n",
      "Iteration 23, loss = 0.00320695\n",
      "Iteration 24, loss = 0.00359360\n",
      "Iteration 25, loss = 0.00393872\n",
      "Iteration 26, loss = 0.00394658\n",
      "Iteration 27, loss = 0.00363032\n",
      "Iteration 28, loss = 0.00324472\n",
      "Iteration 29, loss = 0.00303300\n",
      "Iteration 30, loss = 0.00305821\n",
      "Iteration 31, loss = 0.00320601\n",
      "Iteration 32, loss = 0.00331037\n",
      "Iteration 33, loss = 0.00327376\n",
      "Iteration 34, loss = 0.00311049\n",
      "Iteration 35, loss = 0.00291528\n",
      "Iteration 36, loss = 0.00281023\n",
      "Iteration 37, loss = 0.00285162\n",
      "Iteration 38, loss = 0.00293253\n",
      "Iteration 39, loss = 0.00294619\n",
      "Iteration 40, loss = 0.00287930\n",
      "Iteration 41, loss = 0.00278103\n",
      "Iteration 42, loss = 0.00269622\n",
      "Iteration 43, loss = 0.00265665\n",
      "Iteration 44, loss = 0.00266218\n",
      "Iteration 45, loss = 0.00268126\n",
      "Iteration 46, loss = 0.00268077\n",
      "Iteration 47, loss = 0.00265037\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02412934\n",
      "Iteration 2, loss = 0.05304243\n",
      "Iteration 3, loss = 0.01558886\n",
      "Iteration 4, loss = 0.00583901\n",
      "Iteration 5, loss = 0.01381435\n",
      "Iteration 6, loss = 0.01629288\n",
      "Iteration 7, loss = 0.01109767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.00545644\n",
      "Iteration 9, loss = 0.00418614\n",
      "Iteration 10, loss = 0.00633987\n",
      "Iteration 11, loss = 0.00806773\n",
      "Iteration 12, loss = 0.00746409\n",
      "Iteration 13, loss = 0.00555153\n",
      "Iteration 14, loss = 0.00402936\n",
      "Iteration 15, loss = 0.00373639\n",
      "Iteration 16, loss = 0.00436775\n",
      "Iteration 17, loss = 0.00517031\n",
      "Iteration 18, loss = 0.00553480\n",
      "Iteration 19, loss = 0.00525204\n",
      "Iteration 20, loss = 0.00449129\n",
      "Iteration 21, loss = 0.00364949\n",
      "Iteration 22, loss = 0.00313631\n",
      "Iteration 23, loss = 0.00314245\n",
      "Iteration 24, loss = 0.00351750\n",
      "Iteration 25, loss = 0.00388292\n",
      "Iteration 26, loss = 0.00392566\n",
      "Iteration 27, loss = 0.00362941\n",
      "Iteration 28, loss = 0.00323582\n",
      "Iteration 29, loss = 0.00300043\n",
      "Iteration 30, loss = 0.00300599\n",
      "Iteration 31, loss = 0.00315098\n",
      "Iteration 32, loss = 0.00326859\n",
      "Iteration 33, loss = 0.00325055\n",
      "Iteration 34, loss = 0.00309997\n",
      "Iteration 35, loss = 0.00290770\n",
      "Iteration 36, loss = 0.00278188\n",
      "Iteration 37, loss = 0.00279307\n",
      "Iteration 38, loss = 0.00287264\n",
      "Iteration 39, loss = 0.00291426\n",
      "Iteration 40, loss = 0.00288236\n",
      "Iteration 41, loss = 0.00279037\n",
      "Iteration 42, loss = 0.00268756\n",
      "Iteration 43, loss = 0.00262769\n",
      "Iteration 44, loss = 0.00262546\n",
      "Iteration 45, loss = 0.00265250\n",
      "Iteration 46, loss = 0.00266670\n",
      "Iteration 47, loss = 0.00264391\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02409302\n",
      "Iteration 2, loss = 0.05304204\n",
      "Iteration 3, loss = 0.01553570\n",
      "Iteration 4, loss = 0.00586698\n",
      "Iteration 5, loss = 0.01386768\n",
      "Iteration 6, loss = 0.01632923\n",
      "Iteration 7, loss = 0.01111601\n",
      "Iteration 8, loss = 0.00545126\n",
      "Iteration 9, loss = 0.00415625\n",
      "Iteration 10, loss = 0.00630624\n",
      "Iteration 11, loss = 0.00806651\n",
      "Iteration 12, loss = 0.00747270\n",
      "Iteration 13, loss = 0.00558573\n",
      "Iteration 14, loss = 0.00405606\n",
      "Iteration 15, loss = 0.00374646\n",
      "Iteration 16, loss = 0.00435878\n",
      "Iteration 17, loss = 0.00515721\n",
      "Iteration 18, loss = 0.00552889\n",
      "Iteration 19, loss = 0.00525533\n",
      "Iteration 20, loss = 0.00450381\n",
      "Iteration 21, loss = 0.00366841\n",
      "Iteration 22, loss = 0.00315451\n",
      "Iteration 23, loss = 0.00315207\n",
      "Iteration 24, loss = 0.00351840\n",
      "Iteration 25, loss = 0.00388661\n",
      "Iteration 26, loss = 0.00394096\n",
      "Iteration 27, loss = 0.00365698\n",
      "Iteration 28, loss = 0.00326705\n",
      "Iteration 29, loss = 0.00302448\n",
      "Iteration 30, loss = 0.00301909\n",
      "Iteration 31, loss = 0.00315806\n",
      "Iteration 32, loss = 0.00327903\n",
      "Iteration 33, loss = 0.00327079\n",
      "Iteration 34, loss = 0.00312999\n",
      "Iteration 35, loss = 0.00294258\n",
      "Iteration 36, loss = 0.00281467\n",
      "Iteration 37, loss = 0.00281993\n",
      "Iteration 38, loss = 0.00289700\n",
      "Iteration 39, loss = 0.00293926\n",
      "Iteration 40, loss = 0.00291036\n",
      "Iteration 41, loss = 0.00282038\n",
      "Iteration 42, loss = 0.00271552\n",
      "Iteration 43, loss = 0.00265232\n",
      "Iteration 44, loss = 0.00264796\n",
      "Iteration 45, loss = 0.00267619\n",
      "Iteration 46, loss = 0.00269239\n",
      "Iteration 47, loss = 0.00267091\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02369627\n",
      "Iteration 2, loss = 0.05306628\n",
      "Iteration 3, loss = 0.01536678\n",
      "Iteration 4, loss = 0.00577530\n",
      "Iteration 5, loss = 0.01383359\n",
      "Iteration 6, loss = 0.01625116\n",
      "Iteration 7, loss = 0.01098579\n",
      "Iteration 8, loss = 0.00528995\n",
      "Iteration 9, loss = 0.00401319\n",
      "Iteration 10, loss = 0.00616819\n",
      "Iteration 11, loss = 0.00793158\n",
      "Iteration 12, loss = 0.00734771\n",
      "Iteration 13, loss = 0.00546103\n",
      "Iteration 14, loss = 0.00391943\n",
      "Iteration 15, loss = 0.00359143\n",
      "Iteration 16, loss = 0.00419154\n",
      "Iteration 17, loss = 0.00498892\n",
      "Iteration 18, loss = 0.00536895\n",
      "Iteration 19, loss = 0.00510849\n",
      "Iteration 20, loss = 0.00436283\n",
      "Iteration 21, loss = 0.00352118\n",
      "Iteration 22, loss = 0.00299214\n",
      "Iteration 23, loss = 0.00297255\n",
      "Iteration 24, loss = 0.00333102\n",
      "Iteration 25, loss = 0.00370625\n",
      "Iteration 26, loss = 0.00377691\n",
      "Iteration 27, loss = 0.00350371\n",
      "Iteration 28, loss = 0.00311081\n",
      "Iteration 29, loss = 0.00285349\n",
      "Iteration 30, loss = 0.00283403\n",
      "Iteration 31, loss = 0.00296721\n",
      "Iteration 32, loss = 0.00309242\n",
      "Iteration 33, loss = 0.00309523\n",
      "Iteration 34, loss = 0.00296228\n",
      "Iteration 35, loss = 0.00277342\n",
      "Iteration 36, loss = 0.00263530\n",
      "Iteration 37, loss = 0.00262943\n",
      "Iteration 38, loss = 0.00270622\n",
      "Iteration 39, loss = 0.00275381\n",
      "Iteration 40, loss = 0.00272850\n",
      "Iteration 41, loss = 0.00264004\n",
      "Iteration 42, loss = 0.00253362\n",
      "Iteration 43, loss = 0.00246269\n",
      "Iteration 44, loss = 0.00245325\n",
      "Iteration 45, loss = 0.00248093\n",
      "Iteration 46, loss = 0.00249941\n",
      "Iteration 47, loss = 0.00248023\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02482644\n",
      "Iteration 2, loss = 0.05210408\n",
      "Iteration 3, loss = 0.01572191\n",
      "Iteration 4, loss = 0.00579060\n",
      "Iteration 5, loss = 0.01344568\n",
      "Iteration 6, loss = 0.01589385\n",
      "Iteration 7, loss = 0.01081100\n",
      "Iteration 8, loss = 0.00535589\n",
      "Iteration 9, loss = 0.00415767\n",
      "Iteration 10, loss = 0.00629079\n",
      "Iteration 11, loss = 0.00794192\n",
      "Iteration 12, loss = 0.00728552\n",
      "Iteration 13, loss = 0.00535932\n",
      "Iteration 14, loss = 0.00389592\n",
      "Iteration 15, loss = 0.00370326\n",
      "Iteration 16, loss = 0.00439954\n",
      "Iteration 17, loss = 0.00519773\n",
      "Iteration 18, loss = 0.00548384\n",
      "Iteration 19, loss = 0.00510110\n",
      "Iteration 20, loss = 0.00428471\n",
      "Iteration 21, loss = 0.00346924\n",
      "Iteration 22, loss = 0.00305214\n",
      "Iteration 23, loss = 0.00315653\n",
      "Iteration 24, loss = 0.00356117\n",
      "Iteration 25, loss = 0.00387011\n",
      "Iteration 26, loss = 0.00382165\n",
      "Iteration 27, loss = 0.00347596\n",
      "Iteration 28, loss = 0.00310516\n",
      "Iteration 29, loss = 0.00293442\n",
      "Iteration 30, loss = 0.00299359\n",
      "Iteration 31, loss = 0.00314583\n",
      "Iteration 32, loss = 0.00322782\n",
      "Iteration 33, loss = 0.00316115\n",
      "Iteration 34, loss = 0.00298304\n",
      "Iteration 35, loss = 0.00279932\n",
      "Iteration 36, loss = 0.00272324\n",
      "Iteration 37, loss = 0.00278418\n",
      "Iteration 38, loss = 0.00285873\n",
      "Iteration 39, loss = 0.00286027\n",
      "Iteration 40, loss = 0.00278836\n",
      "Iteration 41, loss = 0.00268564\n",
      "Iteration 42, loss = 0.00260253\n",
      "Iteration 43, loss = 0.00257110\n",
      "Iteration 44, loss = 0.00258476\n",
      "Iteration 45, loss = 0.00260592\n",
      "Iteration 46, loss = 0.00260106\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02498482\n",
      "Iteration 2, loss = 0.05143123\n",
      "Iteration 3, loss = 0.01554453\n",
      "Iteration 4, loss = 0.00547073\n",
      "Iteration 5, loss = 0.01297878\n",
      "Iteration 6, loss = 0.01553458\n",
      "Iteration 7, loss = 0.01061967\n",
      "Iteration 8, loss = 0.00522531\n",
      "Iteration 9, loss = 0.00401597\n",
      "Iteration 10, loss = 0.00606379\n",
      "Iteration 11, loss = 0.00776629\n",
      "Iteration 12, loss = 0.00719629\n",
      "Iteration 13, loss = 0.00529277\n",
      "Iteration 14, loss = 0.00380821\n",
      "Iteration 15, loss = 0.00358716\n",
      "Iteration 16, loss = 0.00428739\n",
      "Iteration 17, loss = 0.00510917\n",
      "Iteration 18, loss = 0.00542819\n",
      "Iteration 19, loss = 0.00507100\n",
      "Iteration 20, loss = 0.00426616\n",
      "Iteration 21, loss = 0.00344715\n",
      "Iteration 22, loss = 0.00301016\n",
      "Iteration 23, loss = 0.00309609\n",
      "Iteration 24, loss = 0.00349729\n",
      "Iteration 25, loss = 0.00382156\n",
      "Iteration 26, loss = 0.00379486\n",
      "Iteration 27, loss = 0.00346472\n",
      "Iteration 28, loss = 0.00309880\n",
      "Iteration 29, loss = 0.00292791\n",
      "Iteration 30, loss = 0.00298714\n",
      "Iteration 31, loss = 0.00314274\n",
      "Iteration 32, loss = 0.00323168\n",
      "Iteration 33, loss = 0.00317206\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02440100\n",
      "Iteration 2, loss = 0.05276509\n",
      "Iteration 3, loss = 0.01571111\n",
      "Iteration 4, loss = 0.00593022\n",
      "Iteration 5, loss = 0.01377631\n",
      "Iteration 6, loss = 0.01620130\n",
      "Iteration 7, loss = 0.01102277\n",
      "Iteration 8, loss = 0.00545862\n",
      "Iteration 9, loss = 0.00420847\n",
      "Iteration 10, loss = 0.00634753\n",
      "Iteration 11, loss = 0.00805952\n",
      "Iteration 12, loss = 0.00746016\n",
      "Iteration 13, loss = 0.00554569\n",
      "Iteration 14, loss = 0.00402737\n",
      "Iteration 15, loss = 0.00374654\n",
      "Iteration 16, loss = 0.00438245\n",
      "Iteration 17, loss = 0.00517937\n",
      "Iteration 18, loss = 0.00552930\n",
      "Iteration 19, loss = 0.00522639\n",
      "Iteration 20, loss = 0.00445570\n",
      "Iteration 21, loss = 0.00361904\n",
      "Iteration 22, loss = 0.00312324\n",
      "Iteration 23, loss = 0.00314681\n",
      "Iteration 24, loss = 0.00352447\n",
      "Iteration 25, loss = 0.00387561\n",
      "Iteration 26, loss = 0.00389637\n",
      "Iteration 27, loss = 0.00358753\n",
      "Iteration 28, loss = 0.00319655\n",
      "Iteration 29, loss = 0.00297174\n",
      "Iteration 30, loss = 0.00298535\n",
      "Iteration 31, loss = 0.00312929\n",
      "Iteration 32, loss = 0.00323899\n",
      "Iteration 33, loss = 0.00320906\n",
      "Iteration 34, loss = 0.00304732\n",
      "Iteration 35, loss = 0.00284757\n",
      "Iteration 36, loss = 0.00273426\n",
      "Iteration 37, loss = 0.00277033\n",
      "Iteration 38, loss = 0.00285132\n",
      "Iteration 39, loss = 0.00286521\n",
      "Iteration 40, loss = 0.00279799\n",
      "Iteration 41, loss = 0.00269763\n",
      "Iteration 42, loss = 0.00261301\n",
      "Iteration 43, loss = 0.00257229\n",
      "Iteration 44, loss = 0.00257521\n",
      "Iteration 45, loss = 0.00259206\n",
      "Iteration 46, loss = 0.00258960\n",
      "Iteration 47, loss = 0.00255772\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02419832\n",
      "Iteration 2, loss = 0.05280780\n",
      "Iteration 3, loss = 0.01562443\n",
      "Iteration 4, loss = 0.00595188\n",
      "Iteration 5, loss = 0.01386051\n",
      "Iteration 6, loss = 0.01624980\n",
      "Iteration 7, loss = 0.01103213\n",
      "Iteration 8, loss = 0.00544940\n",
      "Iteration 9, loss = 0.00419512\n",
      "Iteration 10, loss = 0.00631227\n",
      "Iteration 11, loss = 0.00808736\n",
      "Iteration 12, loss = 0.00754501\n",
      "Iteration 13, loss = 0.00561327\n",
      "Iteration 14, loss = 0.00406128\n",
      "Iteration 15, loss = 0.00375533\n",
      "Iteration 16, loss = 0.00439096\n",
      "Iteration 17, loss = 0.00520091\n",
      "Iteration 18, loss = 0.00556840\n",
      "Iteration 19, loss = 0.00527779\n",
      "Iteration 20, loss = 0.00451005\n",
      "Iteration 21, loss = 0.00366585\n",
      "Iteration 22, loss = 0.00315788\n",
      "Iteration 23, loss = 0.00317433\n",
      "Iteration 24, loss = 0.00355624\n",
      "Iteration 25, loss = 0.00391885\n",
      "Iteration 26, loss = 0.00394972\n",
      "Iteration 27, loss = 0.00364280\n",
      "Iteration 28, loss = 0.00324608\n",
      "Iteration 29, loss = 0.00301438\n",
      "Iteration 30, loss = 0.00302470\n",
      "Iteration 31, loss = 0.00317189\n",
      "Iteration 32, loss = 0.00328930\n",
      "Iteration 33, loss = 0.00326839\n",
      "Iteration 34, loss = 0.00311247\n",
      "Iteration 35, loss = 0.00291243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36, loss = 0.00279245\n",
      "Iteration 37, loss = 0.00282355\n",
      "Iteration 38, loss = 0.00290806\n",
      "Iteration 39, loss = 0.00292861\n",
      "Iteration 40, loss = 0.00286684\n",
      "Iteration 41, loss = 0.00276867\n",
      "Iteration 42, loss = 0.00268400\n",
      "Iteration 43, loss = 0.00264215\n",
      "Iteration 44, loss = 0.00264557\n",
      "Iteration 45, loss = 0.00266519\n",
      "Iteration 46, loss = 0.00266725\n",
      "Iteration 47, loss = 0.00263925\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02386412\n",
      "Iteration 2, loss = 0.05303455\n",
      "Iteration 3, loss = 0.01549372\n",
      "Iteration 4, loss = 0.00586782\n",
      "Iteration 5, loss = 0.01398064\n",
      "Iteration 6, loss = 0.01644635\n",
      "Iteration 7, loss = 0.01117243\n",
      "Iteration 8, loss = 0.00544675\n",
      "Iteration 9, loss = 0.00415698\n",
      "Iteration 10, loss = 0.00636426\n",
      "Iteration 11, loss = 0.00812087\n",
      "Iteration 12, loss = 0.00749220\n",
      "Iteration 13, loss = 0.00558133\n",
      "Iteration 14, loss = 0.00403911\n",
      "Iteration 15, loss = 0.00372089\n",
      "Iteration 16, loss = 0.00433465\n",
      "Iteration 17, loss = 0.00514220\n",
      "Iteration 18, loss = 0.00552703\n",
      "Iteration 19, loss = 0.00526610\n",
      "Iteration 20, loss = 0.00451906\n",
      "Iteration 21, loss = 0.00367659\n",
      "Iteration 22, loss = 0.00314668\n",
      "Iteration 23, loss = 0.00312987\n",
      "Iteration 24, loss = 0.00349398\n",
      "Iteration 25, loss = 0.00386939\n",
      "Iteration 26, loss = 0.00393438\n",
      "Iteration 27, loss = 0.00365539\n",
      "Iteration 28, loss = 0.00326144\n",
      "Iteration 29, loss = 0.00300954\n",
      "Iteration 30, loss = 0.00299612\n",
      "Iteration 31, loss = 0.00313539\n",
      "Iteration 32, loss = 0.00326226\n",
      "Iteration 33, loss = 0.00325936\n",
      "Iteration 34, loss = 0.00311962\n",
      "Iteration 35, loss = 0.00292846\n",
      "Iteration 36, loss = 0.00279457\n",
      "Iteration 37, loss = 0.00279229\n",
      "Iteration 38, loss = 0.00287029\n",
      "Iteration 39, loss = 0.00291914\n",
      "Iteration 40, loss = 0.00289581\n",
      "Iteration 41, loss = 0.00280870\n",
      "Iteration 42, loss = 0.00270294\n",
      "Iteration 43, loss = 0.00263337\n",
      "Iteration 44, loss = 0.00262370\n",
      "Iteration 45, loss = 0.00265007\n",
      "Iteration 46, loss = 0.00266829\n",
      "Iteration 47, loss = 0.00265013\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02392907\n",
      "Iteration 2, loss = 0.05292688\n",
      "Iteration 3, loss = 0.01551410\n",
      "Iteration 4, loss = 0.00599462\n",
      "Iteration 5, loss = 0.01399488\n",
      "Iteration 6, loss = 0.01634078\n",
      "Iteration 7, loss = 0.01106271\n",
      "Iteration 8, loss = 0.00543213\n",
      "Iteration 9, loss = 0.00419915\n",
      "Iteration 10, loss = 0.00638491\n",
      "Iteration 11, loss = 0.00812084\n",
      "Iteration 12, loss = 0.00750005\n",
      "Iteration 13, loss = 0.00556656\n",
      "Iteration 14, loss = 0.00404520\n",
      "Iteration 15, loss = 0.00377888\n",
      "Iteration 16, loss = 0.00443416\n",
      "Iteration 17, loss = 0.00523540\n",
      "Iteration 18, loss = 0.00557090\n",
      "Iteration 19, loss = 0.00524275\n",
      "Iteration 20, loss = 0.00445441\n",
      "Iteration 21, loss = 0.00362497\n",
      "Iteration 22, loss = 0.00316031\n",
      "Iteration 23, loss = 0.00321928\n",
      "Iteration 24, loss = 0.00361754\n",
      "Iteration 25, loss = 0.00396247\n",
      "Iteration 26, loss = 0.00396274\n",
      "Iteration 27, loss = 0.00363725\n",
      "Iteration 28, loss = 0.00324437\n",
      "Iteration 29, loss = 0.00303102\n",
      "Iteration 30, loss = 0.00305767\n",
      "Iteration 31, loss = 0.00320948\n",
      "Iteration 32, loss = 0.00331794\n",
      "Iteration 33, loss = 0.00328217\n",
      "Iteration 34, loss = 0.00311717\n",
      "Iteration 35, loss = 0.00292090\n",
      "Iteration 36, loss = 0.00281649\n",
      "Iteration 37, loss = 0.00286117\n",
      "Iteration 38, loss = 0.00294212\n",
      "Iteration 39, loss = 0.00295312\n",
      "Iteration 40, loss = 0.00288489\n",
      "Iteration 41, loss = 0.00278626\n",
      "Iteration 42, loss = 0.00270288\n",
      "Iteration 43, loss = 0.00266381\n",
      "Iteration 44, loss = 0.00266863\n",
      "Iteration 45, loss = 0.00268703\n",
      "Iteration 46, loss = 0.00268567\n",
      "Iteration 47, loss = 0.00265395\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02419734\n",
      "Iteration 2, loss = 0.05262472\n",
      "Iteration 3, loss = 0.01557118\n",
      "Iteration 4, loss = 0.00593033\n",
      "Iteration 5, loss = 0.01371296\n",
      "Iteration 6, loss = 0.01606071\n",
      "Iteration 7, loss = 0.01086983\n",
      "Iteration 8, loss = 0.00540965\n",
      "Iteration 9, loss = 0.00428319\n",
      "Iteration 10, loss = 0.00645829\n",
      "Iteration 11, loss = 0.00807175\n",
      "Iteration 12, loss = 0.00735550\n",
      "Iteration 13, loss = 0.00543825\n",
      "Iteration 14, loss = 0.00399105\n",
      "Iteration 15, loss = 0.00378910\n",
      "Iteration 16, loss = 0.00446011\n",
      "Iteration 17, loss = 0.00523698\n",
      "Iteration 18, loss = 0.00553624\n",
      "Iteration 19, loss = 0.00518788\n",
      "Iteration 20, loss = 0.00440017\n",
      "Iteration 21, loss = 0.00358586\n",
      "Iteration 22, loss = 0.00314312\n",
      "Iteration 23, loss = 0.00321363\n",
      "Iteration 24, loss = 0.00360250\n",
      "Iteration 25, loss = 0.00392598\n",
      "Iteration 26, loss = 0.00390757\n",
      "Iteration 27, loss = 0.00358035\n",
      "Iteration 28, loss = 0.00320343\n",
      "Iteration 29, loss = 0.00301068\n",
      "Iteration 30, loss = 0.00304972\n",
      "Iteration 31, loss = 0.00319756\n",
      "Iteration 32, loss = 0.00329093\n",
      "Iteration 33, loss = 0.00324129\n",
      "Iteration 34, loss = 0.00307322\n",
      "Iteration 35, loss = 0.00288971\n",
      "Iteration 36, loss = 0.00279238\n",
      "Iteration 37, loss = 0.00282705\n",
      "Iteration 38, loss = 0.00290389\n",
      "Iteration 39, loss = 0.00293448\n",
      "Iteration 40, loss = 0.00288889\n",
      "Iteration 41, loss = 0.00278462\n",
      "Iteration 42, loss = 0.00267978\n",
      "Iteration 43, loss = 0.00262744\n",
      "Iteration 44, loss = 0.00263693\n",
      "Iteration 45, loss = 0.00266772\n",
      "Iteration 46, loss = 0.00267444\n",
      "Iteration 47, loss = 0.00264076\n",
      "Iteration 48, loss = 0.00258610\n",
      "Iteration 49, loss = 0.00254473\n",
      "Iteration 50, loss = 0.00253496\n",
      "Iteration 51, loss = 0.00254379\n",
      "Iteration 52, loss = 0.00254180\n",
      "Iteration 53, loss = 0.00251522\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02482693\n",
      "Iteration 2, loss = 0.05189042\n",
      "Iteration 3, loss = 0.01569049\n",
      "Iteration 4, loss = 0.00573516\n",
      "Iteration 5, loss = 0.01332126\n",
      "Iteration 6, loss = 0.01578867\n",
      "Iteration 7, loss = 0.01076476\n",
      "Iteration 8, loss = 0.00536001\n",
      "Iteration 9, loss = 0.00420294\n",
      "Iteration 10, loss = 0.00631606\n",
      "Iteration 11, loss = 0.00794032\n",
      "Iteration 12, loss = 0.00727731\n",
      "Iteration 13, loss = 0.00535273\n",
      "Iteration 14, loss = 0.00391180\n",
      "Iteration 15, loss = 0.00373447\n",
      "Iteration 16, loss = 0.00442814\n",
      "Iteration 17, loss = 0.00521657\n",
      "Iteration 18, loss = 0.00551129\n",
      "Iteration 19, loss = 0.00514993\n",
      "Iteration 20, loss = 0.00434864\n",
      "Iteration 21, loss = 0.00353771\n",
      "Iteration 22, loss = 0.00310854\n",
      "Iteration 23, loss = 0.00319819\n",
      "Iteration 24, loss = 0.00359603\n",
      "Iteration 25, loss = 0.00391265\n",
      "Iteration 26, loss = 0.00387828\n",
      "Iteration 27, loss = 0.00354238\n",
      "Iteration 28, loss = 0.00317308\n",
      "Iteration 29, loss = 0.00299963\n",
      "Iteration 30, loss = 0.00305682\n",
      "Iteration 31, loss = 0.00321042\n",
      "Iteration 32, loss = 0.00329572\n",
      "Iteration 33, loss = 0.00323060\n",
      "Iteration 34, loss = 0.00305119\n",
      "Iteration 35, loss = 0.00286748\n",
      "Iteration 36, loss = 0.00279479\n",
      "Iteration 37, loss = 0.00286026\n",
      "Iteration 38, loss = 0.00293635\n",
      "Iteration 39, loss = 0.00293754\n",
      "Iteration 40, loss = 0.00286566\n",
      "Iteration 41, loss = 0.00276740\n",
      "Iteration 42, loss = 0.00268785\n",
      "Iteration 43, loss = 0.00265867\n",
      "Iteration 44, loss = 0.00267221\n",
      "Iteration 45, loss = 0.00269268\n",
      "Iteration 46, loss = 0.00268768\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02496437\n",
      "Iteration 2, loss = 0.05140291\n",
      "Iteration 3, loss = 0.01554793\n",
      "Iteration 4, loss = 0.00553214\n",
      "Iteration 5, loss = 0.01300696\n",
      "Iteration 6, loss = 0.01547165\n",
      "Iteration 7, loss = 0.01049345\n",
      "Iteration 8, loss = 0.00515452\n",
      "Iteration 9, loss = 0.00411276\n",
      "Iteration 10, loss = 0.00625790\n",
      "Iteration 11, loss = 0.00782215\n",
      "Iteration 12, loss = 0.00706841\n",
      "Iteration 13, loss = 0.00511974\n",
      "Iteration 14, loss = 0.00371985\n",
      "Iteration 15, loss = 0.00360188\n",
      "Iteration 16, loss = 0.00433336\n",
      "Iteration 17, loss = 0.00512109\n",
      "Iteration 18, loss = 0.00538823\n",
      "Iteration 19, loss = 0.00499698\n",
      "Iteration 20, loss = 0.00418545\n",
      "Iteration 21, loss = 0.00338694\n",
      "Iteration 22, loss = 0.00298389\n",
      "Iteration 23, loss = 0.00309140\n",
      "Iteration 24, loss = 0.00348828\n",
      "Iteration 25, loss = 0.00378844\n",
      "Iteration 26, loss = 0.00373997\n",
      "Iteration 27, loss = 0.00340734\n",
      "Iteration 28, loss = 0.00305487\n",
      "Iteration 29, loss = 0.00289792\n",
      "Iteration 30, loss = 0.00296427\n",
      "Iteration 31, loss = 0.00311739\n",
      "Iteration 32, loss = 0.00319848\n",
      "Iteration 33, loss = 0.00313235\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02416419\n",
      "Iteration 2, loss = 0.05284024\n",
      "Iteration 3, loss = 0.01560361\n",
      "Iteration 4, loss = 0.00595151\n",
      "Iteration 5, loss = 0.01391126\n",
      "Iteration 6, loss = 0.01632116\n",
      "Iteration 7, loss = 0.01108332\n",
      "Iteration 8, loss = 0.00544804\n",
      "Iteration 9, loss = 0.00421975\n",
      "Iteration 10, loss = 0.00641937\n",
      "Iteration 11, loss = 0.00812759\n",
      "Iteration 12, loss = 0.00746893\n",
      "Iteration 13, loss = 0.00554091\n",
      "Iteration 14, loss = 0.00402130\n",
      "Iteration 15, loss = 0.00373564\n",
      "Iteration 16, loss = 0.00436465\n",
      "Iteration 17, loss = 0.00516888\n",
      "Iteration 18, loss = 0.00553607\n",
      "Iteration 19, loss = 0.00525897\n",
      "Iteration 20, loss = 0.00450114\n",
      "Iteration 21, loss = 0.00366524\n",
      "Iteration 22, loss = 0.00315439\n",
      "Iteration 23, loss = 0.00315450\n",
      "Iteration 24, loss = 0.00351941\n",
      "Iteration 25, loss = 0.00387941\n",
      "Iteration 26, loss = 0.00392378\n",
      "Iteration 27, loss = 0.00363366\n",
      "Iteration 28, loss = 0.00324450\n",
      "Iteration 29, loss = 0.00300732\n",
      "Iteration 30, loss = 0.00300688\n",
      "Iteration 31, loss = 0.00314919\n",
      "Iteration 32, loss = 0.00326792\n",
      "Iteration 33, loss = 0.00325225\n",
      "Iteration 34, loss = 0.00310056\n",
      "Iteration 35, loss = 0.00290039\n",
      "Iteration 36, loss = 0.00277701\n",
      "Iteration 37, loss = 0.00280426\n",
      "Iteration 38, loss = 0.00288958\n",
      "Iteration 39, loss = 0.00291312\n",
      "Iteration 40, loss = 0.00285441\n",
      "Iteration 41, loss = 0.00275826\n",
      "Iteration 42, loss = 0.00267498\n",
      "Iteration 43, loss = 0.00263177\n",
      "Iteration 44, loss = 0.00263160\n",
      "Iteration 45, loss = 0.00264824\n",
      "Iteration 46, loss = 0.00264974\n",
      "Iteration 47, loss = 0.00262320\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02387065\n",
      "Iteration 2, loss = 0.05294027\n",
      "Iteration 3, loss = 0.01551478\n",
      "Iteration 4, loss = 0.00593948\n",
      "Iteration 5, loss = 0.01397684\n",
      "Iteration 6, loss = 0.01641617\n",
      "Iteration 7, loss = 0.01116451\n",
      "Iteration 8, loss = 0.00547309\n",
      "Iteration 9, loss = 0.00415303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.00630746\n",
      "Iteration 11, loss = 0.00811459\n",
      "Iteration 12, loss = 0.00760622\n",
      "Iteration 13, loss = 0.00568447\n",
      "Iteration 14, loss = 0.00409861\n",
      "Iteration 15, loss = 0.00374211\n",
      "Iteration 16, loss = 0.00434000\n",
      "Iteration 17, loss = 0.00515196\n",
      "Iteration 18, loss = 0.00555058\n",
      "Iteration 19, loss = 0.00530172\n",
      "Iteration 20, loss = 0.00455969\n",
      "Iteration 21, loss = 0.00371275\n",
      "Iteration 22, loss = 0.00317577\n",
      "Iteration 23, loss = 0.00315660\n",
      "Iteration 24, loss = 0.00352260\n",
      "Iteration 25, loss = 0.00390528\n",
      "Iteration 26, loss = 0.00397140\n",
      "Iteration 27, loss = 0.00368560\n",
      "Iteration 28, loss = 0.00328196\n",
      "Iteration 29, loss = 0.00302594\n",
      "Iteration 30, loss = 0.00301398\n",
      "Iteration 31, loss = 0.00315647\n",
      "Iteration 32, loss = 0.00328588\n",
      "Iteration 33, loss = 0.00328471\n",
      "Iteration 34, loss = 0.00314245\n",
      "Iteration 35, loss = 0.00294149\n",
      "Iteration 36, loss = 0.00280707\n",
      "Iteration 37, loss = 0.00282134\n",
      "Iteration 38, loss = 0.00290766\n",
      "Iteration 39, loss = 0.00293828\n",
      "Iteration 40, loss = 0.00288517\n",
      "Iteration 41, loss = 0.00278865\n",
      "Iteration 42, loss = 0.00270117\n",
      "Iteration 43, loss = 0.00265359\n",
      "Iteration 44, loss = 0.00265192\n",
      "Iteration 45, loss = 0.00267089\n",
      "Iteration 46, loss = 0.00267630\n",
      "Iteration 47, loss = 0.00265197\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02382086\n",
      "Iteration 2, loss = 0.05298554\n",
      "Iteration 3, loss = 0.01549719\n",
      "Iteration 4, loss = 0.00599086\n",
      "Iteration 5, loss = 0.01405637\n",
      "Iteration 6, loss = 0.01643920\n",
      "Iteration 7, loss = 0.01114868\n",
      "Iteration 8, loss = 0.00545772\n",
      "Iteration 9, loss = 0.00418863\n",
      "Iteration 10, loss = 0.00638571\n",
      "Iteration 11, loss = 0.00812870\n",
      "Iteration 12, loss = 0.00748915\n",
      "Iteration 13, loss = 0.00557545\n",
      "Iteration 14, loss = 0.00405420\n",
      "Iteration 15, loss = 0.00376164\n",
      "Iteration 16, loss = 0.00438759\n",
      "Iteration 17, loss = 0.00518166\n",
      "Iteration 18, loss = 0.00553124\n",
      "Iteration 19, loss = 0.00522778\n",
      "Iteration 20, loss = 0.00445775\n",
      "Iteration 21, loss = 0.00363644\n",
      "Iteration 22, loss = 0.00316003\n",
      "Iteration 23, loss = 0.00319904\n",
      "Iteration 24, loss = 0.00358225\n",
      "Iteration 25, loss = 0.00393199\n",
      "Iteration 26, loss = 0.00394991\n",
      "Iteration 27, loss = 0.00363968\n",
      "Iteration 28, loss = 0.00325013\n",
      "Iteration 29, loss = 0.00302818\n",
      "Iteration 30, loss = 0.00304353\n",
      "Iteration 31, loss = 0.00319082\n",
      "Iteration 32, loss = 0.00330460\n",
      "Iteration 33, loss = 0.00327900\n",
      "Iteration 34, loss = 0.00312416\n",
      "Iteration 35, loss = 0.00293089\n",
      "Iteration 36, loss = 0.00281874\n",
      "Iteration 37, loss = 0.00285259\n",
      "Iteration 38, loss = 0.00293387\n",
      "Iteration 39, loss = 0.00295335\n",
      "Iteration 40, loss = 0.00289313\n",
      "Iteration 41, loss = 0.00279615\n",
      "Iteration 42, loss = 0.00270734\n",
      "Iteration 43, loss = 0.00266024\n",
      "Iteration 44, loss = 0.00266074\n",
      "Iteration 45, loss = 0.00268037\n",
      "Iteration 46, loss = 0.00268301\n",
      "Iteration 47, loss = 0.00265334\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02432020\n",
      "Iteration 2, loss = 0.05261758\n",
      "Iteration 3, loss = 0.01564051\n",
      "Iteration 4, loss = 0.00597433\n",
      "Iteration 5, loss = 0.01382094\n",
      "Iteration 6, loss = 0.01618095\n",
      "Iteration 7, loss = 0.01096926\n",
      "Iteration 8, loss = 0.00541333\n",
      "Iteration 9, loss = 0.00424590\n",
      "Iteration 10, loss = 0.00643864\n",
      "Iteration 11, loss = 0.00810188\n",
      "Iteration 12, loss = 0.00741723\n",
      "Iteration 13, loss = 0.00547407\n",
      "Iteration 14, loss = 0.00398192\n",
      "Iteration 15, loss = 0.00376257\n",
      "Iteration 16, loss = 0.00444620\n",
      "Iteration 17, loss = 0.00524458\n",
      "Iteration 18, loss = 0.00555070\n",
      "Iteration 19, loss = 0.00518830\n",
      "Iteration 20, loss = 0.00437554\n",
      "Iteration 21, loss = 0.00354572\n",
      "Iteration 22, loss = 0.00310203\n",
      "Iteration 23, loss = 0.00318504\n",
      "Iteration 24, loss = 0.00358763\n",
      "Iteration 25, loss = 0.00391440\n",
      "Iteration 26, loss = 0.00388669\n",
      "Iteration 27, loss = 0.00354650\n",
      "Iteration 28, loss = 0.00316108\n",
      "Iteration 29, loss = 0.00296769\n",
      "Iteration 30, loss = 0.00301249\n",
      "Iteration 31, loss = 0.00316490\n",
      "Iteration 32, loss = 0.00325843\n",
      "Iteration 33, loss = 0.00320288\n",
      "Iteration 34, loss = 0.00302617\n",
      "Iteration 35, loss = 0.00283183\n",
      "Iteration 36, loss = 0.00274033\n",
      "Iteration 37, loss = 0.00279655\n",
      "Iteration 38, loss = 0.00287492\n",
      "Iteration 39, loss = 0.00287865\n",
      "Iteration 40, loss = 0.00280355\n",
      "Iteration 41, loss = 0.00269951\n",
      "Iteration 42, loss = 0.00261426\n",
      "Iteration 43, loss = 0.00257837\n",
      "Iteration 44, loss = 0.00258776\n",
      "Iteration 45, loss = 0.00260560\n",
      "Iteration 46, loss = 0.00259770\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02464789\n",
      "Iteration 2, loss = 0.05207753\n",
      "Iteration 3, loss = 0.01557949\n",
      "Iteration 4, loss = 0.00580306\n",
      "Iteration 5, loss = 0.01341065\n",
      "Iteration 6, loss = 0.01579037\n",
      "Iteration 7, loss = 0.01069010\n",
      "Iteration 8, loss = 0.00530459\n",
      "Iteration 9, loss = 0.00425611\n",
      "Iteration 10, loss = 0.00642082\n",
      "Iteration 11, loss = 0.00799401\n",
      "Iteration 12, loss = 0.00723296\n",
      "Iteration 13, loss = 0.00527079\n",
      "Iteration 14, loss = 0.00385296\n",
      "Iteration 15, loss = 0.00372112\n",
      "Iteration 16, loss = 0.00444466\n",
      "Iteration 17, loss = 0.00523235\n",
      "Iteration 18, loss = 0.00550232\n",
      "Iteration 19, loss = 0.00511227\n",
      "Iteration 20, loss = 0.00429477\n",
      "Iteration 21, loss = 0.00348299\n",
      "Iteration 22, loss = 0.00307043\n",
      "Iteration 23, loss = 0.00317271\n",
      "Iteration 24, loss = 0.00357205\n",
      "Iteration 25, loss = 0.00387574\n",
      "Iteration 26, loss = 0.00382480\n",
      "Iteration 27, loss = 0.00348015\n",
      "Iteration 28, loss = 0.00311286\n",
      "Iteration 29, loss = 0.00294582\n",
      "Iteration 30, loss = 0.00300817\n",
      "Iteration 31, loss = 0.00316253\n",
      "Iteration 32, loss = 0.00324268\n",
      "Iteration 33, loss = 0.00316906\n",
      "Iteration 34, loss = 0.00298374\n",
      "Iteration 35, loss = 0.00279822\n",
      "Iteration 36, loss = 0.00272733\n",
      "Iteration 37, loss = 0.00279376\n",
      "Iteration 38, loss = 0.00286851\n",
      "Iteration 39, loss = 0.00286806\n",
      "Iteration 40, loss = 0.00279465\n",
      "Iteration 41, loss = 0.00269404\n",
      "Iteration 42, loss = 0.00261224\n",
      "Iteration 43, loss = 0.00258184\n",
      "Iteration 44, loss = 0.00259467\n",
      "Iteration 45, loss = 0.00261387\n",
      "Iteration 46, loss = 0.00260675\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02489114\n",
      "Iteration 2, loss = 0.05174186\n",
      "Iteration 3, loss = 0.01566972\n",
      "Iteration 4, loss = 0.00565879\n",
      "Iteration 5, loss = 0.01321059\n",
      "Iteration 6, loss = 0.01568994\n",
      "Iteration 7, loss = 0.01069012\n",
      "Iteration 8, loss = 0.00529572\n",
      "Iteration 9, loss = 0.00411888\n",
      "Iteration 10, loss = 0.00620605\n",
      "Iteration 11, loss = 0.00784604\n",
      "Iteration 12, loss = 0.00720325\n",
      "Iteration 13, loss = 0.00529406\n",
      "Iteration 14, loss = 0.00385458\n",
      "Iteration 15, loss = 0.00367520\n",
      "Iteration 16, loss = 0.00437510\n",
      "Iteration 17, loss = 0.00516549\n",
      "Iteration 18, loss = 0.00544630\n",
      "Iteration 19, loss = 0.00506284\n",
      "Iteration 20, loss = 0.00424883\n",
      "Iteration 21, loss = 0.00344704\n",
      "Iteration 22, loss = 0.00304347\n",
      "Iteration 23, loss = 0.00315755\n",
      "Iteration 24, loss = 0.00356543\n",
      "Iteration 25, loss = 0.00387226\n",
      "Iteration 26, loss = 0.00382107\n",
      "Iteration 27, loss = 0.00347904\n",
      "Iteration 28, loss = 0.00311681\n",
      "Iteration 29, loss = 0.00295545\n",
      "Iteration 30, loss = 0.00301985\n",
      "Iteration 31, loss = 0.00317174\n",
      "Iteration 32, loss = 0.00325129\n",
      "Iteration 33, loss = 0.00318401\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02483012\n",
      "Iteration 2, loss = 0.05207229\n",
      "Iteration 3, loss = 0.01571501\n",
      "Iteration 4, loss = 0.00578621\n",
      "Iteration 5, loss = 0.01340190\n",
      "Iteration 6, loss = 0.01583338\n",
      "Iteration 7, loss = 0.01077391\n",
      "Iteration 8, loss = 0.00534390\n",
      "Iteration 9, loss = 0.00423357\n",
      "Iteration 10, loss = 0.00640375\n",
      "Iteration 11, loss = 0.00800521\n",
      "Iteration 12, loss = 0.00729744\n",
      "Iteration 13, loss = 0.00535573\n",
      "Iteration 14, loss = 0.00390892\n",
      "Iteration 15, loss = 0.00372821\n",
      "Iteration 16, loss = 0.00442285\n",
      "Iteration 17, loss = 0.00521417\n",
      "Iteration 18, loss = 0.00551564\n",
      "Iteration 19, loss = 0.00516322\n",
      "Iteration 20, loss = 0.00436886\n",
      "Iteration 21, loss = 0.00355481\n",
      "Iteration 22, loss = 0.00310998\n",
      "Iteration 23, loss = 0.00317718\n",
      "Iteration 24, loss = 0.00356141\n",
      "Iteration 25, loss = 0.00388314\n",
      "Iteration 26, loss = 0.00386848\n",
      "Iteration 27, loss = 0.00354929\n",
      "Iteration 28, loss = 0.00318115\n",
      "Iteration 29, loss = 0.00299392\n",
      "Iteration 30, loss = 0.00303498\n",
      "Iteration 31, loss = 0.00318358\n",
      "Iteration 32, loss = 0.00327645\n",
      "Iteration 33, loss = 0.00322696\n",
      "Iteration 34, loss = 0.00306098\n",
      "Iteration 35, loss = 0.00288022\n",
      "Iteration 36, loss = 0.00278674\n",
      "Iteration 37, loss = 0.00282534\n",
      "Iteration 38, loss = 0.00290462\n",
      "Iteration 39, loss = 0.00293781\n",
      "Iteration 40, loss = 0.00289641\n",
      "Iteration 41, loss = 0.00279686\n",
      "Iteration 42, loss = 0.00269373\n",
      "Iteration 43, loss = 0.00263990\n",
      "Iteration 44, loss = 0.00264461\n",
      "Iteration 45, loss = 0.00267357\n",
      "Iteration 46, loss = 0.00268475\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02379365\n",
      "Iteration 2, loss = 0.05299986\n",
      "Iteration 3, loss = 0.01543549\n",
      "Iteration 4, loss = 0.00590328\n",
      "Iteration 5, loss = 0.01400441\n",
      "Iteration 6, loss = 0.01641779\n",
      "Iteration 7, loss = 0.01113117\n",
      "Iteration 8, loss = 0.00542625\n",
      "Iteration 9, loss = 0.00412233\n",
      "Iteration 10, loss = 0.00631827\n",
      "Iteration 11, loss = 0.00809174\n",
      "Iteration 12, loss = 0.00749929\n",
      "Iteration 13, loss = 0.00558640\n",
      "Iteration 14, loss = 0.00403200\n",
      "Iteration 15, loss = 0.00369178\n",
      "Iteration 16, loss = 0.00428915\n",
      "Iteration 17, loss = 0.00509584\n",
      "Iteration 18, loss = 0.00549188\n",
      "Iteration 19, loss = 0.00524879\n",
      "Iteration 20, loss = 0.00451464\n",
      "Iteration 21, loss = 0.00367760\n",
      "Iteration 22, loss = 0.00314588\n",
      "Iteration 23, loss = 0.00311985\n",
      "Iteration 24, loss = 0.00347510\n",
      "Iteration 25, loss = 0.00385154\n",
      "Iteration 26, loss = 0.00392476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27, loss = 0.00365450\n",
      "Iteration 28, loss = 0.00325782\n",
      "Iteration 29, loss = 0.00300185\n",
      "Iteration 30, loss = 0.00298591\n",
      "Iteration 31, loss = 0.00312535\n",
      "Iteration 32, loss = 0.00325535\n",
      "Iteration 33, loss = 0.00325733\n",
      "Iteration 34, loss = 0.00312167\n",
      "Iteration 35, loss = 0.00293076\n",
      "Iteration 36, loss = 0.00279370\n",
      "Iteration 37, loss = 0.00279042\n",
      "Iteration 38, loss = 0.00286942\n",
      "Iteration 39, loss = 0.00291999\n",
      "Iteration 40, loss = 0.00289939\n",
      "Iteration 41, loss = 0.00281641\n",
      "Iteration 42, loss = 0.00271277\n",
      "Iteration 43, loss = 0.00264234\n",
      "Iteration 44, loss = 0.00262987\n",
      "Iteration 45, loss = 0.00265367\n",
      "Iteration 46, loss = 0.00267387\n",
      "Iteration 47, loss = 0.00265897\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02373537\n",
      "Iteration 2, loss = 0.05302049\n",
      "Iteration 3, loss = 0.01545025\n",
      "Iteration 4, loss = 0.00595699\n",
      "Iteration 5, loss = 0.01405832\n",
      "Iteration 6, loss = 0.01644919\n",
      "Iteration 7, loss = 0.01113710\n",
      "Iteration 8, loss = 0.00545692\n",
      "Iteration 9, loss = 0.00417454\n",
      "Iteration 10, loss = 0.00635853\n",
      "Iteration 11, loss = 0.00813619\n",
      "Iteration 12, loss = 0.00755685\n",
      "Iteration 13, loss = 0.00563934\n",
      "Iteration 14, loss = 0.00408918\n",
      "Iteration 15, loss = 0.00375967\n",
      "Iteration 16, loss = 0.00436039\n",
      "Iteration 17, loss = 0.00516091\n",
      "Iteration 18, loss = 0.00554347\n",
      "Iteration 19, loss = 0.00528406\n",
      "Iteration 20, loss = 0.00454157\n",
      "Iteration 21, loss = 0.00370835\n",
      "Iteration 22, loss = 0.00319047\n",
      "Iteration 23, loss = 0.00318583\n",
      "Iteration 24, loss = 0.00355698\n",
      "Iteration 25, loss = 0.00393032\n",
      "Iteration 26, loss = 0.00398445\n",
      "Iteration 27, loss = 0.00369355\n",
      "Iteration 28, loss = 0.00329464\n",
      "Iteration 29, loss = 0.00304755\n",
      "Iteration 30, loss = 0.00304292\n",
      "Iteration 31, loss = 0.00318683\n",
      "Iteration 32, loss = 0.00331382\n",
      "Iteration 33, loss = 0.00330729\n",
      "Iteration 34, loss = 0.00316413\n",
      "Iteration 35, loss = 0.00297163\n",
      "Iteration 36, loss = 0.00283987\n",
      "Iteration 37, loss = 0.00284254\n",
      "Iteration 38, loss = 0.00292079\n",
      "Iteration 39, loss = 0.00296626\n",
      "Iteration 40, loss = 0.00294010\n",
      "Iteration 41, loss = 0.00285039\n",
      "Iteration 42, loss = 0.00274401\n",
      "Iteration 43, loss = 0.00267567\n",
      "Iteration 44, loss = 0.00266611\n",
      "Iteration 45, loss = 0.00269285\n",
      "Iteration 46, loss = 0.00271101\n",
      "Iteration 47, loss = 0.00269113\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02439530\n",
      "Iteration 2, loss = 0.05264242\n",
      "Iteration 3, loss = 0.01567154\n",
      "Iteration 4, loss = 0.00595795\n",
      "Iteration 5, loss = 0.01378800\n",
      "Iteration 6, loss = 0.01616886\n",
      "Iteration 7, loss = 0.01096335\n",
      "Iteration 8, loss = 0.00541295\n",
      "Iteration 9, loss = 0.00424927\n",
      "Iteration 10, loss = 0.00641007\n",
      "Iteration 11, loss = 0.00804437\n",
      "Iteration 12, loss = 0.00737481\n",
      "Iteration 13, loss = 0.00544913\n",
      "Iteration 14, loss = 0.00396472\n",
      "Iteration 15, loss = 0.00373130\n",
      "Iteration 16, loss = 0.00439851\n",
      "Iteration 17, loss = 0.00519057\n",
      "Iteration 18, loss = 0.00550280\n",
      "Iteration 19, loss = 0.00515584\n",
      "Iteration 20, loss = 0.00435696\n",
      "Iteration 21, loss = 0.00353636\n",
      "Iteration 22, loss = 0.00308855\n",
      "Iteration 23, loss = 0.00315877\n",
      "Iteration 24, loss = 0.00355014\n",
      "Iteration 25, loss = 0.00387405\n",
      "Iteration 26, loss = 0.00385300\n",
      "Iteration 27, loss = 0.00352319\n",
      "Iteration 28, loss = 0.00314474\n",
      "Iteration 29, loss = 0.00295139\n",
      "Iteration 30, loss = 0.00299038\n",
      "Iteration 31, loss = 0.00313944\n",
      "Iteration 32, loss = 0.00323325\n",
      "Iteration 33, loss = 0.00318116\n",
      "Iteration 34, loss = 0.00300858\n",
      "Iteration 35, loss = 0.00281697\n",
      "Iteration 36, loss = 0.00272417\n",
      "Iteration 37, loss = 0.00277495\n",
      "Iteration 38, loss = 0.00285191\n",
      "Iteration 39, loss = 0.00285903\n",
      "Iteration 40, loss = 0.00278846\n",
      "Iteration 41, loss = 0.00268453\n",
      "Iteration 42, loss = 0.00259549\n",
      "Iteration 43, loss = 0.00255525\n",
      "Iteration 44, loss = 0.00256101\n",
      "Iteration 45, loss = 0.00257922\n",
      "Iteration 46, loss = 0.00257551\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02146492\n",
      "Iteration 2, loss = 0.05299593\n",
      "Iteration 3, loss = 0.01411842\n",
      "Iteration 4, loss = 0.00494793\n",
      "Iteration 5, loss = 0.01340534\n",
      "Iteration 6, loss = 0.01564647\n",
      "Iteration 7, loss = 0.01016235\n",
      "Iteration 8, loss = 0.00431955\n",
      "Iteration 9, loss = 0.00284468\n",
      "Iteration 10, loss = 0.00493578\n",
      "Iteration 11, loss = 0.00685129\n",
      "Iteration 12, loss = 0.00647892\n",
      "Iteration 13, loss = 0.00469533\n",
      "Iteration 14, loss = 0.00307751\n",
      "Iteration 15, loss = 0.00258306\n",
      "Iteration 16, loss = 0.00306303\n",
      "Iteration 17, loss = 0.00383200\n",
      "Iteration 18, loss = 0.00425828\n",
      "Iteration 19, loss = 0.00407101\n",
      "Iteration 20, loss = 0.00337815\n",
      "Iteration 21, loss = 0.00255140\n",
      "Iteration 22, loss = 0.00200102\n",
      "Iteration 23, loss = 0.00195023\n",
      "Iteration 24, loss = 0.00229787\n",
      "Iteration 25, loss = 0.00268555\n",
      "Iteration 26, loss = 0.00278418\n",
      "Iteration 27, loss = 0.00252703\n",
      "Iteration 28, loss = 0.00212068\n",
      "Iteration 29, loss = 0.00182897\n",
      "Iteration 30, loss = 0.00177373\n",
      "Iteration 31, loss = 0.00189149\n",
      "Iteration 32, loss = 0.00203022\n",
      "Iteration 33, loss = 0.00206219\n",
      "Iteration 34, loss = 0.00195452\n",
      "Iteration 35, loss = 0.00177231\n",
      "Iteration 36, loss = 0.00161919\n",
      "Iteration 37, loss = 0.00158380\n",
      "Iteration 38, loss = 0.00164880\n",
      "Iteration 39, loss = 0.00170020\n",
      "Iteration 40, loss = 0.00168308\n",
      "Iteration 41, loss = 0.00160702\n",
      "Iteration 42, loss = 0.00150852\n",
      "Iteration 43, loss = 0.00143468\n",
      "Iteration 44, loss = 0.00141253\n",
      "Iteration 45, loss = 0.00143206\n",
      "Iteration 46, loss = 0.00145439\n",
      "Iteration 47, loss = 0.00144686\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02387076\n",
      "Iteration 2, loss = 0.05260822\n",
      "Iteration 3, loss = 0.01550771\n",
      "Iteration 4, loss = 0.00593167\n",
      "Iteration 5, loss = 0.01373054\n",
      "Iteration 6, loss = 0.01609972\n",
      "Iteration 7, loss = 0.01093982\n",
      "Iteration 8, loss = 0.00543793\n",
      "Iteration 9, loss = 0.00415689\n",
      "Iteration 10, loss = 0.00628081\n",
      "Iteration 11, loss = 0.00813035\n",
      "Iteration 12, loss = 0.00759051\n",
      "Iteration 13, loss = 0.00563923\n",
      "Iteration 14, loss = 0.00405859\n",
      "Iteration 15, loss = 0.00373241\n",
      "Iteration 16, loss = 0.00436513\n",
      "Iteration 17, loss = 0.00518229\n",
      "Iteration 18, loss = 0.00557127\n",
      "Iteration 19, loss = 0.00531598\n",
      "Iteration 20, loss = 0.00457154\n",
      "Iteration 21, loss = 0.00371946\n",
      "Iteration 22, loss = 0.00317590\n",
      "Iteration 23, loss = 0.00315156\n",
      "Iteration 24, loss = 0.00351301\n",
      "Iteration 25, loss = 0.00389185\n",
      "Iteration 26, loss = 0.00395887\n",
      "Iteration 27, loss = 0.00367573\n",
      "Iteration 28, loss = 0.00327497\n",
      "Iteration 29, loss = 0.00302220\n",
      "Iteration 30, loss = 0.00301207\n",
      "Iteration 31, loss = 0.00315560\n",
      "Iteration 32, loss = 0.00328427\n",
      "Iteration 33, loss = 0.00328115\n",
      "Iteration 34, loss = 0.00314044\n",
      "Iteration 35, loss = 0.00294899\n",
      "Iteration 36, loss = 0.00281612\n",
      "Iteration 37, loss = 0.00279889\n",
      "Iteration 38, loss = 0.00287169\n",
      "Iteration 39, loss = 0.00294909\n",
      "Iteration 40, loss = 0.00295210\n",
      "Iteration 41, loss = 0.00286732\n",
      "Iteration 42, loss = 0.00274726\n",
      "Iteration 43, loss = 0.00266178\n",
      "Iteration 44, loss = 0.00264450\n",
      "Iteration 45, loss = 0.00267594\n",
      "Iteration 46, loss = 0.00270623\n",
      "Iteration 47, loss = 0.00269689\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Config of the regressors and cross val leave one out\n",
    "####\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes = (100, 50), alpha = 0.001,\n",
    "    learning_rate_init = 0.01, max_iter = 1000,\n",
    "    random_state = 9, verbose = True)\n",
    "svr = SVR(kernel = 'linear', C = 0.25, epsilon = 0.01, verbose = True, max_iter = 1000)\n",
    "lr = LinearRegression()\n",
    "\n",
    "full_predict_lr = cross_val_predict(lr, X, target, cv = 10)\n",
    "full_predict_mlp = cross_val_predict(mlp, X, target, cv = loo)\n",
    "full_predict_svr = cross_val_predict(svr, X, target, cv = loo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error in LR: 0.00599490799826504\n",
      "Mean Squared Error in MLP: 0.005956410936480039\n",
      "Mean Squared Error in SVR: 0.005935750349804434\n",
      "R score in LR: 0.8612230866468593\n",
      "R score in MLP: 0.8621142601910127\n",
      "R score in SVR: 0.8625925348280439\n",
      "adjusted R score in LR: 0.8516991808285065\n",
      "adjusted R score in MLP: 0.8526515133413763\n",
      "adjusted R score in SVR: 0.8531626107476156\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Printing some metrics of the regressors\n",
    "####\n",
    "\n",
    "print('Mean Squared Error in LR: %s' %(metrics.mean_squared_error(target, full_predict_lr)))\n",
    "print('Mean Squared Error in MLP: %s' %(metrics.mean_squared_error(target, full_predict_mlp)))\n",
    "print('Mean Squared Error in SVR: %s' %(metrics.mean_squared_error(target, full_predict_svr)))\n",
    "\n",
    "r_squared_lr = metrics.r2_score(target, full_predict_lr)\n",
    "r_squared_mlp = metrics.r2_score(target, full_predict_mlp)\n",
    "r_squared_svr = metrics.r2_score(target, full_predict_svr)\n",
    "\n",
    "print('R score in LR: %s' %(r_squared_lr))\n",
    "print('R score in MLP: %s' %(r_squared_mlp))\n",
    "print('R score in SVR: %s' %(r_squared_svr))\n",
    "\n",
    "adjusted_r_squared_lr = 1 - (1 - r_squared_lr) * (len(target) - 1) / (len(target) - X.shape[1] - 1)\n",
    "adjusted_r_squared_mlp = 1 - (1 - r_squared_mlp) * (len(target) - 1) / (len(target) - X.shape[1] - 1)\n",
    "adjusted_r_squared_svr = 1 - (1 - r_squared_svr) * (len(target) - 1) / (len(target) - X.shape[1] - 1)\n",
    "\n",
    "print('adjusted R score in LR: %s' %(adjusted_r_squared_lr))\n",
    "print('adjusted R score in MLP: %s' %(adjusted_r_squared_mlp))\n",
    "print('adjusted R score in SVR: %s' %(adjusted_r_squared_svr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "## Filling lists with NaN so the len is the same across all lists \n",
    "## so that a graph can be generated\n",
    "####\n",
    "import numpy as np\n",
    "\n",
    "values_to_add = list()\n",
    "for i in range(0, window_size):\n",
    "    values_to_add.append(float('NaN'))\n",
    "    \n",
    "full_predict_svr = np.insert(full_predict_svr, 0, values_to_add)\n",
    "full_predict_svr.shape = (len(full_predict_svr), 1)\n",
    "    \n",
    "full_predict_mlp = np.insert(full_predict_mlp, 0, values_to_add)\n",
    "full_predict_mlp.shape = (len(full_predict_mlp), 1)\n",
    "\n",
    "full_predict_lr = np.insert(full_predict_lr, 0, values_to_add)\n",
    "full_predict_lr.shape = (len(full_predict_lr), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "## Adding the data to plot \n",
    "####\n",
    "\n",
    "data['Predict_lr'] = full_predict_lr\n",
    "data['Predict_mlp'] = full_predict_mlp\n",
    "data['Predict_svr'] = full_predict_svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data.plot(y=['CasosNormalizados', 'Predict_mlp', 'Predict_svr', 'Predict_lr'], style=['-s', '--o'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
