{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "## Celso Antonio Uliana Junior\n",
    "## July 2 2020\n",
    "####\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#####\n",
    "## Consuming and shaping the data to analysis\n",
    "## Covid-19 numbers in Brazil by date\n",
    "## Isolation percentage in Brazil by date\n",
    "#####\n",
    "\n",
    "data_raw_covid = pd.read_csv(\"C:/Users/PCDOMILHAO/Documents/GitHub/trab-siad/scripts/Jupyter-Notebook/dados/covidBrasil.csv\", sep = \";\", decimal = \",\")\n",
    "data_covid = data_raw_covid['Data'].values.copy()\n",
    "data_covid = data_raw_covid.dropna().set_index(\"Data\")\n",
    "\n",
    "####\n",
    "## Shaping a central pandas dataFrame for all our ML needs\n",
    "####\n",
    "\n",
    "data = data_covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Casos</th>\n",
       "      <th>CasosNormalizados</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26/2/20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/3/20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17/6/20</th>\n",
       "      <td>34918</td>\n",
       "      <td>0.637527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18/6/20</th>\n",
       "      <td>32188</td>\n",
       "      <td>0.587683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19/6/20</th>\n",
       "      <td>22765</td>\n",
       "      <td>0.415640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20/6/20</th>\n",
       "      <td>54771</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21/6/20</th>\n",
       "      <td>34666</td>\n",
       "      <td>0.632926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Casos  CasosNormalizados\n",
       "Data                             \n",
       "26/2/20      1           0.000018\n",
       "27/2/20      0           0.000000\n",
       "28/2/20      0           0.000000\n",
       "29/2/20      0           0.000000\n",
       "1/3/20       1           0.000018\n",
       "...        ...                ...\n",
       "17/6/20  34918           0.637527\n",
       "18/6/20  32188           0.587683\n",
       "19/6/20  22765           0.415640\n",
       "20/6/20  54771           1.000000\n",
       "21/6/20  34666           0.632926\n",
       "\n",
       "[117 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "####\n",
    "## normalizing values for both covid and isolation percentage \n",
    "## between range [0,1] using sklearn MinMaxScaler\n",
    "####\n",
    "\n",
    "covid_norm = data_covid[\"Casos\"].values.copy()\n",
    "covid_norm.shape = (len(covid_norm), 1)\n",
    "\n",
    "####\n",
    "## Shaping the central dataFrame with normalized values\n",
    "####\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "covid_norm = min_max_scaler.fit_transform(covid_norm)\n",
    "\n",
    "data[\"CasosNormalizados\"] = covid_norm\n",
    "data.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               E0        E1        E2        E3        E4        E5        E6\n",
      "Data                                                                         \n",
      "26/2/20  0.000018  0.000000  0.000000  0.000000  0.000018  0.000000  0.000000\n",
      "27/2/20  0.000000  0.000000  0.000000  0.000018  0.000000  0.000000  0.000000\n",
      "28/2/20  0.000000  0.000000  0.000018  0.000000  0.000000  0.000000  0.000018\n",
      "29/2/20  0.000000  0.000018  0.000000  0.000000  0.000000  0.000018  0.000091\n",
      "1/3/20   0.000018  0.000000  0.000000  0.000000  0.000018  0.000091  0.000091\n",
      "...           ...       ...       ...       ...       ...       ...       ...\n",
      "11/6/20  0.600920  0.555257  0.474375  0.396268  0.312392  0.376970  0.637527\n",
      "12/6/20  0.555257  0.474375  0.396268  0.312392  0.376970  0.637527  0.587683\n",
      "13/6/20  0.474375  0.396268  0.312392  0.376970  0.637527  0.587683  0.415640\n",
      "14/6/20  0.396268  0.312392  0.376970  0.637527  0.587683  0.415640  1.000000\n",
      "15/6/20  0.312392  0.376970  0.637527  0.587683  0.415640  1.000000  0.632926\n",
      "\n",
      "[111 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Sliding window\n",
    "####\n",
    "df = pd.DataFrame()\n",
    "window_size = 6\n",
    "for i in range(0, window_size + 1):\n",
    "    df['E{}'.format(i)] = data['CasosNormalizados'].shift(-i)\n",
    "df = df.iloc[: -window_size]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.82578372e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.82578372e-05 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.82578372e-05\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.82578372e-05 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.82578372e-05 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.82578372e-05]\n",
      " [1.82578372e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.82578372e-05 9.12891859e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.82578372e-05\n",
      "  9.12891859e-05 9.12891859e-05]\n",
      " [0.00000000e+00 0.00000000e+00 1.82578372e-05 9.12891859e-05\n",
      "  9.12891859e-05 0.00000000e+00]\n",
      " [0.00000000e+00 1.82578372e-05 9.12891859e-05 9.12891859e-05\n",
      "  0.00000000e+00 2.19094046e-04]\n",
      " [1.82578372e-05 9.12891859e-05 9.12891859e-05 0.00000000e+00\n",
      "  2.19094046e-04 0.00000000e+00]\n",
      " [9.12891859e-05 9.12891859e-05 0.00000000e+00 2.19094046e-04\n",
      "  0.00000000e+00 1.64320535e-04]\n",
      " [9.12891859e-05 0.00000000e+00 2.19094046e-04 0.00000000e+00\n",
      "  1.64320535e-04 3.28641069e-04]\n",
      " [0.00000000e+00 2.19094046e-04 0.00000000e+00 1.64320535e-04\n",
      "  3.28641069e-04 4.56445929e-04]\n",
      " [2.19094046e-04 0.00000000e+00 1.64320535e-04 3.28641069e-04\n",
      "  4.56445929e-04 3.83414581e-04]\n",
      " [0.00000000e+00 1.64320535e-04 3.28641069e-04 4.56445929e-04\n",
      "  3.83414581e-04 4.19930255e-04]\n",
      " [1.64320535e-04 3.28641069e-04 4.56445929e-04 3.83414581e-04\n",
      "  4.19930255e-04 1.44236914e-03]\n",
      " [3.28641069e-04 4.56445929e-04 3.83414581e-04 4.19930255e-04\n",
      "  1.44236914e-03 6.20766464e-04]\n",
      " [4.56445929e-04 3.83414581e-04 4.19930255e-04 1.44236914e-03\n",
      "  6.20766464e-04 1.04069672e-03]\n",
      " [3.83414581e-04 4.19930255e-04 1.44236914e-03 6.20766464e-04\n",
      "  1.04069672e-03 2.50132369e-03]\n",
      " [4.19930255e-04 1.44236914e-03 6.20766464e-04 1.04069672e-03\n",
      "  2.50132369e-03 3.52376258e-03]\n",
      " [1.44236914e-03 6.20766464e-04 1.04069672e-03 2.50132369e-03\n",
      "  3.52376258e-03 5.16696792e-03]\n",
      " [6.20766464e-04 1.04069672e-03 2.50132369e-03 3.52376258e-03\n",
      "  5.16696792e-03 4.08975553e-03]\n",
      " [1.04069672e-03 2.50132369e-03 3.52376258e-03 5.16696792e-03\n",
      "  4.08975553e-03 7.63177594e-03]\n",
      " [2.50132369e-03 3.52376258e-03 5.16696792e-03 4.08975553e-03\n",
      "  7.63177594e-03 6.29895383e-03]\n",
      " [3.52376258e-03 5.16696792e-03 4.08975553e-03 7.63177594e-03\n",
      "  6.29895383e-03 5.65992952e-03]\n",
      " [5.16696792e-03 4.08975553e-03 7.63177594e-03 6.29895383e-03\n",
      "  5.65992952e-03 4.23581822e-03]\n",
      " [4.08975553e-03 7.63177594e-03 6.29895383e-03 5.65992952e-03\n",
      "  4.23581822e-03 8.80027752e-03]\n",
      " [7.63177594e-03 6.29895383e-03 5.65992952e-03 4.23581822e-03\n",
      "  8.80027752e-03 9.16543426e-03]\n",
      " [6.29895383e-03 5.65992952e-03 4.23581822e-03 8.80027752e-03\n",
      "  9.16543426e-03 8.89156671e-03]\n",
      " [5.65992952e-03 4.23581822e-03 8.80027752e-03 9.16543426e-03\n",
      "  8.89156671e-03 6.42675869e-03]\n",
      " [4.23581822e-03 8.80027752e-03 9.16543426e-03 8.89156671e-03\n",
      "  6.42675869e-03 5.89728141e-03]\n",
      " [8.80027752e-03 9.16543426e-03 8.89156671e-03 6.42675869e-03\n",
      "  5.89728141e-03 2.07774187e-02]\n",
      " [9.16543426e-03 8.89156671e-03 6.42675869e-03 5.89728141e-03\n",
      "  2.07774187e-02 2.04305198e-02]\n",
      " [8.89156671e-03 6.42675869e-03 5.89728141e-03 2.07774187e-02\n",
      "  2.04305198e-02 1.96089171e-02]\n",
      " [6.42675869e-03 5.89728141e-03 2.07774187e-02 2.04305198e-02\n",
      "  1.96089171e-02 2.09234814e-02]\n",
      " [5.89728141e-03 2.07774187e-02 2.04305198e-02 1.96089171e-02\n",
      "  2.09234814e-02 2.23110770e-02]\n",
      " [2.07774187e-02 2.04305198e-02 1.96089171e-02 2.09234814e-02\n",
      "  2.23110770e-02 1.55556773e-02]\n",
      " [2.04305198e-02 1.96089171e-02 2.09234814e-02 2.23110770e-02\n",
      "  1.55556773e-02 1.69067572e-02]\n",
      " [1.96089171e-02 2.09234814e-02 2.23110770e-02 1.55556773e-02\n",
      "  1.69067572e-02 3.03262676e-02]\n",
      " [2.09234814e-02 2.23110770e-02 1.55556773e-02 1.69067572e-02\n",
      "  3.03262676e-02 4.03498202e-02]\n",
      " [2.23110770e-02 1.55556773e-02 1.69067572e-02 3.03262676e-02\n",
      "  4.03498202e-02 3.52376258e-02]\n",
      " [1.55556773e-02 1.69067572e-02 3.03262676e-02 4.03498202e-02\n",
      "  3.52376258e-02 3.25172080e-02]\n",
      " [1.69067572e-02 3.03262676e-02 4.03498202e-02 3.52376258e-02\n",
      "  3.25172080e-02 1.98827847e-02]\n",
      " [3.03262676e-02 4.03498202e-02 3.52376258e-02 3.25172080e-02\n",
      "  1.98827847e-02 2.63278012e-02]\n",
      " [4.03498202e-02 3.52376258e-02 3.25172080e-02 1.98827847e-02\n",
      "  2.63278012e-02 2.30231327e-02]\n",
      " [3.52376258e-02 3.25172080e-02 1.98827847e-02 2.63278012e-02\n",
      "  2.30231327e-02 3.34483577e-02]\n",
      " [3.25172080e-02 1.98827847e-02 2.63278012e-02 2.30231327e-02\n",
      "  3.34483577e-02 5.58324661e-02]\n",
      " [1.98827847e-02 2.63278012e-02 2.30231327e-02 3.34483577e-02\n",
      "  5.58324661e-02 3.84327473e-02]\n",
      " [2.63278012e-02 2.30231327e-02 3.34483577e-02 5.58324661e-02\n",
      "  3.84327473e-02 5.94657757e-02]\n",
      " [2.30231327e-02 3.34483577e-02 5.58324661e-02 3.84327473e-02\n",
      "  5.94657757e-02 5.32581110e-02]\n",
      " [3.34483577e-02 5.58324661e-02 3.84327473e-02 5.94657757e-02\n",
      "  5.32581110e-02 3.75198554e-02]\n",
      " [5.58324661e-02 3.84327473e-02 5.94657757e-02 5.32581110e-02\n",
      "  3.75198554e-02 3.51828522e-02]\n",
      " [3.84327473e-02 5.94657757e-02 5.32581110e-02 3.75198554e-02\n",
      "  3.51828522e-02 4.56080773e-02]\n",
      " [5.94657757e-02 5.32581110e-02 3.75198554e-02 3.51828522e-02\n",
      "  4.56080773e-02 4.88944880e-02]\n",
      " [5.32581110e-02 3.75198554e-02 3.51828522e-02 4.56080773e-02\n",
      "  4.88944880e-02 6.81930219e-02]\n",
      " [3.75198554e-02 3.51828522e-02 4.56080773e-02 4.88944880e-02\n",
      "  6.81930219e-02 6.39572036e-02]\n",
      " [3.51828522e-02 4.56080773e-02 4.88944880e-02 6.81930219e-02\n",
      "  6.39572036e-02 1.00673714e-01]\n",
      " [4.56080773e-02 4.88944880e-02 6.81930219e-02 6.39572036e-02\n",
      "  1.00673714e-01 6.16932318e-02]\n",
      " [4.88944880e-02 6.81930219e-02 6.39572036e-02 1.00673714e-01\n",
      "  6.16932318e-02 8.42234029e-02]\n",
      " [6.81930219e-02 6.39572036e-02 1.00673714e-01 6.16932318e-02\n",
      "  8.42234029e-02 9.83184532e-02]\n",
      " [6.39572036e-02 1.00673714e-01 6.16932318e-02 8.42234029e-02\n",
      "  9.83184532e-02 1.14586186e-01]\n",
      " [1.00673714e-01 6.16932318e-02 8.42234029e-02 9.83184532e-02\n",
      "  1.14586186e-01 1.31785069e-01]\n",
      " [6.16932318e-02 8.42234029e-02 9.83184532e-02 1.14586186e-01\n",
      "  1.31785069e-01 1.13362911e-01]\n",
      " [8.42234029e-02 9.83184532e-02 1.14586186e-01 1.31785069e-01\n",
      "  1.13362911e-01 9.07414508e-02]\n",
      " [9.83184532e-02 1.14586186e-01 1.31785069e-01 1.13362911e-01\n",
      "  9.07414508e-02 8.37669570e-02]\n",
      " [1.14586186e-01 1.31785069e-01 1.13362911e-01 9.07414508e-02\n",
      "  8.37669570e-02 1.21104234e-01]\n",
      " [1.31785069e-01 1.13362911e-01 9.07414508e-02 8.37669570e-02\n",
      "  1.21104234e-01 1.26618101e-01]\n",
      " [1.13362911e-01 9.07414508e-02 8.37669570e-02 1.21104234e-01\n",
      "  1.26618101e-01 1.91762064e-01]\n",
      " [9.07414508e-02 8.37669570e-02 1.21104234e-01 1.26618101e-01\n",
      "  1.91762064e-01 1.80533494e-01]\n",
      " [8.37669570e-02 1.21104234e-01 1.26618101e-01 1.91762064e-01\n",
      "  1.80533494e-01 1.86631612e-01]\n",
      " [1.21104234e-01 1.26618101e-01 1.91762064e-01 1.80533494e-01\n",
      "  1.86631612e-01 1.93733910e-01]\n",
      " [1.26618101e-01 1.91762064e-01 1.80533494e-01 1.86631612e-01\n",
      "  1.93733910e-01 1.23422979e-01]\n",
      " [1.91762064e-01 1.80533494e-01 1.86631612e-01 1.93733910e-01\n",
      "  1.23422979e-01 1.02828139e-01]\n",
      " [1.80533494e-01 1.86631612e-01 1.93733910e-01 1.23422979e-01\n",
      "  1.02828139e-01 1.69031057e-01]\n",
      " [1.86631612e-01 1.93733910e-01 1.23422979e-01 1.02828139e-01\n",
      "  1.69031057e-01 2.07865476e-01]\n",
      " [1.93733910e-01 1.23422979e-01 1.02828139e-01 1.69031057e-01\n",
      "  2.07865476e-01 2.54587282e-01]\n",
      " [1.23422979e-01 1.02828139e-01 1.69031057e-01 2.07865476e-01\n",
      "  2.54587282e-01 2.79436198e-01]\n",
      " [1.02828139e-01 1.69031057e-01 2.07865476e-01 2.54587282e-01\n",
      "  2.79436198e-01 2.72388673e-01]\n",
      " [1.69031057e-01 2.07865476e-01 2.54587282e-01 2.79436198e-01\n",
      "  2.72388673e-01 1.44930712e-01]\n",
      " [2.07865476e-01 2.54587282e-01 2.79436198e-01 2.72388673e-01\n",
      "  1.44930712e-01 2.39907981e-01]\n",
      " [2.54587282e-01 2.79436198e-01 2.72388673e-01 1.44930712e-01\n",
      "  2.39907981e-01 3.17832430e-01]\n",
      " [2.79436198e-01 2.72388673e-01 1.44930712e-01 2.39907981e-01\n",
      "  3.17832430e-01 3.64262110e-01]\n",
      " [2.72388673e-01 1.44930712e-01 2.39907981e-01 3.17832430e-01\n",
      "  3.64262110e-01 3.37916050e-01]\n",
      " [1.44930712e-01 2.39907981e-01 3.17832430e-01 3.64262110e-01\n",
      "  3.37916050e-01 3.79817787e-01]\n",
      " [2.39907981e-01 3.17832430e-01 3.64262110e-01 3.37916050e-01\n",
      "  3.79817787e-01 3.01400376e-01]\n",
      " [3.17832430e-01 3.64262110e-01 3.37916050e-01 3.79817787e-01\n",
      "  3.01400376e-01 2.88711179e-01]\n",
      " [3.64262110e-01 3.37916050e-01 3.79817787e-01 3.01400376e-01\n",
      "  2.88711179e-01 2.13379343e-01]\n",
      " [3.37916050e-01 3.79817787e-01 3.01400376e-01 2.88711179e-01\n",
      "  2.13379343e-01 2.98040934e-01]\n",
      " [3.79817787e-01 3.01400376e-01 2.88711179e-01 2.13379343e-01\n",
      "  2.98040934e-01 3.76093188e-01]\n",
      " [3.01400376e-01 2.88711179e-01 2.13379343e-01 2.98040934e-01\n",
      "  3.76093188e-01 4.82317285e-01]\n",
      " [2.88711179e-01 2.13379343e-01 2.98040934e-01 3.76093188e-01\n",
      "  4.82317285e-01 4.91647039e-01]\n",
      " [2.13379343e-01 2.98040934e-01 3.76093188e-01 4.82317285e-01\n",
      "  4.91647039e-01 6.07511274e-01]\n",
      " [2.98040934e-01 3.76093188e-01 4.82317285e-01 4.91647039e-01\n",
      "  6.07511274e-01 2.99592850e-01]\n",
      " [3.76093188e-01 4.82317285e-01 4.91647039e-01 6.07511274e-01\n",
      "  2.99592850e-01 2.11754396e-01]\n",
      " [4.82317285e-01 4.91647039e-01 6.07511274e-01 2.99592850e-01\n",
      "  2.11754396e-01 5.28308777e-01]\n",
      " [4.91647039e-01 6.07511274e-01 2.99592850e-01 2.11754396e-01\n",
      "  5.28308777e-01 5.22776652e-01]\n",
      " [6.07511274e-01 2.99592850e-01 2.11754396e-01 5.28308777e-01\n",
      "  5.22776652e-01 5.64459294e-01]\n",
      " [2.99592850e-01 2.11754396e-01 5.28308777e-01 5.22776652e-01\n",
      "  5.64459294e-01 5.62889120e-01]\n",
      " [2.11754396e-01 5.28308777e-01 5.22776652e-01 5.64459294e-01\n",
      "  5.62889120e-01 4.94330942e-01]\n",
      " [5.28308777e-01 5.22776652e-01 5.64459294e-01 5.62889120e-01\n",
      "  4.94330942e-01 3.45456537e-01]\n",
      " [5.22776652e-01 5.64459294e-01 5.62889120e-01 4.94330942e-01\n",
      "  3.45456537e-01 2.85808183e-01]\n",
      " [5.64459294e-01 5.62889120e-01 4.94330942e-01 3.45456537e-01\n",
      "  2.85808183e-01 5.85912253e-01]\n",
      " [5.62889120e-01 4.94330942e-01 3.45456537e-01 2.85808183e-01\n",
      "  5.85912253e-01 6.00920195e-01]\n",
      " [4.94330942e-01 3.45456537e-01 2.85808183e-01 5.85912253e-01\n",
      "  6.00920195e-01 5.55257344e-01]\n",
      " [3.45456537e-01 2.85808183e-01 5.85912253e-01 6.00920195e-01\n",
      "  5.55257344e-01 4.74375126e-01]\n",
      " [2.85808183e-01 5.85912253e-01 6.00920195e-01 5.55257344e-01\n",
      "  4.74375126e-01 3.96268098e-01]\n",
      " [5.85912253e-01 6.00920195e-01 5.55257344e-01 4.74375126e-01\n",
      "  3.96268098e-01 3.12391594e-01]\n",
      " [6.00920195e-01 5.55257344e-01 4.74375126e-01 3.96268098e-01\n",
      "  3.12391594e-01 3.76969564e-01]\n",
      " [5.55257344e-01 4.74375126e-01 3.96268098e-01 3.12391594e-01\n",
      "  3.76969564e-01 6.37527159e-01]\n",
      " [4.74375126e-01 3.96268098e-01 3.12391594e-01 3.76969564e-01\n",
      "  6.37527159e-01 5.87683263e-01]\n",
      " [3.96268098e-01 3.12391594e-01 3.76969564e-01 6.37527159e-01\n",
      "  5.87683263e-01 4.15639663e-01]\n",
      " [3.12391594e-01 3.76969564e-01 6.37527159e-01 5.87683263e-01\n",
      "  4.15639663e-01 1.00000000e+00]]\n",
      "[0.00000000e+00 0.00000000e+00 1.82578372e-05 9.12891859e-05\n",
      " 9.12891859e-05 0.00000000e+00 2.19094046e-04 0.00000000e+00\n",
      " 1.64320535e-04 3.28641069e-04 4.56445929e-04 3.83414581e-04\n",
      " 4.19930255e-04 1.44236914e-03 6.20766464e-04 1.04069672e-03\n",
      " 2.50132369e-03 3.52376258e-03 5.16696792e-03 4.08975553e-03\n",
      " 7.63177594e-03 6.29895383e-03 5.65992952e-03 4.23581822e-03\n",
      " 8.80027752e-03 9.16543426e-03 8.89156671e-03 6.42675869e-03\n",
      " 5.89728141e-03 2.07774187e-02 2.04305198e-02 1.96089171e-02\n",
      " 2.09234814e-02 2.23110770e-02 1.55556773e-02 1.69067572e-02\n",
      " 3.03262676e-02 4.03498202e-02 3.52376258e-02 3.25172080e-02\n",
      " 1.98827847e-02 2.63278012e-02 2.30231327e-02 3.34483577e-02\n",
      " 5.58324661e-02 3.84327473e-02 5.94657757e-02 5.32581110e-02\n",
      " 3.75198554e-02 3.51828522e-02 4.56080773e-02 4.88944880e-02\n",
      " 6.81930219e-02 6.39572036e-02 1.00673714e-01 6.16932318e-02\n",
      " 8.42234029e-02 9.83184532e-02 1.14586186e-01 1.31785069e-01\n",
      " 1.13362911e-01 9.07414508e-02 8.37669570e-02 1.21104234e-01\n",
      " 1.26618101e-01 1.91762064e-01 1.80533494e-01 1.86631612e-01\n",
      " 1.93733910e-01 1.23422979e-01 1.02828139e-01 1.69031057e-01\n",
      " 2.07865476e-01 2.54587282e-01 2.79436198e-01 2.72388673e-01\n",
      " 1.44930712e-01 2.39907981e-01 3.17832430e-01 3.64262110e-01\n",
      " 3.37916050e-01 3.79817787e-01 3.01400376e-01 2.88711179e-01\n",
      " 2.13379343e-01 2.98040934e-01 3.76093188e-01 4.82317285e-01\n",
      " 4.91647039e-01 6.07511274e-01 2.99592850e-01 2.11754396e-01\n",
      " 5.28308777e-01 5.22776652e-01 5.64459294e-01 5.62889120e-01\n",
      " 4.94330942e-01 3.45456537e-01 2.85808183e-01 5.85912253e-01\n",
      " 6.00920195e-01 5.55257344e-01 4.74375126e-01 3.96268098e-01\n",
      " 3.12391594e-01 3.76969564e-01 6.37527159e-01 5.87683263e-01\n",
      " 4.15639663e-01 1.00000000e+00 6.32926184e-01]\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Manipulating the data to split into X(a window size of values)\n",
    "## and target, or Y, the value X \"produces\"\n",
    "####\n",
    "\n",
    "arr = df.values\n",
    "\n",
    "X = arr[:, : -1]\n",
    "target = arr[:, -1]\n",
    "print(X)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.02272646\n",
      "Iteration 2, loss = 0.00601602\n",
      "Iteration 3, loss = 0.01036731\n",
      "Iteration 4, loss = 0.01263170\n",
      "Iteration 5, loss = 0.00856519\n",
      "Iteration 6, loss = 0.00440394\n",
      "Iteration 7, loss = 0.00370121\n",
      "Iteration 8, loss = 0.00564778\n",
      "Iteration 9, loss = 0.00721086\n",
      "Iteration 10, loss = 0.00679693\n",
      "Iteration 11, loss = 0.00515472\n",
      "Iteration 12, loss = 0.00383529\n",
      "Iteration 13, loss = 0.00373159\n",
      "Iteration 14, loss = 0.00454160\n",
      "Iteration 15, loss = 0.00521450\n",
      "Iteration 16, loss = 0.00502877\n",
      "Iteration 17, loss = 0.00418000\n",
      "Iteration 18, loss = 0.00336404\n",
      "Iteration 19, loss = 0.00311406\n",
      "Iteration 20, loss = 0.00342386\n",
      "Iteration 21, loss = 0.00385686\n",
      "Iteration 22, loss = 0.00398880\n",
      "Iteration 23, loss = 0.00373419\n",
      "Iteration 24, loss = 0.00333676\n",
      "Iteration 25, loss = 0.00310877\n",
      "Iteration 26, loss = 0.00316901\n",
      "Iteration 27, loss = 0.00338544\n",
      "Iteration 28, loss = 0.00351597\n",
      "Iteration 29, loss = 0.00342285\n",
      "Iteration 30, loss = 0.00317917\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272646\n",
      "Iteration 2, loss = 0.00601602\n",
      "Iteration 3, loss = 0.01036731\n",
      "Iteration 4, loss = 0.01263171\n",
      "Iteration 5, loss = 0.00856519\n",
      "Iteration 6, loss = 0.00440394\n",
      "Iteration 7, loss = 0.00370121\n",
      "Iteration 8, loss = 0.00564778\n",
      "Iteration 9, loss = 0.00721086\n",
      "Iteration 10, loss = 0.00679693\n",
      "Iteration 11, loss = 0.00515472\n",
      "Iteration 12, loss = 0.00383529\n",
      "Iteration 13, loss = 0.00373159\n",
      "Iteration 14, loss = 0.00454160\n",
      "Iteration 15, loss = 0.00521450\n",
      "Iteration 16, loss = 0.00502878\n",
      "Iteration 17, loss = 0.00418000\n",
      "Iteration 18, loss = 0.00336405\n",
      "Iteration 19, loss = 0.00311406\n",
      "Iteration 20, loss = 0.00342386\n",
      "Iteration 21, loss = 0.00385686\n",
      "Iteration 22, loss = 0.00398881\n",
      "Iteration 23, loss = 0.00373419\n",
      "Iteration 24, loss = 0.00333676\n",
      "Iteration 25, loss = 0.00310877\n",
      "Iteration 26, loss = 0.00316901\n",
      "Iteration 27, loss = 0.00338544\n",
      "Iteration 28, loss = 0.00351597\n",
      "Iteration 29, loss = 0.00342285\n",
      "Iteration 30, loss = 0.00317917\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272646\n",
      "Iteration 2, loss = 0.00601604\n",
      "Iteration 3, loss = 0.01036733\n",
      "Iteration 4, loss = 0.01263170\n",
      "Iteration 5, loss = 0.00856518\n",
      "Iteration 6, loss = 0.00440394\n",
      "Iteration 7, loss = 0.00370121\n",
      "Iteration 8, loss = 0.00564779\n",
      "Iteration 9, loss = 0.00721087\n",
      "Iteration 10, loss = 0.00679692\n",
      "Iteration 11, loss = 0.00515471\n",
      "Iteration 12, loss = 0.00383529\n",
      "Iteration 13, loss = 0.00373160\n",
      "Iteration 14, loss = 0.00454161\n",
      "Iteration 15, loss = 0.00521450\n",
      "Iteration 16, loss = 0.00502877\n",
      "Iteration 17, loss = 0.00418000\n",
      "Iteration 18, loss = 0.00336404\n",
      "Iteration 19, loss = 0.00311406\n",
      "Iteration 20, loss = 0.00342387\n",
      "Iteration 21, loss = 0.00385686\n",
      "Iteration 22, loss = 0.00398881\n",
      "Iteration 23, loss = 0.00373419\n",
      "Iteration 24, loss = 0.00333676\n",
      "Iteration 25, loss = 0.00310877\n",
      "Iteration 26, loss = 0.00316902\n",
      "Iteration 27, loss = 0.00338544\n",
      "Iteration 28, loss = 0.00351597\n",
      "Iteration 29, loss = 0.00342285\n",
      "Iteration 30, loss = 0.00317917\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272645\n",
      "Iteration 2, loss = 0.00601609\n",
      "Iteration 3, loss = 0.01036736\n",
      "Iteration 4, loss = 0.01263169\n",
      "Iteration 5, loss = 0.00856516\n",
      "Iteration 6, loss = 0.00440393\n",
      "Iteration 7, loss = 0.00370123\n",
      "Iteration 8, loss = 0.00564781\n",
      "Iteration 9, loss = 0.00721087\n",
      "Iteration 10, loss = 0.00679691\n",
      "Iteration 11, loss = 0.00515471\n",
      "Iteration 12, loss = 0.00383529\n",
      "Iteration 13, loss = 0.00373162\n",
      "Iteration 14, loss = 0.00454164\n",
      "Iteration 15, loss = 0.00521452\n",
      "Iteration 16, loss = 0.00502877\n",
      "Iteration 17, loss = 0.00417998\n",
      "Iteration 18, loss = 0.00336404\n",
      "Iteration 19, loss = 0.00311407\n",
      "Iteration 20, loss = 0.00342388\n",
      "Iteration 21, loss = 0.00385687\n",
      "Iteration 22, loss = 0.00398881\n",
      "Iteration 23, loss = 0.00373419\n",
      "Iteration 24, loss = 0.00333675\n",
      "Iteration 25, loss = 0.00310878\n",
      "Iteration 26, loss = 0.00316903\n",
      "Iteration 27, loss = 0.00338545\n",
      "Iteration 28, loss = 0.00351597\n",
      "Iteration 29, loss = 0.00342284\n",
      "Iteration 30, loss = 0.00317916\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272645\n",
      "Iteration 2, loss = 0.00601609\n",
      "Iteration 3, loss = 0.01036736\n",
      "Iteration 4, loss = 0.01263168\n",
      "Iteration 5, loss = 0.00856515\n",
      "Iteration 6, loss = 0.00440393\n",
      "Iteration 7, loss = 0.00370122\n",
      "Iteration 8, loss = 0.00564781\n",
      "Iteration 9, loss = 0.00721087\n",
      "Iteration 10, loss = 0.00679691\n",
      "Iteration 11, loss = 0.00515471\n",
      "Iteration 12, loss = 0.00383529\n",
      "Iteration 13, loss = 0.00373161\n",
      "Iteration 14, loss = 0.00454163\n",
      "Iteration 15, loss = 0.00521451\n",
      "Iteration 16, loss = 0.00502877\n",
      "Iteration 17, loss = 0.00417998\n",
      "Iteration 18, loss = 0.00336404\n",
      "Iteration 19, loss = 0.00311406\n",
      "Iteration 20, loss = 0.00342388\n",
      "Iteration 21, loss = 0.00385687\n",
      "Iteration 22, loss = 0.00398880\n",
      "Iteration 23, loss = 0.00373418\n",
      "Iteration 24, loss = 0.00333675\n",
      "Iteration 25, loss = 0.00310878\n",
      "Iteration 26, loss = 0.00316903\n",
      "Iteration 27, loss = 0.00338545\n",
      "Iteration 28, loss = 0.00351597\n",
      "Iteration 29, loss = 0.00342284\n",
      "Iteration 30, loss = 0.00317916\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272646\n",
      "Iteration 2, loss = 0.00601602\n",
      "Iteration 3, loss = 0.01036730\n",
      "Iteration 4, loss = 0.01263170\n",
      "Iteration 5, loss = 0.00856518\n",
      "Iteration 6, loss = 0.00440394\n",
      "Iteration 7, loss = 0.00370120\n",
      "Iteration 8, loss = 0.00564778\n",
      "Iteration 9, loss = 0.00721086\n",
      "Iteration 10, loss = 0.00679693\n",
      "Iteration 11, loss = 0.00515472\n",
      "Iteration 12, loss = 0.00383529\n",
      "Iteration 13, loss = 0.00373158\n",
      "Iteration 14, loss = 0.00454160\n",
      "Iteration 15, loss = 0.00521449\n",
      "Iteration 16, loss = 0.00502877\n",
      "Iteration 17, loss = 0.00418000\n",
      "Iteration 18, loss = 0.00336404\n",
      "Iteration 19, loss = 0.00311405\n",
      "Iteration 20, loss = 0.00342386\n",
      "Iteration 21, loss = 0.00385686\n",
      "Iteration 22, loss = 0.00398880\n",
      "Iteration 23, loss = 0.00373419\n",
      "Iteration 24, loss = 0.00333676\n",
      "Iteration 25, loss = 0.00310877\n",
      "Iteration 26, loss = 0.00316901\n",
      "Iteration 27, loss = 0.00338543\n",
      "Iteration 28, loss = 0.00351596\n",
      "Iteration 29, loss = 0.00342284\n",
      "Iteration 30, loss = 0.00317916\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272644\n",
      "Iteration 2, loss = 0.00601620\n",
      "Iteration 3, loss = 0.01036744\n",
      "Iteration 4, loss = 0.01263166\n",
      "Iteration 5, loss = 0.00856511\n",
      "Iteration 6, loss = 0.00440392\n",
      "Iteration 7, loss = 0.00370126\n",
      "Iteration 8, loss = 0.00564785\n",
      "Iteration 9, loss = 0.00721089\n",
      "Iteration 10, loss = 0.00679690\n",
      "Iteration 11, loss = 0.00515469\n",
      "Iteration 12, loss = 0.00383531\n",
      "Iteration 13, loss = 0.00373166\n",
      "Iteration 14, loss = 0.00454169\n",
      "Iteration 15, loss = 0.00521454\n",
      "Iteration 16, loss = 0.00502876\n",
      "Iteration 17, loss = 0.00417996\n",
      "Iteration 18, loss = 0.00336403\n",
      "Iteration 19, loss = 0.00311408\n",
      "Iteration 20, loss = 0.00342391\n",
      "Iteration 21, loss = 0.00385689\n",
      "Iteration 22, loss = 0.00398881\n",
      "Iteration 23, loss = 0.00373418\n",
      "Iteration 24, loss = 0.00333675\n",
      "Iteration 25, loss = 0.00310879\n",
      "Iteration 26, loss = 0.00316905\n",
      "Iteration 27, loss = 0.00338547\n",
      "Iteration 28, loss = 0.00351598\n",
      "Iteration 29, loss = 0.00342284\n",
      "Iteration 30, loss = 0.00317915\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272646\n",
      "Iteration 2, loss = 0.00601600\n",
      "Iteration 3, loss = 0.01036727\n",
      "Iteration 4, loss = 0.01263169\n",
      "Iteration 5, loss = 0.00856519\n",
      "Iteration 6, loss = 0.00440394\n",
      "Iteration 7, loss = 0.00370119\n",
      "Iteration 8, loss = 0.00564776\n",
      "Iteration 9, loss = 0.00721085\n",
      "Iteration 10, loss = 0.00679693\n",
      "Iteration 11, loss = 0.00515472\n",
      "Iteration 12, loss = 0.00383529\n",
      "Iteration 13, loss = 0.00373157\n",
      "Iteration 14, loss = 0.00454158\n",
      "Iteration 15, loss = 0.00521448\n",
      "Iteration 16, loss = 0.00502877\n",
      "Iteration 17, loss = 0.00418000\n",
      "Iteration 18, loss = 0.00336404\n",
      "Iteration 19, loss = 0.00311404\n",
      "Iteration 20, loss = 0.00342385\n",
      "Iteration 21, loss = 0.00385685\n",
      "Iteration 22, loss = 0.00398880\n",
      "Iteration 23, loss = 0.00373419\n",
      "Iteration 24, loss = 0.00333676\n",
      "Iteration 25, loss = 0.00310877\n",
      "Iteration 26, loss = 0.00316900\n",
      "Iteration 27, loss = 0.00338542\n",
      "Iteration 28, loss = 0.00351596\n",
      "Iteration 29, loss = 0.00342284\n",
      "Iteration 30, loss = 0.00317916\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272644\n",
      "Iteration 2, loss = 0.00601617\n",
      "Iteration 3, loss = 0.01036741\n",
      "Iteration 4, loss = 0.01263166\n",
      "Iteration 5, loss = 0.00856512\n",
      "Iteration 6, loss = 0.00440392\n",
      "Iteration 7, loss = 0.00370125\n",
      "Iteration 8, loss = 0.00564784\n",
      "Iteration 9, loss = 0.00721088\n",
      "Iteration 10, loss = 0.00679691\n",
      "Iteration 11, loss = 0.00515470\n",
      "Iteration 12, loss = 0.00383531\n",
      "Iteration 13, loss = 0.00373165\n",
      "Iteration 14, loss = 0.00454167\n",
      "Iteration 15, loss = 0.00521453\n",
      "Iteration 16, loss = 0.00502876\n",
      "Iteration 17, loss = 0.00417997\n",
      "Iteration 18, loss = 0.00336403\n",
      "Iteration 19, loss = 0.00311407\n",
      "Iteration 20, loss = 0.00342390\n",
      "Iteration 21, loss = 0.00385688\n",
      "Iteration 22, loss = 0.00398881\n",
      "Iteration 23, loss = 0.00373418\n",
      "Iteration 24, loss = 0.00333675\n",
      "Iteration 25, loss = 0.00310879\n",
      "Iteration 26, loss = 0.00316904\n",
      "Iteration 27, loss = 0.00338547\n",
      "Iteration 28, loss = 0.00351598\n",
      "Iteration 29, loss = 0.00342284\n",
      "Iteration 30, loss = 0.00317916\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272644\n",
      "Iteration 2, loss = 0.00601622\n",
      "Iteration 3, loss = 0.01036743\n",
      "Iteration 4, loss = 0.01263164\n",
      "Iteration 5, loss = 0.00856509\n",
      "Iteration 6, loss = 0.00440391\n",
      "Iteration 7, loss = 0.00370125\n",
      "Iteration 8, loss = 0.00564785\n",
      "Iteration 9, loss = 0.00721088\n",
      "Iteration 10, loss = 0.00679690\n",
      "Iteration 11, loss = 0.00515469\n",
      "Iteration 12, loss = 0.00383531\n",
      "Iteration 13, loss = 0.00373166\n",
      "Iteration 14, loss = 0.00454168\n",
      "Iteration 15, loss = 0.00521453\n",
      "Iteration 16, loss = 0.00502875\n",
      "Iteration 17, loss = 0.00417995\n",
      "Iteration 18, loss = 0.00336402\n",
      "Iteration 19, loss = 0.00311407\n",
      "Iteration 20, loss = 0.00342390\n",
      "Iteration 21, loss = 0.00385688\n",
      "Iteration 22, loss = 0.00398880\n",
      "Iteration 23, loss = 0.00373417\n",
      "Iteration 24, loss = 0.00333675\n",
      "Iteration 25, loss = 0.00310879\n",
      "Iteration 26, loss = 0.00316905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27, loss = 0.00338547\n",
      "Iteration 28, loss = 0.00351598\n",
      "Iteration 29, loss = 0.00342283\n",
      "Iteration 30, loss = 0.00317915\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272642\n",
      "Iteration 2, loss = 0.00601642\n",
      "Iteration 3, loss = 0.01036757\n",
      "Iteration 4, loss = 0.01263158\n",
      "Iteration 5, loss = 0.00856500\n",
      "Iteration 6, loss = 0.00440389\n",
      "Iteration 7, loss = 0.00370131\n",
      "Iteration 8, loss = 0.00564792\n",
      "Iteration 9, loss = 0.00721090\n",
      "Iteration 10, loss = 0.00679686\n",
      "Iteration 11, loss = 0.00515466\n",
      "Iteration 12, loss = 0.00383533\n",
      "Iteration 13, loss = 0.00373174\n",
      "Iteration 14, loss = 0.00454178\n",
      "Iteration 15, loss = 0.00521457\n",
      "Iteration 16, loss = 0.00502872\n",
      "Iteration 17, loss = 0.00417991\n",
      "Iteration 18, loss = 0.00336400\n",
      "Iteration 19, loss = 0.00311409\n",
      "Iteration 20, loss = 0.00342394\n",
      "Iteration 21, loss = 0.00385691\n",
      "Iteration 22, loss = 0.00398880\n",
      "Iteration 23, loss = 0.00373416\n",
      "Iteration 24, loss = 0.00333673\n",
      "Iteration 25, loss = 0.00310878\n",
      "Iteration 26, loss = 0.00316903\n",
      "Iteration 27, loss = 0.00338544\n",
      "Iteration 28, loss = 0.00351595\n",
      "Iteration 29, loss = 0.00342280\n",
      "Iteration 30, loss = 0.00317912\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272644\n",
      "Iteration 2, loss = 0.00601626\n",
      "Iteration 3, loss = 0.01036743\n",
      "Iteration 4, loss = 0.01263160\n",
      "Iteration 5, loss = 0.00856506\n",
      "Iteration 6, loss = 0.00440390\n",
      "Iteration 7, loss = 0.00370125\n",
      "Iteration 8, loss = 0.00564785\n",
      "Iteration 9, loss = 0.00721088\n",
      "Iteration 10, loss = 0.00679689\n",
      "Iteration 11, loss = 0.00515469\n",
      "Iteration 12, loss = 0.00383532\n",
      "Iteration 13, loss = 0.00373167\n",
      "Iteration 14, loss = 0.00454169\n",
      "Iteration 15, loss = 0.00521453\n",
      "Iteration 16, loss = 0.00502873\n",
      "Iteration 17, loss = 0.00417994\n",
      "Iteration 18, loss = 0.00336401\n",
      "Iteration 19, loss = 0.00311407\n",
      "Iteration 20, loss = 0.00342390\n",
      "Iteration 21, loss = 0.00385688\n",
      "Iteration 22, loss = 0.00398880\n",
      "Iteration 23, loss = 0.00373417\n",
      "Iteration 24, loss = 0.00333674\n",
      "Iteration 25, loss = 0.00310879\n",
      "Iteration 26, loss = 0.00316905\n",
      "Iteration 27, loss = 0.00338547\n",
      "Iteration 28, loss = 0.00351597\n",
      "Iteration 29, loss = 0.00342283\n",
      "Iteration 30, loss = 0.00317914\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272643\n",
      "Iteration 2, loss = 0.00601633\n",
      "Iteration 3, loss = 0.01036746\n",
      "Iteration 4, loss = 0.01263156\n",
      "Iteration 5, loss = 0.00856502\n",
      "Iteration 6, loss = 0.00440389\n",
      "Iteration 7, loss = 0.00370127\n",
      "Iteration 8, loss = 0.00564787\n",
      "Iteration 9, loss = 0.00721088\n",
      "Iteration 10, loss = 0.00679688\n",
      "Iteration 11, loss = 0.00515469\n",
      "Iteration 12, loss = 0.00383533\n",
      "Iteration 13, loss = 0.00373169\n",
      "Iteration 14, loss = 0.00454171\n",
      "Iteration 15, loss = 0.00521453\n",
      "Iteration 16, loss = 0.00502872\n",
      "Iteration 17, loss = 0.00417992\n",
      "Iteration 18, loss = 0.00336400\n",
      "Iteration 19, loss = 0.00311407\n",
      "Iteration 20, loss = 0.00342391\n",
      "Iteration 21, loss = 0.00385688\n",
      "Iteration 22, loss = 0.00398879\n",
      "Iteration 23, loss = 0.00373417\n",
      "Iteration 24, loss = 0.00333674\n",
      "Iteration 25, loss = 0.00310877\n",
      "Iteration 26, loss = 0.00316900\n",
      "Iteration 27, loss = 0.00338541\n",
      "Iteration 28, loss = 0.00351593\n",
      "Iteration 29, loss = 0.00342279\n",
      "Iteration 30, loss = 0.00317912\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272635\n",
      "Iteration 2, loss = 0.00601715\n",
      "Iteration 3, loss = 0.01036808\n",
      "Iteration 4, loss = 0.01263138\n",
      "Iteration 5, loss = 0.00856467\n",
      "Iteration 6, loss = 0.00440381\n",
      "Iteration 7, loss = 0.00370153\n",
      "Iteration 8, loss = 0.00564820\n",
      "Iteration 9, loss = 0.00721099\n",
      "Iteration 10, loss = 0.00679675\n",
      "Iteration 11, loss = 0.00515456\n",
      "Iteration 12, loss = 0.00383542\n",
      "Iteration 13, loss = 0.00373203\n",
      "Iteration 14, loss = 0.00454212\n",
      "Iteration 15, loss = 0.00521473\n",
      "Iteration 16, loss = 0.00502865\n",
      "Iteration 17, loss = 0.00417975\n",
      "Iteration 18, loss = 0.00336392\n",
      "Iteration 19, loss = 0.00311418\n",
      "Iteration 20, loss = 0.00342411\n",
      "Iteration 21, loss = 0.00385704\n",
      "Iteration 22, loss = 0.00398880\n",
      "Iteration 23, loss = 0.00373409\n",
      "Iteration 24, loss = 0.00333669\n",
      "Iteration 25, loss = 0.00310885\n",
      "Iteration 26, loss = 0.00316917\n",
      "Iteration 27, loss = 0.00338559\n",
      "Iteration 28, loss = 0.00351601\n",
      "Iteration 29, loss = 0.00342277\n",
      "Iteration 30, loss = 0.00317906\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272641\n",
      "Iteration 2, loss = 0.00601641\n",
      "Iteration 3, loss = 0.01036742\n",
      "Iteration 4, loss = 0.01263147\n",
      "Iteration 5, loss = 0.00856496\n",
      "Iteration 6, loss = 0.00440386\n",
      "Iteration 7, loss = 0.00370124\n",
      "Iteration 8, loss = 0.00564783\n",
      "Iteration 9, loss = 0.00721085\n",
      "Iteration 10, loss = 0.00679686\n",
      "Iteration 11, loss = 0.00515469\n",
      "Iteration 12, loss = 0.00383534\n",
      "Iteration 13, loss = 0.00373169\n",
      "Iteration 14, loss = 0.00454169\n",
      "Iteration 15, loss = 0.00521449\n",
      "Iteration 16, loss = 0.00502868\n",
      "Iteration 17, loss = 0.00417989\n",
      "Iteration 18, loss = 0.00336397\n",
      "Iteration 19, loss = 0.00311404\n",
      "Iteration 20, loss = 0.00342387\n",
      "Iteration 21, loss = 0.00385685\n",
      "Iteration 22, loss = 0.00398877\n",
      "Iteration 23, loss = 0.00373416\n",
      "Iteration 24, loss = 0.00333673\n",
      "Iteration 25, loss = 0.00310876\n",
      "Iteration 26, loss = 0.00316899\n",
      "Iteration 27, loss = 0.00338539\n",
      "Iteration 28, loss = 0.00351590\n",
      "Iteration 29, loss = 0.00342277\n",
      "Iteration 30, loss = 0.00317909\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272638\n",
      "Iteration 2, loss = 0.00601683\n",
      "Iteration 3, loss = 0.01036774\n",
      "Iteration 4, loss = 0.01263137\n",
      "Iteration 5, loss = 0.00856477\n",
      "Iteration 6, loss = 0.00440382\n",
      "Iteration 7, loss = 0.00370139\n",
      "Iteration 8, loss = 0.00564802\n",
      "Iteration 9, loss = 0.00721092\n",
      "Iteration 10, loss = 0.00679681\n",
      "Iteration 11, loss = 0.00515464\n",
      "Iteration 12, loss = 0.00383540\n",
      "Iteration 13, loss = 0.00373188\n",
      "Iteration 14, loss = 0.00454192\n",
      "Iteration 15, loss = 0.00521461\n",
      "Iteration 16, loss = 0.00502865\n",
      "Iteration 17, loss = 0.00417981\n",
      "Iteration 18, loss = 0.00336394\n",
      "Iteration 19, loss = 0.00311411\n",
      "Iteration 20, loss = 0.00342400\n",
      "Iteration 21, loss = 0.00385695\n",
      "Iteration 22, loss = 0.00398879\n",
      "Iteration 23, loss = 0.00373413\n",
      "Iteration 24, loss = 0.00333672\n",
      "Iteration 25, loss = 0.00310883\n",
      "Iteration 26, loss = 0.00316909\n",
      "Iteration 27, loss = 0.00338550\n",
      "Iteration 28, loss = 0.00351596\n",
      "Iteration 29, loss = 0.00342276\n",
      "Iteration 30, loss = 0.00317907\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272628\n",
      "Iteration 2, loss = 0.00601774\n",
      "Iteration 3, loss = 0.01036839\n",
      "Iteration 4, loss = 0.01263111\n",
      "Iteration 5, loss = 0.00856435\n",
      "Iteration 6, loss = 0.00440371\n",
      "Iteration 7, loss = 0.00370165\n",
      "Iteration 8, loss = 0.00564837\n",
      "Iteration 9, loss = 0.00721102\n",
      "Iteration 10, loss = 0.00679665\n",
      "Iteration 11, loss = 0.00515448\n",
      "Iteration 12, loss = 0.00383550\n",
      "Iteration 13, loss = 0.00373223\n",
      "Iteration 14, loss = 0.00454234\n",
      "Iteration 15, loss = 0.00521479\n",
      "Iteration 16, loss = 0.00502854\n",
      "Iteration 17, loss = 0.00417959\n",
      "Iteration 18, loss = 0.00336383\n",
      "Iteration 19, loss = 0.00311420\n",
      "Iteration 20, loss = 0.00342420\n",
      "Iteration 21, loss = 0.00385709\n",
      "Iteration 22, loss = 0.00398878\n",
      "Iteration 23, loss = 0.00373402\n",
      "Iteration 24, loss = 0.00333665\n",
      "Iteration 25, loss = 0.00310890\n",
      "Iteration 26, loss = 0.00316925\n",
      "Iteration 27, loss = 0.00338564\n",
      "Iteration 28, loss = 0.00351597\n",
      "Iteration 29, loss = 0.00342267\n",
      "Iteration 30, loss = 0.00317899\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272614\n",
      "Iteration 2, loss = 0.00601882\n",
      "Iteration 3, loss = 0.01036910\n",
      "Iteration 4, loss = 0.01263075\n",
      "Iteration 5, loss = 0.00856382\n",
      "Iteration 6, loss = 0.00440355\n",
      "Iteration 7, loss = 0.00370193\n",
      "Iteration 8, loss = 0.00564872\n",
      "Iteration 9, loss = 0.00721109\n",
      "Iteration 10, loss = 0.00679644\n",
      "Iteration 11, loss = 0.00515430\n",
      "Iteration 12, loss = 0.00383560\n",
      "Iteration 13, loss = 0.00373263\n",
      "Iteration 14, loss = 0.00454279\n",
      "Iteration 15, loss = 0.00521497\n",
      "Iteration 16, loss = 0.00502837\n",
      "Iteration 17, loss = 0.00417933\n",
      "Iteration 18, loss = 0.00336368\n",
      "Iteration 19, loss = 0.00311429\n",
      "Iteration 20, loss = 0.00342439\n",
      "Iteration 21, loss = 0.00385722\n",
      "Iteration 22, loss = 0.00398874\n",
      "Iteration 23, loss = 0.00373387\n",
      "Iteration 24, loss = 0.00333655\n",
      "Iteration 25, loss = 0.00310897\n",
      "Iteration 26, loss = 0.00316943\n",
      "Iteration 27, loss = 0.00338582\n",
      "Iteration 28, loss = 0.00351602\n",
      "Iteration 29, loss = 0.00342260\n",
      "Iteration 30, loss = 0.00317887\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272600\n",
      "Iteration 2, loss = 0.00601973\n",
      "Iteration 3, loss = 0.01036964\n",
      "Iteration 4, loss = 0.01263037\n",
      "Iteration 5, loss = 0.00856331\n",
      "Iteration 6, loss = 0.00440338\n",
      "Iteration 7, loss = 0.00370213\n",
      "Iteration 8, loss = 0.00564901\n",
      "Iteration 9, loss = 0.00721115\n",
      "Iteration 10, loss = 0.00679627\n",
      "Iteration 11, loss = 0.00515415\n",
      "Iteration 12, loss = 0.00383569\n",
      "Iteration 13, loss = 0.00373296\n",
      "Iteration 14, loss = 0.00454317\n",
      "Iteration 15, loss = 0.00521510\n",
      "Iteration 16, loss = 0.00502821\n",
      "Iteration 17, loss = 0.00417906\n",
      "Iteration 18, loss = 0.00336353\n",
      "Iteration 19, loss = 0.00311434\n",
      "Iteration 20, loss = 0.00342455\n",
      "Iteration 21, loss = 0.00385732\n",
      "Iteration 22, loss = 0.00398870\n",
      "Iteration 23, loss = 0.00373374\n",
      "Iteration 24, loss = 0.00333647\n",
      "Iteration 25, loss = 0.00310904\n",
      "Iteration 26, loss = 0.00316957\n",
      "Iteration 27, loss = 0.00338596\n",
      "Iteration 28, loss = 0.00351606\n",
      "Iteration 29, loss = 0.00342251\n",
      "Iteration 30, loss = 0.00317875\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272611\n",
      "Iteration 2, loss = 0.00601877\n",
      "Iteration 3, loss = 0.01036861\n",
      "Iteration 4, loss = 0.01263038\n",
      "Iteration 5, loss = 0.00856368\n",
      "Iteration 6, loss = 0.00440345\n",
      "Iteration 7, loss = 0.00370171\n",
      "Iteration 8, loss = 0.00564846\n",
      "Iteration 9, loss = 0.00721094\n",
      "Iteration 10, loss = 0.00679646\n",
      "Iteration 11, loss = 0.00515439\n",
      "Iteration 12, loss = 0.00383562\n",
      "Iteration 13, loss = 0.00373248\n",
      "Iteration 14, loss = 0.00454255\n",
      "Iteration 15, loss = 0.00521473\n",
      "Iteration 16, loss = 0.00502823\n",
      "Iteration 17, loss = 0.00417926\n",
      "Iteration 18, loss = 0.00336359\n",
      "Iteration 19, loss = 0.00311413\n",
      "Iteration 20, loss = 0.00342419\n",
      "Iteration 21, loss = 0.00385704\n",
      "Iteration 22, loss = 0.00398865\n",
      "Iteration 23, loss = 0.00373387\n",
      "Iteration 24, loss = 0.00333657\n",
      "Iteration 25, loss = 0.00310895\n",
      "Iteration 26, loss = 0.00316931\n",
      "Iteration 27, loss = 0.00338567\n",
      "Iteration 28, loss = 0.00351589\n",
      "Iteration 29, loss = 0.00342250\n",
      "Iteration 30, loss = 0.00317879\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.00602176\n",
      "Iteration 3, loss = 0.01037080\n",
      "Iteration 4, loss = 0.01262946\n",
      "Iteration 5, loss = 0.00856214\n",
      "Iteration 6, loss = 0.00440300\n",
      "Iteration 7, loss = 0.00370260\n",
      "Iteration 8, loss = 0.00564965\n",
      "Iteration 9, loss = 0.00721127\n",
      "Iteration 10, loss = 0.00679588\n",
      "Iteration 11, loss = 0.00515383\n",
      "Iteration 12, loss = 0.00383591\n",
      "Iteration 13, loss = 0.00373369\n",
      "Iteration 14, loss = 0.00454401\n",
      "Iteration 15, loss = 0.00521537\n",
      "Iteration 16, loss = 0.00502783\n",
      "Iteration 17, loss = 0.00417850\n",
      "Iteration 18, loss = 0.00336320\n",
      "Iteration 19, loss = 0.00311446\n",
      "Iteration 20, loss = 0.00342490\n",
      "Iteration 21, loss = 0.00385754\n",
      "Iteration 22, loss = 0.00398862\n",
      "Iteration 23, loss = 0.00373347\n",
      "Iteration 24, loss = 0.00333630\n",
      "Iteration 25, loss = 0.00310920\n",
      "Iteration 26, loss = 0.00316989\n",
      "Iteration 27, loss = 0.00338628\n",
      "Iteration 28, loss = 0.00351612\n",
      "Iteration 29, loss = 0.00342228\n",
      "Iteration 30, loss = 0.00317845\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272586\n",
      "Iteration 2, loss = 0.00602005\n",
      "Iteration 3, loss = 0.01036906\n",
      "Iteration 4, loss = 0.01262961\n",
      "Iteration 5, loss = 0.00856291\n",
      "Iteration 6, loss = 0.00440321\n",
      "Iteration 7, loss = 0.00370191\n",
      "Iteration 8, loss = 0.00564873\n",
      "Iteration 9, loss = 0.00721096\n",
      "Iteration 10, loss = 0.00679627\n",
      "Iteration 11, loss = 0.00515431\n",
      "Iteration 12, loss = 0.00383583\n",
      "Iteration 13, loss = 0.00373290\n",
      "Iteration 14, loss = 0.00454297\n",
      "Iteration 15, loss = 0.00521481\n",
      "Iteration 16, loss = 0.00502796\n",
      "Iteration 17, loss = 0.00417893\n",
      "Iteration 18, loss = 0.00336340\n",
      "Iteration 19, loss = 0.00311417\n",
      "Iteration 20, loss = 0.00342433\n",
      "Iteration 21, loss = 0.00385712\n",
      "Iteration 22, loss = 0.00398860\n",
      "Iteration 23, loss = 0.00373376\n",
      "Iteration 24, loss = 0.00333654\n",
      "Iteration 25, loss = 0.00310909\n",
      "Iteration 26, loss = 0.00316950\n",
      "Iteration 27, loss = 0.00338584\n",
      "Iteration 28, loss = 0.00351592\n",
      "Iteration 29, loss = 0.00342239\n",
      "Iteration 30, loss = 0.00317865\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272583\n",
      "Iteration 2, loss = 0.00602010\n",
      "Iteration 3, loss = 0.01036885\n",
      "Iteration 4, loss = 0.01262937\n",
      "Iteration 5, loss = 0.00856279\n",
      "Iteration 6, loss = 0.00440316\n",
      "Iteration 7, loss = 0.00370186\n",
      "Iteration 8, loss = 0.00564866\n",
      "Iteration 9, loss = 0.00721093\n",
      "Iteration 10, loss = 0.00679632\n",
      "Iteration 11, loss = 0.00515441\n",
      "Iteration 12, loss = 0.00383591\n",
      "Iteration 13, loss = 0.00373290\n",
      "Iteration 14, loss = 0.00454292\n",
      "Iteration 15, loss = 0.00521475\n",
      "Iteration 16, loss = 0.00502792\n",
      "Iteration 17, loss = 0.00417893\n",
      "Iteration 18, loss = 0.00336340\n",
      "Iteration 19, loss = 0.00311414\n",
      "Iteration 20, loss = 0.00342428\n",
      "Iteration 21, loss = 0.00385709\n",
      "Iteration 22, loss = 0.00398861\n",
      "Iteration 23, loss = 0.00373381\n",
      "Iteration 24, loss = 0.00333662\n",
      "Iteration 25, loss = 0.00310915\n",
      "Iteration 26, loss = 0.00316951\n",
      "Iteration 27, loss = 0.00338582\n",
      "Iteration 28, loss = 0.00351590\n",
      "Iteration 29, loss = 0.00342239\n",
      "Iteration 30, loss = 0.00317866\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272615\n",
      "Iteration 2, loss = 0.00601774\n",
      "Iteration 3, loss = 0.01036682\n",
      "Iteration 4, loss = 0.01262981\n",
      "Iteration 5, loss = 0.00856385\n",
      "Iteration 6, loss = 0.00440341\n",
      "Iteration 7, loss = 0.00370103\n",
      "Iteration 8, loss = 0.00564759\n",
      "Iteration 9, loss = 0.00721060\n",
      "Iteration 10, loss = 0.00679677\n",
      "Iteration 11, loss = 0.00515491\n",
      "Iteration 12, loss = 0.00383570\n",
      "Iteration 13, loss = 0.00373186\n",
      "Iteration 14, loss = 0.00454166\n",
      "Iteration 15, loss = 0.00521410\n",
      "Iteration 16, loss = 0.00502812\n",
      "Iteration 17, loss = 0.00417946\n",
      "Iteration 18, loss = 0.00336365\n",
      "Iteration 19, loss = 0.00311377\n",
      "Iteration 20, loss = 0.00342353\n",
      "Iteration 21, loss = 0.00385649\n",
      "Iteration 22, loss = 0.00398850\n",
      "Iteration 23, loss = 0.00373406\n",
      "Iteration 24, loss = 0.00333680\n",
      "Iteration 25, loss = 0.00310890\n",
      "Iteration 26, loss = 0.00316894\n",
      "Iteration 27, loss = 0.00338518\n",
      "Iteration 28, loss = 0.00351555\n",
      "Iteration 29, loss = 0.00342240\n",
      "Iteration 30, loss = 0.00317880\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272540\n",
      "Iteration 2, loss = 0.00602229\n",
      "Iteration 3, loss = 0.01037030\n",
      "Iteration 4, loss = 0.01262853\n",
      "Iteration 5, loss = 0.00856158\n",
      "Iteration 6, loss = 0.00440278\n",
      "Iteration 7, loss = 0.00370244\n",
      "Iteration 8, loss = 0.00564945\n",
      "Iteration 9, loss = 0.00721111\n",
      "Iteration 10, loss = 0.00679588\n",
      "Iteration 11, loss = 0.00515402\n",
      "Iteration 12, loss = 0.00383613\n",
      "Iteration 13, loss = 0.00373375\n",
      "Iteration 14, loss = 0.00454390\n",
      "Iteration 15, loss = 0.00521511\n",
      "Iteration 16, loss = 0.00502757\n",
      "Iteration 17, loss = 0.00417834\n",
      "Iteration 18, loss = 0.00336308\n",
      "Iteration 19, loss = 0.00311433\n",
      "Iteration 20, loss = 0.00342473\n",
      "Iteration 21, loss = 0.00385739\n",
      "Iteration 22, loss = 0.00398856\n",
      "Iteration 23, loss = 0.00373353\n",
      "Iteration 24, loss = 0.00333643\n",
      "Iteration 25, loss = 0.00310931\n",
      "Iteration 26, loss = 0.00316989\n",
      "Iteration 27, loss = 0.00338620\n",
      "Iteration 28, loss = 0.00351608\n",
      "Iteration 29, loss = 0.00342232\n",
      "Iteration 30, loss = 0.00317850\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272553\n",
      "Iteration 2, loss = 0.00602146\n",
      "Iteration 3, loss = 0.01036945\n",
      "Iteration 4, loss = 0.01262861\n",
      "Iteration 5, loss = 0.00856196\n",
      "Iteration 6, loss = 0.00440286\n",
      "Iteration 7, loss = 0.00370208\n",
      "Iteration 8, loss = 0.00564897\n",
      "Iteration 9, loss = 0.00721094\n",
      "Iteration 10, loss = 0.00679605\n",
      "Iteration 11, loss = 0.00515422\n",
      "Iteration 12, loss = 0.00383607\n",
      "Iteration 13, loss = 0.00373334\n",
      "Iteration 14, loss = 0.00454338\n",
      "Iteration 15, loss = 0.00521483\n",
      "Iteration 16, loss = 0.00502761\n",
      "Iteration 17, loss = 0.00417851\n",
      "Iteration 18, loss = 0.00336314\n",
      "Iteration 19, loss = 0.00311416\n",
      "Iteration 20, loss = 0.00342443\n",
      "Iteration 21, loss = 0.00385717\n",
      "Iteration 22, loss = 0.00398853\n",
      "Iteration 23, loss = 0.00373364\n",
      "Iteration 24, loss = 0.00333651\n",
      "Iteration 25, loss = 0.00310923\n",
      "Iteration 26, loss = 0.00316969\n",
      "Iteration 27, loss = 0.00338598\n",
      "Iteration 28, loss = 0.00351597\n",
      "Iteration 29, loss = 0.00342234\n",
      "Iteration 30, loss = 0.00317857\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272545\n",
      "Iteration 2, loss = 0.00602170\n",
      "Iteration 3, loss = 0.01036936\n",
      "Iteration 4, loss = 0.01262827\n",
      "Iteration 5, loss = 0.00856169\n",
      "Iteration 6, loss = 0.00440273\n",
      "Iteration 7, loss = 0.00370202\n",
      "Iteration 8, loss = 0.00564891\n",
      "Iteration 9, loss = 0.00721086\n",
      "Iteration 10, loss = 0.00679597\n",
      "Iteration 11, loss = 0.00515419\n",
      "Iteration 12, loss = 0.00383608\n",
      "Iteration 13, loss = 0.00373334\n",
      "Iteration 14, loss = 0.00454334\n",
      "Iteration 15, loss = 0.00521471\n",
      "Iteration 16, loss = 0.00502744\n",
      "Iteration 17, loss = 0.00417836\n",
      "Iteration 18, loss = 0.00336301\n",
      "Iteration 19, loss = 0.00311407\n",
      "Iteration 20, loss = 0.00342434\n",
      "Iteration 21, loss = 0.00385708\n",
      "Iteration 22, loss = 0.00398844\n",
      "Iteration 23, loss = 0.00373357\n",
      "Iteration 24, loss = 0.00333648\n",
      "Iteration 25, loss = 0.00310922\n",
      "Iteration 26, loss = 0.00316965\n",
      "Iteration 27, loss = 0.00338592\n",
      "Iteration 28, loss = 0.00351588\n",
      "Iteration 29, loss = 0.00342224\n",
      "Iteration 30, loss = 0.00317847\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272592\n",
      "Iteration 2, loss = 0.00601902\n",
      "Iteration 3, loss = 0.01036720\n",
      "Iteration 4, loss = 0.01262896\n",
      "Iteration 5, loss = 0.00856302\n",
      "Iteration 6, loss = 0.00440310\n",
      "Iteration 7, loss = 0.00370116\n",
      "Iteration 8, loss = 0.00564779\n",
      "Iteration 9, loss = 0.00721055\n",
      "Iteration 10, loss = 0.00679653\n",
      "Iteration 11, loss = 0.00515477\n",
      "Iteration 12, loss = 0.00383585\n",
      "Iteration 13, loss = 0.00373221\n",
      "Iteration 14, loss = 0.00454198\n",
      "Iteration 15, loss = 0.00521408\n",
      "Iteration 16, loss = 0.00502775\n",
      "Iteration 17, loss = 0.00417903\n",
      "Iteration 18, loss = 0.00336336\n",
      "Iteration 19, loss = 0.00311371\n",
      "Iteration 20, loss = 0.00342356\n",
      "Iteration 21, loss = 0.00385647\n",
      "Iteration 22, loss = 0.00398836\n",
      "Iteration 23, loss = 0.00373387\n",
      "Iteration 24, loss = 0.00333669\n",
      "Iteration 25, loss = 0.00310896\n",
      "Iteration 26, loss = 0.00316903\n",
      "Iteration 27, loss = 0.00338525\n",
      "Iteration 28, loss = 0.00351547\n",
      "Iteration 29, loss = 0.00342218\n",
      "Iteration 30, loss = 0.00317856\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272578\n",
      "Iteration 2, loss = 0.00601984\n",
      "Iteration 3, loss = 0.01036785\n",
      "Iteration 4, loss = 0.01262878\n",
      "Iteration 5, loss = 0.00856268\n",
      "Iteration 6, loss = 0.00440307\n",
      "Iteration 7, loss = 0.00370149\n",
      "Iteration 8, loss = 0.00564818\n",
      "Iteration 9, loss = 0.00721071\n",
      "Iteration 10, loss = 0.00679646\n",
      "Iteration 11, loss = 0.00515471\n",
      "Iteration 12, loss = 0.00383603\n",
      "Iteration 13, loss = 0.00373262\n",
      "Iteration 14, loss = 0.00454245\n",
      "Iteration 15, loss = 0.00521434\n",
      "Iteration 16, loss = 0.00502776\n",
      "Iteration 17, loss = 0.00417896\n",
      "Iteration 18, loss = 0.00336336\n",
      "Iteration 19, loss = 0.00311392\n",
      "Iteration 20, loss = 0.00342392\n",
      "Iteration 21, loss = 0.00385679\n",
      "Iteration 22, loss = 0.00398854\n",
      "Iteration 23, loss = 0.00373393\n",
      "Iteration 24, loss = 0.00333677\n",
      "Iteration 25, loss = 0.00310917\n",
      "Iteration 26, loss = 0.00316933\n",
      "Iteration 27, loss = 0.00338557\n",
      "Iteration 28, loss = 0.00351577\n",
      "Iteration 29, loss = 0.00342240\n",
      "Iteration 30, loss = 0.00317872\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272313\n",
      "Iteration 2, loss = 0.00602972\n",
      "Iteration 3, loss = 0.01037542\n",
      "Iteration 4, loss = 0.01262509\n",
      "Iteration 5, loss = 0.00855653\n",
      "Iteration 6, loss = 0.00440077\n",
      "Iteration 7, loss = 0.00370413\n",
      "Iteration 8, loss = 0.00565196\n",
      "Iteration 9, loss = 0.00721127\n",
      "Iteration 10, loss = 0.00679357\n",
      "Iteration 11, loss = 0.00515175\n",
      "Iteration 12, loss = 0.00383623\n",
      "Iteration 13, loss = 0.00373638\n",
      "Iteration 14, loss = 0.00454713\n",
      "Iteration 15, loss = 0.00521607\n",
      "Iteration 16, loss = 0.00502569\n",
      "Iteration 17, loss = 0.00417548\n",
      "Iteration 18, loss = 0.00336127\n",
      "Iteration 19, loss = 0.00311450\n",
      "Iteration 20, loss = 0.00342597\n",
      "Iteration 21, loss = 0.00385808\n",
      "Iteration 22, loss = 0.00398778\n",
      "Iteration 23, loss = 0.00373176\n",
      "Iteration 24, loss = 0.00333506\n",
      "Iteration 25, loss = 0.00310929\n",
      "Iteration 26, loss = 0.00317079\n",
      "Iteration 27, loss = 0.00338711\n",
      "Iteration 28, loss = 0.00351598\n",
      "Iteration 29, loss = 0.00342108\n",
      "Iteration 30, loss = 0.00317696\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272329\n",
      "Iteration 2, loss = 0.00602865\n",
      "Iteration 3, loss = 0.01037360\n",
      "Iteration 4, loss = 0.01262473\n",
      "Iteration 5, loss = 0.00855699\n",
      "Iteration 6, loss = 0.00440092\n",
      "Iteration 7, loss = 0.00370346\n",
      "Iteration 8, loss = 0.00565102\n",
      "Iteration 9, loss = 0.00721090\n",
      "Iteration 10, loss = 0.00679396\n",
      "Iteration 11, loss = 0.00515237\n",
      "Iteration 12, loss = 0.00383638\n",
      "Iteration 13, loss = 0.00373579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 0.00454620\n",
      "Iteration 15, loss = 0.00521551\n",
      "Iteration 16, loss = 0.00502576\n",
      "Iteration 17, loss = 0.00417588\n",
      "Iteration 18, loss = 0.00336149\n",
      "Iteration 19, loss = 0.00311429\n",
      "Iteration 20, loss = 0.00342549\n",
      "Iteration 21, loss = 0.00385775\n",
      "Iteration 22, loss = 0.00398788\n",
      "Iteration 23, loss = 0.00373221\n",
      "Iteration 24, loss = 0.00333552\n",
      "Iteration 25, loss = 0.00310945\n",
      "Iteration 26, loss = 0.00317065\n",
      "Iteration 27, loss = 0.00338689\n",
      "Iteration 28, loss = 0.00351596\n",
      "Iteration 29, loss = 0.00342136\n",
      "Iteration 30, loss = 0.00317738\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272315\n",
      "Iteration 2, loss = 0.00602876\n",
      "Iteration 3, loss = 0.01037326\n",
      "Iteration 4, loss = 0.01262377\n",
      "Iteration 5, loss = 0.00855727\n",
      "Iteration 6, loss = 0.00440104\n",
      "Iteration 7, loss = 0.00370319\n",
      "Iteration 8, loss = 0.00565067\n",
      "Iteration 9, loss = 0.00721079\n",
      "Iteration 10, loss = 0.00679405\n",
      "Iteration 11, loss = 0.00515230\n",
      "Iteration 12, loss = 0.00383630\n",
      "Iteration 13, loss = 0.00373578\n",
      "Iteration 14, loss = 0.00454625\n",
      "Iteration 15, loss = 0.00521544\n",
      "Iteration 16, loss = 0.00502550\n",
      "Iteration 17, loss = 0.00417556\n",
      "Iteration 18, loss = 0.00336117\n",
      "Iteration 19, loss = 0.00311411\n",
      "Iteration 20, loss = 0.00342544\n",
      "Iteration 21, loss = 0.00385774\n",
      "Iteration 22, loss = 0.00398781\n",
      "Iteration 23, loss = 0.00373206\n",
      "Iteration 24, loss = 0.00333536\n",
      "Iteration 25, loss = 0.00310936\n",
      "Iteration 26, loss = 0.00317063\n",
      "Iteration 27, loss = 0.00338682\n",
      "Iteration 28, loss = 0.00351585\n",
      "Iteration 29, loss = 0.00342118\n",
      "Iteration 30, loss = 0.00317711\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272337\n",
      "Iteration 2, loss = 0.00602781\n",
      "Iteration 3, loss = 0.01037197\n",
      "Iteration 4, loss = 0.01262422\n",
      "Iteration 5, loss = 0.00855721\n",
      "Iteration 6, loss = 0.00440095\n",
      "Iteration 7, loss = 0.00370306\n",
      "Iteration 8, loss = 0.00565051\n",
      "Iteration 9, loss = 0.00721085\n",
      "Iteration 10, loss = 0.00679448\n",
      "Iteration 11, loss = 0.00515305\n",
      "Iteration 12, loss = 0.00383667\n",
      "Iteration 13, loss = 0.00373543\n",
      "Iteration 14, loss = 0.00454559\n",
      "Iteration 15, loss = 0.00521508\n",
      "Iteration 16, loss = 0.00502575\n",
      "Iteration 17, loss = 0.00417618\n",
      "Iteration 18, loss = 0.00336164\n",
      "Iteration 19, loss = 0.00311415\n",
      "Iteration 20, loss = 0.00342515\n",
      "Iteration 21, loss = 0.00385753\n",
      "Iteration 22, loss = 0.00398801\n",
      "Iteration 23, loss = 0.00373262\n",
      "Iteration 24, loss = 0.00333595\n",
      "Iteration 25, loss = 0.00310967\n",
      "Iteration 26, loss = 0.00317053\n",
      "Iteration 27, loss = 0.00338667\n",
      "Iteration 28, loss = 0.00351584\n",
      "Iteration 29, loss = 0.00342138\n",
      "Iteration 30, loss = 0.00317743\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272176\n",
      "Iteration 2, loss = 0.00603118\n",
      "Iteration 3, loss = 0.01037374\n",
      "Iteration 4, loss = 0.01262274\n",
      "Iteration 5, loss = 0.00855621\n",
      "Iteration 6, loss = 0.00440059\n",
      "Iteration 7, loss = 0.00370333\n",
      "Iteration 8, loss = 0.00565110\n",
      "Iteration 9, loss = 0.00721104\n",
      "Iteration 10, loss = 0.00679395\n",
      "Iteration 11, loss = 0.00515208\n",
      "Iteration 12, loss = 0.00383647\n",
      "Iteration 13, loss = 0.00373647\n",
      "Iteration 14, loss = 0.00454715\n",
      "Iteration 15, loss = 0.00521574\n",
      "Iteration 16, loss = 0.00502503\n",
      "Iteration 17, loss = 0.00417477\n",
      "Iteration 18, loss = 0.00336052\n",
      "Iteration 19, loss = 0.00311392\n",
      "Iteration 20, loss = 0.00342560\n",
      "Iteration 21, loss = 0.00385788\n",
      "Iteration 22, loss = 0.00398768\n",
      "Iteration 23, loss = 0.00373170\n",
      "Iteration 24, loss = 0.00333506\n",
      "Iteration 25, loss = 0.00310936\n",
      "Iteration 26, loss = 0.00317078\n",
      "Iteration 27, loss = 0.00338698\n",
      "Iteration 28, loss = 0.00351575\n",
      "Iteration 29, loss = 0.00342075\n",
      "Iteration 30, loss = 0.00317657\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272421\n",
      "Iteration 2, loss = 0.00602429\n",
      "Iteration 3, loss = 0.01036759\n",
      "Iteration 4, loss = 0.01262513\n",
      "Iteration 5, loss = 0.00856093\n",
      "Iteration 6, loss = 0.00440247\n",
      "Iteration 7, loss = 0.00370113\n",
      "Iteration 8, loss = 0.00564793\n",
      "Iteration 9, loss = 0.00721057\n",
      "Iteration 10, loss = 0.00679642\n",
      "Iteration 11, loss = 0.00515474\n",
      "Iteration 12, loss = 0.00383660\n",
      "Iteration 13, loss = 0.00373360\n",
      "Iteration 14, loss = 0.00454344\n",
      "Iteration 15, loss = 0.00521423\n",
      "Iteration 16, loss = 0.00502652\n",
      "Iteration 17, loss = 0.00417745\n",
      "Iteration 18, loss = 0.00336212\n",
      "Iteration 19, loss = 0.00311329\n",
      "Iteration 20, loss = 0.00342368\n",
      "Iteration 21, loss = 0.00385656\n",
      "Iteration 22, loss = 0.00398809\n",
      "Iteration 23, loss = 0.00373335\n",
      "Iteration 24, loss = 0.00333642\n",
      "Iteration 25, loss = 0.00310928\n",
      "Iteration 26, loss = 0.00316952\n",
      "Iteration 27, loss = 0.00338559\n",
      "Iteration 28, loss = 0.00351536\n",
      "Iteration 29, loss = 0.00342155\n",
      "Iteration 30, loss = 0.00317770\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272403\n",
      "Iteration 2, loss = 0.00602458\n",
      "Iteration 3, loss = 0.01036642\n",
      "Iteration 4, loss = 0.01262387\n",
      "Iteration 5, loss = 0.00856015\n",
      "Iteration 6, loss = 0.00440233\n",
      "Iteration 7, loss = 0.00370083\n",
      "Iteration 8, loss = 0.00564736\n",
      "Iteration 9, loss = 0.00721002\n",
      "Iteration 10, loss = 0.00679608\n",
      "Iteration 11, loss = 0.00515460\n",
      "Iteration 12, loss = 0.00383654\n",
      "Iteration 13, loss = 0.00373344\n",
      "Iteration 14, loss = 0.00454281\n",
      "Iteration 15, loss = 0.00521323\n",
      "Iteration 16, loss = 0.00502555\n",
      "Iteration 17, loss = 0.00417677\n",
      "Iteration 18, loss = 0.00336169\n",
      "Iteration 19, loss = 0.00311291\n",
      "Iteration 20, loss = 0.00342310\n",
      "Iteration 21, loss = 0.00385584\n",
      "Iteration 22, loss = 0.00398749\n",
      "Iteration 23, loss = 0.00373298\n",
      "Iteration 24, loss = 0.00333622\n",
      "Iteration 25, loss = 0.00310901\n",
      "Iteration 26, loss = 0.00316909\n",
      "Iteration 27, loss = 0.00338497\n",
      "Iteration 28, loss = 0.00351471\n",
      "Iteration 29, loss = 0.00342095\n",
      "Iteration 30, loss = 0.00317719\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272046\n",
      "Iteration 2, loss = 0.00603310\n",
      "Iteration 3, loss = 0.01037366\n",
      "Iteration 4, loss = 0.01262033\n",
      "Iteration 5, loss = 0.00855400\n",
      "Iteration 6, loss = 0.00439981\n",
      "Iteration 7, loss = 0.00370342\n",
      "Iteration 8, loss = 0.00565137\n",
      "Iteration 9, loss = 0.00721086\n",
      "Iteration 10, loss = 0.00679337\n",
      "Iteration 11, loss = 0.00515168\n",
      "Iteration 12, loss = 0.00383666\n",
      "Iteration 13, loss = 0.00373708\n",
      "Iteration 14, loss = 0.00454767\n",
      "Iteration 15, loss = 0.00521550\n",
      "Iteration 16, loss = 0.00502414\n",
      "Iteration 17, loss = 0.00417380\n",
      "Iteration 18, loss = 0.00335993\n",
      "Iteration 19, loss = 0.00311377\n",
      "Iteration 20, loss = 0.00342566\n",
      "Iteration 21, loss = 0.00385787\n",
      "Iteration 22, loss = 0.00398750\n",
      "Iteration 23, loss = 0.00373147\n",
      "Iteration 24, loss = 0.00333500\n",
      "Iteration 25, loss = 0.00310950\n",
      "Iteration 26, loss = 0.00317100\n",
      "Iteration 27, loss = 0.00338708\n",
      "Iteration 28, loss = 0.00351560\n",
      "Iteration 29, loss = 0.00342040\n",
      "Iteration 30, loss = 0.00317619\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02271611\n",
      "Iteration 2, loss = 0.00603895\n",
      "Iteration 3, loss = 0.01037656\n",
      "Iteration 4, loss = 0.01261490\n",
      "Iteration 5, loss = 0.00854729\n",
      "Iteration 6, loss = 0.00439655\n",
      "Iteration 7, loss = 0.00370398\n",
      "Iteration 8, loss = 0.00565253\n",
      "Iteration 9, loss = 0.00720954\n",
      "Iteration 10, loss = 0.00678963\n",
      "Iteration 11, loss = 0.00514812\n",
      "Iteration 12, loss = 0.00383573\n",
      "Iteration 13, loss = 0.00373907\n",
      "Iteration 14, loss = 0.00455015\n",
      "Iteration 15, loss = 0.00521567\n",
      "Iteration 16, loss = 0.00502145\n",
      "Iteration 17, loss = 0.00417016\n",
      "Iteration 18, loss = 0.00335747\n",
      "Iteration 19, loss = 0.00311346\n",
      "Iteration 20, loss = 0.00342623\n",
      "Iteration 21, loss = 0.00385787\n",
      "Iteration 22, loss = 0.00398611\n",
      "Iteration 23, loss = 0.00372949\n",
      "Iteration 24, loss = 0.00333336\n",
      "Iteration 25, loss = 0.00310915\n",
      "Iteration 26, loss = 0.00317152\n",
      "Iteration 27, loss = 0.00338755\n",
      "Iteration 28, loss = 0.00351522\n",
      "Iteration 29, loss = 0.00341901\n",
      "Iteration 30, loss = 0.00317454\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02271863\n",
      "Iteration 2, loss = 0.00603506\n",
      "Iteration 3, loss = 0.01037254\n",
      "Iteration 4, loss = 0.01261731\n",
      "Iteration 5, loss = 0.00855152\n",
      "Iteration 6, loss = 0.00439855\n",
      "Iteration 7, loss = 0.00370300\n",
      "Iteration 8, loss = 0.00565063\n",
      "Iteration 9, loss = 0.00720978\n",
      "Iteration 10, loss = 0.00679222\n",
      "Iteration 11, loss = 0.00515113\n",
      "Iteration 12, loss = 0.00383689\n",
      "Iteration 13, loss = 0.00373742\n",
      "Iteration 14, loss = 0.00454751\n",
      "Iteration 15, loss = 0.00521470\n",
      "Iteration 16, loss = 0.00502298\n",
      "Iteration 17, loss = 0.00417281\n",
      "Iteration 18, loss = 0.00335928\n",
      "Iteration 19, loss = 0.00311343\n",
      "Iteration 20, loss = 0.00342529\n",
      "Iteration 21, loss = 0.00385735\n",
      "Iteration 22, loss = 0.00398698\n",
      "Iteration 23, loss = 0.00373116\n",
      "Iteration 24, loss = 0.00333496\n",
      "Iteration 25, loss = 0.00310967\n",
      "Iteration 26, loss = 0.00317108\n",
      "Iteration 27, loss = 0.00338702\n",
      "Iteration 28, loss = 0.00351532\n",
      "Iteration 29, loss = 0.00342000\n",
      "Iteration 30, loss = 0.00317580\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02271932\n",
      "Iteration 2, loss = 0.00603366\n",
      "Iteration 3, loss = 0.01037078\n",
      "Iteration 4, loss = 0.01261764\n",
      "Iteration 5, loss = 0.00855266\n",
      "Iteration 6, loss = 0.00439905\n",
      "Iteration 7, loss = 0.00370258\n",
      "Iteration 8, loss = 0.00564994\n",
      "Iteration 9, loss = 0.00720962\n",
      "Iteration 10, loss = 0.00679281\n",
      "Iteration 11, loss = 0.00515190\n",
      "Iteration 12, loss = 0.00383711\n",
      "Iteration 13, loss = 0.00373679\n",
      "Iteration 14, loss = 0.00454657\n",
      "Iteration 15, loss = 0.00521415\n",
      "Iteration 16, loss = 0.00502313\n",
      "Iteration 17, loss = 0.00417335\n",
      "Iteration 18, loss = 0.00335960\n",
      "Iteration 19, loss = 0.00311325\n",
      "Iteration 20, loss = 0.00342480\n",
      "Iteration 21, loss = 0.00385701\n",
      "Iteration 22, loss = 0.00398706\n",
      "Iteration 23, loss = 0.00373156\n",
      "Iteration 24, loss = 0.00333533\n",
      "Iteration 25, loss = 0.00310972\n",
      "Iteration 26, loss = 0.00317075\n",
      "Iteration 27, loss = 0.00338659\n",
      "Iteration 28, loss = 0.00351508\n",
      "Iteration 29, loss = 0.00342002\n",
      "Iteration 30, loss = 0.00317592\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272281\n",
      "Iteration 2, loss = 0.00602625\n",
      "Iteration 3, loss = 0.01036371\n",
      "Iteration 4, loss = 0.01262010\n",
      "Iteration 5, loss = 0.00855793\n",
      "Iteration 6, loss = 0.00440131\n",
      "Iteration 7, loss = 0.00370020\n",
      "Iteration 8, loss = 0.00564625\n",
      "Iteration 9, loss = 0.00720891\n",
      "Iteration 10, loss = 0.00679551\n",
      "Iteration 11, loss = 0.00515475\n",
      "Iteration 12, loss = 0.00383708\n",
      "Iteration 13, loss = 0.00373331\n",
      "Iteration 14, loss = 0.00454201\n",
      "Iteration 15, loss = 0.00521193\n",
      "Iteration 16, loss = 0.00502430\n",
      "Iteration 17, loss = 0.00417603\n",
      "Iteration 18, loss = 0.00336109\n",
      "Iteration 19, loss = 0.00311226\n",
      "Iteration 20, loss = 0.00342222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.00385490\n",
      "Iteration 22, loss = 0.00398680\n",
      "Iteration 23, loss = 0.00373269\n",
      "Iteration 24, loss = 0.00333629\n",
      "Iteration 25, loss = 0.00310918\n",
      "Iteration 26, loss = 0.00316874\n",
      "Iteration 27, loss = 0.00338429\n",
      "Iteration 28, loss = 0.00351387\n",
      "Iteration 29, loss = 0.00342019\n",
      "Iteration 30, loss = 0.00317659\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272057\n",
      "Iteration 2, loss = 0.00603111\n",
      "Iteration 3, loss = 0.01036806\n",
      "Iteration 4, loss = 0.01261870\n",
      "Iteration 5, loss = 0.00855502\n",
      "Iteration 6, loss = 0.00440042\n",
      "Iteration 7, loss = 0.00370195\n",
      "Iteration 8, loss = 0.00564880\n",
      "Iteration 9, loss = 0.00720982\n",
      "Iteration 10, loss = 0.00679453\n",
      "Iteration 11, loss = 0.00515376\n",
      "Iteration 12, loss = 0.00383773\n",
      "Iteration 13, loss = 0.00373586\n",
      "Iteration 14, loss = 0.00454499\n",
      "Iteration 15, loss = 0.00521339\n",
      "Iteration 16, loss = 0.00502382\n",
      "Iteration 17, loss = 0.00417479\n",
      "Iteration 18, loss = 0.00336065\n",
      "Iteration 19, loss = 0.00311325\n",
      "Iteration 20, loss = 0.00342410\n",
      "Iteration 21, loss = 0.00385649\n",
      "Iteration 22, loss = 0.00398739\n",
      "Iteration 23, loss = 0.00373253\n",
      "Iteration 24, loss = 0.00333630\n",
      "Iteration 25, loss = 0.00311003\n",
      "Iteration 26, loss = 0.00317041\n",
      "Iteration 27, loss = 0.00338608\n",
      "Iteration 28, loss = 0.00351499\n",
      "Iteration 29, loss = 0.00342050\n",
      "Iteration 30, loss = 0.00317661\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272310\n",
      "Iteration 2, loss = 0.00602481\n",
      "Iteration 3, loss = 0.01036197\n",
      "Iteration 4, loss = 0.01262045\n",
      "Iteration 5, loss = 0.00855891\n",
      "Iteration 6, loss = 0.00440150\n",
      "Iteration 7, loss = 0.00369911\n",
      "Iteration 8, loss = 0.00564482\n",
      "Iteration 9, loss = 0.00720837\n",
      "Iteration 10, loss = 0.00679691\n",
      "Iteration 11, loss = 0.00515679\n",
      "Iteration 12, loss = 0.00383790\n",
      "Iteration 13, loss = 0.00373267\n",
      "Iteration 14, loss = 0.00454102\n",
      "Iteration 15, loss = 0.00521173\n",
      "Iteration 16, loss = 0.00502519\n",
      "Iteration 17, loss = 0.00417714\n",
      "Iteration 18, loss = 0.00336148\n",
      "Iteration 19, loss = 0.00311162\n",
      "Iteration 20, loss = 0.00342115\n",
      "Iteration 21, loss = 0.00385411\n",
      "Iteration 22, loss = 0.00398682\n",
      "Iteration 23, loss = 0.00373324\n",
      "Iteration 24, loss = 0.00333663\n",
      "Iteration 25, loss = 0.00310891\n",
      "Iteration 26, loss = 0.00316792\n",
      "Iteration 27, loss = 0.00338312\n",
      "Iteration 28, loss = 0.00351301\n",
      "Iteration 29, loss = 0.00341983\n",
      "Iteration 30, loss = 0.00317643\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02271885\n",
      "Iteration 2, loss = 0.00603313\n",
      "Iteration 3, loss = 0.01036922\n",
      "Iteration 4, loss = 0.01261659\n",
      "Iteration 5, loss = 0.00855237\n",
      "Iteration 6, loss = 0.00439911\n",
      "Iteration 7, loss = 0.00370226\n",
      "Iteration 8, loss = 0.00564949\n",
      "Iteration 9, loss = 0.00720954\n",
      "Iteration 10, loss = 0.00679357\n",
      "Iteration 11, loss = 0.00515300\n",
      "Iteration 12, loss = 0.00383768\n",
      "Iteration 13, loss = 0.00373650\n",
      "Iteration 14, loss = 0.00454583\n",
      "Iteration 15, loss = 0.00521367\n",
      "Iteration 16, loss = 0.00502325\n",
      "Iteration 17, loss = 0.00417379\n",
      "Iteration 18, loss = 0.00335993\n",
      "Iteration 19, loss = 0.00311314\n",
      "Iteration 20, loss = 0.00342440\n",
      "Iteration 21, loss = 0.00385674\n",
      "Iteration 22, loss = 0.00398723\n",
      "Iteration 23, loss = 0.00373211\n",
      "Iteration 24, loss = 0.00333591\n",
      "Iteration 25, loss = 0.00310992\n",
      "Iteration 26, loss = 0.00317065\n",
      "Iteration 27, loss = 0.00338631\n",
      "Iteration 28, loss = 0.00351494\n",
      "Iteration 29, loss = 0.00342014\n",
      "Iteration 30, loss = 0.00317615\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02271081\n",
      "Iteration 2, loss = 0.00604290\n",
      "Iteration 3, loss = 0.01037738\n",
      "Iteration 4, loss = 0.01260908\n",
      "Iteration 5, loss = 0.00854109\n",
      "Iteration 6, loss = 0.00439335\n",
      "Iteration 7, loss = 0.00370400\n",
      "Iteration 8, loss = 0.00565312\n",
      "Iteration 9, loss = 0.00720853\n",
      "Iteration 10, loss = 0.00678714\n",
      "Iteration 11, loss = 0.00514627\n",
      "Iteration 12, loss = 0.00383578\n",
      "Iteration 13, loss = 0.00374050\n",
      "Iteration 14, loss = 0.00455152\n",
      "Iteration 15, loss = 0.00521515\n",
      "Iteration 16, loss = 0.00501902\n",
      "Iteration 17, loss = 0.00416731\n",
      "Iteration 18, loss = 0.00335560\n",
      "Iteration 19, loss = 0.00311306\n",
      "Iteration 20, loss = 0.00342665\n",
      "Iteration 21, loss = 0.00385807\n",
      "Iteration 22, loss = 0.00398551\n",
      "Iteration 23, loss = 0.00372847\n",
      "Iteration 24, loss = 0.00333270\n",
      "Iteration 25, loss = 0.00310936\n",
      "Iteration 26, loss = 0.00317218\n",
      "Iteration 27, loss = 0.00338809\n",
      "Iteration 28, loss = 0.00351499\n",
      "Iteration 29, loss = 0.00341805\n",
      "Iteration 30, loss = 0.00317339\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02271637\n",
      "Iteration 2, loss = 0.00603571\n",
      "Iteration 3, loss = 0.01037028\n",
      "Iteration 4, loss = 0.01261383\n",
      "Iteration 5, loss = 0.00854920\n",
      "Iteration 6, loss = 0.00439748\n",
      "Iteration 7, loss = 0.00370253\n",
      "Iteration 8, loss = 0.00565007\n",
      "Iteration 9, loss = 0.00720895\n",
      "Iteration 10, loss = 0.00679191\n",
      "Iteration 11, loss = 0.00515152\n",
      "Iteration 12, loss = 0.00383732\n",
      "Iteration 13, loss = 0.00373743\n",
      "Iteration 14, loss = 0.00454703\n",
      "Iteration 15, loss = 0.00521380\n",
      "Iteration 16, loss = 0.00502200\n",
      "Iteration 17, loss = 0.00417210\n",
      "Iteration 18, loss = 0.00335875\n",
      "Iteration 19, loss = 0.00311301\n",
      "Iteration 20, loss = 0.00342461\n",
      "Iteration 21, loss = 0.00385671\n",
      "Iteration 22, loss = 0.00398655\n",
      "Iteration 23, loss = 0.00373126\n",
      "Iteration 24, loss = 0.00333519\n",
      "Iteration 25, loss = 0.00310983\n",
      "Iteration 26, loss = 0.00317089\n",
      "Iteration 27, loss = 0.00338646\n",
      "Iteration 28, loss = 0.00351478\n",
      "Iteration 29, loss = 0.00341953\n",
      "Iteration 30, loss = 0.00317539\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02270545\n",
      "Iteration 2, loss = 0.00604585\n",
      "Iteration 3, loss = 0.01037862\n",
      "Iteration 4, loss = 0.01260384\n",
      "Iteration 5, loss = 0.00853496\n",
      "Iteration 6, loss = 0.00439007\n",
      "Iteration 7, loss = 0.00370406\n",
      "Iteration 8, loss = 0.00565401\n",
      "Iteration 9, loss = 0.00720762\n",
      "Iteration 10, loss = 0.00678464\n",
      "Iteration 11, loss = 0.00514383\n",
      "Iteration 12, loss = 0.00383491\n",
      "Iteration 13, loss = 0.00374131\n",
      "Iteration 14, loss = 0.00455264\n",
      "Iteration 15, loss = 0.00521459\n",
      "Iteration 16, loss = 0.00501644\n",
      "Iteration 17, loss = 0.00416412\n",
      "Iteration 18, loss = 0.00335323\n",
      "Iteration 19, loss = 0.00311218\n",
      "Iteration 20, loss = 0.00342662\n",
      "Iteration 21, loss = 0.00385776\n",
      "Iteration 22, loss = 0.00398424\n",
      "Iteration 23, loss = 0.00372655\n",
      "Iteration 24, loss = 0.00333101\n",
      "Iteration 25, loss = 0.00310857\n",
      "Iteration 26, loss = 0.00317194\n",
      "Iteration 27, loss = 0.00338785\n",
      "Iteration 28, loss = 0.00351397\n",
      "Iteration 29, loss = 0.00341624\n",
      "Iteration 30, loss = 0.00317133\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02271078\n",
      "Iteration 2, loss = 0.00604094\n",
      "Iteration 3, loss = 0.01037258\n",
      "Iteration 4, loss = 0.01260803\n",
      "Iteration 5, loss = 0.00854262\n",
      "Iteration 6, loss = 0.00439466\n",
      "Iteration 7, loss = 0.00370354\n",
      "Iteration 8, loss = 0.00565186\n",
      "Iteration 9, loss = 0.00720861\n",
      "Iteration 10, loss = 0.00678957\n",
      "Iteration 11, loss = 0.00514955\n",
      "Iteration 12, loss = 0.00383757\n",
      "Iteration 13, loss = 0.00373973\n",
      "Iteration 14, loss = 0.00454957\n",
      "Iteration 15, loss = 0.00521407\n",
      "Iteration 16, loss = 0.00501978\n",
      "Iteration 17, loss = 0.00416927\n",
      "Iteration 18, loss = 0.00335707\n",
      "Iteration 19, loss = 0.00311322\n",
      "Iteration 20, loss = 0.00342584\n",
      "Iteration 21, loss = 0.00385755\n",
      "Iteration 22, loss = 0.00398627\n",
      "Iteration 23, loss = 0.00373027\n",
      "Iteration 24, loss = 0.00333460\n",
      "Iteration 25, loss = 0.00311043\n",
      "Iteration 26, loss = 0.00317212\n",
      "Iteration 27, loss = 0.00338764\n",
      "Iteration 28, loss = 0.00351497\n",
      "Iteration 29, loss = 0.00341873\n",
      "Iteration 30, loss = 0.00317435\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02271405\n",
      "Iteration 2, loss = 0.00603638\n",
      "Iteration 3, loss = 0.01036746\n",
      "Iteration 4, loss = 0.01261008\n",
      "Iteration 5, loss = 0.00854714\n",
      "Iteration 6, loss = 0.00439699\n",
      "Iteration 7, loss = 0.00370237\n",
      "Iteration 8, loss = 0.00564980\n",
      "Iteration 9, loss = 0.00720898\n",
      "Iteration 10, loss = 0.00679345\n",
      "Iteration 11, loss = 0.00515388\n",
      "Iteration 12, loss = 0.00383924\n",
      "Iteration 13, loss = 0.00373832\n",
      "Iteration 14, loss = 0.00454739\n",
      "Iteration 15, loss = 0.00521373\n",
      "Iteration 16, loss = 0.00502204\n",
      "Iteration 17, loss = 0.00417252\n",
      "Iteration 18, loss = 0.00335897\n",
      "Iteration 19, loss = 0.00311295\n",
      "Iteration 20, loss = 0.00342432\n",
      "Iteration 21, loss = 0.00385649\n",
      "Iteration 22, loss = 0.00398688\n",
      "Iteration 23, loss = 0.00373207\n",
      "Iteration 24, loss = 0.00333619\n",
      "Iteration 25, loss = 0.00311077\n",
      "Iteration 26, loss = 0.00317103\n",
      "Iteration 27, loss = 0.00338607\n",
      "Iteration 28, loss = 0.00351422\n",
      "Iteration 29, loss = 0.00341912\n",
      "Iteration 30, loss = 0.00317519\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02272028\n",
      "Iteration 2, loss = 0.00602763\n",
      "Iteration 3, loss = 0.01035929\n",
      "Iteration 4, loss = 0.01261458\n",
      "Iteration 5, loss = 0.00855483\n",
      "Iteration 6, loss = 0.00439974\n",
      "Iteration 7, loss = 0.00369842\n",
      "Iteration 8, loss = 0.00564412\n",
      "Iteration 9, loss = 0.00720738\n",
      "Iteration 10, loss = 0.00679635\n",
      "Iteration 11, loss = 0.00515710\n",
      "Iteration 12, loss = 0.00383865\n",
      "Iteration 13, loss = 0.00373289\n",
      "Iteration 14, loss = 0.00454067\n",
      "Iteration 15, loss = 0.00521025\n",
      "Iteration 16, loss = 0.00502310\n",
      "Iteration 17, loss = 0.00417548\n",
      "Iteration 18, loss = 0.00336016\n",
      "Iteration 19, loss = 0.00311071\n",
      "Iteration 20, loss = 0.00342024\n",
      "Iteration 21, loss = 0.00385312\n",
      "Iteration 22, loss = 0.00398596\n",
      "Iteration 23, loss = 0.00373267\n",
      "Iteration 24, loss = 0.00333653\n",
      "Iteration 25, loss = 0.00310921\n",
      "Iteration 26, loss = 0.00316773\n",
      "Iteration 27, loss = 0.00338255\n",
      "Iteration 28, loss = 0.00351206\n",
      "Iteration 29, loss = 0.00341862\n",
      "Iteration 30, loss = 0.00317524\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02271292\n",
      "Iteration 2, loss = 0.00603710\n",
      "Iteration 3, loss = 0.01036700\n",
      "Iteration 4, loss = 0.01260846\n",
      "Iteration 5, loss = 0.00854588\n",
      "Iteration 6, loss = 0.00439642\n",
      "Iteration 7, loss = 0.00370231\n",
      "Iteration 8, loss = 0.00564974\n",
      "Iteration 9, loss = 0.00720871\n",
      "Iteration 10, loss = 0.00679312\n",
      "Iteration 11, loss = 0.00515377\n",
      "Iteration 12, loss = 0.00383939\n",
      "Iteration 13, loss = 0.00373846\n",
      "Iteration 14, loss = 0.00454726\n",
      "Iteration 15, loss = 0.00521312\n",
      "Iteration 16, loss = 0.00502144\n",
      "Iteration 17, loss = 0.00417226\n",
      "Iteration 18, loss = 0.00335879\n",
      "Iteration 19, loss = 0.00311264\n",
      "Iteration 20, loss = 0.00342390\n",
      "Iteration 21, loss = 0.00385607\n",
      "Iteration 22, loss = 0.00398662\n",
      "Iteration 23, loss = 0.00373197\n",
      "Iteration 24, loss = 0.00333614\n",
      "Iteration 25, loss = 0.00311058\n",
      "Iteration 26, loss = 0.00317076\n",
      "Iteration 27, loss = 0.00338564\n",
      "Iteration 28, loss = 0.00351376\n",
      "Iteration 29, loss = 0.00341866\n",
      "Iteration 30, loss = 0.00317473\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02271266\n",
      "Iteration 2, loss = 0.00603776\n",
      "Iteration 3, loss = 0.01036828\n",
      "Iteration 4, loss = 0.01260873\n",
      "Iteration 5, loss = 0.00854551\n",
      "Iteration 6, loss = 0.00439632\n",
      "Iteration 7, loss = 0.00370270\n",
      "Iteration 8, loss = 0.00565039\n",
      "Iteration 9, loss = 0.00720903\n",
      "Iteration 10, loss = 0.00679290\n",
      "Iteration 11, loss = 0.00515335\n",
      "Iteration 12, loss = 0.00383931\n",
      "Iteration 13, loss = 0.00373903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 0.00454814\n",
      "Iteration 15, loss = 0.00521387\n",
      "Iteration 16, loss = 0.00502150\n",
      "Iteration 17, loss = 0.00417174\n",
      "Iteration 18, loss = 0.00335858\n",
      "Iteration 19, loss = 0.00311307\n",
      "Iteration 20, loss = 0.00342475\n",
      "Iteration 21, loss = 0.00385678\n",
      "Iteration 22, loss = 0.00398684\n",
      "Iteration 23, loss = 0.00373182\n",
      "Iteration 24, loss = 0.00333602\n",
      "Iteration 25, loss = 0.00311084\n",
      "Iteration 26, loss = 0.00317141\n",
      "Iteration 27, loss = 0.00338640\n",
      "Iteration 28, loss = 0.00351430\n",
      "Iteration 29, loss = 0.00341890\n",
      "Iteration 30, loss = 0.00317488\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02270344\n",
      "Iteration 2, loss = 0.00604481\n",
      "Iteration 3, loss = 0.01037375\n",
      "Iteration 4, loss = 0.01260075\n",
      "Iteration 5, loss = 0.00853468\n",
      "Iteration 6, loss = 0.00439082\n",
      "Iteration 7, loss = 0.00370422\n",
      "Iteration 8, loss = 0.00565376\n",
      "Iteration 9, loss = 0.00720826\n",
      "Iteration 10, loss = 0.00678676\n",
      "Iteration 11, loss = 0.00514687\n",
      "Iteration 12, loss = 0.00383726\n",
      "Iteration 13, loss = 0.00374186\n",
      "Iteration 14, loss = 0.00455198\n",
      "Iteration 15, loss = 0.00521403\n",
      "Iteration 16, loss = 0.00501696\n",
      "Iteration 17, loss = 0.00416561\n",
      "Iteration 18, loss = 0.00335476\n",
      "Iteration 19, loss = 0.00311314\n",
      "Iteration 20, loss = 0.00342709\n",
      "Iteration 21, loss = 0.00385848\n",
      "Iteration 22, loss = 0.00398592\n",
      "Iteration 23, loss = 0.00372907\n",
      "Iteration 24, loss = 0.00333372\n",
      "Iteration 25, loss = 0.00311071\n",
      "Iteration 26, loss = 0.00317326\n",
      "Iteration 27, loss = 0.00338873\n",
      "Iteration 28, loss = 0.00351501\n",
      "Iteration 29, loss = 0.00341769\n",
      "Iteration 30, loss = 0.00317301\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02270561\n",
      "Iteration 2, loss = 0.00604315\n",
      "Iteration 3, loss = 0.01037254\n",
      "Iteration 4, loss = 0.01260282\n",
      "Iteration 5, loss = 0.00853714\n",
      "Iteration 6, loss = 0.00439207\n",
      "Iteration 7, loss = 0.00370387\n",
      "Iteration 8, loss = 0.00565285\n",
      "Iteration 9, loss = 0.00720828\n",
      "Iteration 10, loss = 0.00678889\n",
      "Iteration 11, loss = 0.00514947\n",
      "Iteration 12, loss = 0.00383858\n",
      "Iteration 13, loss = 0.00374164\n",
      "Iteration 14, loss = 0.00455154\n",
      "Iteration 15, loss = 0.00521462\n",
      "Iteration 16, loss = 0.00501881\n",
      "Iteration 17, loss = 0.00416780\n",
      "Iteration 18, loss = 0.00335604\n",
      "Iteration 19, loss = 0.00311316\n",
      "Iteration 20, loss = 0.00342643\n",
      "Iteration 21, loss = 0.00385802\n",
      "Iteration 22, loss = 0.00398634\n",
      "Iteration 23, loss = 0.00373011\n",
      "Iteration 24, loss = 0.00333462\n",
      "Iteration 25, loss = 0.00311092\n",
      "Iteration 26, loss = 0.00317282\n",
      "Iteration 27, loss = 0.00338820\n",
      "Iteration 28, loss = 0.00351502\n",
      "Iteration 29, loss = 0.00341821\n",
      "Iteration 30, loss = 0.00317361\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02267101\n",
      "Iteration 2, loss = 0.00605288\n",
      "Iteration 3, loss = 0.01037884\n",
      "Iteration 4, loss = 0.01257297\n",
      "Iteration 5, loss = 0.00849974\n",
      "Iteration 6, loss = 0.00436932\n",
      "Iteration 7, loss = 0.00369997\n",
      "Iteration 8, loss = 0.00565396\n",
      "Iteration 9, loss = 0.00719810\n",
      "Iteration 10, loss = 0.00676468\n",
      "Iteration 11, loss = 0.00512464\n",
      "Iteration 12, loss = 0.00382596\n",
      "Iteration 13, loss = 0.00374295\n",
      "Iteration 14, loss = 0.00455575\n",
      "Iteration 15, loss = 0.00520823\n",
      "Iteration 16, loss = 0.00499914\n",
      "Iteration 17, loss = 0.00414353\n",
      "Iteration 18, loss = 0.00333812\n",
      "Iteration 19, loss = 0.00310581\n",
      "Iteration 20, loss = 0.00342522\n",
      "Iteration 21, loss = 0.00385470\n",
      "Iteration 22, loss = 0.00397541\n",
      "Iteration 23, loss = 0.00371416\n",
      "Iteration 24, loss = 0.00332000\n",
      "Iteration 25, loss = 0.00310270\n",
      "Iteration 26, loss = 0.00316994\n",
      "Iteration 27, loss = 0.00338585\n",
      "Iteration 28, loss = 0.00350789\n",
      "Iteration 29, loss = 0.00340588\n",
      "Iteration 30, loss = 0.00315997\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02270719\n",
      "Iteration 2, loss = 0.00604026\n",
      "Iteration 3, loss = 0.01036631\n",
      "Iteration 4, loss = 0.01260246\n",
      "Iteration 5, loss = 0.00854009\n",
      "Iteration 6, loss = 0.00439379\n",
      "Iteration 7, loss = 0.00370214\n",
      "Iteration 8, loss = 0.00564974\n",
      "Iteration 9, loss = 0.00720737\n",
      "Iteration 10, loss = 0.00679097\n",
      "Iteration 11, loss = 0.00515238\n",
      "Iteration 12, loss = 0.00383944\n",
      "Iteration 13, loss = 0.00373925\n",
      "Iteration 14, loss = 0.00454773\n",
      "Iteration 15, loss = 0.00521195\n",
      "Iteration 16, loss = 0.00501854\n",
      "Iteration 17, loss = 0.00416906\n",
      "Iteration 18, loss = 0.00335800\n",
      "Iteration 19, loss = 0.00311068\n",
      "Iteration 20, loss = 0.00341829\n",
      "Iteration 21, loss = 0.00384846\n",
      "Iteration 22, loss = 0.00397874\n",
      "Iteration 23, loss = 0.00372631\n",
      "Iteration 24, loss = 0.00333403\n",
      "Iteration 25, loss = 0.00310929\n",
      "Iteration 26, loss = 0.00316664\n",
      "Iteration 27, loss = 0.00338308\n",
      "Iteration 28, loss = 0.00351178\n",
      "Iteration 29, loss = 0.00341589\n",
      "Iteration 30, loss = 0.00317011\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02268367\n",
      "Iteration 2, loss = 0.00605075\n",
      "Iteration 3, loss = 0.01037465\n",
      "Iteration 4, loss = 0.01258396\n",
      "Iteration 5, loss = 0.00851651\n",
      "Iteration 6, loss = 0.00438159\n",
      "Iteration 7, loss = 0.00370456\n",
      "Iteration 8, loss = 0.00565627\n",
      "Iteration 9, loss = 0.00720551\n",
      "Iteration 10, loss = 0.00677884\n",
      "Iteration 11, loss = 0.00513949\n",
      "Iteration 12, loss = 0.00383511\n",
      "Iteration 13, loss = 0.00374490\n",
      "Iteration 14, loss = 0.00455550\n",
      "Iteration 15, loss = 0.00521228\n",
      "Iteration 16, loss = 0.00500933\n",
      "Iteration 17, loss = 0.00415636\n",
      "Iteration 18, loss = 0.00334847\n",
      "Iteration 19, loss = 0.00311154\n",
      "Iteration 20, loss = 0.00342825\n",
      "Iteration 21, loss = 0.00385886\n",
      "Iteration 22, loss = 0.00398342\n",
      "Iteration 23, loss = 0.00372467\n",
      "Iteration 24, loss = 0.00333000\n",
      "Iteration 25, loss = 0.00310959\n",
      "Iteration 26, loss = 0.00317399\n",
      "Iteration 27, loss = 0.00338935\n",
      "Iteration 28, loss = 0.00351330\n",
      "Iteration 29, loss = 0.00341356\n",
      "Iteration 30, loss = 0.00316822\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02268404\n",
      "Iteration 2, loss = 0.00605017\n",
      "Iteration 3, loss = 0.01037344\n",
      "Iteration 4, loss = 0.01258354\n",
      "Iteration 5, loss = 0.00851873\n",
      "Iteration 6, loss = 0.00438394\n",
      "Iteration 7, loss = 0.00370420\n",
      "Iteration 8, loss = 0.00565447\n",
      "Iteration 9, loss = 0.00720493\n",
      "Iteration 10, loss = 0.00678047\n",
      "Iteration 11, loss = 0.00514210\n",
      "Iteration 12, loss = 0.00383761\n",
      "Iteration 13, loss = 0.00374591\n",
      "Iteration 14, loss = 0.00455484\n",
      "Iteration 15, loss = 0.00521153\n",
      "Iteration 16, loss = 0.00500997\n",
      "Iteration 17, loss = 0.00415830\n",
      "Iteration 18, loss = 0.00335031\n",
      "Iteration 19, loss = 0.00311211\n",
      "Iteration 20, loss = 0.00342775\n",
      "Iteration 21, loss = 0.00385862\n",
      "Iteration 22, loss = 0.00398450\n",
      "Iteration 23, loss = 0.00372695\n",
      "Iteration 24, loss = 0.00333260\n",
      "Iteration 25, loss = 0.00311147\n",
      "Iteration 26, loss = 0.00317479\n",
      "Iteration 27, loss = 0.00338961\n",
      "Iteration 28, loss = 0.00351392\n",
      "Iteration 29, loss = 0.00341502\n",
      "Iteration 30, loss = 0.00317007\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02265302\n",
      "Iteration 2, loss = 0.00605294\n",
      "Iteration 3, loss = 0.01037313\n",
      "Iteration 4, loss = 0.01256043\n",
      "Iteration 5, loss = 0.00849054\n",
      "Iteration 6, loss = 0.00436682\n",
      "Iteration 7, loss = 0.00370238\n",
      "Iteration 8, loss = 0.00565722\n",
      "Iteration 9, loss = 0.00719953\n",
      "Iteration 10, loss = 0.00676502\n",
      "Iteration 11, loss = 0.00512616\n",
      "Iteration 12, loss = 0.00382962\n",
      "Iteration 13, loss = 0.00374762\n",
      "Iteration 14, loss = 0.00455942\n",
      "Iteration 15, loss = 0.00520914\n",
      "Iteration 16, loss = 0.00499807\n",
      "Iteration 17, loss = 0.00414268\n",
      "Iteration 18, loss = 0.00333912\n",
      "Iteration 19, loss = 0.00310904\n",
      "Iteration 20, loss = 0.00342979\n",
      "Iteration 21, loss = 0.00385932\n",
      "Iteration 22, loss = 0.00397949\n",
      "Iteration 23, loss = 0.00371801\n",
      "Iteration 24, loss = 0.00332429\n",
      "Iteration 25, loss = 0.00310765\n",
      "Iteration 26, loss = 0.00317517\n",
      "Iteration 27, loss = 0.00339058\n",
      "Iteration 28, loss = 0.00351142\n",
      "Iteration 29, loss = 0.00340839\n",
      "Iteration 30, loss = 0.00316226\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02265459\n",
      "Iteration 2, loss = 0.00605297\n",
      "Iteration 3, loss = 0.01037222\n",
      "Iteration 4, loss = 0.01256092\n",
      "Iteration 5, loss = 0.00849397\n",
      "Iteration 6, loss = 0.00437017\n",
      "Iteration 7, loss = 0.00370323\n",
      "Iteration 8, loss = 0.00565704\n",
      "Iteration 9, loss = 0.00720072\n",
      "Iteration 10, loss = 0.00676812\n",
      "Iteration 11, loss = 0.00512985\n",
      "Iteration 12, loss = 0.00383310\n",
      "Iteration 13, loss = 0.00374968\n",
      "Iteration 14, loss = 0.00455973\n",
      "Iteration 15, loss = 0.00520952\n",
      "Iteration 16, loss = 0.00499994\n",
      "Iteration 17, loss = 0.00414583\n",
      "Iteration 18, loss = 0.00334211\n",
      "Iteration 19, loss = 0.00311057\n",
      "Iteration 20, loss = 0.00343011\n",
      "Iteration 21, loss = 0.00386001\n",
      "Iteration 22, loss = 0.00398163\n",
      "Iteration 23, loss = 0.00372135\n",
      "Iteration 24, loss = 0.00332781\n",
      "Iteration 25, loss = 0.00311035\n",
      "Iteration 26, loss = 0.00317670\n",
      "Iteration 27, loss = 0.00339153\n",
      "Iteration 28, loss = 0.00351279\n",
      "Iteration 29, loss = 0.00341070\n",
      "Iteration 30, loss = 0.00316503\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02266708\n",
      "Iteration 2, loss = 0.00605187\n",
      "Iteration 3, loss = 0.01036834\n",
      "Iteration 4, loss = 0.01257036\n",
      "Iteration 5, loss = 0.00850877\n",
      "Iteration 6, loss = 0.00438019\n",
      "Iteration 7, loss = 0.00370409\n",
      "Iteration 8, loss = 0.00565454\n",
      "Iteration 9, loss = 0.00720356\n",
      "Iteration 10, loss = 0.00677951\n",
      "Iteration 11, loss = 0.00514351\n",
      "Iteration 12, loss = 0.00384021\n",
      "Iteration 13, loss = 0.00374853\n",
      "Iteration 14, loss = 0.00455622\n",
      "Iteration 15, loss = 0.00521049\n",
      "Iteration 16, loss = 0.00500727\n",
      "Iteration 17, loss = 0.00415572\n",
      "Iteration 18, loss = 0.00335023\n",
      "Iteration 19, loss = 0.00311066\n",
      "Iteration 20, loss = 0.00342277\n",
      "Iteration 21, loss = 0.00385163\n",
      "Iteration 22, loss = 0.00397746\n",
      "Iteration 23, loss = 0.00372263\n",
      "Iteration 24, loss = 0.00333199\n",
      "Iteration 25, loss = 0.00311099\n",
      "Iteration 26, loss = 0.00317135\n",
      "Iteration 27, loss = 0.00338732\n",
      "Iteration 28, loss = 0.00351221\n",
      "Iteration 29, loss = 0.00341241\n",
      "Iteration 30, loss = 0.00316547\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02268513\n",
      "Iteration 2, loss = 0.00604749\n",
      "Iteration 3, loss = 0.01036304\n",
      "Iteration 4, loss = 0.01258053\n",
      "Iteration 5, loss = 0.00852414\n",
      "Iteration 6, loss = 0.00438847\n",
      "Iteration 7, loss = 0.00370217\n",
      "Iteration 8, loss = 0.00564936\n",
      "Iteration 9, loss = 0.00720397\n",
      "Iteration 10, loss = 0.00678632\n",
      "Iteration 11, loss = 0.00515037\n",
      "Iteration 12, loss = 0.00384193\n",
      "Iteration 13, loss = 0.00374347\n",
      "Iteration 14, loss = 0.00454953\n",
      "Iteration 15, loss = 0.00520804\n",
      "Iteration 16, loss = 0.00501069\n",
      "Iteration 17, loss = 0.00416145\n",
      "Iteration 18, loss = 0.00335355\n",
      "Iteration 19, loss = 0.00310933\n",
      "Iteration 20, loss = 0.00341838\n",
      "Iteration 21, loss = 0.00384830\n",
      "Iteration 22, loss = 0.00397739\n",
      "Iteration 23, loss = 0.00372506\n",
      "Iteration 24, loss = 0.00333403\n",
      "Iteration 25, loss = 0.00311016\n",
      "Iteration 26, loss = 0.00316816\n",
      "Iteration 27, loss = 0.00338391\n",
      "Iteration 28, loss = 0.00351088\n",
      "Iteration 29, loss = 0.00341337\n",
      "Iteration 30, loss = 0.00316708\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02269347\n",
      "Iteration 2, loss = 0.00604230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.01034949\n",
      "Iteration 4, loss = 0.01258330\n",
      "Iteration 5, loss = 0.00853361\n",
      "Iteration 6, loss = 0.00439171\n",
      "Iteration 7, loss = 0.00369604\n",
      "Iteration 8, loss = 0.00564001\n",
      "Iteration 9, loss = 0.00720050\n",
      "Iteration 10, loss = 0.00679050\n",
      "Iteration 11, loss = 0.00515685\n",
      "Iteration 12, loss = 0.00384184\n",
      "Iteration 13, loss = 0.00373556\n",
      "Iteration 14, loss = 0.00453944\n",
      "Iteration 15, loss = 0.00520264\n",
      "Iteration 16, loss = 0.00501162\n",
      "Iteration 17, loss = 0.00416457\n",
      "Iteration 18, loss = 0.00335372\n",
      "Iteration 19, loss = 0.00310432\n",
      "Iteration 20, loss = 0.00341025\n",
      "Iteration 21, loss = 0.00384107\n",
      "Iteration 22, loss = 0.00397344\n",
      "Iteration 23, loss = 0.00372340\n",
      "Iteration 24, loss = 0.00333213\n",
      "Iteration 25, loss = 0.00310565\n",
      "Iteration 26, loss = 0.00316083\n",
      "Iteration 27, loss = 0.00337554\n",
      "Iteration 28, loss = 0.00350410\n",
      "Iteration 29, loss = 0.00340870\n",
      "Iteration 30, loss = 0.00316331\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02265730\n",
      "Iteration 2, loss = 0.00605219\n",
      "Iteration 3, loss = 0.01036195\n",
      "Iteration 4, loss = 0.01256331\n",
      "Iteration 5, loss = 0.00850495\n",
      "Iteration 6, loss = 0.00437924\n",
      "Iteration 7, loss = 0.00370501\n",
      "Iteration 8, loss = 0.00565523\n",
      "Iteration 9, loss = 0.00720357\n",
      "Iteration 10, loss = 0.00677979\n",
      "Iteration 11, loss = 0.00514387\n",
      "Iteration 12, loss = 0.00384028\n",
      "Iteration 13, loss = 0.00374812\n",
      "Iteration 14, loss = 0.00455545\n",
      "Iteration 15, loss = 0.00520918\n",
      "Iteration 16, loss = 0.00500564\n",
      "Iteration 17, loss = 0.00415424\n",
      "Iteration 18, loss = 0.00334939\n",
      "Iteration 19, loss = 0.00311061\n",
      "Iteration 20, loss = 0.00342319\n",
      "Iteration 21, loss = 0.00385230\n",
      "Iteration 22, loss = 0.00397804\n",
      "Iteration 23, loss = 0.00372297\n",
      "Iteration 24, loss = 0.00333238\n",
      "Iteration 25, loss = 0.00311158\n",
      "Iteration 26, loss = 0.00317221\n",
      "Iteration 27, loss = 0.00338815\n",
      "Iteration 28, loss = 0.00351261\n",
      "Iteration 29, loss = 0.00341227\n",
      "Iteration 30, loss = 0.00316511\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02266103\n",
      "Iteration 2, loss = 0.00605162\n",
      "Iteration 3, loss = 0.01035966\n",
      "Iteration 4, loss = 0.01256477\n",
      "Iteration 5, loss = 0.00850818\n",
      "Iteration 6, loss = 0.00438102\n",
      "Iteration 7, loss = 0.00370412\n",
      "Iteration 8, loss = 0.00565329\n",
      "Iteration 9, loss = 0.00720313\n",
      "Iteration 10, loss = 0.00678135\n",
      "Iteration 11, loss = 0.00514589\n",
      "Iteration 12, loss = 0.00384088\n",
      "Iteration 13, loss = 0.00374662\n",
      "Iteration 14, loss = 0.00455300\n",
      "Iteration 15, loss = 0.00520760\n",
      "Iteration 16, loss = 0.00500570\n",
      "Iteration 17, loss = 0.00415516\n",
      "Iteration 18, loss = 0.00334974\n",
      "Iteration 19, loss = 0.00310958\n",
      "Iteration 20, loss = 0.00342104\n",
      "Iteration 21, loss = 0.00385016\n",
      "Iteration 22, loss = 0.00397689\n",
      "Iteration 23, loss = 0.00372304\n",
      "Iteration 24, loss = 0.00333232\n",
      "Iteration 25, loss = 0.00311053\n",
      "Iteration 26, loss = 0.00317000\n",
      "Iteration 27, loss = 0.00338524\n",
      "Iteration 28, loss = 0.00351007\n",
      "Iteration 29, loss = 0.00341064\n",
      "Iteration 30, loss = 0.00316396\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02257057\n",
      "Iteration 2, loss = 0.00604157\n",
      "Iteration 3, loss = 0.01035982\n",
      "Iteration 4, loss = 0.01250321\n",
      "Iteration 5, loss = 0.00842996\n",
      "Iteration 6, loss = 0.00433219\n",
      "Iteration 7, loss = 0.00369519\n",
      "Iteration 8, loss = 0.00565633\n",
      "Iteration 9, loss = 0.00718410\n",
      "Iteration 10, loss = 0.00673587\n",
      "Iteration 11, loss = 0.00509867\n",
      "Iteration 12, loss = 0.00381669\n",
      "Iteration 13, loss = 0.00374889\n",
      "Iteration 14, loss = 0.00456086\n",
      "Iteration 15, loss = 0.00519762\n",
      "Iteration 16, loss = 0.00497294\n",
      "Iteration 17, loss = 0.00411462\n",
      "Iteration 18, loss = 0.00332035\n",
      "Iteration 19, loss = 0.00310318\n",
      "Iteration 20, loss = 0.00343111\n",
      "Iteration 21, loss = 0.00385852\n",
      "Iteration 22, loss = 0.00397190\n",
      "Iteration 23, loss = 0.00370585\n",
      "Iteration 24, loss = 0.00331419\n",
      "Iteration 25, loss = 0.00310398\n",
      "Iteration 26, loss = 0.00317658\n",
      "Iteration 27, loss = 0.00339154\n",
      "Iteration 28, loss = 0.00350682\n",
      "Iteration 29, loss = 0.00339789\n",
      "Iteration 30, loss = 0.00315054\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02259676\n",
      "Iteration 2, loss = 0.00604843\n",
      "Iteration 3, loss = 0.01036443\n",
      "Iteration 4, loss = 0.01252299\n",
      "Iteration 5, loss = 0.00845469\n",
      "Iteration 6, loss = 0.00435012\n",
      "Iteration 7, loss = 0.00370254\n",
      "Iteration 8, loss = 0.00566023\n",
      "Iteration 9, loss = 0.00719431\n",
      "Iteration 10, loss = 0.00675372\n",
      "Iteration 11, loss = 0.00511765\n",
      "Iteration 12, loss = 0.00382959\n",
      "Iteration 13, loss = 0.00375413\n",
      "Iteration 14, loss = 0.00456391\n",
      "Iteration 15, loss = 0.00520542\n",
      "Iteration 16, loss = 0.00498742\n",
      "Iteration 17, loss = 0.00413179\n",
      "Iteration 18, loss = 0.00333438\n",
      "Iteration 19, loss = 0.00311143\n",
      "Iteration 20, loss = 0.00343584\n",
      "Iteration 21, loss = 0.00386426\n",
      "Iteration 22, loss = 0.00398148\n",
      "Iteration 23, loss = 0.00371870\n",
      "Iteration 24, loss = 0.00332660\n",
      "Iteration 25, loss = 0.00311306\n",
      "Iteration 26, loss = 0.00318243\n",
      "Iteration 27, loss = 0.00339672\n",
      "Iteration 28, loss = 0.00351424\n",
      "Iteration 29, loss = 0.00340835\n",
      "Iteration 30, loss = 0.00316205\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02255705\n",
      "Iteration 2, loss = 0.00604030\n",
      "Iteration 3, loss = 0.01035937\n",
      "Iteration 4, loss = 0.01249665\n",
      "Iteration 5, loss = 0.00842419\n",
      "Iteration 6, loss = 0.00432987\n",
      "Iteration 7, loss = 0.00369563\n",
      "Iteration 8, loss = 0.00565792\n",
      "Iteration 9, loss = 0.00718325\n",
      "Iteration 10, loss = 0.00673207\n",
      "Iteration 11, loss = 0.00509420\n",
      "Iteration 12, loss = 0.00381609\n",
      "Iteration 13, loss = 0.00375277\n",
      "Iteration 14, loss = 0.00456405\n",
      "Iteration 15, loss = 0.00519696\n",
      "Iteration 16, loss = 0.00496880\n",
      "Iteration 17, loss = 0.00410934\n",
      "Iteration 18, loss = 0.00331721\n",
      "Iteration 19, loss = 0.00310245\n",
      "Iteration 20, loss = 0.00343241\n",
      "Iteration 21, loss = 0.00385887\n",
      "Iteration 22, loss = 0.00396967\n",
      "Iteration 23, loss = 0.00370203\n",
      "Iteration 24, loss = 0.00331089\n",
      "Iteration 25, loss = 0.00310252\n",
      "Iteration 26, loss = 0.00317816\n",
      "Iteration 27, loss = 0.00339205\n",
      "Iteration 28, loss = 0.00350502\n",
      "Iteration 29, loss = 0.00339435\n",
      "Iteration 30, loss = 0.00314729\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02257062\n",
      "Iteration 2, loss = 0.00604620\n",
      "Iteration 3, loss = 0.01035726\n",
      "Iteration 4, loss = 0.01250845\n",
      "Iteration 5, loss = 0.00844563\n",
      "Iteration 6, loss = 0.00434891\n",
      "Iteration 7, loss = 0.00370512\n",
      "Iteration 8, loss = 0.00566229\n",
      "Iteration 9, loss = 0.00719333\n",
      "Iteration 10, loss = 0.00675071\n",
      "Iteration 11, loss = 0.00511543\n",
      "Iteration 12, loss = 0.00383017\n",
      "Iteration 13, loss = 0.00375619\n",
      "Iteration 14, loss = 0.00456401\n",
      "Iteration 15, loss = 0.00520170\n",
      "Iteration 16, loss = 0.00498104\n",
      "Iteration 17, loss = 0.00412551\n",
      "Iteration 18, loss = 0.00333036\n",
      "Iteration 19, loss = 0.00311015\n",
      "Iteration 20, loss = 0.00343607\n",
      "Iteration 21, loss = 0.00386426\n",
      "Iteration 22, loss = 0.00397983\n",
      "Iteration 23, loss = 0.00371597\n",
      "Iteration 24, loss = 0.00332405\n",
      "Iteration 25, loss = 0.00311134\n",
      "Iteration 26, loss = 0.00318189\n",
      "Iteration 27, loss = 0.00339576\n",
      "Iteration 28, loss = 0.00351155\n",
      "Iteration 29, loss = 0.00340375\n",
      "Iteration 30, loss = 0.00315673\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02265229\n",
      "Iteration 2, loss = 0.00604935\n",
      "Iteration 3, loss = 0.01033493\n",
      "Iteration 4, loss = 0.01254853\n",
      "Iteration 5, loss = 0.00850922\n",
      "Iteration 6, loss = 0.00438317\n",
      "Iteration 7, loss = 0.00369234\n",
      "Iteration 8, loss = 0.00563267\n",
      "Iteration 9, loss = 0.00718887\n",
      "Iteration 10, loss = 0.00677890\n",
      "Iteration 11, loss = 0.00514998\n",
      "Iteration 12, loss = 0.00384009\n",
      "Iteration 13, loss = 0.00373415\n",
      "Iteration 14, loss = 0.00453423\n",
      "Iteration 15, loss = 0.00519022\n",
      "Iteration 16, loss = 0.00499512\n",
      "Iteration 17, loss = 0.00414948\n",
      "Iteration 18, loss = 0.00334275\n",
      "Iteration 19, loss = 0.00309698\n",
      "Iteration 20, loss = 0.00340573\n",
      "Iteration 21, loss = 0.00383780\n",
      "Iteration 22, loss = 0.00396955\n",
      "Iteration 23, loss = 0.00371880\n",
      "Iteration 24, loss = 0.00332702\n",
      "Iteration 25, loss = 0.00310125\n",
      "Iteration 26, loss = 0.00315730\n",
      "Iteration 27, loss = 0.00337193\n",
      "Iteration 28, loss = 0.00349845\n",
      "Iteration 29, loss = 0.00340054\n",
      "Iteration 30, loss = 0.00315379\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02268948\n",
      "Iteration 2, loss = 0.00603156\n",
      "Iteration 3, loss = 0.01030410\n",
      "Iteration 4, loss = 0.01255839\n",
      "Iteration 5, loss = 0.00852541\n",
      "Iteration 6, loss = 0.00438171\n",
      "Iteration 7, loss = 0.00366961\n",
      "Iteration 8, loss = 0.00560411\n",
      "Iteration 9, loss = 0.00717384\n",
      "Iteration 10, loss = 0.00678014\n",
      "Iteration 11, loss = 0.00515252\n",
      "Iteration 12, loss = 0.00382977\n",
      "Iteration 13, loss = 0.00370984\n",
      "Iteration 14, loss = 0.00450316\n",
      "Iteration 15, loss = 0.00517047\n",
      "Iteration 16, loss = 0.00499130\n",
      "Iteration 17, loss = 0.00415142\n",
      "Iteration 18, loss = 0.00333821\n",
      "Iteration 19, loss = 0.00308413\n",
      "Iteration 20, loss = 0.00338903\n",
      "Iteration 21, loss = 0.00382415\n",
      "Iteration 22, loss = 0.00396382\n",
      "Iteration 23, loss = 0.00371812\n",
      "Iteration 24, loss = 0.00332327\n",
      "Iteration 25, loss = 0.00309120\n",
      "Iteration 26, loss = 0.00314424\n",
      "Iteration 27, loss = 0.00335539\n",
      "Iteration 28, loss = 0.00348561\n",
      "Iteration 29, loss = 0.00339448\n",
      "Iteration 30, loss = 0.00315271\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02264005\n",
      "Iteration 2, loss = 0.00605152\n",
      "Iteration 3, loss = 0.01034471\n",
      "Iteration 4, loss = 0.01254763\n",
      "Iteration 5, loss = 0.00850043\n",
      "Iteration 6, loss = 0.00437944\n",
      "Iteration 7, loss = 0.00370183\n",
      "Iteration 8, loss = 0.00564842\n",
      "Iteration 9, loss = 0.00719860\n",
      "Iteration 10, loss = 0.00677782\n",
      "Iteration 11, loss = 0.00514410\n",
      "Iteration 12, loss = 0.00384121\n",
      "Iteration 13, loss = 0.00374556\n",
      "Iteration 14, loss = 0.00454950\n",
      "Iteration 15, loss = 0.00520082\n",
      "Iteration 16, loss = 0.00499806\n",
      "Iteration 17, loss = 0.00414903\n",
      "Iteration 18, loss = 0.00334581\n",
      "Iteration 19, loss = 0.00310689\n",
      "Iteration 20, loss = 0.00341866\n",
      "Iteration 21, loss = 0.00384877\n",
      "Iteration 22, loss = 0.00397596\n",
      "Iteration 23, loss = 0.00372267\n",
      "Iteration 24, loss = 0.00333246\n",
      "Iteration 25, loss = 0.00310967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26, loss = 0.00316866\n",
      "Iteration 27, loss = 0.00338364\n",
      "Iteration 28, loss = 0.00350795\n",
      "Iteration 29, loss = 0.00340765\n",
      "Iteration 30, loss = 0.00316060\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02255969\n",
      "Iteration 2, loss = 0.00604774\n",
      "Iteration 3, loss = 0.01035454\n",
      "Iteration 4, loss = 0.01250455\n",
      "Iteration 5, loss = 0.00844352\n",
      "Iteration 6, loss = 0.00434972\n",
      "Iteration 7, loss = 0.00371024\n",
      "Iteration 8, loss = 0.00566842\n",
      "Iteration 9, loss = 0.00719728\n",
      "Iteration 10, loss = 0.00675218\n",
      "Iteration 11, loss = 0.00511554\n",
      "Iteration 12, loss = 0.00383130\n",
      "Iteration 13, loss = 0.00375954\n",
      "Iteration 14, loss = 0.00456915\n",
      "Iteration 15, loss = 0.00520547\n",
      "Iteration 16, loss = 0.00498174\n",
      "Iteration 17, loss = 0.00412473\n",
      "Iteration 18, loss = 0.00333032\n",
      "Iteration 19, loss = 0.00311253\n",
      "Iteration 20, loss = 0.00343980\n",
      "Iteration 21, loss = 0.00386738\n",
      "Iteration 22, loss = 0.00398121\n",
      "Iteration 23, loss = 0.00371578\n",
      "Iteration 24, loss = 0.00332407\n",
      "Iteration 25, loss = 0.00311322\n",
      "Iteration 26, loss = 0.00318410\n",
      "Iteration 27, loss = 0.00339804\n",
      "Iteration 28, loss = 0.00351274\n",
      "Iteration 29, loss = 0.00340377\n",
      "Iteration 30, loss = 0.00315660\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02248494\n",
      "Iteration 2, loss = 0.00602854\n",
      "Iteration 3, loss = 0.01034734\n",
      "Iteration 4, loss = 0.01246025\n",
      "Iteration 5, loss = 0.00838825\n",
      "Iteration 6, loss = 0.00431453\n",
      "Iteration 7, loss = 0.00370224\n",
      "Iteration 8, loss = 0.00566923\n",
      "Iteration 9, loss = 0.00718304\n",
      "Iteration 10, loss = 0.00672089\n",
      "Iteration 11, loss = 0.00508423\n",
      "Iteration 12, loss = 0.00381506\n",
      "Iteration 13, loss = 0.00376050\n",
      "Iteration 14, loss = 0.00457311\n",
      "Iteration 15, loss = 0.00519827\n",
      "Iteration 16, loss = 0.00496142\n",
      "Iteration 17, loss = 0.00410049\n",
      "Iteration 18, loss = 0.00331497\n",
      "Iteration 19, loss = 0.00310976\n",
      "Iteration 20, loss = 0.00344399\n",
      "Iteration 21, loss = 0.00386874\n",
      "Iteration 22, loss = 0.00397443\n",
      "Iteration 23, loss = 0.00370326\n",
      "Iteration 24, loss = 0.00331312\n",
      "Iteration 25, loss = 0.00310884\n",
      "Iteration 26, loss = 0.00318630\n",
      "Iteration 27, loss = 0.00340034\n",
      "Iteration 28, loss = 0.00350973\n",
      "Iteration 29, loss = 0.00339507\n",
      "Iteration 30, loss = 0.00314646\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02243201\n",
      "Iteration 2, loss = 0.00601336\n",
      "Iteration 3, loss = 0.01033809\n",
      "Iteration 4, loss = 0.01243117\n",
      "Iteration 5, loss = 0.00835831\n",
      "Iteration 6, loss = 0.00429678\n",
      "Iteration 7, loss = 0.00369678\n",
      "Iteration 8, loss = 0.00566615\n",
      "Iteration 9, loss = 0.00717394\n",
      "Iteration 10, loss = 0.00670583\n",
      "Iteration 11, loss = 0.00507016\n",
      "Iteration 12, loss = 0.00380854\n",
      "Iteration 13, loss = 0.00376023\n",
      "Iteration 14, loss = 0.00457148\n",
      "Iteration 15, loss = 0.00519131\n",
      "Iteration 16, loss = 0.00495003\n",
      "Iteration 17, loss = 0.00408881\n",
      "Iteration 18, loss = 0.00330776\n",
      "Iteration 19, loss = 0.00310775\n",
      "Iteration 20, loss = 0.00344584\n",
      "Iteration 21, loss = 0.00386919\n",
      "Iteration 22, loss = 0.00397258\n",
      "Iteration 23, loss = 0.00370020\n",
      "Iteration 24, loss = 0.00331089\n",
      "Iteration 25, loss = 0.00310908\n",
      "Iteration 26, loss = 0.00318998\n",
      "Iteration 27, loss = 0.00340327\n",
      "Iteration 28, loss = 0.00351043\n",
      "Iteration 29, loss = 0.00339398\n",
      "Iteration 30, loss = 0.00314590\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02239610\n",
      "Iteration 2, loss = 0.00599770\n",
      "Iteration 3, loss = 0.01032874\n",
      "Iteration 4, loss = 0.01240507\n",
      "Iteration 5, loss = 0.00833049\n",
      "Iteration 6, loss = 0.00427975\n",
      "Iteration 7, loss = 0.00369099\n",
      "Iteration 8, loss = 0.00566198\n",
      "Iteration 9, loss = 0.00716370\n",
      "Iteration 10, loss = 0.00669132\n",
      "Iteration 11, loss = 0.00505536\n",
      "Iteration 12, loss = 0.00379504\n",
      "Iteration 13, loss = 0.00374876\n",
      "Iteration 14, loss = 0.00455915\n",
      "Iteration 15, loss = 0.00517657\n",
      "Iteration 16, loss = 0.00492945\n",
      "Iteration 17, loss = 0.00406570\n",
      "Iteration 18, loss = 0.00329119\n",
      "Iteration 19, loss = 0.00309650\n",
      "Iteration 20, loss = 0.00343223\n",
      "Iteration 21, loss = 0.00384849\n",
      "Iteration 22, loss = 0.00394832\n",
      "Iteration 23, loss = 0.00367804\n",
      "Iteration 24, loss = 0.00329268\n",
      "Iteration 25, loss = 0.00309308\n",
      "Iteration 26, loss = 0.00317446\n",
      "Iteration 27, loss = 0.00338350\n",
      "Iteration 28, loss = 0.00348861\n",
      "Iteration 29, loss = 0.00337269\n",
      "Iteration 30, loss = 0.00312731\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02264198\n",
      "Iteration 2, loss = 0.00604668\n",
      "Iteration 3, loss = 0.01030840\n",
      "Iteration 4, loss = 0.01252866\n",
      "Iteration 5, loss = 0.00849953\n",
      "Iteration 6, loss = 0.00437527\n",
      "Iteration 7, loss = 0.00367602\n",
      "Iteration 8, loss = 0.00560847\n",
      "Iteration 9, loss = 0.00716822\n",
      "Iteration 10, loss = 0.00676968\n",
      "Iteration 11, loss = 0.00514585\n",
      "Iteration 12, loss = 0.00383151\n",
      "Iteration 13, loss = 0.00371581\n",
      "Iteration 14, loss = 0.00450555\n",
      "Iteration 15, loss = 0.00516313\n",
      "Iteration 16, loss = 0.00497644\n",
      "Iteration 17, loss = 0.00413647\n",
      "Iteration 18, loss = 0.00332908\n",
      "Iteration 19, loss = 0.00307906\n",
      "Iteration 20, loss = 0.00338226\n",
      "Iteration 21, loss = 0.00381372\n",
      "Iteration 22, loss = 0.00394980\n",
      "Iteration 23, loss = 0.00370497\n",
      "Iteration 24, loss = 0.00331525\n",
      "Iteration 25, loss = 0.00308574\n",
      "Iteration 26, loss = 0.00313803\n",
      "Iteration 27, loss = 0.00335018\n",
      "Iteration 28, loss = 0.00347821\n",
      "Iteration 29, loss = 0.00338318\n",
      "Iteration 30, loss = 0.00313920\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02250977\n",
      "Iteration 2, loss = 0.00604615\n",
      "Iteration 3, loss = 0.01034476\n",
      "Iteration 4, loss = 0.01248205\n",
      "Iteration 5, loss = 0.00842763\n",
      "Iteration 6, loss = 0.00434835\n",
      "Iteration 7, loss = 0.00371597\n",
      "Iteration 8, loss = 0.00567074\n",
      "Iteration 9, loss = 0.00719218\n",
      "Iteration 10, loss = 0.00674457\n",
      "Iteration 11, loss = 0.00511128\n",
      "Iteration 12, loss = 0.00383102\n",
      "Iteration 13, loss = 0.00375905\n",
      "Iteration 14, loss = 0.00456578\n",
      "Iteration 15, loss = 0.00519820\n",
      "Iteration 16, loss = 0.00497365\n",
      "Iteration 17, loss = 0.00411911\n",
      "Iteration 18, loss = 0.00332880\n",
      "Iteration 19, loss = 0.00310932\n",
      "Iteration 20, loss = 0.00343139\n",
      "Iteration 21, loss = 0.00385550\n",
      "Iteration 22, loss = 0.00396999\n",
      "Iteration 23, loss = 0.00370836\n",
      "Iteration 24, loss = 0.00332123\n",
      "Iteration 25, loss = 0.00310915\n",
      "Iteration 26, loss = 0.00317635\n",
      "Iteration 27, loss = 0.00339028\n",
      "Iteration 28, loss = 0.00350591\n",
      "Iteration 29, loss = 0.00339724\n",
      "Iteration 30, loss = 0.00314891\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02238967\n",
      "Iteration 2, loss = 0.00601222\n",
      "Iteration 3, loss = 0.01033712\n",
      "Iteration 4, loss = 0.01241933\n",
      "Iteration 5, loss = 0.00835115\n",
      "Iteration 6, loss = 0.00430231\n",
      "Iteration 7, loss = 0.00371008\n",
      "Iteration 8, loss = 0.00567635\n",
      "Iteration 9, loss = 0.00717492\n",
      "Iteration 10, loss = 0.00670191\n",
      "Iteration 11, loss = 0.00506857\n",
      "Iteration 12, loss = 0.00380827\n",
      "Iteration 13, loss = 0.00375946\n",
      "Iteration 14, loss = 0.00456832\n",
      "Iteration 15, loss = 0.00518525\n",
      "Iteration 16, loss = 0.00494161\n",
      "Iteration 17, loss = 0.00407990\n",
      "Iteration 18, loss = 0.00329963\n",
      "Iteration 19, loss = 0.00310094\n",
      "Iteration 20, loss = 0.00343909\n",
      "Iteration 21, loss = 0.00386150\n",
      "Iteration 22, loss = 0.00396315\n",
      "Iteration 23, loss = 0.00368963\n",
      "Iteration 24, loss = 0.00329919\n",
      "Iteration 25, loss = 0.00309684\n",
      "Iteration 26, loss = 0.00317728\n",
      "Iteration 27, loss = 0.00339038\n",
      "Iteration 28, loss = 0.00349667\n",
      "Iteration 29, loss = 0.00337887\n",
      "Iteration 30, loss = 0.00312932\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02221179\n",
      "Iteration 2, loss = 0.00595610\n",
      "Iteration 3, loss = 0.01030215\n",
      "Iteration 4, loss = 0.01232193\n",
      "Iteration 5, loss = 0.00824256\n",
      "Iteration 6, loss = 0.00423778\n",
      "Iteration 7, loss = 0.00370129\n",
      "Iteration 8, loss = 0.00568229\n",
      "Iteration 9, loss = 0.00714874\n",
      "Iteration 10, loss = 0.00664350\n",
      "Iteration 11, loss = 0.00500581\n",
      "Iteration 12, loss = 0.00377137\n",
      "Iteration 13, loss = 0.00375327\n",
      "Iteration 14, loss = 0.00456589\n",
      "Iteration 15, loss = 0.00516054\n",
      "Iteration 16, loss = 0.00488910\n",
      "Iteration 17, loss = 0.00401788\n",
      "Iteration 18, loss = 0.00326278\n",
      "Iteration 19, loss = 0.00309338\n",
      "Iteration 20, loss = 0.00343942\n",
      "Iteration 21, loss = 0.00384777\n",
      "Iteration 22, loss = 0.00393208\n",
      "Iteration 23, loss = 0.00365311\n",
      "Iteration 24, loss = 0.00326906\n",
      "Iteration 25, loss = 0.00307788\n",
      "Iteration 26, loss = 0.00317087\n",
      "Iteration 27, loss = 0.00337925\n",
      "Iteration 28, loss = 0.00347506\n",
      "Iteration 29, loss = 0.00335051\n",
      "Iteration 30, loss = 0.00310311\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02241862\n",
      "Iteration 2, loss = 0.00603514\n",
      "Iteration 3, loss = 0.01033160\n",
      "Iteration 4, loss = 0.01243671\n",
      "Iteration 5, loss = 0.00838748\n",
      "Iteration 6, loss = 0.00433310\n",
      "Iteration 7, loss = 0.00372039\n",
      "Iteration 8, loss = 0.00567518\n",
      "Iteration 9, loss = 0.00718320\n",
      "Iteration 10, loss = 0.00672700\n",
      "Iteration 11, loss = 0.00509866\n",
      "Iteration 12, loss = 0.00382864\n",
      "Iteration 13, loss = 0.00376510\n",
      "Iteration 14, loss = 0.00457199\n",
      "Iteration 15, loss = 0.00519674\n",
      "Iteration 16, loss = 0.00496372\n",
      "Iteration 17, loss = 0.00410735\n",
      "Iteration 18, loss = 0.00332271\n",
      "Iteration 19, loss = 0.00311153\n",
      "Iteration 20, loss = 0.00343781\n",
      "Iteration 21, loss = 0.00385981\n",
      "Iteration 22, loss = 0.00396891\n",
      "Iteration 23, loss = 0.00370359\n",
      "Iteration 24, loss = 0.00331733\n",
      "Iteration 25, loss = 0.00310854\n",
      "Iteration 26, loss = 0.00317963\n",
      "Iteration 27, loss = 0.00339297\n",
      "Iteration 28, loss = 0.00350395\n",
      "Iteration 29, loss = 0.00339061\n",
      "Iteration 30, loss = 0.00314172\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02220855\n",
      "Iteration 2, loss = 0.00597487\n",
      "Iteration 3, loss = 0.01031216\n",
      "Iteration 4, loss = 0.01233165\n",
      "Iteration 5, loss = 0.00825738\n",
      "Iteration 6, loss = 0.00425842\n",
      "Iteration 7, loss = 0.00372112\n",
      "Iteration 8, loss = 0.00569611\n",
      "Iteration 9, loss = 0.00716006\n",
      "Iteration 10, loss = 0.00665847\n",
      "Iteration 11, loss = 0.00502455\n",
      "Iteration 12, loss = 0.00378900\n",
      "Iteration 13, loss = 0.00376826\n",
      "Iteration 14, loss = 0.00457968\n",
      "Iteration 15, loss = 0.00517637\n",
      "Iteration 16, loss = 0.00490742\n",
      "Iteration 17, loss = 0.00403738\n",
      "Iteration 18, loss = 0.00328144\n",
      "Iteration 19, loss = 0.00311042\n",
      "Iteration 20, loss = 0.00345357\n",
      "Iteration 21, loss = 0.00386058\n",
      "Iteration 22, loss = 0.00394530\n",
      "Iteration 23, loss = 0.00366734\n",
      "Iteration 24, loss = 0.00328430\n",
      "Iteration 25, loss = 0.00309296\n",
      "Iteration 26, loss = 0.00318608\n",
      "Iteration 27, loss = 0.00339411\n",
      "Iteration 28, loss = 0.00349111\n",
      "Iteration 29, loss = 0.00336851\n",
      "Iteration 30, loss = 0.00312259\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02239575\n",
      "Iteration 2, loss = 0.00603768\n",
      "Iteration 3, loss = 0.01032713\n",
      "Iteration 4, loss = 0.01242555\n",
      "Iteration 5, loss = 0.00837741\n",
      "Iteration 6, loss = 0.00433006\n",
      "Iteration 7, loss = 0.00372334\n",
      "Iteration 8, loss = 0.00567720\n",
      "Iteration 9, loss = 0.00717875\n",
      "Iteration 10, loss = 0.00671638\n",
      "Iteration 11, loss = 0.00508640\n",
      "Iteration 12, loss = 0.00382209\n",
      "Iteration 13, loss = 0.00376348\n",
      "Iteration 14, loss = 0.00457106\n",
      "Iteration 15, loss = 0.00519129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.00495443\n",
      "Iteration 17, loss = 0.00409852\n",
      "Iteration 18, loss = 0.00331789\n",
      "Iteration 19, loss = 0.00311005\n",
      "Iteration 20, loss = 0.00343679\n",
      "Iteration 21, loss = 0.00385723\n",
      "Iteration 22, loss = 0.00396483\n",
      "Iteration 23, loss = 0.00369943\n",
      "Iteration 24, loss = 0.00331467\n",
      "Iteration 25, loss = 0.00310735\n",
      "Iteration 26, loss = 0.00317931\n",
      "Iteration 27, loss = 0.00339218\n",
      "Iteration 28, loss = 0.00350249\n",
      "Iteration 29, loss = 0.00338914\n",
      "Iteration 30, loss = 0.00314063\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02247422\n",
      "Iteration 2, loss = 0.00605072\n",
      "Iteration 3, loss = 0.01031098\n",
      "Iteration 4, loss = 0.01245807\n",
      "Iteration 5, loss = 0.00842612\n",
      "Iteration 6, loss = 0.00435082\n",
      "Iteration 7, loss = 0.00369879\n",
      "Iteration 8, loss = 0.00563584\n",
      "Iteration 9, loss = 0.00716285\n",
      "Iteration 10, loss = 0.00673310\n",
      "Iteration 11, loss = 0.00511176\n",
      "Iteration 12, loss = 0.00382723\n",
      "Iteration 13, loss = 0.00374119\n",
      "Iteration 14, loss = 0.00454254\n",
      "Iteration 15, loss = 0.00517884\n",
      "Iteration 16, loss = 0.00496557\n",
      "Iteration 17, loss = 0.00411895\n",
      "Iteration 18, loss = 0.00332904\n",
      "Iteration 19, loss = 0.00310129\n",
      "Iteration 20, loss = 0.00341722\n",
      "Iteration 21, loss = 0.00384290\n",
      "Iteration 22, loss = 0.00396491\n",
      "Iteration 23, loss = 0.00371022\n",
      "Iteration 24, loss = 0.00332404\n",
      "Iteration 25, loss = 0.00310771\n",
      "Iteration 26, loss = 0.00317007\n",
      "Iteration 27, loss = 0.00338248\n",
      "Iteration 28, loss = 0.00350253\n",
      "Iteration 29, loss = 0.00339925\n",
      "Iteration 30, loss = 0.00315372\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02265714\n",
      "Iteration 2, loss = 0.00601167\n",
      "Iteration 3, loss = 0.01017870\n",
      "Iteration 4, loss = 0.01245390\n",
      "Iteration 5, loss = 0.00846939\n",
      "Iteration 6, loss = 0.00431813\n",
      "Iteration 7, loss = 0.00355198\n",
      "Iteration 8, loss = 0.00545032\n",
      "Iteration 9, loss = 0.00703842\n",
      "Iteration 10, loss = 0.00668324\n",
      "Iteration 11, loss = 0.00507947\n",
      "Iteration 12, loss = 0.00375317\n",
      "Iteration 13, loss = 0.00361019\n",
      "Iteration 14, loss = 0.00438890\n",
      "Iteration 15, loss = 0.00504830\n",
      "Iteration 16, loss = 0.00488108\n",
      "Iteration 17, loss = 0.00405854\n",
      "Iteration 18, loss = 0.00325207\n",
      "Iteration 19, loss = 0.00298695\n",
      "Iteration 20, loss = 0.00327581\n",
      "Iteration 21, loss = 0.00370789\n",
      "Iteration 22, loss = 0.00385770\n",
      "Iteration 23, loss = 0.00362748\n",
      "Iteration 24, loss = 0.00324406\n",
      "Iteration 25, loss = 0.00301166\n",
      "Iteration 26, loss = 0.00305161\n",
      "Iteration 27, loss = 0.00325521\n",
      "Iteration 28, loss = 0.00338686\n",
      "Iteration 29, loss = 0.00330334\n",
      "Iteration 30, loss = 0.00306735\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02247637\n",
      "Iteration 2, loss = 0.00605110\n",
      "Iteration 3, loss = 0.01031249\n",
      "Iteration 4, loss = 0.01245625\n",
      "Iteration 5, loss = 0.00842476\n",
      "Iteration 6, loss = 0.00434970\n",
      "Iteration 7, loss = 0.00370183\n",
      "Iteration 8, loss = 0.00564223\n",
      "Iteration 9, loss = 0.00716631\n",
      "Iteration 10, loss = 0.00673026\n",
      "Iteration 11, loss = 0.00510735\n",
      "Iteration 12, loss = 0.00382698\n",
      "Iteration 13, loss = 0.00374679\n",
      "Iteration 14, loss = 0.00454720\n",
      "Iteration 15, loss = 0.00517685\n",
      "Iteration 16, loss = 0.00495809\n",
      "Iteration 17, loss = 0.00411157\n",
      "Iteration 18, loss = 0.00332483\n",
      "Iteration 19, loss = 0.00310289\n",
      "Iteration 20, loss = 0.00341959\n",
      "Iteration 21, loss = 0.00384129\n",
      "Iteration 22, loss = 0.00395930\n",
      "Iteration 23, loss = 0.00370368\n",
      "Iteration 24, loss = 0.00331898\n",
      "Iteration 25, loss = 0.00310643\n",
      "Iteration 26, loss = 0.00317060\n",
      "Iteration 27, loss = 0.00338217\n",
      "Iteration 28, loss = 0.00349939\n",
      "Iteration 29, loss = 0.00339332\n",
      "Iteration 30, loss = 0.00314716\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02235586\n",
      "Iteration 2, loss = 0.00602257\n",
      "Iteration 3, loss = 0.01032835\n",
      "Iteration 4, loss = 0.01241099\n",
      "Iteration 5, loss = 0.00835927\n",
      "Iteration 6, loss = 0.00432003\n",
      "Iteration 7, loss = 0.00372429\n",
      "Iteration 8, loss = 0.00568173\n",
      "Iteration 9, loss = 0.00717749\n",
      "Iteration 10, loss = 0.00670852\n",
      "Iteration 11, loss = 0.00507981\n",
      "Iteration 12, loss = 0.00382005\n",
      "Iteration 13, loss = 0.00376714\n",
      "Iteration 14, loss = 0.00457404\n",
      "Iteration 15, loss = 0.00519084\n",
      "Iteration 16, loss = 0.00495026\n",
      "Iteration 17, loss = 0.00409350\n",
      "Iteration 18, loss = 0.00331412\n",
      "Iteration 19, loss = 0.00311324\n",
      "Iteration 20, loss = 0.00344710\n",
      "Iteration 21, loss = 0.00386819\n",
      "Iteration 22, loss = 0.00397237\n",
      "Iteration 23, loss = 0.00370152\n",
      "Iteration 24, loss = 0.00331343\n",
      "Iteration 25, loss = 0.00311032\n",
      "Iteration 26, loss = 0.00318604\n",
      "Iteration 27, loss = 0.00339780\n",
      "Iteration 28, loss = 0.00350478\n",
      "Iteration 29, loss = 0.00338920\n",
      "Iteration 30, loss = 0.00314154\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02195525\n",
      "Iteration 2, loss = 0.00587266\n",
      "Iteration 3, loss = 0.01025756\n",
      "Iteration 4, loss = 0.01219801\n",
      "Iteration 5, loss = 0.00810390\n",
      "Iteration 6, loss = 0.00415938\n",
      "Iteration 7, loss = 0.00369475\n",
      "Iteration 8, loss = 0.00568412\n",
      "Iteration 9, loss = 0.00711002\n",
      "Iteration 10, loss = 0.00657529\n",
      "Iteration 11, loss = 0.00493907\n",
      "Iteration 12, loss = 0.00373169\n",
      "Iteration 13, loss = 0.00374522\n",
      "Iteration 14, loss = 0.00455120\n",
      "Iteration 15, loss = 0.00512396\n",
      "Iteration 16, loss = 0.00483865\n",
      "Iteration 17, loss = 0.00397271\n",
      "Iteration 18, loss = 0.00323277\n",
      "Iteration 19, loss = 0.00307632\n",
      "Iteration 20, loss = 0.00342274\n",
      "Iteration 21, loss = 0.00382229\n",
      "Iteration 22, loss = 0.00389494\n",
      "Iteration 23, loss = 0.00360876\n",
      "Iteration 24, loss = 0.00323057\n",
      "Iteration 25, loss = 0.00305448\n",
      "Iteration 26, loss = 0.00314894\n",
      "Iteration 27, loss = 0.00336123\n",
      "Iteration 28, loss = 0.00345018\n",
      "Iteration 29, loss = 0.00331536\n",
      "Iteration 30, loss = 0.00306554\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02196983\n",
      "Iteration 2, loss = 0.00589752\n",
      "Iteration 3, loss = 0.01027070\n",
      "Iteration 4, loss = 0.01221433\n",
      "Iteration 5, loss = 0.00812733\n",
      "Iteration 6, loss = 0.00418649\n",
      "Iteration 7, loss = 0.00371958\n",
      "Iteration 8, loss = 0.00570617\n",
      "Iteration 9, loss = 0.00712778\n",
      "Iteration 10, loss = 0.00659293\n",
      "Iteration 11, loss = 0.00495759\n",
      "Iteration 12, loss = 0.00375501\n",
      "Iteration 13, loss = 0.00376977\n",
      "Iteration 14, loss = 0.00458017\n",
      "Iteration 15, loss = 0.00515245\n",
      "Iteration 16, loss = 0.00486360\n",
      "Iteration 17, loss = 0.00399471\n",
      "Iteration 18, loss = 0.00325241\n",
      "Iteration 19, loss = 0.00310000\n",
      "Iteration 20, loss = 0.00345159\n",
      "Iteration 21, loss = 0.00385364\n",
      "Iteration 22, loss = 0.00392417\n",
      "Iteration 23, loss = 0.00363489\n",
      "Iteration 24, loss = 0.00325483\n",
      "Iteration 25, loss = 0.00307800\n",
      "Iteration 26, loss = 0.00317505\n",
      "Iteration 27, loss = 0.00338368\n",
      "Iteration 28, loss = 0.00347049\n",
      "Iteration 29, loss = 0.00333708\n",
      "Iteration 30, loss = 0.00308939\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02145146\n",
      "Iteration 2, loss = 0.00563542\n",
      "Iteration 3, loss = 0.01010325\n",
      "Iteration 4, loss = 0.01190505\n",
      "Iteration 5, loss = 0.00777142\n",
      "Iteration 6, loss = 0.00392584\n",
      "Iteration 7, loss = 0.00359160\n",
      "Iteration 8, loss = 0.00561651\n",
      "Iteration 9, loss = 0.00695208\n",
      "Iteration 10, loss = 0.00630958\n",
      "Iteration 11, loss = 0.00465415\n",
      "Iteration 12, loss = 0.00352438\n",
      "Iteration 13, loss = 0.00363754\n",
      "Iteration 14, loss = 0.00448883\n",
      "Iteration 15, loss = 0.00499188\n",
      "Iteration 16, loss = 0.00462696\n",
      "Iteration 17, loss = 0.00374329\n",
      "Iteration 18, loss = 0.00304809\n",
      "Iteration 19, loss = 0.00295139\n",
      "Iteration 20, loss = 0.00332678\n",
      "Iteration 21, loss = 0.00370886\n",
      "Iteration 22, loss = 0.00373784\n",
      "Iteration 23, loss = 0.00342373\n",
      "Iteration 24, loss = 0.00305620\n",
      "Iteration 25, loss = 0.00290928\n",
      "Iteration 26, loss = 0.00303155\n",
      "Iteration 27, loss = 0.00324063\n",
      "Iteration 28, loss = 0.00330305\n",
      "Iteration 29, loss = 0.00314790\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02254755\n",
      "Iteration 2, loss = 0.00604298\n",
      "Iteration 3, loss = 0.01021227\n",
      "Iteration 4, loss = 0.01241473\n",
      "Iteration 5, loss = 0.00842721\n",
      "Iteration 6, loss = 0.00431639\n",
      "Iteration 7, loss = 0.00359024\n",
      "Iteration 8, loss = 0.00548931\n",
      "Iteration 9, loss = 0.00704597\n",
      "Iteration 10, loss = 0.00667909\n",
      "Iteration 11, loss = 0.00507784\n",
      "Iteration 12, loss = 0.00376914\n",
      "Iteration 13, loss = 0.00364554\n",
      "Iteration 14, loss = 0.00440589\n",
      "Iteration 15, loss = 0.00505971\n",
      "Iteration 16, loss = 0.00489101\n",
      "Iteration 17, loss = 0.00406690\n",
      "Iteration 18, loss = 0.00326636\n",
      "Iteration 19, loss = 0.00301017\n",
      "Iteration 20, loss = 0.00329657\n",
      "Iteration 21, loss = 0.00372247\n",
      "Iteration 22, loss = 0.00387008\n",
      "Iteration 23, loss = 0.00364062\n",
      "Iteration 24, loss = 0.00325278\n",
      "Iteration 25, loss = 0.00301513\n",
      "Iteration 26, loss = 0.00305966\n",
      "Iteration 27, loss = 0.00326144\n",
      "Iteration 28, loss = 0.00339338\n",
      "Iteration 29, loss = 0.00330996\n",
      "Iteration 30, loss = 0.00307267\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02267997\n",
      "Iteration 2, loss = 0.00595371\n",
      "Iteration 3, loss = 0.01000414\n",
      "Iteration 4, loss = 0.01235124\n",
      "Iteration 5, loss = 0.00842255\n",
      "Iteration 6, loss = 0.00423390\n",
      "Iteration 7, loss = 0.00340299\n",
      "Iteration 8, loss = 0.00528411\n",
      "Iteration 9, loss = 0.00693148\n",
      "Iteration 10, loss = 0.00666244\n",
      "Iteration 11, loss = 0.00509042\n",
      "Iteration 12, loss = 0.00373063\n",
      "Iteration 13, loss = 0.00353859\n",
      "Iteration 14, loss = 0.00428412\n",
      "Iteration 15, loss = 0.00497234\n",
      "Iteration 16, loss = 0.00487202\n",
      "Iteration 17, loss = 0.00408472\n",
      "Iteration 18, loss = 0.00325652\n",
      "Iteration 19, loss = 0.00293745\n",
      "Iteration 20, loss = 0.00318298\n",
      "Iteration 21, loss = 0.00361630\n",
      "Iteration 22, loss = 0.00380220\n",
      "Iteration 23, loss = 0.00360284\n",
      "Iteration 24, loss = 0.00322607\n",
      "Iteration 25, loss = 0.00297714\n",
      "Iteration 26, loss = 0.00300000\n",
      "Iteration 27, loss = 0.00320716\n",
      "Iteration 28, loss = 0.00335559\n",
      "Iteration 29, loss = 0.00329029\n",
      "Iteration 30, loss = 0.00306207\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02199611\n",
      "Iteration 2, loss = 0.00594889\n",
      "Iteration 3, loss = 0.01028564\n",
      "Iteration 4, loss = 0.01225582\n",
      "Iteration 5, loss = 0.00820005\n",
      "Iteration 6, loss = 0.00424336\n",
      "Iteration 7, loss = 0.00372682\n",
      "Iteration 8, loss = 0.00568438\n",
      "Iteration 9, loss = 0.00711575\n",
      "Iteration 10, loss = 0.00660558\n",
      "Iteration 11, loss = 0.00498009\n",
      "Iteration 12, loss = 0.00375974\n",
      "Iteration 13, loss = 0.00374971\n",
      "Iteration 14, loss = 0.00455372\n",
      "Iteration 15, loss = 0.00513652\n",
      "Iteration 16, loss = 0.00485716\n",
      "Iteration 17, loss = 0.00398668\n",
      "Iteration 18, loss = 0.00323856\n",
      "Iteration 19, loss = 0.00307575\n",
      "Iteration 20, loss = 0.00341890\n",
      "Iteration 21, loss = 0.00381810\n",
      "Iteration 22, loss = 0.00389203\n",
      "Iteration 23, loss = 0.00360750\n",
      "Iteration 24, loss = 0.00322427\n",
      "Iteration 25, loss = 0.00303598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26, loss = 0.00313435\n",
      "Iteration 27, loss = 0.00334049\n",
      "Iteration 28, loss = 0.00343047\n",
      "Iteration 29, loss = 0.00330101\n",
      "Iteration 30, loss = 0.00305231\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02183182\n",
      "Iteration 2, loss = 0.00588222\n",
      "Iteration 3, loss = 0.01026024\n",
      "Iteration 4, loss = 0.01217814\n",
      "Iteration 5, loss = 0.00810994\n",
      "Iteration 6, loss = 0.00419284\n",
      "Iteration 7, loss = 0.00372986\n",
      "Iteration 8, loss = 0.00570821\n",
      "Iteration 9, loss = 0.00711664\n",
      "Iteration 10, loss = 0.00656561\n",
      "Iteration 11, loss = 0.00493598\n",
      "Iteration 12, loss = 0.00374289\n",
      "Iteration 13, loss = 0.00376216\n",
      "Iteration 14, loss = 0.00458619\n",
      "Iteration 15, loss = 0.00515531\n",
      "Iteration 16, loss = 0.00485675\n",
      "Iteration 17, loss = 0.00398682\n",
      "Iteration 18, loss = 0.00324660\n",
      "Iteration 19, loss = 0.00310231\n",
      "Iteration 20, loss = 0.00346480\n",
      "Iteration 21, loss = 0.00386794\n",
      "Iteration 22, loss = 0.00392932\n",
      "Iteration 23, loss = 0.00363242\n",
      "Iteration 24, loss = 0.00325224\n",
      "Iteration 25, loss = 0.00308314\n",
      "Iteration 26, loss = 0.00318759\n",
      "Iteration 27, loss = 0.00340231\n",
      "Iteration 28, loss = 0.00348442\n",
      "Iteration 29, loss = 0.00334116\n",
      "Iteration 30, loss = 0.00308693\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02198943\n",
      "Iteration 2, loss = 0.00594501\n",
      "Iteration 3, loss = 0.01028970\n",
      "Iteration 4, loss = 0.01226073\n",
      "Iteration 5, loss = 0.00820764\n",
      "Iteration 6, loss = 0.00425255\n",
      "Iteration 7, loss = 0.00373563\n",
      "Iteration 8, loss = 0.00569719\n",
      "Iteration 9, loss = 0.00713269\n",
      "Iteration 10, loss = 0.00661036\n",
      "Iteration 11, loss = 0.00498238\n",
      "Iteration 12, loss = 0.00377046\n",
      "Iteration 13, loss = 0.00376663\n",
      "Iteration 14, loss = 0.00458354\n",
      "Iteration 15, loss = 0.00516078\n",
      "Iteration 16, loss = 0.00488109\n",
      "Iteration 17, loss = 0.00402082\n",
      "Iteration 18, loss = 0.00327426\n",
      "Iteration 19, loss = 0.00311257\n",
      "Iteration 20, loss = 0.00346124\n",
      "Iteration 21, loss = 0.00386512\n",
      "Iteration 22, loss = 0.00393937\n",
      "Iteration 23, loss = 0.00365413\n",
      "Iteration 24, loss = 0.00327404\n",
      "Iteration 25, loss = 0.00309330\n",
      "Iteration 26, loss = 0.00318793\n",
      "Iteration 27, loss = 0.00339552\n",
      "Iteration 28, loss = 0.00348652\n",
      "Iteration 29, loss = 0.00335690\n",
      "Iteration 30, loss = 0.00310928\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02218817\n",
      "Iteration 2, loss = 0.00601407\n",
      "Iteration 3, loss = 0.01030232\n",
      "Iteration 4, loss = 0.01233441\n",
      "Iteration 5, loss = 0.00829991\n",
      "Iteration 6, loss = 0.00430287\n",
      "Iteration 7, loss = 0.00372001\n",
      "Iteration 8, loss = 0.00565820\n",
      "Iteration 9, loss = 0.00712473\n",
      "Iteration 10, loss = 0.00664100\n",
      "Iteration 11, loss = 0.00502578\n",
      "Iteration 12, loss = 0.00379564\n",
      "Iteration 13, loss = 0.00376844\n",
      "Iteration 14, loss = 0.00458146\n",
      "Iteration 15, loss = 0.00516999\n",
      "Iteration 16, loss = 0.00491115\n",
      "Iteration 17, loss = 0.00405717\n",
      "Iteration 18, loss = 0.00330069\n",
      "Iteration 19, loss = 0.00311181\n",
      "Iteration 20, loss = 0.00344239\n",
      "Iteration 21, loss = 0.00385067\n",
      "Iteration 22, loss = 0.00394423\n",
      "Iteration 23, loss = 0.00367421\n",
      "Iteration 24, loss = 0.00329773\n",
      "Iteration 25, loss = 0.00310025\n",
      "Iteration 26, loss = 0.00318222\n",
      "Iteration 27, loss = 0.00339341\n",
      "Iteration 28, loss = 0.00349280\n",
      "Iteration 29, loss = 0.00337150\n",
      "Iteration 30, loss = 0.00312505\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02200869\n",
      "Iteration 2, loss = 0.00596474\n",
      "Iteration 3, loss = 0.01029234\n",
      "Iteration 4, loss = 0.01226243\n",
      "Iteration 5, loss = 0.00820867\n",
      "Iteration 6, loss = 0.00425607\n",
      "Iteration 7, loss = 0.00374435\n",
      "Iteration 8, loss = 0.00571040\n",
      "Iteration 9, loss = 0.00714317\n",
      "Iteration 10, loss = 0.00661623\n",
      "Iteration 11, loss = 0.00498745\n",
      "Iteration 12, loss = 0.00377313\n",
      "Iteration 13, loss = 0.00376899\n",
      "Iteration 14, loss = 0.00458663\n",
      "Iteration 15, loss = 0.00516021\n",
      "Iteration 16, loss = 0.00487656\n",
      "Iteration 17, loss = 0.00401431\n",
      "Iteration 18, loss = 0.00327002\n",
      "Iteration 19, loss = 0.00311179\n",
      "Iteration 20, loss = 0.00346200\n",
      "Iteration 21, loss = 0.00386198\n",
      "Iteration 22, loss = 0.00393236\n",
      "Iteration 23, loss = 0.00364541\n",
      "Iteration 24, loss = 0.00326686\n",
      "Iteration 25, loss = 0.00308852\n",
      "Iteration 26, loss = 0.00318545\n",
      "Iteration 27, loss = 0.00339213\n",
      "Iteration 28, loss = 0.00348093\n",
      "Iteration 29, loss = 0.00335020\n",
      "Iteration 30, loss = 0.00310372\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02253056\n",
      "Iteration 2, loss = 0.00603903\n",
      "Iteration 3, loss = 0.01015345\n",
      "Iteration 4, loss = 0.01238437\n",
      "Iteration 5, loss = 0.00843136\n",
      "Iteration 6, loss = 0.00431708\n",
      "Iteration 7, loss = 0.00356804\n",
      "Iteration 8, loss = 0.00546406\n",
      "Iteration 9, loss = 0.00704791\n",
      "Iteration 10, loss = 0.00671274\n",
      "Iteration 11, loss = 0.00512601\n",
      "Iteration 12, loss = 0.00380550\n",
      "Iteration 13, loss = 0.00365772\n",
      "Iteration 14, loss = 0.00441234\n",
      "Iteration 15, loss = 0.00507854\n",
      "Iteration 16, loss = 0.00493425\n",
      "Iteration 17, loss = 0.00412330\n",
      "Iteration 18, loss = 0.00331274\n",
      "Iteration 19, loss = 0.00303670\n",
      "Iteration 20, loss = 0.00331073\n",
      "Iteration 21, loss = 0.00373916\n",
      "Iteration 22, loss = 0.00390181\n",
      "Iteration 23, loss = 0.00368608\n",
      "Iteration 24, loss = 0.00330599\n",
      "Iteration 25, loss = 0.00306853\n",
      "Iteration 26, loss = 0.00310290\n",
      "Iteration 27, loss = 0.00330734\n",
      "Iteration 28, loss = 0.00344650\n",
      "Iteration 29, loss = 0.00337438\n",
      "Iteration 30, loss = 0.00314448\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02267617\n",
      "Iteration 2, loss = 0.00591776\n",
      "Iteration 3, loss = 0.00988985\n",
      "Iteration 4, loss = 0.01226291\n",
      "Iteration 5, loss = 0.00837095\n",
      "Iteration 6, loss = 0.00416617\n",
      "Iteration 7, loss = 0.00328270\n",
      "Iteration 8, loss = 0.00514261\n",
      "Iteration 9, loss = 0.00681862\n",
      "Iteration 10, loss = 0.00659259\n",
      "Iteration 11, loss = 0.00505192\n",
      "Iteration 12, loss = 0.00368857\n",
      "Iteration 13, loss = 0.00348470\n",
      "Iteration 14, loss = 0.00422142\n",
      "Iteration 15, loss = 0.00490974\n",
      "Iteration 16, loss = 0.00481900\n",
      "Iteration 17, loss = 0.00403971\n",
      "Iteration 18, loss = 0.00320305\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02182718\n",
      "Iteration 2, loss = 0.00591025\n",
      "Iteration 3, loss = 0.01027145\n",
      "Iteration 4, loss = 0.01217618\n",
      "Iteration 5, loss = 0.00811630\n",
      "Iteration 6, loss = 0.00420233\n",
      "Iteration 7, loss = 0.00372086\n",
      "Iteration 8, loss = 0.00567325\n",
      "Iteration 9, loss = 0.00706930\n",
      "Iteration 10, loss = 0.00652096\n",
      "Iteration 11, loss = 0.00490358\n",
      "Iteration 12, loss = 0.00372637\n",
      "Iteration 13, loss = 0.00375061\n",
      "Iteration 14, loss = 0.00456630\n",
      "Iteration 15, loss = 0.00511975\n",
      "Iteration 16, loss = 0.00481477\n",
      "Iteration 17, loss = 0.00394732\n",
      "Iteration 18, loss = 0.00321601\n",
      "Iteration 19, loss = 0.00307481\n",
      "Iteration 20, loss = 0.00342829\n",
      "Iteration 21, loss = 0.00381671\n",
      "Iteration 22, loss = 0.00387018\n",
      "Iteration 23, loss = 0.00357478\n",
      "Iteration 24, loss = 0.00320284\n",
      "Iteration 25, loss = 0.00303990\n",
      "Iteration 26, loss = 0.00314418\n",
      "Iteration 27, loss = 0.00334807\n",
      "Iteration 28, loss = 0.00342372\n",
      "Iteration 29, loss = 0.00328190\n",
      "Iteration 30, loss = 0.00303552\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02177308\n",
      "Iteration 2, loss = 0.00588176\n",
      "Iteration 3, loss = 0.01025348\n",
      "Iteration 4, loss = 0.01216221\n",
      "Iteration 5, loss = 0.00810962\n",
      "Iteration 6, loss = 0.00420374\n",
      "Iteration 7, loss = 0.00373649\n",
      "Iteration 8, loss = 0.00570545\n",
      "Iteration 9, loss = 0.00711009\n",
      "Iteration 10, loss = 0.00656156\n",
      "Iteration 11, loss = 0.00493660\n",
      "Iteration 12, loss = 0.00375119\n",
      "Iteration 13, loss = 0.00377508\n",
      "Iteration 14, loss = 0.00459695\n",
      "Iteration 15, loss = 0.00515693\n",
      "Iteration 16, loss = 0.00485576\n",
      "Iteration 17, loss = 0.00398985\n",
      "Iteration 18, loss = 0.00325854\n",
      "Iteration 19, loss = 0.00311921\n",
      "Iteration 20, loss = 0.00348118\n",
      "Iteration 21, loss = 0.00387920\n",
      "Iteration 22, loss = 0.00393710\n",
      "Iteration 23, loss = 0.00364155\n",
      "Iteration 24, loss = 0.00326479\n",
      "Iteration 25, loss = 0.00309748\n",
      "Iteration 26, loss = 0.00320417\n",
      "Iteration 27, loss = 0.00341225\n",
      "Iteration 28, loss = 0.00349265\n",
      "Iteration 29, loss = 0.00335190\n",
      "Iteration 30, loss = 0.00310270\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02208550\n",
      "Iteration 2, loss = 0.00600195\n",
      "Iteration 3, loss = 0.01030179\n",
      "Iteration 4, loss = 0.01229598\n",
      "Iteration 5, loss = 0.00825025\n",
      "Iteration 6, loss = 0.00427938\n",
      "Iteration 7, loss = 0.00373589\n",
      "Iteration 8, loss = 0.00568210\n",
      "Iteration 9, loss = 0.00712403\n",
      "Iteration 10, loss = 0.00661842\n",
      "Iteration 11, loss = 0.00499928\n",
      "Iteration 12, loss = 0.00378819\n",
      "Iteration 13, loss = 0.00378162\n",
      "Iteration 14, loss = 0.00459430\n",
      "Iteration 15, loss = 0.00517406\n",
      "Iteration 16, loss = 0.00490001\n",
      "Iteration 17, loss = 0.00404162\n",
      "Iteration 18, loss = 0.00328794\n",
      "Iteration 19, loss = 0.00311842\n",
      "Iteration 20, loss = 0.00346077\n",
      "Iteration 21, loss = 0.00386339\n",
      "Iteration 22, loss = 0.00394127\n",
      "Iteration 23, loss = 0.00365915\n",
      "Iteration 24, loss = 0.00328147\n",
      "Iteration 25, loss = 0.00310226\n",
      "Iteration 26, loss = 0.00319335\n",
      "Iteration 27, loss = 0.00340161\n",
      "Iteration 28, loss = 0.00349257\n",
      "Iteration 29, loss = 0.00336552\n",
      "Iteration 30, loss = 0.00312015\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02243801\n",
      "Iteration 2, loss = 0.00605274\n",
      "Iteration 3, loss = 0.01023668\n",
      "Iteration 4, loss = 0.01238681\n",
      "Iteration 5, loss = 0.00839227\n",
      "Iteration 6, loss = 0.00431992\n",
      "Iteration 7, loss = 0.00364432\n",
      "Iteration 8, loss = 0.00556691\n",
      "Iteration 9, loss = 0.00710278\n",
      "Iteration 10, loss = 0.00669747\n",
      "Iteration 11, loss = 0.00509499\n",
      "Iteration 12, loss = 0.00381921\n",
      "Iteration 13, loss = 0.00373751\n",
      "Iteration 14, loss = 0.00452049\n",
      "Iteration 15, loss = 0.00514894\n",
      "Iteration 16, loss = 0.00493802\n",
      "Iteration 17, loss = 0.00408811\n",
      "Iteration 18, loss = 0.00329778\n",
      "Iteration 19, loss = 0.00307203\n",
      "Iteration 20, loss = 0.00338086\n",
      "Iteration 21, loss = 0.00379555\n",
      "Iteration 22, loss = 0.00391192\n",
      "Iteration 23, loss = 0.00365577\n",
      "Iteration 24, loss = 0.00327039\n",
      "Iteration 25, loss = 0.00306264\n",
      "Iteration 26, loss = 0.00312926\n",
      "Iteration 27, loss = 0.00334112\n",
      "Iteration 28, loss = 0.00345470\n",
      "Iteration 29, loss = 0.00334499\n",
      "Iteration 30, loss = 0.00309725\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02238139\n",
      "Iteration 2, loss = 0.00605289\n",
      "Iteration 3, loss = 0.01022678\n",
      "Iteration 4, loss = 0.01237079\n",
      "Iteration 5, loss = 0.00838943\n",
      "Iteration 6, loss = 0.00432921\n",
      "Iteration 7, loss = 0.00365344\n",
      "Iteration 8, loss = 0.00556737\n",
      "Iteration 9, loss = 0.00710197\n",
      "Iteration 10, loss = 0.00670306\n",
      "Iteration 11, loss = 0.00510584\n",
      "Iteration 12, loss = 0.00382462\n",
      "Iteration 13, loss = 0.00373109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 0.00451089\n",
      "Iteration 15, loss = 0.00514683\n",
      "Iteration 16, loss = 0.00494886\n",
      "Iteration 17, loss = 0.00410742\n",
      "Iteration 18, loss = 0.00332045\n",
      "Iteration 19, loss = 0.00309297\n",
      "Iteration 20, loss = 0.00340172\n",
      "Iteration 21, loss = 0.00382208\n",
      "Iteration 22, loss = 0.00394861\n",
      "Iteration 23, loss = 0.00370248\n",
      "Iteration 24, loss = 0.00331627\n",
      "Iteration 25, loss = 0.00309534\n",
      "Iteration 26, loss = 0.00316033\n",
      "Iteration 27, loss = 0.00336698\n",
      "Iteration 28, loss = 0.00348761\n",
      "Iteration 29, loss = 0.00339002\n",
      "Iteration 30, loss = 0.00314905\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02257437\n",
      "Iteration 2, loss = 0.00602086\n",
      "Iteration 3, loss = 0.01009639\n",
      "Iteration 4, loss = 0.01236796\n",
      "Iteration 5, loss = 0.00843906\n",
      "Iteration 6, loss = 0.00430399\n",
      "Iteration 7, loss = 0.00351690\n",
      "Iteration 8, loss = 0.00540129\n",
      "Iteration 9, loss = 0.00701019\n",
      "Iteration 10, loss = 0.00670794\n",
      "Iteration 11, loss = 0.00513183\n",
      "Iteration 12, loss = 0.00379770\n",
      "Iteration 13, loss = 0.00362471\n",
      "Iteration 14, loss = 0.00436652\n",
      "Iteration 15, loss = 0.00504024\n",
      "Iteration 16, loss = 0.00491971\n",
      "Iteration 17, loss = 0.00412306\n",
      "Iteration 18, loss = 0.00330974\n",
      "Iteration 19, loss = 0.00301413\n",
      "Iteration 20, loss = 0.00327068\n",
      "Iteration 21, loss = 0.00370007\n",
      "Iteration 22, loss = 0.00388621\n",
      "Iteration 23, loss = 0.00369631\n",
      "Iteration 24, loss = 0.00331763\n",
      "Iteration 25, loss = 0.00305644\n",
      "Iteration 26, loss = 0.00307564\n",
      "Iteration 27, loss = 0.00327165\n",
      "Iteration 28, loss = 0.00342249\n",
      "Iteration 29, loss = 0.00336973\n",
      "Iteration 30, loss = 0.00314933\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02252717\n",
      "Iteration 2, loss = 0.00603438\n",
      "Iteration 3, loss = 0.01010281\n",
      "Iteration 4, loss = 0.01235846\n",
      "Iteration 5, loss = 0.00841997\n",
      "Iteration 6, loss = 0.00429298\n",
      "Iteration 7, loss = 0.00351851\n",
      "Iteration 8, loss = 0.00540629\n",
      "Iteration 9, loss = 0.00699768\n",
      "Iteration 10, loss = 0.00665882\n",
      "Iteration 11, loss = 0.00507749\n",
      "Iteration 12, loss = 0.00376428\n",
      "Iteration 13, loss = 0.00363761\n",
      "Iteration 14, loss = 0.00441849\n",
      "Iteration 15, loss = 0.00506975\n",
      "Iteration 16, loss = 0.00488965\n",
      "Iteration 17, loss = 0.00405574\n",
      "Iteration 18, loss = 0.00325580\n",
      "Iteration 19, loss = 0.00300127\n",
      "Iteration 20, loss = 0.00329155\n",
      "Iteration 21, loss = 0.00371938\n",
      "Iteration 22, loss = 0.00386681\n",
      "Iteration 23, loss = 0.00363679\n",
      "Iteration 24, loss = 0.00325486\n",
      "Iteration 25, loss = 0.00302523\n",
      "Iteration 26, loss = 0.00307210\n",
      "Iteration 27, loss = 0.00328060\n",
      "Iteration 28, loss = 0.00341122\n",
      "Iteration 29, loss = 0.00332415\n",
      "Iteration 30, loss = 0.00308387\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02163658\n",
      "Iteration 2, loss = 0.00583545\n",
      "Iteration 3, loss = 0.01023987\n",
      "Iteration 4, loss = 0.01211089\n",
      "Iteration 5, loss = 0.00803411\n",
      "Iteration 6, loss = 0.00414826\n",
      "Iteration 7, loss = 0.00371379\n",
      "Iteration 8, loss = 0.00568400\n",
      "Iteration 9, loss = 0.00706346\n",
      "Iteration 10, loss = 0.00648987\n",
      "Iteration 11, loss = 0.00485725\n",
      "Iteration 12, loss = 0.00368904\n",
      "Iteration 13, loss = 0.00373700\n",
      "Iteration 14, loss = 0.00456764\n",
      "Iteration 15, loss = 0.00511661\n",
      "Iteration 16, loss = 0.00479496\n",
      "Iteration 17, loss = 0.00391792\n",
      "Iteration 18, loss = 0.00319130\n",
      "Iteration 19, loss = 0.00306634\n",
      "Iteration 20, loss = 0.00343552\n",
      "Iteration 21, loss = 0.00382781\n",
      "Iteration 22, loss = 0.00387208\n",
      "Iteration 23, loss = 0.00356542\n",
      "Iteration 24, loss = 0.00318988\n",
      "Iteration 25, loss = 0.00303272\n",
      "Iteration 26, loss = 0.00314608\n",
      "Iteration 27, loss = 0.00335479\n",
      "Iteration 28, loss = 0.00342721\n",
      "Iteration 29, loss = 0.00327929\n",
      "Iteration 30, loss = 0.00303020\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02186257\n",
      "Iteration 2, loss = 0.00590391\n",
      "Iteration 3, loss = 0.01027064\n",
      "Iteration 4, loss = 0.01220841\n",
      "Iteration 5, loss = 0.00815653\n",
      "Iteration 6, loss = 0.00422972\n",
      "Iteration 7, loss = 0.00374214\n",
      "Iteration 8, loss = 0.00571310\n",
      "Iteration 9, loss = 0.00713094\n",
      "Iteration 10, loss = 0.00659075\n",
      "Iteration 11, loss = 0.00496508\n",
      "Iteration 12, loss = 0.00376553\n",
      "Iteration 13, loss = 0.00377619\n",
      "Iteration 14, loss = 0.00459621\n",
      "Iteration 15, loss = 0.00516418\n",
      "Iteration 16, loss = 0.00487383\n",
      "Iteration 17, loss = 0.00401292\n",
      "Iteration 18, loss = 0.00327514\n",
      "Iteration 19, loss = 0.00312545\n",
      "Iteration 20, loss = 0.00348116\n",
      "Iteration 21, loss = 0.00388249\n",
      "Iteration 22, loss = 0.00394860\n",
      "Iteration 23, loss = 0.00365855\n",
      "Iteration 24, loss = 0.00328072\n",
      "Iteration 25, loss = 0.00310676\n",
      "Iteration 26, loss = 0.00320806\n",
      "Iteration 27, loss = 0.00341613\n",
      "Iteration 28, loss = 0.00350202\n",
      "Iteration 29, loss = 0.00336677\n",
      "Iteration 30, loss = 0.00311887\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02244988\n",
      "Iteration 2, loss = 0.00605020\n",
      "Iteration 3, loss = 0.01020840\n",
      "Iteration 4, loss = 0.01238516\n",
      "Iteration 5, loss = 0.00839231\n",
      "Iteration 6, loss = 0.00430363\n",
      "Iteration 7, loss = 0.00361468\n",
      "Iteration 8, loss = 0.00553879\n",
      "Iteration 9, loss = 0.00708322\n",
      "Iteration 10, loss = 0.00668871\n",
      "Iteration 11, loss = 0.00508285\n",
      "Iteration 12, loss = 0.00379441\n",
      "Iteration 13, loss = 0.00370263\n",
      "Iteration 14, loss = 0.00448151\n",
      "Iteration 15, loss = 0.00511449\n",
      "Iteration 16, loss = 0.00491045\n",
      "Iteration 17, loss = 0.00406815\n",
      "Iteration 18, loss = 0.00327781\n",
      "Iteration 19, loss = 0.00304575\n",
      "Iteration 20, loss = 0.00334694\n",
      "Iteration 21, loss = 0.00376116\n",
      "Iteration 22, loss = 0.00388528\n",
      "Iteration 23, loss = 0.00363631\n",
      "Iteration 24, loss = 0.00324852\n",
      "Iteration 25, loss = 0.00303109\n",
      "Iteration 26, loss = 0.00309887\n",
      "Iteration 27, loss = 0.00330632\n",
      "Iteration 28, loss = 0.00342132\n",
      "Iteration 29, loss = 0.00331693\n",
      "Iteration 30, loss = 0.00307279\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01966844\n",
      "Iteration 2, loss = 0.00459357\n",
      "Iteration 3, loss = 0.00930874\n",
      "Iteration 4, loss = 0.01066143\n",
      "Iteration 5, loss = 0.00641969\n",
      "Iteration 6, loss = 0.00284378\n",
      "Iteration 7, loss = 0.00278703\n",
      "Iteration 8, loss = 0.00475121\n",
      "Iteration 9, loss = 0.00577360\n",
      "Iteration 10, loss = 0.00491474\n",
      "Iteration 11, loss = 0.00328530\n",
      "Iteration 12, loss = 0.00231413\n",
      "Iteration 13, loss = 0.00254852\n",
      "Iteration 14, loss = 0.00331362\n",
      "Iteration 15, loss = 0.00358369\n",
      "Iteration 16, loss = 0.00304396\n",
      "Iteration 17, loss = 0.00218728\n",
      "Iteration 18, loss = 0.00170266\n",
      "Iteration 19, loss = 0.00185763\n",
      "Iteration 20, loss = 0.00232558\n",
      "Iteration 21, loss = 0.00255997\n",
      "Iteration 22, loss = 0.00234962\n",
      "Iteration 23, loss = 0.00193019\n",
      "Iteration 24, loss = 0.00166699\n",
      "Iteration 25, loss = 0.00171995\n",
      "Iteration 26, loss = 0.00193763\n",
      "Iteration 27, loss = 0.00204089\n",
      "Iteration 28, loss = 0.00190515\n",
      "Iteration 29, loss = 0.00164840\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02164165\n",
      "Iteration 2, loss = 0.00587939\n",
      "Iteration 3, loss = 0.01025180\n",
      "Iteration 4, loss = 0.01211204\n",
      "Iteration 5, loss = 0.00805092\n",
      "Iteration 6, loss = 0.00418415\n",
      "Iteration 7, loss = 0.00376286\n",
      "Iteration 8, loss = 0.00573722\n",
      "Iteration 9, loss = 0.00710138\n",
      "Iteration 10, loss = 0.00651578\n",
      "Iteration 11, loss = 0.00488992\n",
      "Iteration 12, loss = 0.00372956\n",
      "Iteration 13, loss = 0.00378995\n",
      "Iteration 14, loss = 0.00462237\n",
      "Iteration 15, loss = 0.00515533\n",
      "Iteration 16, loss = 0.00482660\n",
      "Iteration 17, loss = 0.00395586\n",
      "Iteration 18, loss = 0.00324531\n",
      "Iteration 19, loss = 0.00312147\n",
      "Iteration 20, loss = 0.00348592\n",
      "Iteration 21, loss = 0.00387501\n",
      "Iteration 22, loss = 0.00391922\n",
      "Iteration 23, loss = 0.00361887\n",
      "Iteration 24, loss = 0.00325240\n",
      "Iteration 25, loss = 0.00308860\n",
      "Iteration 26, loss = 0.00320367\n",
      "Iteration 27, loss = 0.00341626\n",
      "Iteration 28, loss = 0.00348853\n",
      "Iteration 29, loss = 0.00333737\n",
      "Iteration 30, loss = 0.00308658\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Config of the regressors and cross val leave one out\n",
    "####\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes = (50,), alpha = 0.001,\n",
    "    learning_rate_init = 0.01, max_iter = 1000,\n",
    "    random_state = 9, tol = 0.0001, verbose = True)\n",
    "svr = SVR(kernel = 'linear', C = 0.25, epsilon = 0.01, verbose = True, max_iter = 1000)\n",
    "lr = LinearRegression()\n",
    "\n",
    "full_predict_lr = cross_val_predict(lr, X, target, cv = loo)\n",
    "full_predict_mlp = cross_val_predict(mlp, X, target, cv = loo)\n",
    "full_predict_svr = cross_val_predict(svr, X, target, cv = loo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error in MLP: 0.007172284489446285\n",
      "Mean Squared Error in SVR: 0.006799635526165946\n",
      "Mean Squared Error in LR: 0.007608342850615478\n",
      "RÂ² score in MLP: 0.8335484222014908\n",
      "RÂ² score in SVR: 0.8421967138293902\n",
      "RÂ² score in LR: 0.823428550027479\n",
      "adjusted RÂ² score in MLP: 0.8239454465592692\n",
      "adjusted RÂ² score in SVR: 0.8330926780887782\n",
      "adjusted RÂ² score in LR: 0.8132417356059873\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Printing some metrics of the regressors\n",
    "####\n",
    "print('Mean Squared Error in MLP: %s' %(metrics.mean_squared_error(target, full_predict_mlp)))\n",
    "print('Mean Squared Error in SVR: %s' %(metrics.mean_squared_error(target, full_predict_svr)))\n",
    "print('Mean Squared Error in LR: %s' %(metrics.mean_squared_error(target, full_predict_lr)))\n",
    "\n",
    "r_squared_mlp = metrics.r2_score(target, full_predict_mlp)\n",
    "r_squared_svr = metrics.r2_score(target, full_predict_svr)\n",
    "r_squared_lr = metrics.r2_score(target, full_predict_lr)\n",
    "\n",
    "print('RÂ² score in MLP: %s' %(r_squared_mlp))\n",
    "print('RÂ² score in SVR: %s' %(r_squared_svr))\n",
    "print('RÂ² score in LR: %s' %(r_squared_lr))\n",
    "\n",
    "adjusted_r_squared_mlp = 1 - (1 - r_squared_mlp) * (len(target) - 1) / (len(target) - X.shape[1] - 1)\n",
    "adjusted_r_squared_svr = 1 - (1 - r_squared_svr) * (len(target) - 1) / (len(target) - X.shape[1] - 1)\n",
    "adjusted_r_squared_lr = 1 - (1 - r_squared_lr) * (len(target) - 1) / (len(target) - X.shape[1] - 1)\n",
    "\n",
    "print('adjusted RÂ² score in MLP: %s' %(adjusted_r_squared_mlp))\n",
    "print('adjusted RÂ² score in SVR: %s' %(adjusted_r_squared_svr))\n",
    "print('adjusted RÂ² score in LR: %s' %(adjusted_r_squared_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117, 1)\n",
      "(117, 1)\n",
      "(117, 1)\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Filling lists with NaN so the len is the same across all lists \n",
    "## so that a graph can be generated\n",
    "####\n",
    "import numpy as np\n",
    "\n",
    "values_to_add = list()\n",
    "for i in range(0, window_size):\n",
    "    values_to_add.append(float('NaN'))\n",
    "    \n",
    "full_predict_svr = np.insert(full_predict_svr, 0, values_to_add)\n",
    "full_predict_svr.shape = (len(full_predict_svr), 1)\n",
    "    \n",
    "full_predict_mlp = np.insert(full_predict_mlp, 0, values_to_add)\n",
    "full_predict_mlp.shape = (len(full_predict_mlp), 1)\n",
    "\n",
    "full_predict_lr = np.insert(full_predict_lr, 0, values_to_add)\n",
    "full_predict_lr.shape = (len(full_predict_lr), 1)\n",
    "\n",
    "print(full_predict_svr.shape)\n",
    "print(full_predict_mlp.shape)\n",
    "print(full_predict_lr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Casos</th>\n",
       "      <th>CasosNormalizados</th>\n",
       "      <th>Predict_lr</th>\n",
       "      <th>Predict_mlp</th>\n",
       "      <th>Predict_svr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26/2/20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/3/20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17/6/20</th>\n",
       "      <td>34918</td>\n",
       "      <td>0.637527</td>\n",
       "      <td>0.535564</td>\n",
       "      <td>0.512767</td>\n",
       "      <td>0.508200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18/6/20</th>\n",
       "      <td>32188</td>\n",
       "      <td>0.587683</td>\n",
       "      <td>0.613958</td>\n",
       "      <td>0.571534</td>\n",
       "      <td>0.588058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19/6/20</th>\n",
       "      <td>22765</td>\n",
       "      <td>0.415640</td>\n",
       "      <td>0.562513</td>\n",
       "      <td>0.546907</td>\n",
       "      <td>0.539008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20/6/20</th>\n",
       "      <td>54771</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.414571</td>\n",
       "      <td>0.421622</td>\n",
       "      <td>0.413640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21/6/20</th>\n",
       "      <td>34666</td>\n",
       "      <td>0.632926</td>\n",
       "      <td>0.721179</td>\n",
       "      <td>0.582612</td>\n",
       "      <td>0.651739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Casos  CasosNormalizados  Predict_lr  Predict_mlp  Predict_svr\n",
       "Data                                                                   \n",
       "26/2/20      1           0.000018         NaN          NaN          NaN\n",
       "27/2/20      0           0.000000         NaN          NaN          NaN\n",
       "28/2/20      0           0.000000         NaN          NaN          NaN\n",
       "29/2/20      0           0.000000         NaN          NaN          NaN\n",
       "1/3/20       1           0.000018         NaN          NaN          NaN\n",
       "...        ...                ...         ...          ...          ...\n",
       "17/6/20  34918           0.637527    0.535564     0.512767     0.508200\n",
       "18/6/20  32188           0.587683    0.613958     0.571534     0.588058\n",
       "19/6/20  22765           0.415640    0.562513     0.546907     0.539008\n",
       "20/6/20  54771           1.000000    0.414571     0.421622     0.413640\n",
       "21/6/20  34666           0.632926    0.721179     0.582612     0.651739\n",
       "\n",
       "[117 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####\n",
    "## Adding the data to plot \n",
    "####\n",
    "data['Predict_lr'] = full_predict_lr\n",
    "data['Predict_mlp'] = full_predict_mlp\n",
    "data['Predict_svr'] = full_predict_svr\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEGCAYAAAB1iW6ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzde3xU1bnw8d/ac5/J/UJICBBAIiJBwKhYUVDEqlQUW1F6qlKq1rev5fSm0taj1npO0bYq1mprrZf61gvaoljUqqhH8QYBuQgCAgYIgYTcJpe5773eP/bM5DZJhpCYAOv7+fRDZmbPnjXT+MyTZ6/1LCGlRFEURTn6aQM9AEVRFKVvqICuKIpyjFABXVEU5RihArqiKMoxQgV0RVGUY4R1oF44JydHFhUVDdTLK4qiHJXWrVtXI6XMTfTYgAX0oqIiysrKBurlFUVRjkpCiD1dPaZKLoqiKMcIFdAVRVGOESqgK4qiHCMGrIaeSDgcpqKigkAgMNBDUQYJp9NJYWEhNpttoIeiKIPeoAroFRUVpKamUlRUhBBioIejDDApJbW1tVRUVDBq1KiBHo6iDHo9BnQhxOPAN4BqKeWEBI8LYClwMeADFkgp1/dmMIFAQAVzJU4IQXZ2NocOHRrooShKr5Te/SY1zaFO9+ek2Cm7bVafv14yNfQngQu7efwiYGz0fzcAjxzJgFQwV9pSvw/K0SxRMO/u/iPVY0CXUr4H1HVzyKXA36TpYyBDCJHfVwNUFEVRktMXs1yGAfva3K6I3teJEOIGIUSZEKJsMP8ZffDgQa666irGjBnD+PHjufjii9mxY0e/vd6dd96J2+2muro6fl9KSkq/vV4i5eXlTJhgVtTKyspYtGjREZ9zwYIFvPjii0d8HkVRktMXF0UT/U2ccNcMKeWjwKMApaWlR7SzRn/VpqSUzJ07l2uvvZbnnnsOgA0bNlBVVUVxcXGvz9uTnJwcfv/733PPPfcc9nOllEgp0bS+mYVaWlpKaWlpn5xLUZSvTl9EgApgeJvbhUBlH5y3W/1Vm3rnnXew2WzceOON8fsmTZrE5MmTmTlzJlOmTKGkpISXX34ZgJaWFmbPns0pp5zChAkTeP755wFYtWoVkydPpqSkhIULFxIMBgFYvHgx48ePZ+LEifzsZz+Lv8bChQt5/vnnqavrXN267777mDBhAhMmTOCBBx4AzIz6pJNO4gc/+AFTpkxh3759pKSkcOutt3Lqqady/vnns2bNGmbMmMHo0aNZsWJF/Hlnn302U6ZMYcqUKXz44YedXu/dd9/lG9/4BgAXX3wxkyZNYtKkSaSnp/PUU091eQ4pJTfddBPjx49n9uzZ7f7iONzPQ1GUw9cXGfoK4CYhxHPAGYBXSnngSE/6q1e2sLWysVfPvfLPHyW8f3xBGndccnK3z/3ss8849dRTO93vdDpZvnw5aWlp1NTUMHXqVObMmcPrr79OQUEBK1euBMDr9RIIBFiwYAGrVq2iuLiYa665hkceeYRrrrmG5cuXs23bNoQQNDQ0xM+fkpLCwoULWbp0Kb/61a/i969bt44nnniCTz75BCklZ5xxBtOnTyczM5Pt27fzxBNP8PDDDwPml8uMGTO45557mDt3LrfddhtvvvkmW7du5dprr2XOnDkMGTKEN998E6fTyRdffMH8+fO77anz6quvxsfx3e9+l8suuwybzZbwHMuXL2f79u1s3ryZqqoqxo8fz8KFC3v1eSjKsSAnxd5lJaE/9JihCyGeBT4CThRCVAghvieEuFEIEUthXwV2AzuBvwA/6JeRDjApJb/4xS+YOHEi559/Pvv376eqqoqSkhLeeustbr31Vt5//33S09PZvn07o0aNipdorr32Wt577z3S0tJwOp1cd911/POf/8Ttdrd7jUWLFvHUU0/R2Nj6RbZ69Wrmzp2Lx+MhJSWFyy+/nPfffx+AkSNHMnXq1PixdrudCy80JySVlJQwffp0bDYbJSUllJeXA+bireuvv56SkhKuuOIKtm7d2uN7r6mp4eqrr+aZZ54hPT29y3O89957zJ8/H4vFQkFBAeeddx5Arz8PRTnald02i/Ilsxk3NJVUp5XyJbMpXzK7X6YsQhIZupRyfg+PS+D/9tmIonrKpIsWr+zysee/f2avX/fkk09OeCHv73//O4cOHWLdunXYbDaKiooIBAIUFxezbt06Xn31VX7+859zwQUXMGfOnITntlqtrFmzhlWrVvHcc8/x0EMP8fbbb8cfz8jI4Nvf/nY84wbzi6QrHo+n3W2bzRaf5qdpGg6HI/5zJBIB4P777ycvL4+NGzdiGAZOp7Pbz0PXda666ipuv/32+EXT7s6RaJphV++hp89DUY4VYd0gGDb6/XVUL5cOzjvvPILBIH/5y1/i961du5Y9e/YwZMgQbDYb77zzDnv2mB0sKysrcbvdfOc73+FnP/sZ69evZ9y4cZSXl7Nz504Ann76aaZPn05zczNer5eLL76YBx54gA0bNnR6/Z/85Cf8+c9/jgfgc845h5deegmfz0dLSwvLly/n7LPP7vX783q95Ofno2kaTz/9NLqud3v84sWLmThxIldddVWP5zjnnHN47rnn0HWdAwcO8M477wAc0eehKMeCsC4J6Qa6cURzQXo0qJb+H47+qk0JIVi+fDk/+tGPWLJkCU6nk6KiIu68804WLVpEaWkpkyZNYty4cQBs3ryZm2++GU3TsNlsPPLIIzidTp544gmuuOIKIpEIp512GjfeeCN1dXVceumlBAIBpJTcf//9ncefk8PcuXPjj02ZMoUFCxZw+umnA3DdddcxefLkeAnlcP3gBz/gm9/8Ji+88ALnnntupyy/o9/97necfPLJTJo0CYC77rqry3PMnTuXt99+m5KSEoqLi5k+fTrAEX0einIsCOtmdh6M6Ljt/Rd2RXd/0ven0tJS2fFi3Oeff85JJ500IONRBi/1e6Ec7WLTrNfddj7ZKY4jOpcQYp2UMuG8YlVyURRF6Wdh3UycA5H+raOrgK4oitLPYiWXQLj7a1ZHSgV0RVGUfqYCuqIoyjFAStlaclEBXVEU5egVC+YAgX6ei64CuqIoSj+KlVtAZeiKoihHtYjK0JO0aRncPwHuzDD/3bTsiE9psViYNGkSEyZM4IorrsDn8/X6XG27Fq5YsYIlS5Z0eWxDQ0O7Jf9H4s477+R3v/tdn5xLUZQjE2qToftVht6FTcvglUXg3QdI899XFh1xUHe5XGzYsIHPPvsMu93On/70p3aPSykxjMP/lp0zZw6LFy/u8vG+DOiKogweX2XJZXAv/X9iduf7Tr4MTr8e3voVhP3tHwv74bVbYeI8aKmFZde0f/y7XTf0SuTss89m06ZNlJeXc9FFF3Huuefy0Ucf8dJLL7F9+3buuOMOgsEgY8aM4YknniAlJYXXX3+dH/3oR+Tk5DBlypT4uZ588knKysp46KGHqKqq4sYbb2T37t0APPLIIzz44IPs2rWLSZMmMWvWLH772992Gs+7777LHXfcQV5eHhs2bODyyy+npKSEpUuX4vf7eemllxgzZky758yYMYNJkyaxZs0aGhsbefzxx+NtBBRF6X+qhp6Mxv2J7/d3t/1p8iKRCK+99holJSWA2QL2mmuu4dNPP8Xj8XD33Xfz1ltvsX79ekpLS7nvvvsIBAJcf/31vPLKK7z//vscPHgw4bkXLVrE9OnT2bhxI+vXr+fkk09myZIljBkzhg0bNiQM5jEbN25k6dKlbN68maeffpodO3awZs0arrvuOv7whz8kfE5LSwsffvghDz/8MAsXLjzyD0dRlKS1DejBfl4pOrgz9O4y6vTCaLml4/3RzZM82YedkQP4/f54I6qzzz6b733ve1RWVrbrPf7xxx+zdetWzjrrLABCoRBnnnkm27ZtY9SoUYwdOxaA73znOzz66KOdXuPtt9/mb3/7G2DW7NPT06mvr09qfKeddhr5+eYe3GPGjOGCCy4AzP7nse6GHc2fb3ZAPuecc2hsbKShoYGMjIykXk9RlCPTdtqiP3Q8l1y6M/N2s2betuxic5n3H4FYDb2jtl0JpZTMmjWLZ599tt0xGzZsSNgPvC/FepxD1z3PO+o4pv4eo6IorVTJJRkT58ElD0YzcmH+e8mD5v39bOrUqXzwwQfx/t4+n48dO3Ywbtw4vvzyS3bt2gXQKeDHzJw5k0ceeQQwN5BobGwkNTWVpqamfhlvbJ/T1atXk56eTnp6er+8jqIonbUL6BEV0Ls2cR78+DO4s8H89ysI5gC5ubk8+eSTzJ8/n4kTJzJ16lS2bduG0+nk0UcfZfbs2UybNo2RI0cmfP7SpUt55513KCkp4dRTT2XLli1kZ2dz1llnMWHCBG6++eY+HW9mZiZf+9rXuPHGG/nrX//ap+dWFKV7oUjbkkv/1tBVP/Rj3IwZM/jd735HaWnC9slHBfV7oRzN3ttxiGseXwPA7In5/PHbU3p4RvdUP3RFUZQBEmmzbiV4XM9DP05t3ryZq6++ut19DoeDTz755LDP9e677/bRqBRF6Y1YycVls/T70n8V0AehkpIStWGyohwjYhdFU51WtfRfURTlaNY2oKtpi4qiKEex1oBuUwFdURTlaBaKrhQ1M3TVPldRFOWoFYlm6GkulaF/5Y6FfuiKogwesZJLmqqhf/WOxX7ovR2zoihHLhwvudgIHK/dFu9Zcw/b6rb16TnHZY3j1tNvTfr4wdYP/cCBA1x55ZU0NjYSiUR45JFH+Oyzz/jyyy+5995746+zbt06fvrTn3Yac1etCBRF6T+haBBPcVjRDUlYN7BZ+ieXVhl6FwZjP/RnnnmGr3/962zYsIGNGzcyadIkvvWtb/HPf/4zfszzzz/PlVde2WnMKpgrxwu9qYmWjz4a6GHEhXUDqyZw2y1A/25DN2gz9MPJpPvSYO6Hftppp7Fw4ULC4TCXXXYZkyZNIjU1ldGjR/Pxxx8zduxYtm/fzllnncWePXvajVlRjhfe5cupWnIPJ65dg9am7fVAiRgSm0XDYTMDeiCsk+a09ctrJRXQhRAXAksBC/CYlHJJh8dHAE8BGdFjFkspX+3jsX4lBnM/9HPOOYf33nuPlStXcvXVV3PzzTdzzTXXcOWVV7Js2TLGjRvH3Llz42PwDIJfZkX5qunNzWAYGD7foAjooYiBzSJwWs2CSLAfpy72WHIRQliAPwIXAeOB+UKI8R0Ouw1YJqWcDFwFHNPTNQaqH/qePXsYMmQI119/Pd/73vdYv349AJdffjkvvfQSzz77bLzcoijHKxkMAWAEgwM8ElNYN7BbNZxtMvT+kkwN/XRgp5Ryt5QyBDwHXNrhGAmkRX9OByr7boiDz0D1Q3/33XeZNGkSkydP5h//+Af/+Z//CZj9zsePH8+ePXvUBtDKcU8GAu3+HWhmDV3DZev/GnqP/dCFEN8CLpRSXhe9fTVwhpTypjbH5ANvAJmABzhfSrkuwbluAG4AGDFixKl79uxp97jqe60kon4vlMNx4Fe/ouHZ5yh68UVcE04e6OHw4+c3ULanjt/Mnch3/voJy75/JqePyur1+Y60H3qionDHb4H5wJNSykLgYuBpIUSnc0spH5VSlkopS3Nzc5N4aUVRlMMTK7nIgL+HI78asWmKTpsZEvuz5JLMRdEKYHib24V0Lql8D7gQQEr5kRDCCeQA1X0xyONNX/ZDV5TjjYzWzo3AIKqhW1pr6AM9bXEtMFYIMQrYj3nR89sdjtkLzASeFEKcBDiBQ3050OOJ6oeuKL0nQ2Ygl8HBUkOX0Qx9EFwUlVJGgJuAfwOfY85m2SKEuEsIMSd62E+B64UQG4FngQVyoDYrVRTluGbEM/TBEtANrBYRL7n057TFpOahR+eUv9rhvtvb/LwVOKtvh6YoinL4Wmvog6PkYs5Db5OhRwZ22qKiKMpRI15DHzQllw419JAK6IqiKEmJlVwGS4ZuLv1vXSnan5tcqIDewWDuh15eXs6ECRN6PR5FOR7EMvTBclE0VnKxWjRsFqFKLl+lo7EfeiQS6dXzFOVYFC+5+AdHQG/bLtdptQz4PPQBcfB//ofg533bD91x0jiG/uIXSR8/2Pqht/Xkk0+ycuVKAoEALS0tvP322737UBTlGGMMymmL5vpMh61/A7rK0LswGPuhd/TRRx/x1FNPqWCuKG3Em3MNkhp62wzdZdf6tYY+aDP0w8mk+9Jg7ofe0axZs8jK6n1PCEU5FsVr6INmHrrEZj3OSy4DZTD3Q+9uTIqimP9ttk5bHDwZuj1WQ1cll8FnoPqhK4rSPRkOt/48aDJ0cws6gBShM37dKoLRGNHXVEDvhYHqh64oSvfaBvHBtLAoVnLJ0P18462/4Svr1F28T6iSSwfNzc2d7isqKuKzzz5rd995553H2rVrOx174YUXsm1b59k5CxYsYMGCBQDk5eXx8ssvdzrmmWee6XZsbcfR9nyKophkmzLLYFhYJKWMN+cCSJHmFGPN7eqX11MZuqIoxwwjOsMFBkdzrohh9ii0R6cteqRZEtJc/RPQVYY+CKl+6IrSO7HWuTA4auhh3ZyiGMvQPboZ0MXxEtCllF/pTJHBSPVDb6W6MCuHI1Zy0VJSBkUNPRwxf3/jAd0w/4LQXO5+eb1BVXJxOp3U1taq/4gVwAzmtbW1OJ3OgR6KcpSIBXRLWtqgqKGH4hm6maS6jWjJpZ9q6IMqQy8sLKSiooJDh9RmR4rJ6XRSWFg40MNQjhKxGrqWkY5e3jDAo+lccnFGzC8Z0U9JyqAK6DabjVGjRg30MBRFOUrFauiWtHSCfbCwSG9qQvN4EFrvihmdAno0Qw/bnTiOeHSdDaqSi6IoypGIzWyxpKWBrrdbaHS49KYmdk6fQdO//93rc4T1aA09Og/dETb/gghZ7b0+Z3dUQFcU5ZgRa8xlSU8Hjmz5f2jvXgyfj9CePb0+RyxDj01bdERLLkFrf+TnKqArinIMiV8UTU8zb/v9vT5X5MABAHRvY6/PEQvo1mjJxhYJEtKsBPqp4eKgqqEriqIciVgNXUs78gw9fMBsf617vb0/R6yGHi252ENBAhY79n5q0KUydEVRjhlGm2mLcGSLi8LxDL33AT0Un4dullys4SB+q73fOi6qgK4oyjGjtYZuBvQj2eQictAM6MYRBPSIEauhm6FWBFsIOf2sq/641+fsjiq5KIpyzIivFE2NZuhHsFo0XNlzhl5695vUNIc63Z+TYqfstlmdpi0SbCHoCFLtq+r1uLqjArqiKMcMGQoiHA40l7lw50gadIUP9lxDTxTM294fK7lYLYLSu9/k5j3VWFzw+PsHefTVlUBr8O8LquSiKMoxwwiYAV04zIAue3lRVEYiRKqrAdAbj3yWi92iUdMcwqmHCNoE0midttjVl0JvqICuKMoxQwaDCIcdzWkGzN5eFI1UV4NhYBs+HBkI9DrT71hycUWCBG0g9f5Z+q8CuqIoxwwZCqLZHfFeKYa/l4E4OsPFOe5EoPdz0SMdVoo6I2ECNsBQAV1RFKVbRjCEcDrRHI7o7d4GdLN+7jhxnHmext7NdOnYbdGhR8wM3eiflaLqoqiiKINGT7NGehIrucQy9N620A0fqATaZuiJA3pOir3TeFNDLbgzzFk28ZKLFsvQIwTtIPspQ08qoAshLgSWAhbgMSnlkgTHzAPuBCSwUUr57T4cp6Iox4GeZo30RAbNkkssQ+9p2mJXXyA/2bqWr6elYc3PB7oO6LEvmQvu/192VjcjDYNlHy0lZ/6VwIXtVormuq3YdYOAVQPDFj9HTkrfNerqMaALISzAH4FZQAWwVgixQkq5tc0xY4GfA2dJKeuFEEP6bISKoihJMqLTFrHZwGLpcWFRV18UqY212PLzsaRnAD3X0FuCOqcVZbH3sx1QX0fzu/9L7qJFrd0WLYKPf3IWO54BXA7Kl1xy+G8uCcnU0E8Hdkopd0spQ8BzwKUdjrke+KOUsh5ASlndt8NUFEXpmQyGzJKLEGgOR69nueT6GrANHRpfcdrT8v/mYIQTh6ZymhkCCXz+OXpDA6FIa8nF8PnMg139Uz+H5AL6MGBfm9sV0fvaKgaKhRAfCCE+jpZoOhFC3CCEKBNClKldiRRF6WuyoQpt7/twZwbC8GFUbkl4XOndb1K0eGX8dkrIx7e3vYFNjwCQ62/AWpCPlpICFgu6t+vdj6SUtAQjvFBWgWv/3tidXL3oTyxd9QUAmibinR/7a4NoSC6gJ9qxueOmn1ZgLDADmA88JoTI6PQkKR+VUpZKKUtzc3MPd6yKoihd27QMo+EAwvABEqFFkLs+gE3LOh3asdQy9eAWrt72BufuW4fHtZa0sA/b0HyEEFjS0jC6WVwUjBhEDIk/rDOy6SA1znT8FjunHPqi3XFGNKBrgSq4MwPun5BwbEcimYBeAQxvc7sQqExwzMtSyrCU8ktgO2aAVxRFSVpXFwiTunC46i5kBITFvKlZJEbYgFV39fjUzEATAHP2vMXwlBcBsOYPBczOjXpD1yWXlmAk/vPIxoPsyfezdWgGk2p2tjvO2Pwv83wiBEjw7oNXFvVpUE8moK8FxgohRgkh7MBVwIoOx7wEnAsghMjBLMHs7rNRKopyXCi7bRZnnZANwKLzTqB8yWzKl8xOrteJtwJpmIEczMAudQHeih6fmhUN6GPq6/naFjMsNnjXwv0T0Jq/QN/0apeBtyVotsLVDJ0RzQepGBKmbvRBRjRVc1XwrfhxxsdPAGC1tNndIuxP6gsnWT3OcpFSRoQQNwH/xpy2+LiUcosQ4i6gTEq5IvrYBUKIrYAO3CylrO2zUSqKctyobDAvZPpCyfUMj009XG3PRuoCEQ3omkVi6ALSC3s8R2a4hppUgSsE31oTAgRbdj5OkWjGYs9C94XMbBpg4rx2z22OZujf8b+B1ZBU5GhU5FqZ9b7BT+tfwFfgBGYjvTVAFnZrh4p1El84yUpqpaiU8lUpZbGUcoyU8r+j990eDeZI00+klOOllCVSyuf6bISKohw3pJRUNpi1Zl+Sm0DE6uH3RuZhtAnowiLRIxq/bJzb7fPnaKuZFN5KdYYkdXQLIiIwgLIss8xjsRvoIa3LbLolZAb077a8DsDeXMHuPIFhNwhXa9xiNTN7w2b+5WG3dNh/LokvnGSppf+Kogwa9b4wwehUP3+SGXrMvyJnghQIzQzoEauFikgOf/dPbT1o0zK4fwK7nf/BavsifmV9nCW2x7AFJF6PoHB0EyDxeSTr3eb0wnhAh4TZdCxD9zQGkEhaMnXcSCqHSXxVDgqEWayIjLkYAEfbgG5zwczbD+t9dkct/VcUZdCIZecAvlCkmyM7sxnm8bEa+i5RgGz7nbBpmVk2CfvRgEKthqvFW2gCrD6Bzy3xpOqkDg9Qq9nYbXfSoGlY7BIjJJASREbnbDp2UTTYkkpDuqRIRvD4DdaMslP4JRz051AABNLHAOCyGBCOloJm3t6phHMkVEBXFGXQiAV0u0VLuoYe44jOIV8jxnGe3IjNGkHoberVq+4yyyZtaAKMCNhDgojbzJyHnVnPQZcDcLLB4WCi3QAEhnRjSZBNxwK6P5hDeW4No8NhRofCrMx1AgZWXxg2LSPYYk59dGkG3FYN1r5b8h9/P31+RkVRlF464DUviI7K8Rx2ycVuhAF4ma+x2ZqJ3RLBobeZb97FxcdIIDrP0WUGdKHBhFAIq5SsdzrQbOb9+rQ72mfT0fLNvJUTed+yCL2qjvJcQVFYMNUfoMFjLuFx+gPwyiKC+zYStkCKzdkvwRxUQFcUZRCpbPBjt2gUZrqSztCz3GajqwzdnHpopO/ghkIXNksQu9GmbNPFxcdYQLc5zcBtSHAYkhOCBv+0j+QG7acA6PnTWp8UK9949yGQ5LY0gG6wL0cwRjcoikRwOszx6wHzgmpk7waCNvA40pL/QA6TKrkoijJoVHoD5Gc4cTus+DvMcumqM6LHbgbkmbZtAIi0XYQ12GAdwdf0Xa2LkmbeDq8somE7NFc6KZxWj0/aacg5F9iM06FTYeRwb2QeK4xpOFpexZb5Aac5EmwW3aF8E2o0Q+m+XEGxvxEBTBABDJyEol8Y4UCAgM1B+tBT+uKjSkhl6IqiDBoHGvzkpztx2yydLop21RmxJaRzY+Y6fiT+AUDAYZZt9toysUiDtbfOMA+cOA/OuZWWGg9NFS4CpLI4fB31Q88F4GnXt5gWepAVhpmJ676RCE3n2oxnzNtt+7lEFzHVbfdwcH0atZ+nYAiJP0MnxZkHQGkwQLML6sNmsNelk4AdUmbeceQfVBdUQFcUZdCobPBTkO7CZbckXXKZo61mke8P8d7nRrTuMMpuLlY32m4UbXOh+83SyidDfsgKYxrhmv0YwC+umkn5ktlsv/tCLJpgwakzANiaapZ02vVzSS/EX2un6tN0vLvd6GGN9SWSkVLHe9Yv8Uk7wyIRGjwQCFjA5sJwDiFogxRbSu8/oB6ogK4oyqCgG5KqpiAFGS7cdgv+kI6UHfsAtmfPeYvmwpeJWMI0SDOcnREySyHDHOZF0HYtdA9uQo+YJZgan9kgUN+7mUYP5GrmN4HDamFktpuKGgvZzmy2RwN6u5LLzNvxBc1SimdWPWO+Uc3DF1kZY8/CPuUqFoevw2PJxusRhIMWuORBpLSbAX1D/627VAFdUZRBobopgG5I8jOceBxWIoaM78nZFYt7F2s9Fhbk5/GG0w3ARYEW7IbEazdnmbTb5OLgJvSIuWDIvXcbQkj0mkM0eCAnY1T8sLFDUviiuokTMk9gR4oHYdfaN+gacSY7DfOL4a5hGVRYLTRrGmN8TaR9sZyVnM0/J76A1wMyYDPLPX4/QRu4+jHsqoCuKMqgEJuDXpDuwmUzs9+epi4Ki59hIYP9Visf2M2APsyIkKPrNNjN8Bbfhi4Sgupt6NGbE7c9z3BbEzS00OAR5Ka2zoIZOySV8lofo9NOYJdFYHEI9LYll70fURutja/NtPPTIWa2P6bpEOKVRVzl+AivD3wugcMXnQ7pD6LbQHj6r3W4CuiKogwKsaZcsZILtG/QlaiFrrD4MXwjebiyjhGB1pWiObIyi1IAACAASURBVIakPpqhR3zR2SjefUjNieE356sHm6yMtx7E0hSmJUXgtLZu3Dw2LwXdkGRYh+MXEiM7r33JZc+HNIftBOxwdXMTnzvMsY0JhSHsZxHP0uAL0eRyYg+bvdC1QAjdCniy++YDS0AFdEVRBoUDXjPw5mc4cSUI6GW3zaJ8yWx+fH5x/D5h8bFHH8Vn/rO4psYM1CIjn5pIIbXRDH3eg+9StHglRb/dxkz/AwCErOZUwxPFfhwtBuHU1k2bAU4YYl64FGFzk2h/uqf9LJeCyYTDdkJOyaL6Bk73BygIR8gyzGw8V9ZQ1xKi2pkFQKS2FkswgrRKcOf0zQeWgJqHrijKgOo4v3zinW/Ef05UcvGFo9MZRRihhRG6nfmWt3k9UspY9iFuWk35U7eQaT8EhLDr4fhzww1m2WRnPozfp1Hi3Y7FEBiZqe1eY0xuCkJAU5MZkJukl7T61rF4J1yGteUehEtgBR45WE2T1pof11tzqfeFcNnNLwb9YCXWkGHuJ5pW0LsPKgkqQ1cUZUB1Nb8cEjfo8od0Mtw21v7XWQDcNX04dqFzVsl4AHNz6Egq/uhKTbseZo62mi2O77JMLAZge6FZjsk7eBAA67Cidq/htFkYkeVmT41BgSObusgB9Ppac4XofSex4/5i0vwSR0Y6PmnHDmRHs3NsLt7M/z51LWFcDvPLxL9rPbaIhKwCSBnSuw8qCSpDVxTlK9fVqs+OEvVE94d03DYL3qAXW0SSXlcFgOEwSxnCbkdGUglFo9vX5Gaut72OW4TYKNPIBLYPE4Ckpi6boTTjyB3a6XXGDklhZ1UzY04aTZX7ICc2NsW7NW5LS6XIB6kp1bygn8Plni2kBg/GOyjurZxIw67dVFuHALto2LUVAK0fN4gGFdAVRRkAyQRz6KrkouO0W2gMNXLDawb5/nfhrFSk5gGrFWG1YkTSCEWvcc6RH+MW5utV6DYyAUd6hKBdI6dyPwDuIe0DetsvnHLDRa5HQ0YMDH8AzQrbbDZO8YHHFmamtoEV573Nf5wxMv78LO9uIoZkv92cOdOytxwroHl3HcandPhUyUVRlEEr0WpRf0jHbTcz9BGHJNZ9Tci8CchQCM1uzjaRkVSC0eucaUZz/Lk1ETOHHSuC7M8Ca3RmTPqWx9vtGdr2C0cPDuVQanTGjN8MmbuFHZsOVodOgaglxdE+N850RxcvyeE0OyF0sAYAm71/Q64K6IqiDEpztNVc+Ob5cGcG3D8hHnB9oQhumxVvyEuOF0RIEjnnfzCCAYTDXDSUac+Ol1y8EfPCZBhoCVswNEmxDLE/2wzSQStkBw+Z5ZQEG0GfHT5AbbRBYrjFQgioi34xWBwGlTIbj71DQPeY3yaRcCZeN1BjfqlYXU76kwroiqIMOnO01SyxPUZK4AAgwbsvHnD9YQOn3ULTjndIjS4SCj16NfLA9nhAX7P4MiI2M2CXhYuRwsIOux1XQGA4JOND4XhAb0iBXF1PuGfoHG01v5cvUB+dBBP2Wdhpt+H2mc8Vbiv3Rubh6SJDx3DS6BE46s2/BOzu/uvjAiqgK4oyyJQvmc3S3Ffide+4aMD1hyJMD7xDsOxf8YdCB+qQ5Z8gMJ9j0SykOTwYQnIokk71+Q+wISOPFD/YnRpF4RCHMs0+MQ2eaECHTptg3GJdRhYh3E7z8bDPwna7nVS/+dy9k25ghTGtU8klyxNbBCXwpbqwRHdOcnj6rxc6qICuKMoASLTqs+39oovdhfBW4AvpzK37K0Zza+OuUJMVGdHRQnXx+3JT8onYLKAL9uTPZuPJF5MdtOEcU4olrRBnqpk1N3vAHWsC1mETjAJh1r5HyTBNbknEZ2Gb3U5OSzSgF10EgMdhafe8TE/r+wumtJZZnEPHJn5ffUQFdEVRvnJlt81i4x0XAHDb7JMoXzKb8iWzKbttlnlANLDWfeFm7zttlsqnF+IP6WRGqhEtZvjSbAahJiuGLuIZOkCOAWGLweW8z+RnSthY+SHZITuWzAxapv2SLHcEAwi6osHc5jI3waD1i6VSmlMhx4bCVKUJVksPy1M9jI5eZ/U6zBJKxww91WHFqkXLMq7WBmPOUWccycfWIxXQFUUZEBX1PgAKMxPMzZ55O0FsNO5x0VLlwNCJB1x/WMdrz8PSrCGRuIcEzQxdF4joLBc2LSP3wGaCVpC6oF4GqAx5SWkOYcnIoOGEy9gY+hp//brG9gk6pA+HSx6M7xkaazNwoPRmfNLOVY1NZDrDjGgwOCMYptRZjLDbaRLmxc+ONXQhBBmxOnpaRvx+d2pW336IHaiArijKgKioN3u3FGa6Oz84cR6vaecTqDODYtjvNHuKl1yBP6zzwcgf4GzWaPGYc8pDzRYM3YI2ZIz5/FV3kRMO4beBrgvedbtASmwtZkD3hXTW+c/lzSkaTJ8LP/6s/QbQUe7S+SwOX4fTMZQT7CEymjUePPPXjPCMx5KVRUtIRwjizcTayorOdNGyWleGeoI1R/qxdUsFdEVRBkRrQE+8evLt4IVIwyxbhJslDD+dQNhAStg77Bu4fRb8KRJ7qg5SEGpxI7JHmE/2VpAb0alNEwR8FlamuBnfHAaDaECPYARzcVtTGZk2MuHrg9nT5V9yGn+a/BK2y36FEQZj1EXo9fVYsjJpDup47FaEEJ2eG8vQrbmt50/J7fq1+oIK6IqiDIiKeh8pDivpLlvCx8dWta6qDLdYoPyDeG8Xt91CWhOEhhfh+PFrABiBUHzaIumF5Og6+7Mh1GhjvcPBxXXmF4glIzO6AtXCHVP+ysIJC7sco9NmoSjbw46qZmz55mrS8IEDROrrsGZk0hKMdLogGpMVDejOYeMAiGiQkjUq4bF9RQV0RVEGREW9n8JMV8LslqotXPjli3jT0sFiIRxMgT0f4o/2dnFoOpleA31IJvaiovjTtGhfcmbeTq6wUpEj0MKCrCaY3mhe/LRkZtASXYE6PLUAl7X7/irFeansqGrClm+20g0fOIBeV48lK4vmUKRT/TwmNtNleOQAAAEbuP48PeHipb6ierkoijIgYgEdMIPcqrvMeeDphciR05CHDD47wclJ+5tJDXlg07MM2/D/2OnQaHxFUBXOQziCWDIysGRmotfXI+zRDH3iPHKC9VS8/gcAzjxgkDPx/7DvX0/HSy5AvO96d4rzUnhj60H0HLMPe+TgQfS6OixZZobecYZL2z4wc7TVnFT2GF/YcwjZQMQWSEXH2NdUhq4oyoCoqPeZF0Q3LTODnHcfsVWhofdfQA9p7B1ezcE0nYb6FjB0BGAVBt6AWaaxNm+ETcviWXq85ALkTlnA/uiMR6OyBD1zImDW0GNNvxJdzOyoeGgqhoQ90glWK6HyPRgtLVizssySS4dl/237wNxiXUaaCNHshkisspRgRWpfUQFdUZSvnNcfpikQMTP0VXeZQa4N3yGzXPFhkcahdEGopX2oavGbgdjpCMCqu7CPMmvTwtG6oMdhcSAzUml0QV6VFX9tLWAG9FjJpWMwTqQ4z1z3/0WNH+uQXAKff26eJzPLvCjaRckFWhcmBdySiK11IVTHFal9JamALoS4UAixXQixU4hoh/jEx31LCCGFEKV9N0RFUY417eagR4Obd48T7x4nUoK/xk7EZVCVCcEUiaNFIFvX5xDwmUHU7YqAtyKeoWuO9s2vhqeNoDYvlRGN9TRXm6tILWlp+A+j5FKU7cFmEWyvasKWX9Aa0OMll67PEVuY5BsfoO7kNq0MOqxI7Ss9BnQhhAX4I3ARMB6YL4QYn+C4VGAR8ElfD1JRlGNLuzno6YWE/RoHPsmk8qMs9n+QSUu1nYoCSbZucKLdjyYF5aHW7DvssxDRINWhQ3oh9iJzOmDbkgvAQ+c9RPbYsxjRWIW/phYtLQ1hteIL6Vg1gd3afQgsvftNim97jbAueeTdXbxaZWA0mtvYxUouKc6uM/R7I/PwSTtzc+q5Is/8C6HtitS+lkyGfjqwU0q5W0oZAp4DLk1w3K+Be4FAH45PUZRj0L66Nhn6zNup256KlJA9rommSicRn5W1IyxMDQQ4yWaGlA2R1gVIhk+jLhUyLHaYeTuOBCUXgFx3Lp6x40gL+4h8uRtLhrlq0xftqd6Tjhtx1LjS4z9bMjNpDnY9ywVghTGNxeHrqDByMKTotCK1ryUT0IcB+9rcrojeFyeEmAwMl1L+i24IIW4QQpQJIcoOHTp02INVFOUosmmZ2ce8Qz9zMDP02Bx0PX8aDTtdpI0MM2RSE0Xn1yBGBXjrZAtT/QHyPGaY+jLiQQIRqaG1WKhJg7SL74OJ87AXFZF+6Rw8Z3TulZJ6ojk7xbZ9a5uAHsGdRP28o2pXZuuN9AyCEYOUDufp2HhshTGNaaEHOd32QpcrUvtKMu8owSRR4tV9IYQG3A8s6OlEUspHgUcBSktLZQ+HK4oyiHW1L2hOip2yOfXx/TeB1n7mABPntZuDXv/Qf2NENBpuuong2Zcyek8Za1bfRb1HMLLWTcPFv4AVv6W5xcIf5rzHfW9U8GL4Thqyg9hOmQ+AsNkouOeehOPMObmYvYAlFMSSYWbYyWboHR1ymV8IutAYd+8HIDR+/+YOfv/mjtb3HmswNgCSydArgOFtbhcClW1upwITgHeFEOXAVGCFujCqKMe2rvYFrWkOJZy5QtjPgX/+nKLFK3nr8yqKq19j3y/GUPfSmwQLw3yn7knmvTKPFR4nHxefQ65jOHP9S6kZ+01kdgY5Xp0vmtejSQNnfQstWcnt/uMeVoDPatbW25Zckrkg2um9RQN6o92NFJ3DZ7J7pfaXZAL6WmCsEGKUEMIOXAWsiD0opfRKKXOklEVSyiLgY2COlLKsX0asKMrg18W0vDxZyxxtNevsN7DU9jDu3X70oIV7ptsZGwwy0TWUX67+JR9UfsBJGVMA8Id1XIUjyW+yUu5bS57egkWXBLKT2/1HCEFVlrnKs23JJZkpix1Vu83ne+2ew37uV6HHdySljAghbgL+DViAx6WUW4QQdwFlUsoV3Z9BUZSjWcfSyhxtNbdYl7HbUUOlzOHeyDxWGNPaPym9MLpQqL166WGJ7THcIkQkqFG9NZVNoyAwVOfxA9WkNlu5b9o1PL31aU4bcjYrieAL6diHDaOgcieVoY0UhU4BIJKT0en8XanPLWRUdTnWTLMG7g/pre1tu5GTYm/33ptsbgIWW7wP+mCT1FeUlPJV4NUO9yWcdyOlnHHkw1IUZbDoGMxjARmgUNSwxPYYhGkf1GfeDisWQaRN2cXqQoSJP/fQ5lSMsOAf58FfDlaTaRjgreCW027hhpIb2F8ngNX4QxFswwpIfSNA2AhwsncTAHp+m40veuAbWghbWjP0lpBOQUbPJZe29fDrnlrL3jofGSeOpap5cGboaqWooihJu8W6rNNen24R4hZrh4ZTE+fB6dcTaraw770sIkEB4+eQgbnVT6DBSsMuN/+eojHN3kKOHl01FF1wk+HMiM9C8YV0bAUFiIjOsBrBNzauZttoO/rIgqTHrQ8vAloDuj+kH/YslzFDUiiv8ZH/yCP8uWTOYT33q6ICuqIoSYstZe98f2385/i0vQt+TfP439Bc6aSxbhQ0VnJQZiElVH+aTtgOL04TXNEY3c+tw4Kb2CwUX0jHNsycKf391614AiGeOk+Q7ky+5KKXTOaNEadhnVIaPWfksGe5jMlNIaQbHLB4cGemJzymq71Svyqq26KiKEmrlDkUJgjqlTKbDxefR0FG+1a0of3mepOmcgtZ+e+Tr4G/xkZLlYPl58LpMsDQiE69SCXzkvvazdGOzULxRzN0gBMrArw9UbArV2eOI3FQ7Sh+DWDKldx//5r4/S+uq+DXl01I+r2fMMSsm++qbuaNH09nyq/fZPFF47hx+pikz9HfVEBXFCVp90bmcY/tMRrWumg+4MTq1BFuyZ8nXsIFVU2tAb3xAPzrRwQ/N5tg+fY0E56kYXMZNOzyYNgk/5ps5aGgC/HNv5CZaPs3W5sMvcDM0INWG8+fY5Zn0u3JBfSuphLGeqsna0yuGdB3HmrGFm0ZMLEwuTF8VVRAVxSlW21neqwwpjFd30RJxRdYHAaaQ+Lb76Ap280XVU2ce+IQc0Xoa7eCv46m7fmU5wvGHBA07XORXuSjca+T9SdLCqxw+vWbIdEGF4DVomG3aPjCETS3G/eZU3lBFNKU+ibQQpoj7Sv8FCDdZSM31cHO6mbCEfNLZcKwwRXQVQ1dUZRuld02i8kjMjhleHSVZEhDD1l4c+RpRM620Wh3M7ahgh1Vza29zf116CGBpUXw8Ykae3Nhf4WbmnI3UtdYdqqNa+rqE+9W1IbLbon3Lh/5xBP8u/hcsi1mmSTZDL0vjcn1sOtQM5v2exmd6yHNmXj7vIGiArqiKN2qbwmxYV8DM4pzAShuMeeX/6/nFJbql5OZ1cw3vB9yy+ffNDPz6ArRL/3m6sxxTj/biw2s1TZ2f5HGFwXwLVsDl1syE79gG267BV+otTTiD+sMs5sLjnLduX36PpNxwpAUdlY3s6migVMKk78o+1VRJRdFURLquKBo6aovEBgUNNdQSyrFqfu41baM5iwHtZ+nkBM6gDRamz+tCXqYDMyyNmJkCRrIIaNJICa1cIk/grik5xaybTN0MGenFHum87Nzz2ZUev9uuJzImNwUmgIRmgIRSgZZuQVUQFcUpQuJLiYWikOIZjCAnBFv4faF0LM0kIJggw1XThgAnxAcaHEwUZMMcYcRniyasyHSZDA634G45HdJdR00M3RzMwrDkATCBm67jZLc4qTfR8fVnjHprsMLfx2/4O7611bu+tfWAW/I1ZYquSiKkjSvTGF10wnUpMOTuTaqLBacWWaQ89e11pNf87jJrROIVB3hcMFF91D4l39wx7RFPDjln0m3kHXbrPGSSyBi/nu4TbXKbptF+ZLZvPuzGQBcNGEoAI8vOO2wztNtM7JBQgV0RVGS1ogHi7+ZqiwNQ8Dj6WlYXQYWh06g3lxUI4Hn0lIpqjFIy7LGN3RoGDaKNWlFnBjdozMZLrslPr3QdxgbOycyMttNlsfOh7vMRVAu27FXoDj23pGiKP2m1LqGgsYWDk4sZk5OGi/yKQu9jTizwgSiGfpndjtfWG3kNERwXPl9SldkUvPMyvg5/uvlLfzXy1u6LVV0LG8ULW59vsvWu4AuhGDKiAze+rwa6P0Xw2CmMnRFUZI2yfMCniCcdMpMrp/+Gwxh4a/pabiywgQbrRgRwbK0FEbVGAgpcIwZ06tSRXeP9WanoZjJI1pn1qiArijKcaNtX5I52mrecP6Q7eb1SfLsjRSmFnLp2Lm8kJaCP1sHKaj12njd4+Yb+819QC9fWZno1EfEZe9d2Cq9+01+++/t8dun/88qihavpPTuN/tqaANOBXRFURIqu20W44am8vPCzTzoeYKy1CDpDeakRPvnD8OmZdww8QaktHJ3sblq89MWDwFN47SD5krKipS+nyve29r3kV7U7Krx1kA35GpL1dAV5TjS7T6gHerZYd1g96EWrvI8AWE/72bnckqNDgJsjhZYdRcFEz+juXI+Hw17mu+7dKq9Tk4M6HxZN5Z0VzPB6NZvfWmgSiWDZWpid1RAV5TjSHdZauzCYyy476ltIaQbpIWqaBGCtU4nV9YEsaeA0IhvM5chp9BYu59tBW8xag8Uv1tCam0F+1Lz+uU9HIu1776iSi6KorQTC/rbDzYzR1sNQuNjl5OIEOTVgS0lWkiPbkZRdtssPv/x/dSdNR6rLrj6003k++r4IqOw29fprlTR3WPOXs5yOR6oDF1RlITE5mUssT2GkDrvuV2k6DrWJg1HbqTTZhQWzcJNP3+BUc3LsYc1MgJN1Lg6L41P1DM9kbbljRm/fYeTC9I5fVQWd6zYojL0bqgMXVGUhM748iHcIoQE3nc5mVkTREY0bGlGfLFQW0IIctyphCw2qj1ZGFpr4HXaNNx2C0PTnIc9jlE5ZofD1oVFvctDj4aLmkdKZeiKchzo6mJodzIjh5AGfL42i9k7BaUWM6DaU8JdLt2PZdZ7a32c89t3+M3lJcw/fQTXPL6GupYgmtZ9u9xERuem8NHu2nhPF4e1d3no0XBR80ipgK4ox4HDDeaBsE6tzCanqQHxpZOLv5SAWSqxF/Q8FbEw04XbbmH7wSbA3LattKjndrmJjM71EAibM25cNkuvvhSOF6rkoijHmTnaat6u+BHvfvhD3rctMi98djDuv17ngfBlBBrMnO/JK3QKptaTV+rDdmnPbW81TVCcl8q2g434Qzr7G/zxLdwO1+gc83lbKr2qft4DlaErynFkjraaJbbHOLQ3FV+1g5H1h/j9kMc5NbSDq7O2gbeCSpnDvZF5eEmlsdGGLmBUWoD0U3LMC6FJdko8KT+V1z87yK5DzUDrJsuHa3SuB4DyWh+FmT1fUD2eqQxdUY4jt1iX4TJC+GvNC4GNe13YjABXW95Ca6xAE1Co1bDE9hhOgqz2FnMgC37r/yn8+LOkgznAiXmp1PvCfLzb7G7Y2wx9SKoDTzQz721jruOFCuiKchwpEDX4621IXaDZDZr2uZASOpal3SLEUtvDjG2sZF+uQA90P6c8kROHmu0A/rXpAJow29f2hhCC0dEvA1Vy6Z4K6IpyHEhzWpmjrcZAw3fIXI6fO6GJiN+CvybxtD2pC9yN0JglmUPZYb/muKFm3/MN+xoYnuU+ogVBo3LMssvhbm5xvFEBXVGOA78Yvpl7bI9hFQa+ajv2tDDpo3wIi6Rxb+K54QGvFYHAmR7iFuuyw37NTI+dIanml0dvyy0xsTr6kbTOPR6oT0dRjnFSSqbv+xMuEUIa4K+xkzbCT8QuSckP0LjPRd7kRrM/SxvVTWbmnpcapEBrPqzX7Djv/e1t1RQtXtmr/Tfbnit2HkjcUOx4pwK6ohxrNi2DVXdheCuoNLK5NzKPB2yHQEDQa8UIazx/goPlw4fx1Kh69AoXvho7niHt56pXNTmw26DYEUBzH14NvS/33zwa9vIcLJIquQghLhRCbBdC7BRCLE7w+E+EEFuFEJuEEKuEECP7fqiKovRo0zJ4ZRF496EhKdRqeMD2MLFrng01Zgnk5dF2/ELwy0lpCKtB9YY0GoMW3nM50aPHhrw29udAMdZ2fVuUwavHgC6EsAB/BC4CxgPzhRDjOxz2KVAqpZwIvAjc29cDVRQlCavugrC/3V2aACHAJwQfNqZxKA1+Gqrj3upaNqXY+d8LIvi9Nja8ncfdtlyeT8thv8zBVafRnC2wJejbogxOyZRcTgd2Sil3AwghngMuBbbGDpBSvtPm+I+B7/TlIBXleNLdJhSQuNSQk2KnbE49ePclPKcEbsvJ5or9Alt+kHOs2TD2Am748iUeOsXDv3Mkv3ghwm/+FuHu/0jnWdst3Ov7FZaJpSqYH0WSCejDgLa/JRXAGd0c/z3gtUQPCCFuAG4AGDFiRJJDVJTjy+HUjOdoq7nFuoyCcA38s+seJ39JT6N5n5MMn0Feph9+vBuAH2w8ncBHd5PnbCD7XAe+1en86EU/y2c8CUD2yZOP/A0pX5lkAnqi3xKZ8EAhvgOUAtMTPS6lfBR4FKC0tDThORRF6V48iIsaoO2ioNb/pELNFup3ekgb4WfNMAvrD6Tyn6/ouLLDtAx3kxU9znLKVdx8ylW8uvkA5/59Pa8uySbjhwtZsHIPAGOmnNerMeak2Lv9K2OgznWsSyagVwDD29wuBDpt5S2EOB/4JTBdShnsm+EpitJWrBeLW3Q/w6P+Cw9121Oo25aCLw9+WGXgHhIiZ1ozv3F8n//ucHxemnmxdEvaMF4p/SY//+RFmtyCE0eW9GqcfTmdUE1NTF4yAX0tMFYIMQrYD1wFfLvtAUKIycCfgQullNV9PkpFOU61zcYrZQ5uEegxmAO0HHTgyA3ywskOTt8E9mEhRn7dhXbhEv47QU08L7rxxJov63gvfypzrg6SbQ2jaWrt4dGkx4AupYwIIW4C/g1YgMellFuEEHcBZVLKFcBvgRTgBSEEwF4p5Zx+HLeiHPNi2biLEFKHQmsNMkGhMoxZbIkVIMI+jaDXxidf0/l/U608OmwhmWICZbd0nekOSTUD+idf1gEw5Ybb4/1TlKNHUguLpJSvAq92uO/2Nj+f38fjUpTjVqxmfIt1GW4RonpTKt4v3Yz5RhVah1YmhoTnd+bRJC1cMr6KgrBO80GzfPLCSXaChy5Abymmhu6zertVI9tjZ2+dz+y4mNm7RlrKwFJ/TynKILNy0dnM0VYzTNRg6NCw003Eb6F5v5lFx7J0acCGddmcus7CjPVwhzWf/yO/y/O142nwwC7bZEK1M5J+3VjZpSDDhb2X27wpA0v9v6Yog8WmZXD/BIbcl2eu7hTQVOFED1kQmqThy2jWLMxgXvFJBq6dDlZNdSFdDs5cb+GDER8yufIAnw7LJnBwHofzn/jQdDOgF2V7+uHNKV8F1ctFUQaD2JL9sB+BubIToGGXB4snQmR0kJbP3IR9Gja3Qc22FJr3uHlmukbT7Jv51gm7+dpzz/LeCSHS/LA2dRbJBvOOC5lW76zpdSMtZWCpDF1RBoMES/aDjRZ81Q7+PUlj0dfSQAq85W5CTRYObUmlrBgqJ0ka604g69prEIbk+yttAGzILU76pVXzq2OHCuiKMhh4KzrdVb/bgyEkL062MtQdYutw2LMnhXUbsvFZBM+cL/hlXTVbKr3Yhw8n9fzzyQo2syutgHpnWrtzqUU4xwdVclGUPta2hNF2HnmVyCH/8t+YvVGiLW7x7gNhoePi60hYUFXu4dOxGt8Lefl2TTNPjc3B/bYNt1fjvXMjPOmtxW0bQlVDkJrmIFnfXUDTG2/gOPNMANb8cmZ8OqJyfFABXVH6WNtgvsT2GE49hLfczdDRNWadfO/HsPGZyVf0nAAAFs5JREFU1hKL1OPP3W+18HoolRPedpETFDSdHOAmkYEonce1Zc+wzZqBnqVzw5BqhOZix6k3wyr4/EAjZ0+ezLAHHuCp/U6GVkVUMD8OqYCuKP1gjraa+2x/wioM6nZ5qPo0Hc0qSS/yQ9lfEz6nRQiWNuTx7dchbIcnv17MC/J7/PDH5ho964ipnBD5FVa9CpExnF82zuXvq4YCcPVf18TPYxHNnHdSXv+/SWXQUQFdUfpAxzLLkuj+nQAN5S7z3y/dZkDvwtMig/94HSyZYRacejcNztT2B0ych7PNsv2/R7di+//tnXmUXFWdxz+/2nrvdKe7g0kaSAIBSQIECEFDWATZRRwdgVFGxnXAQQccXEEFhjmDcRZUPMdBVNRRJILMyQyILGGLbAkhISQhZBWyJ93ppPeu5Td/3FvdryvVnep0pbtT/fucU6de3Xffe/f33r3fuu93t0ySCidOHJNz2m3yq8LBBN0w8kBQENMjPAE6miJ07onRXpmCHTHirWGiZcn9jl8fiVC9qAQRpfJ9SlO4Yr84A2Eggm5dEwsH6+ViGHkmPa0tQOPGUpIh5faPRgFhj6+tB1Hgkd1jOWkTlJ/Uxr9FB7+gxIwBCLpROFgN3TD6ob/Vg5bcegGz7nySOW0LWRTrmRGxiXLG0kIqBdvfLWPFsSGmVbTx5tHFTP5LOXXTWkh2Ce+uGMP2UIRVdWHOfD3Mvjrlt0d9ggWpuYNO9+n/8lSvdBqjAxN0w+iHAw26mdO2sNf85PXiZkRUhUf3VXFsmxCb0s5tO/fy0DHVlC2Mce+ucUxfGqayWaAI5rwNCRFunP1l1mvvlbwG68e2wUGjCxN0wxgEQX95mpTAPdVjqFhexsTiFHPKQnw1/veEy+PMiD7K2QsjtJbATy+dwqOlV1DaXEUsmaCxxLlJcq1V99WYaYxeTNANI0cyF5v48rcWcXd0d684+0LC1+tqia4t4vq3UoSPT3FO8oduZwgqJ7dz4u71zJv1SXaGx0IntPhK+EDdI8G4k/ro8WKMLkzQDSMHMpd+q5fd3BW9jybKqexoY/OiaqiL892zSinbGOa6x5KUje9kwol7INFznl9Ou7TPa1ht2xgsJuiGkaZ7OP5mKKkGYENRY59Lv5VKFyXaxe515bTvLiLVEOPmNUJYU5TVdVF/5h5CNfVsuukyq0EbQ4J1WzRGF37OcW6rct9vzAfgltu/TdvD/+DmVkGhvRHaG93qPaHdVNMCQMu2Iv6ysIZEhy86Cg0bylh9NHzr84Ic305lfQf1ZzcSKimG87/TR0LyS1+NpzY4aHRhNXSjMAnWtsfU9wirn3MccOL9h8/D/97IHak2wtIzQVYqCbveqCTZJYw/fS8SglRcWL+kmlhriKUrq5l9WgPvbi9F28I8cyrc2tbKjLMre18zy4LMhwLrmmiACbpRQKT7jGf6u9n7rhPySMl+c44DEG8lLD0/O/dG2PJSNZ1Nbm7xSHGKcSc3s2JNFbHWEMunwMlri7jnxBqO2hRjbAVcX7aL98YTcNM7WdNmPVKMocAE3TjsSA/mSfc42aPliMCrtLA1lt3fTbwd9asBpelqDtPRFKWivqN7haB1jSW0L6yiMwo/+niI2WtTnLesgr0xIbyqhMXTlcun72TjzjpmLCziiCaIn9LGtEScbVLL+D7SHKxB9zdYyTAGgwm6MfLJaKx8Nt5KWbSzW4RrpKU7anpgDzh/964VFdTPbSRamuol5u0NUd55roZUV4jIzBYiJ7bxCJWc+UIxLZXw2F/FOT3axrpxUSbsjvHe5eW0F8OZ03ZTHUoSPrmJLX8ei4aUaUfvg2gJ4y//15zMMfeIcagwQTeGj8xFHjQJJWPdvvY9rqdJohPirT3HtDdSLr1PowodjVFQKKmNI+J84NuXjCHeGmHb4iqOPLsRBHaHQ2zYVUrJsxXsKRM2ThbOWFbO/dWVzF2ZojipTJ6zm3Nb4uylnGr2smxOEdufr6b0+DaOCccB6JoQITIpSWVxO5EjJg6pv9ww+sIE3Th0ZAi2apLGlHOPVPleI6G0OKcXeWhv7Dk+uJ2FVFxo2lTCjvXl0BQhGVLGnd/IuJpOGteVEW+N8OrxMHtNMbfsncBTJwlzlwuffirF1lpYfFkHp9JBa0cp1zwTA1GOPLuRUJlwU/yLLEjN7fHHn7u9+7ptGuOO5KdYMNPNubLppsvydssMYzCYoBsHT189SdIijtC9tJomEaAm1NLHyfonAazVGE0aZh9hwuuLqFsZI9YhbHgPvHCBcNESaPtzDZvPbyK0qpLVk4WlF3YypTnMVQtDfGB9gvq1EeL1Xcw4vYPprcVMkHYaZ4VoCcOYmjaajqhgXvzK7gmyFqTmQhzvr29gq9YwL3FlXibQMox8Y4JuOLKJc9CFkM09EhTsdBfAXih9ocBbqRird1RQvT3E2B0hoq1C8dg4FXWdVB7VQVGlG2K5Phzh1aV1zFwtVAFV/hxLj4UXZysnV7TyjeYW3j2riMTj1YQeqyKShNBpLdy9ay+dp4TZ9Hgd9Wsj1E5vpnR6J99MfI4FXQFRPsl/B9oqN92Vrnlfxqw7z7OGTGPEI6p9F7pDyaxZs3TJkiXDcu3Dnn5rxj2jHGlv7Ns3nbk/KM7Q87vP/QNDUxBvC7O9LcbyzlI6t8c4fqMQSUFTKayfIDRUwNStytE7IBVW3p4dZ9uMOOXPl3HGGmiY3klFeZxSUlTXdFJTGQeFVoroIkoVLWzfXkzTc9V0TO3i1NMaaNMYv0+ezTkNy6llH011FTnXsHsE3TBGDiLymqrOyrpvVAl6lqHdAxK97v0Hc0wezzlIcc0nXc1hUgmhqCrR3eukC1hdFGNdezGlr5cwdkeI8mYIaU9rZnOZ0jGlixMmNlNWFWdLNMI70Qgbo1G2dkWZ8XyME9YLTaVQ1QZlp+7jyKktAfFuzer++HBoETe3z2diWQPhmnq+vOvyg3KP2DzixkjFBB2cmAdHCRoDoiMhvLypmjYRWqvdWpkTV0Wp3uy8dl0lKfZMTLK+LsSb1RHe0wCXLlYSYVhzjNI4BporlSNLOpgbaeXIqO+Nou7vKUmIMCkafZ/yMdrKlg1VtL5ZRN0JzXRMLRmw73rTXQObQ8VE3Dgc6E/QR48P/ek7TMw9KWBnKkJDZ5R97WG62iPEOiDWLoRjSlFFnNLyBGWlCcojKV5pKif8YgVH7O19noYKeOCcEA0VcPIGYfo7IeasgzkoilI0pYP6GXuZWZLKEOwilARbUgdoYKz3H+jl286FtG871xGa5l4xCoHRI+h7Nw93CgaEqvM7AySB9lSIdg3RkgjT2hqhvTkKAuGSJKGo0twSpaspAp0hOoqV9iKItoWobBIq90EiDIkohJJQ1i4Uxd3MbFWBayZCEPHXbPOfjcUwrgN2VynNlzRzSmkr8eYIybhwTF0XM0PiBuycBNGTlEgCEu1hFGgtL+Gr8etY0Hloe4T0V7O2EZrGaCInQReRi4EfAGHgPlW9K2N/EfAr4DSgAbhKVTflM6F9FcZcWRSroT60m8dX1FG8M2B22uPk3buSiwdKe+KpgPqJ90JJCKV6wjPjh/y3Ss/+SByiCUEF4hFIhZTiTqGoq++pMKP+E6QEaC6G5nKo2wGlHUpbKTRVK1umKCmFUFyQkLKnOEVpUZKSkiQVxXFKixPESyAVhaak0NESJdUcQVtDhFrCdBUled8xewiHFQGkOkmYFFu0lnldV3b31/5aZD4Twg3sLKt2Ne9+hDybCA90itmB1qrNnWIUOgcUdBEJAz8GLgA2A4tFZIGqrgpE+yywR1WPFZGrge8BV+UzoYOd2Ghe4kruit5HV6nSVXEA1Zb+d6v0xJFUQNxDkAoFTqGBc4n2CLkXdwCNgoTViX7CnTwRU5IxIKxE1X0iISUcVqKRFMXlSUpK44RROjojpLqE2tI4x0S7iNK710cqwzddRSt7tKx7cE96/5ZULT9u9eIcWcTX6uYzYdzunv1a26t/diYLUnN7dwPsh3y4N6xWbRj7k0sNfTawTlU3AIjI74ArgKCgXwHc5rcfAu4REdHhanHNQvcAkcnzmTBlR1ZRO5Do9ew/mGPyd84tWsu8xKd6xLd4PhOkja1am5dBLwMR53zSl7/bGisNIzdyEfSJwLuB35uBM/qKo6oJEdkL1AC9FlwUkS8AXwA46qijGGqGS6gOJUNp04Fq1rm4xfqrWZtoG8bgyEXQszkgMmveucRBVe8F7gXXbTGHaxuHESbIhjG85CLom4EjA7/rga19xNksIhFgDND/zErGsJDNfTHYmrVhGCODXAR9MTBVRCYDW4CrgU9kxFkAXAu8BPw1sDDf/vPRuOLLUPmOrWZtGIXBAQXd+8RvAP6E67b4c1VdKSJ3AEtUdQHwM+DXIrIOVzO/Ot8JNdExDMPon5z6oavqY8BjGWHfCWx3AB/Pb9IMwzCMgdDX2BXDMAzjMMME3TAMo0AwQTcMwygQTNANwzAKhGGbD11EdgF/OcjDa8kYhVoAFJpNhWYPFJ5NhWYPFJ5N2ew5WlXrskUeNkEfDCKypK8J3g9XCs2mQrMHCs+mQrMHCs+mgdpjLhfDMIwCwQTdMAyjQDhcBf3e4U7AIaDQbCo0e6DwbCo0e6DwbBqQPYelD90wDMPYn8O1hm4YhmFkYIJuGIZRIAypoIvIkSLyjIisFpGVIvKPgX1fEpE1PnxexnGviUiliDwqIm/5OJkLVY8XkSdEZKaIvOTjvCEiVwXiTBaRV0RkrYg8KCKDnuRbRH4uIjtF5M1A2IMissx/NonIsiz2xETkcRFZ7tP6E79+azrO+0XkpyJygY+/wn+fF4hzmg9fJyI/FJEDrIZ6UPZVichD/r6vFpH3B9MXiHeUiLSIyM0Zx/+XiJwpIt/353hDRB4RkapAnG96G9aIyEV5Tv9g8lws8HtB8BkH78FQP6NseS5Xe0TkWR8nnT/HBeIMSxnKYt8mf8+WiciSQHh3nhORkwJpXCEixYF43xSRT4rIV0RklbfhaRE5OhDnWm/DWhG5Nt82DBuqOmQfYDxwqt+uAN4GpgEfAJ4Civy+cYFjJuHmWy8FPuDDYsALwCWBeJ8G/gk4DpjqwyYA24Aq/3s+cLXf/glwfR5sOhs4FXizj/3/Dnwn0x6/Xem/BXg4nTYfdjvwMeAUYIIPmwFsCcR5FXi/P/6PwfuRx2f2S+BzgfteFUxfIN7DwO+BmzOOX4abdvlCIOLDvgd8z29PA5YDRcBkYD0QHgl5LvD7o8BvM5/xcD2jbHkuV3uAZ4FZfZx3WMpQlnRsAmqzhKfvdwR4AzjZh9cE8wzwDFDn70mpD7seeNBvjwU2+O9qv12dbzuG4zOkNXRV3aaqS/12M7Aatx7p9cBdqtrp9+0MHHYJ8LiqtqnqM35/F7AUt3pSmouBP6rq26q61sfbCuwE6nzN6DzcItbghOojebDpefpYnclf80rggUx7/LH7fFgEJ5bBFurzgadU9XVvB8BKoFhEikRkPO4P4SV1ufRX+bAnI/2VOPH4mU9vl6o2BdPn430EVyhWZhx/AvC2qiZV9QlVTfhdL9Pz7K4Afqeqnaq6EViHW5g8Lwwmz3kbyoGvAHdmOf2wPKM+8lxO9hyAYSlDAyCd5y4E3lDV5T6NDaqahO48G1PVXar6jKq2+WODee4i4ElVbVTVPcCTONsPe4bNhy4ik3A1m1dwNYKz/KvccyJyeiDqxWRkRv+6fjnwtP8dBo5X1VUZ8WbjhHI97l+8KSAqm3EF+1ByFrAjXTg8vewRkT/hCkwzvqCISC0QV9W9Gef7GPC6L7QTcTakORT2TAF2Ab8QkddF5D4RKQumT0TKgK/jak+Z9CUkn8HVViH7IuSH5LkcZJ77Z9xbVltg/0h6RmkGUoZ+4d0Z3067gEZYGVLgCe8m+oJPR/B+HweoiPxJRJaKyNcCx34QrwsZfJZhyHNDTU4LXOQbX+t5GLhRVfeJW4e0GngfcDowX0SmAFGgXlU3BI6N4Gq8PwyEn4ErpMFrjAd+DVyrqqk+fJeHus/m3xConXt/Yy97VPUi7//7Da728ySuBvJE8EQiMh3nqrgwHZTlevm2J4J7tf+Sqr4iIj8AvoGr5abTdzvwn6rakuUWX4R7je9GRG4BEjh7YWjsOKg8JyIzgWNV9Sb/ZxBkpDyjNLmWoU+q6hYRqcDdj7/FvTmMpDJ0pqpu9f79J0XkLVztOn2/I8BcnJ1twNMi8pqqPo378/pFhh3XALOAc9JBWa5ZEP23h7yGLiJRXEb6jar+wQdvBv6gjleBFG5SmrOARRmnuBdYq6p3B8J61QT9a9ejwK2q+rIP3g1U+YIM2Re7zhv+Oh8FHgwEZ7MHdSs+LcC5H2B/e+qBR4BPqep6H7yZ3i6nQ2HPZmCzqqYL+kM4gQ+m7wxgnohsAm4EviUiN4hIKc7v2p0m3/j0IZyoaOAaB1qEfFAMIs+9HzjN27YIOE5EnvX7RsozSpNTGVLVLf67GdcukHZvjZgylM4z3m30iE9jMH2bgedUdbd3qTyGy5f4uK8G7PggcAvw4bQ7iiHIc8PGYBzwA/3g/hl/BdydEX4dcIffPg73OiTA94GLA/HuxBXMUMbxL9LTwBjDvXLdmOX6v6d3g84X82TXJPZvMLsYl+mCYd32AOXAeL8dwQn/Dd7u5fQM+qryvz+W5bqLcTWydIPbpYfgmb2AexUHuM3b0J2+jLi34RtFgctwPt3g/VgF1GUcM53ejaIbyG+j6KDyXLZnPBKeUWaey8Uen89q/XYU9wd93UgoQ4HzlwEVge0XcWIevN/VuDa0Um/TUz6/Tce1x6TPdQrOVTQ14xpjgY3+PNV+e2y+y85wfIb2Yu41SXEt1Mv851Kfgf4beNM/qPN8/MVAid+u98euDhz7OVxr9sLANa4B4oE4y4CZft8U3L/3Op8xi/Jg0wO4XgBx3D//Z334/enCEogbtOcI//sNXEPaj3zmnAXcHzjmVqA1w55xft8sf8/WA/eQRWTzYN9MYIlP5//gXnPv7yPubfQI+j3AuYF963Aik7bhJ4F9t3gb1pDnnjqDyXMZ55lEj6AP6zPKludysQcnkK8F8twPcD2QhrUMZdg2BSfey30ab8m834E0rvT2zvNhNwN/F4jzFLAjYEOw59JnvA3rgE/nu9wM12fEDv33r7A/VdVLDhDvGpyP8K7+4g03A7DnVmCdqv5uaFI2MHJNn4gsBc5Q1fjQpGzwFMozSlMoZWgAee5JnMtr29CkbOQxYgXdMAzDGBg29N8wDKNAMEE3DMMoEEzQDcMwCgQTdMMwjALBBN0YNYhI0g95XylulsuviEi/ZUBEJonIJ4YqjYYxGEzQjdFEu6rOVNXpwAW4/ujfPcAxkwATdOOwwLotGqMGEWlR1fLA7ym4gTe1wNG4eUvK/O4bVPVFEXkZOAE3mvCXuKHo+8UbIhMMo19M0I1RQ6ag+7A9wHtxs12mVLVDRKYCD6jqLBE5Fzf69UM+fmm2eENriWFkZ1hmWzSMEUR65r0ocI+fYTGJmw8lG7nGM4whxwTdGLV4l0sSNx/9d3HzfpyMa1vq6OOwm3KMZxhDjjWKGqMSEanDzRZ4jzq/4xhgm6qmcHOEp9d3bcYtXZemr3iGMeyYD90YNYhIEliBc5skcI2b/6Fu8YapuKmZ23BrUn5JVcv9XOqP4xpO7wf+L1u8obbFMLJhgm4YhlEgmMvFMAyjQDBBNwzDKBBM0A3DMAoEE3TDMIwCwQTdMAyjQDBBNwzDKBBM0A3DMAqE/wf3gMuNt85qeAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data.plot(y=['CasosNormalizados', 'Predict_mlp', 'Predict_svr', 'Predict_lr'], style=['-s', '--o'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
