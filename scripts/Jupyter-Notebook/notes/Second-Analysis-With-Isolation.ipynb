{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "## Celso Antonio Uliana Junior\n",
    "## July 2 2020\n",
    "####\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#####\n",
    "## Consuming and shaping the data to analysis\n",
    "## Covid-19 numbers in Brazil by date\n",
    "## Isolation percentage in Brazil by date\n",
    "#####\n",
    "\n",
    "data_raw_covid = pd.read_csv(\"C:/Users/PCDOMILHAO/Documents/GitHub/trab-siad/scripts/Jupyter-Notebook/dados/covidBrasil.csv\", sep = \";\", decimal = \",\")\n",
    "data_raw_isolation = pd.read_csv(\"C:/Users/PCDOMILHAO/Documents/GitHub/trab-siad/scripts/Jupyter-Notebook/dados/isolamento.csv\", sep = \";\", decimal = \",\")\n",
    "data_covid = data_raw_covid['Data'].values.copy()\n",
    "data_covid = data_raw_covid.dropna().set_index(\"Data\")\n",
    "data_isolation = data_raw_isolation['Data'].values.copy()\n",
    "data_isolation = data_raw_isolation.dropna().set_index(\"Data\")\n",
    "\n",
    "####\n",
    "## Shaping a central pandas dataFrame for all our ML needs\n",
    "####\n",
    "\n",
    "data = data_covid\n",
    "data['Taxa'] = data_isolation['Taxa'].values.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Casos</th>\n",
       "      <th>Taxa</th>\n",
       "      <th>CasosNormalizados</th>\n",
       "      <th>TaxaNormalizadas</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26/2/20</th>\n",
       "      <td>1</td>\n",
       "      <td>24.7</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>27.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>31.4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/3/20</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.461333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>27.7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>30.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/3/20</th>\n",
       "      <td>1</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6/3/20</th>\n",
       "      <td>5</td>\n",
       "      <td>28.5</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.101333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7/3/20</th>\n",
       "      <td>5</td>\n",
       "      <td>31.8</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.189333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.434667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9/3/20</th>\n",
       "      <td>12</td>\n",
       "      <td>29.2</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10/3/20</th>\n",
       "      <td>0</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/3/20</th>\n",
       "      <td>9</td>\n",
       "      <td>28.3</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.096000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/3/20</th>\n",
       "      <td>18</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13/3/20</th>\n",
       "      <td>25</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14/3/20</th>\n",
       "      <td>21</td>\n",
       "      <td>35.7</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.293333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15/3/20</th>\n",
       "      <td>23</td>\n",
       "      <td>42.6</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.477333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16/3/20</th>\n",
       "      <td>79</td>\n",
       "      <td>32.1</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.197333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Casos  Taxa  CasosNormalizados  TaxaNormalizadas\n",
       "Data                                                     \n",
       "26/2/20      1  24.7           0.000018          0.000000\n",
       "27/2/20      0  27.5           0.000000          0.074667\n",
       "28/2/20      0  26.6           0.000000          0.050667\n",
       "29/2/20      0  31.4           0.000000          0.178667\n",
       "1/3/20       1    42           0.000018          0.461333\n",
       "2/3/20       0  27.7           0.000000          0.080000\n",
       "3/3/20       0    29           0.000000          0.114667\n",
       "4/3/20       0  30.2           0.000000          0.146667\n",
       "5/3/20       1  29.7           0.000018          0.133333\n",
       "6/3/20       5  28.5           0.000091          0.101333\n",
       "7/3/20       5  31.8           0.000091          0.189333\n",
       "8/3/20       0    41           0.000000          0.434667\n",
       "9/3/20      12  29.2           0.000219          0.120000\n",
       "10/3/20      0  29.7           0.000000          0.133333\n",
       "11/3/20      9  28.3           0.000164          0.096000\n",
       "12/3/20     18  30.1           0.000329          0.144000\n",
       "13/3/20     25  30.1           0.000456          0.144000\n",
       "14/3/20     21  35.7           0.000383          0.293333\n",
       "15/3/20     23  42.6           0.000420          0.477333\n",
       "16/3/20     79  32.1           0.001442          0.197333"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "####\n",
    "## normalizing values for both covid and isolation percentage \n",
    "## between range [0,1] using sklearn MinMaxScaler\n",
    "####\n",
    "\n",
    "covid_norm = data_covid[\"Casos\"].values.copy()\n",
    "covid_norm.shape = (len(covid_norm), 1)\n",
    "\n",
    "isolation_norm = data_isolation[\"Taxa\"].values.copy()\n",
    "isolation_norm.shape = (len(isolation_norm), 1)\n",
    "\n",
    "####\n",
    "## Shaping the central dataFrame with normalized values\n",
    "####\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "covid_norm = min_max_scaler.fit_transform(covid_norm)\n",
    "isolation_norm = min_max_scaler.fit_transform(isolation_norm)\n",
    "\n",
    "data[\"CasosNormalizados\"] = covid_norm\n",
    "data[\"TaxaNormalizadas\"] = isolation_norm\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               E0        E1        E2        E3        E4\n",
      "Data                                                     \n",
      "26/2/20  0.000018  0.000000  0.000000  0.074667  0.000000\n",
      "27/2/20  0.000000  0.000000  0.074667  0.050667  0.000000\n",
      "28/2/20  0.000000  0.000000  0.050667  0.178667  0.000000\n",
      "29/2/20  0.000000  0.000018  0.178667  0.461333  0.000018\n",
      "1/3/20   0.000018  0.000000  0.461333  0.080000  0.000000\n",
      "...           ...       ...       ...       ...       ...\n",
      "16/6/20  0.376970  0.637527  0.384000  0.336000  0.637527\n",
      "17/6/20  0.637527  0.587683  0.336000  0.368000  0.587683\n",
      "18/6/20  0.587683  0.415640  0.368000  0.266667  0.415640\n",
      "19/6/20  0.415640  1.000000  0.266667  0.384000  1.000000\n",
      "20/6/20  1.000000  0.632926  0.384000  0.602667  0.632926\n",
      "\n",
      "[116 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Sliding window\n",
    "####\n",
    "\n",
    "df = pd.DataFrame()\n",
    "window_size = 1\n",
    "for i in range(0, window_size + 1):\n",
    "    df['E{}'.format(i)] = data['CasosNormalizados'].shift(-i)\n",
    "    if(i == window_size):\n",
    "        for j in range(0, window_size + 1):\n",
    "             df['E{}'.format(j + i + 1)] = data['TaxaNormalizadas'].shift(-j)\n",
    "        df['E{}'.format(window_size * 2 + 2)] = data['CasosNormalizados'].shift(-window_size)\n",
    "df = df.iloc[: -window_size]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "## Manipulating the data to split into X(a window size of values)\n",
    "## and target, or Y, the value X \"produces\"\n",
    "####\n",
    "\n",
    "arr = df.values\n",
    "\n",
    "X = arr[:, : -1]\n",
    "target = arr[:, -1]\n",
    "#print(X)\n",
    "#print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.06799594\n",
      "Iteration 2, loss = 0.03454402\n",
      "Iteration 3, loss = 0.01817950\n",
      "Iteration 4, loss = 0.01613405\n",
      "Iteration 5, loss = 0.02032831\n",
      "Iteration 6, loss = 0.02227652\n",
      "Iteration 7, loss = 0.01967983\n",
      "Iteration 8, loss = 0.01436826\n",
      "Iteration 9, loss = 0.00890014\n",
      "Iteration 10, loss = 0.00493624\n",
      "Iteration 11, loss = 0.00313237\n",
      "Iteration 12, loss = 0.00316933\n",
      "Iteration 13, loss = 0.00414376\n",
      "Iteration 14, loss = 0.00510754\n",
      "Iteration 15, loss = 0.00542522\n",
      "Iteration 16, loss = 0.00497026\n",
      "Iteration 17, loss = 0.00407825\n",
      "Iteration 18, loss = 0.00324354\n",
      "Iteration 19, loss = 0.00288300\n",
      "Iteration 20, loss = 0.00307314\n",
      "Iteration 21, loss = 0.00356206\n",
      "Iteration 22, loss = 0.00393049\n",
      "Iteration 23, loss = 0.00390457\n",
      "Iteration 24, loss = 0.00346868\n",
      "Iteration 25, loss = 0.00281193\n",
      "Iteration 26, loss = 0.00217973\n",
      "Iteration 27, loss = 0.00174951\n",
      "Iteration 28, loss = 0.00157112\n",
      "Iteration 29, loss = 0.00158704\n",
      "Iteration 30, loss = 0.00167944\n",
      "Iteration 31, loss = 0.00173816\n",
      "Iteration 32, loss = 0.00170587\n",
      "Iteration 33, loss = 0.00158709\n",
      "Iteration 34, loss = 0.00142856\n",
      "Iteration 35, loss = 0.00129087\n",
      "Iteration 36, loss = 0.00121848\n",
      "Iteration 37, loss = 0.00122274\n",
      "Iteration 38, loss = 0.00128039\n",
      "Iteration 39, loss = 0.00134821\n",
      "Iteration 40, loss = 0.00138340\n",
      "Iteration 41, loss = 0.00136265\n",
      "Iteration 42, loss = 0.00128929\n",
      "Iteration 43, loss = 0.00118772\n",
      "Iteration 44, loss = 0.00108989\n",
      "Iteration 45, loss = 0.00101975\n",
      "Iteration 46, loss = 0.00098401\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06799515\n",
      "Iteration 2, loss = 0.03454571\n",
      "Iteration 3, loss = 0.01818153\n",
      "Iteration 4, loss = 0.01613309\n",
      "Iteration 5, loss = 0.02032663\n",
      "Iteration 6, loss = 0.02227643\n",
      "Iteration 7, loss = 0.01967993\n",
      "Iteration 8, loss = 0.01436935\n",
      "Iteration 9, loss = 0.00890216\n",
      "Iteration 10, loss = 0.00493954\n",
      "Iteration 11, loss = 0.00313572\n",
      "Iteration 12, loss = 0.00317131\n",
      "Iteration 13, loss = 0.00414457\n",
      "Iteration 14, loss = 0.00510781\n",
      "Iteration 15, loss = 0.00542545\n",
      "Iteration 16, loss = 0.00497040\n",
      "Iteration 17, loss = 0.00407866\n",
      "Iteration 18, loss = 0.00324418\n",
      "Iteration 19, loss = 0.00288293\n",
      "Iteration 20, loss = 0.00307292\n",
      "Iteration 21, loss = 0.00356219\n",
      "Iteration 22, loss = 0.00393136\n",
      "Iteration 23, loss = 0.00390664\n",
      "Iteration 24, loss = 0.00347131\n",
      "Iteration 25, loss = 0.00281459\n",
      "Iteration 26, loss = 0.00218190\n",
      "Iteration 27, loss = 0.00175144\n",
      "Iteration 28, loss = 0.00157278\n",
      "Iteration 29, loss = 0.00158876\n",
      "Iteration 30, loss = 0.00168157\n",
      "Iteration 31, loss = 0.00174032\n",
      "Iteration 32, loss = 0.00170792\n",
      "Iteration 33, loss = 0.00158875\n",
      "Iteration 34, loss = 0.00142986\n",
      "Iteration 35, loss = 0.00129183\n",
      "Iteration 36, loss = 0.00121916\n",
      "Iteration 37, loss = 0.00122327\n",
      "Iteration 38, loss = 0.00128095\n",
      "Iteration 39, loss = 0.00134896\n",
      "Iteration 40, loss = 0.00138445\n",
      "Iteration 41, loss = 0.00136402\n",
      "Iteration 42, loss = 0.00129091\n",
      "Iteration 43, loss = 0.00118952\n",
      "Iteration 44, loss = 0.00109177\n",
      "Iteration 45, loss = 0.00102166\n",
      "Iteration 46, loss = 0.00098593\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06798815\n",
      "Iteration 2, loss = 0.03455360\n",
      "Iteration 3, loss = 0.01820330\n",
      "Iteration 4, loss = 0.01616359\n",
      "Iteration 5, loss = 0.02035459\n",
      "Iteration 6, loss = 0.02229735\n",
      "Iteration 7, loss = 0.01969533\n",
      "Iteration 8, loss = 0.01438213\n",
      "Iteration 9, loss = 0.00891521\n",
      "Iteration 10, loss = 0.00495487\n",
      "Iteration 11, loss = 0.00315446\n",
      "Iteration 12, loss = 0.00319298\n",
      "Iteration 13, loss = 0.00416653\n",
      "Iteration 14, loss = 0.00512773\n",
      "Iteration 15, loss = 0.00544156\n",
      "Iteration 16, loss = 0.00498277\n",
      "Iteration 17, loss = 0.00408712\n",
      "Iteration 18, loss = 0.00324960\n",
      "Iteration 19, loss = 0.00288725\n",
      "Iteration 20, loss = 0.00307646\n",
      "Iteration 21, loss = 0.00356417\n",
      "Iteration 22, loss = 0.00393119\n",
      "Iteration 23, loss = 0.00390365\n",
      "Iteration 24, loss = 0.00346642\n",
      "Iteration 25, loss = 0.00280894\n",
      "Iteration 26, loss = 0.00217679\n",
      "Iteration 27, loss = 0.00174713\n",
      "Iteration 28, loss = 0.00156941\n",
      "Iteration 29, loss = 0.00158583\n",
      "Iteration 30, loss = 0.00167834\n",
      "Iteration 31, loss = 0.00173700\n",
      "Iteration 32, loss = 0.00170430\n",
      "Iteration 33, loss = 0.00158517\n",
      "Iteration 34, loss = 0.00142655\n",
      "Iteration 35, loss = 0.00128906\n",
      "Iteration 36, loss = 0.00121698\n",
      "Iteration 37, loss = 0.00122152\n",
      "Iteration 38, loss = 0.00127922\n",
      "Iteration 39, loss = 0.00134685\n",
      "Iteration 40, loss = 0.00138164\n",
      "Iteration 41, loss = 0.00136062\n",
      "Iteration 42, loss = 0.00128717\n",
      "Iteration 43, loss = 0.00118562\n",
      "Iteration 44, loss = 0.00108805\n",
      "Iteration 45, loss = 0.00101833\n",
      "Iteration 46, loss = 0.00098302\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06794960\n",
      "Iteration 2, loss = 0.03455512\n",
      "Iteration 3, loss = 0.01822773\n",
      "Iteration 4, loss = 0.01618969\n",
      "Iteration 5, loss = 0.02037515\n",
      "Iteration 6, loss = 0.02231193\n",
      "Iteration 7, loss = 0.01970617\n",
      "Iteration 8, loss = 0.01439223\n",
      "Iteration 9, loss = 0.00892601\n",
      "Iteration 10, loss = 0.00496816\n",
      "Iteration 11, loss = 0.00317410\n",
      "Iteration 12, loss = 0.00321941\n",
      "Iteration 13, loss = 0.00419601\n",
      "Iteration 14, loss = 0.00515332\n",
      "Iteration 15, loss = 0.00545503\n",
      "Iteration 16, loss = 0.00498393\n",
      "Iteration 17, loss = 0.00407794\n",
      "Iteration 18, loss = 0.00323557\n",
      "Iteration 19, loss = 0.00287505\n",
      "Iteration 20, loss = 0.00307064\n",
      "Iteration 21, loss = 0.00356360\n",
      "Iteration 22, loss = 0.00393523\n",
      "Iteration 23, loss = 0.00390766\n",
      "Iteration 24, loss = 0.00346457\n",
      "Iteration 25, loss = 0.00279950\n",
      "Iteration 26, loss = 0.00216421\n",
      "Iteration 27, loss = 0.00173443\n",
      "Iteration 28, loss = 0.00156196\n",
      "Iteration 29, loss = 0.00158595\n",
      "Iteration 30, loss = 0.00168159\n",
      "Iteration 31, loss = 0.00173912\n",
      "Iteration 32, loss = 0.00170261\n",
      "Iteration 33, loss = 0.00157587\n",
      "Iteration 34, loss = 0.00141036\n",
      "Iteration 35, loss = 0.00127042\n",
      "Iteration 36, loss = 0.00120071\n",
      "Iteration 37, loss = 0.00121081\n",
      "Iteration 38, loss = 0.00127434\n",
      "Iteration 39, loss = 0.00134560\n",
      "Iteration 40, loss = 0.00138036\n",
      "Iteration 41, loss = 0.00135611\n",
      "Iteration 42, loss = 0.00127889\n",
      "Iteration 43, loss = 0.00117631\n",
      "Iteration 44, loss = 0.00108155\n",
      "Iteration 45, loss = 0.00101750\n",
      "Iteration 46, loss = 0.00098805\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06798176\n",
      "Iteration 2, loss = 0.03455381\n",
      "Iteration 3, loss = 0.01819423\n",
      "Iteration 4, loss = 0.01613772\n",
      "Iteration 5, loss = 0.02033247\n",
      "Iteration 6, loss = 0.02229712\n",
      "Iteration 7, loss = 0.01971577\n",
      "Iteration 8, loss = 0.01441278\n",
      "Iteration 9, loss = 0.00894692\n",
      "Iteration 10, loss = 0.00498099\n",
      "Iteration 11, loss = 0.00317447\n",
      "Iteration 12, loss = 0.00320820\n",
      "Iteration 13, loss = 0.00417983\n",
      "Iteration 14, loss = 0.00514105\n",
      "Iteration 15, loss = 0.00545308\n",
      "Iteration 16, loss = 0.00499311\n",
      "Iteration 17, loss = 0.00409324\n",
      "Iteration 18, loss = 0.00324565\n",
      "Iteration 19, loss = 0.00287244\n",
      "Iteration 20, loss = 0.00305393\n",
      "Iteration 21, loss = 0.00354236\n",
      "Iteration 22, loss = 0.00391800\n",
      "Iteration 23, loss = 0.00390282\n",
      "Iteration 24, loss = 0.00347454\n",
      "Iteration 25, loss = 0.00281846\n",
      "Iteration 26, loss = 0.00218172\n",
      "Iteration 27, loss = 0.00174393\n",
      "Iteration 28, loss = 0.00156095\n",
      "Iteration 29, loss = 0.00157616\n",
      "Iteration 30, loss = 0.00167018\n",
      "Iteration 31, loss = 0.00173299\n",
      "Iteration 32, loss = 0.00170629\n",
      "Iteration 33, loss = 0.00158987\n",
      "Iteration 34, loss = 0.00142995\n",
      "Iteration 35, loss = 0.00128986\n",
      "Iteration 36, loss = 0.00121465\n",
      "Iteration 37, loss = 0.00121627\n",
      "Iteration 38, loss = 0.00127243\n",
      "Iteration 39, loss = 0.00134032\n",
      "Iteration 40, loss = 0.00137714\n",
      "Iteration 41, loss = 0.00135873\n",
      "Iteration 42, loss = 0.00128744\n",
      "Iteration 43, loss = 0.00118719\n",
      "Iteration 44, loss = 0.00108980\n",
      "Iteration 45, loss = 0.00101964\n",
      "Iteration 46, loss = 0.00098371\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06799158\n",
      "Iteration 2, loss = 0.03455092\n",
      "Iteration 3, loss = 0.01819580\n",
      "Iteration 4, loss = 0.01615020\n",
      "Iteration 5, loss = 0.02034169\n",
      "Iteration 6, loss = 0.02228799\n",
      "Iteration 7, loss = 0.01968901\n",
      "Iteration 8, loss = 0.01437734\n",
      "Iteration 9, loss = 0.00891038\n",
      "Iteration 10, loss = 0.00494893\n",
      "Iteration 11, loss = 0.00314684\n",
      "Iteration 12, loss = 0.00318401\n",
      "Iteration 13, loss = 0.00415724\n",
      "Iteration 14, loss = 0.00511950\n",
      "Iteration 15, loss = 0.00543540\n",
      "Iteration 16, loss = 0.00497849\n",
      "Iteration 17, loss = 0.00408480\n",
      "Iteration 18, loss = 0.00324886\n",
      "Iteration 19, loss = 0.00288699\n",
      "Iteration 20, loss = 0.00307708\n",
      "Iteration 21, loss = 0.00356597\n",
      "Iteration 22, loss = 0.00393428\n",
      "Iteration 23, loss = 0.00390824\n",
      "Iteration 24, loss = 0.00347160\n",
      "Iteration 25, loss = 0.00281403\n",
      "Iteration 26, loss = 0.00218108\n",
      "Iteration 27, loss = 0.00175096\n",
      "Iteration 28, loss = 0.00157277\n",
      "Iteration 29, loss = 0.00158907\n",
      "Iteration 30, loss = 0.00168194\n",
      "Iteration 31, loss = 0.00174047\n",
      "Iteration 32, loss = 0.00170751\n",
      "Iteration 33, loss = 0.00158782\n",
      "Iteration 34, loss = 0.00142866\n",
      "Iteration 35, loss = 0.00129061\n",
      "Iteration 36, loss = 0.00121815\n",
      "Iteration 37, loss = 0.00122251\n",
      "Iteration 38, loss = 0.00128034\n",
      "Iteration 39, loss = 0.00134828\n",
      "Iteration 40, loss = 0.00138346\n",
      "Iteration 41, loss = 0.00136277\n",
      "Iteration 42, loss = 0.00128952\n",
      "Iteration 43, loss = 0.00118806\n",
      "Iteration 44, loss = 0.00109040\n",
      "Iteration 45, loss = 0.00102047\n",
      "Iteration 46, loss = 0.00098500\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.06798818\n",
      "Iteration 2, loss = 0.03455303\n",
      "Iteration 3, loss = 0.01819991\n",
      "Iteration 4, loss = 0.01615803\n",
      "Iteration 5, loss = 0.02035275\n",
      "Iteration 6, loss = 0.02229756\n",
      "Iteration 7, loss = 0.01969731\n",
      "Iteration 8, loss = 0.01438490\n",
      "Iteration 9, loss = 0.00891780\n",
      "Iteration 10, loss = 0.00495647\n",
      "Iteration 11, loss = 0.00315469\n",
      "Iteration 12, loss = 0.00319197\n",
      "Iteration 13, loss = 0.00416502\n",
      "Iteration 14, loss = 0.00512625\n",
      "Iteration 15, loss = 0.00544075\n",
      "Iteration 16, loss = 0.00498215\n",
      "Iteration 17, loss = 0.00408714\n",
      "Iteration 18, loss = 0.00325036\n",
      "Iteration 19, loss = 0.00288829\n",
      "Iteration 20, loss = 0.00307838\n",
      "Iteration 21, loss = 0.00356729\n",
      "Iteration 22, loss = 0.00393547\n",
      "Iteration 23, loss = 0.00390904\n",
      "Iteration 24, loss = 0.00347194\n",
      "Iteration 25, loss = 0.00281399\n",
      "Iteration 26, loss = 0.00218084\n",
      "Iteration 27, loss = 0.00175066\n",
      "Iteration 28, loss = 0.00157251\n",
      "Iteration 29, loss = 0.00158888\n",
      "Iteration 30, loss = 0.00168184\n",
      "Iteration 31, loss = 0.00174042\n",
      "Iteration 32, loss = 0.00170734\n",
      "Iteration 33, loss = 0.00158747\n",
      "Iteration 34, loss = 0.00142808\n",
      "Iteration 35, loss = 0.00128986\n",
      "Iteration 36, loss = 0.00121734\n",
      "Iteration 37, loss = 0.00122174\n",
      "Iteration 38, loss = 0.00127966\n",
      "Iteration 39, loss = 0.00134769\n",
      "Iteration 40, loss = 0.00138289\n",
      "Iteration 41, loss = 0.00136219\n",
      "Iteration 42, loss = 0.00128891\n",
      "Iteration 43, loss = 0.00118742\n",
      "Iteration 44, loss = 0.00108981\n",
      "Iteration 45, loss = 0.00102003\n",
      "Iteration 46, loss = 0.00098472\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06798819\n",
      "Iteration 2, loss = 0.03455278\n",
      "Iteration 3, loss = 0.01819853\n",
      "Iteration 4, loss = 0.01615556\n",
      "Iteration 5, loss = 0.02035253\n",
      "Iteration 6, loss = 0.02229835\n",
      "Iteration 7, loss = 0.01969896\n",
      "Iteration 8, loss = 0.01438691\n",
      "Iteration 9, loss = 0.00891965\n",
      "Iteration 10, loss = 0.00495776\n",
      "Iteration 11, loss = 0.00315522\n",
      "Iteration 12, loss = 0.00319183\n",
      "Iteration 13, loss = 0.00416445\n",
      "Iteration 14, loss = 0.00512571\n",
      "Iteration 15, loss = 0.00544049\n",
      "Iteration 16, loss = 0.00498224\n",
      "Iteration 17, loss = 0.00408752\n",
      "Iteration 18, loss = 0.00325068\n",
      "Iteration 19, loss = 0.00288808\n",
      "Iteration 20, loss = 0.00307774\n",
      "Iteration 21, loss = 0.00356648\n",
      "Iteration 22, loss = 0.00393480\n",
      "Iteration 23, loss = 0.00390871\n",
      "Iteration 24, loss = 0.00347193\n",
      "Iteration 25, loss = 0.00281418\n",
      "Iteration 26, loss = 0.00218104\n",
      "Iteration 27, loss = 0.00175079\n",
      "Iteration 28, loss = 0.00157254\n",
      "Iteration 29, loss = 0.00158885\n",
      "Iteration 30, loss = 0.00168189\n",
      "Iteration 31, loss = 0.00174061\n",
      "Iteration 32, loss = 0.00170768\n",
      "Iteration 33, loss = 0.00158789\n",
      "Iteration 34, loss = 0.00142857\n",
      "Iteration 35, loss = 0.00129033\n",
      "Iteration 36, loss = 0.00121772\n",
      "Iteration 37, loss = 0.00122203\n",
      "Iteration 38, loss = 0.00127992\n",
      "Iteration 39, loss = 0.00134797\n",
      "Iteration 40, loss = 0.00138327\n",
      "Iteration 41, loss = 0.00136268\n",
      "Iteration 42, loss = 0.00128948\n",
      "Iteration 43, loss = 0.00118803\n",
      "Iteration 44, loss = 0.00109042\n",
      "Iteration 45, loss = 0.00102059\n",
      "Iteration 46, loss = 0.00098523\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06799097\n",
      "Iteration 2, loss = 0.03455108\n",
      "Iteration 3, loss = 0.01819452\n",
      "Iteration 4, loss = 0.01615148\n",
      "Iteration 5, loss = 0.02034295\n",
      "Iteration 6, loss = 0.02229023\n",
      "Iteration 7, loss = 0.01969235\n",
      "Iteration 8, loss = 0.01438125\n",
      "Iteration 9, loss = 0.00891421\n",
      "Iteration 10, loss = 0.00495208\n",
      "Iteration 11, loss = 0.00314897\n",
      "Iteration 12, loss = 0.00318527\n",
      "Iteration 13, loss = 0.00415810\n",
      "Iteration 14, loss = 0.00512010\n",
      "Iteration 15, loss = 0.00543622\n",
      "Iteration 16, loss = 0.00497967\n",
      "Iteration 17, loss = 0.00408622\n",
      "Iteration 18, loss = 0.00325009\n",
      "Iteration 19, loss = 0.00288749\n",
      "Iteration 20, loss = 0.00307696\n",
      "Iteration 21, loss = 0.00356546\n",
      "Iteration 22, loss = 0.00393387\n",
      "Iteration 23, loss = 0.00390827\n",
      "Iteration 24, loss = 0.00347207\n",
      "Iteration 25, loss = 0.00281477\n",
      "Iteration 26, loss = 0.00218179\n",
      "Iteration 27, loss = 0.00175146\n",
      "Iteration 28, loss = 0.00157294\n",
      "Iteration 29, loss = 0.00158899\n",
      "Iteration 30, loss = 0.00168195\n",
      "Iteration 31, loss = 0.00174078\n",
      "Iteration 32, loss = 0.00170785\n",
      "Iteration 33, loss = 0.00158811\n",
      "Iteration 34, loss = 0.00142886\n",
      "Iteration 35, loss = 0.00129069\n",
      "Iteration 36, loss = 0.00121811\n",
      "Iteration 37, loss = 0.00122238\n",
      "Iteration 38, loss = 0.00128019\n",
      "Iteration 39, loss = 0.00134818\n",
      "Iteration 40, loss = 0.00138344\n",
      "Iteration 41, loss = 0.00136284\n",
      "Iteration 42, loss = 0.00128970\n",
      "Iteration 43, loss = 0.00118836\n",
      "Iteration 44, loss = 0.00109083\n",
      "Iteration 45, loss = 0.00102103\n",
      "Iteration 46, loss = 0.00098566\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06798494\n",
      "Iteration 2, loss = 0.03455455\n",
      "Iteration 3, loss = 0.01820485\n",
      "Iteration 4, loss = 0.01616364\n",
      "Iteration 5, loss = 0.02035975\n",
      "Iteration 6, loss = 0.02230450\n",
      "Iteration 7, loss = 0.01970188\n",
      "Iteration 8, loss = 0.01438819\n",
      "Iteration 9, loss = 0.00892114\n",
      "Iteration 10, loss = 0.00496071\n",
      "Iteration 11, loss = 0.00316037\n",
      "Iteration 12, loss = 0.00319862\n",
      "Iteration 13, loss = 0.00417167\n",
      "Iteration 14, loss = 0.00513184\n",
      "Iteration 15, loss = 0.00544429\n",
      "Iteration 16, loss = 0.00498470\n",
      "Iteration 17, loss = 0.00408830\n",
      "Iteration 18, loss = 0.00325011\n",
      "Iteration 19, loss = 0.00288760\n",
      "Iteration 20, loss = 0.00307710\n",
      "Iteration 21, loss = 0.00356609\n",
      "Iteration 22, loss = 0.00393475\n",
      "Iteration 23, loss = 0.00390822\n",
      "Iteration 24, loss = 0.00347077\n",
      "Iteration 25, loss = 0.00281205\n",
      "Iteration 26, loss = 0.00217839\n",
      "Iteration 27, loss = 0.00174765\n",
      "Iteration 28, loss = 0.00156962\n",
      "Iteration 29, loss = 0.00158665\n",
      "Iteration 30, loss = 0.00167976\n",
      "Iteration 31, loss = 0.00173883\n",
      "Iteration 32, loss = 0.00170625\n",
      "Iteration 33, loss = 0.00158657\n",
      "Iteration 34, loss = 0.00142714\n",
      "Iteration 35, loss = 0.00128879\n",
      "Iteration 36, loss = 0.00121619\n",
      "Iteration 37, loss = 0.00122081\n",
      "Iteration 38, loss = 0.00127887\n",
      "Iteration 39, loss = 0.00134703\n",
      "Iteration 40, loss = 0.00138225\n",
      "Iteration 41, loss = 0.00136139\n",
      "Iteration 42, loss = 0.00128782\n",
      "Iteration 43, loss = 0.00118606\n",
      "Iteration 44, loss = 0.00108836\n",
      "Iteration 45, loss = 0.00101866\n",
      "Iteration 46, loss = 0.00098355\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06795201\n",
      "Iteration 2, loss = 0.03455555\n",
      "Iteration 3, loss = 0.01822621\n",
      "Iteration 4, loss = 0.01618723\n",
      "Iteration 5, loss = 0.02037458\n",
      "Iteration 6, loss = 0.02231394\n",
      "Iteration 7, loss = 0.01971155\n",
      "Iteration 8, loss = 0.01439702\n",
      "Iteration 9, loss = 0.00892974\n",
      "Iteration 10, loss = 0.00497201\n",
      "Iteration 11, loss = 0.00317800\n",
      "Iteration 12, loss = 0.00322238\n",
      "Iteration 13, loss = 0.00419810\n",
      "Iteration 14, loss = 0.00515526\n",
      "Iteration 15, loss = 0.00545877\n",
      "Iteration 16, loss = 0.00498947\n",
      "Iteration 17, loss = 0.00408500\n",
      "Iteration 18, loss = 0.00324101\n",
      "Iteration 19, loss = 0.00287663\n",
      "Iteration 20, loss = 0.00306582\n",
      "Iteration 21, loss = 0.00355477\n",
      "Iteration 22, loss = 0.00392501\n",
      "Iteration 23, loss = 0.00390164\n",
      "Iteration 24, loss = 0.00346648\n",
      "Iteration 25, loss = 0.00280796\n",
      "Iteration 26, loss = 0.00217318\n",
      "Iteration 27, loss = 0.00173927\n",
      "Iteration 28, loss = 0.00156027\n",
      "Iteration 29, loss = 0.00157792\n",
      "Iteration 30, loss = 0.00167143\n",
      "Iteration 31, loss = 0.00173172\n",
      "Iteration 32, loss = 0.00170123\n",
      "Iteration 33, loss = 0.00158221\n",
      "Iteration 34, loss = 0.00142172\n",
      "Iteration 35, loss = 0.00128300\n",
      "Iteration 36, loss = 0.00121018\n",
      "Iteration 37, loss = 0.00121441\n",
      "Iteration 38, loss = 0.00127205\n",
      "Iteration 39, loss = 0.00134004\n",
      "Iteration 40, loss = 0.00137569\n",
      "Iteration 41, loss = 0.00135560\n",
      "Iteration 42, loss = 0.00128305\n",
      "Iteration 43, loss = 0.00118246\n",
      "Iteration 44, loss = 0.00108588\n",
      "Iteration 45, loss = 0.00101721\n",
      "Iteration 46, loss = 0.00098283\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06797840\n",
      "Iteration 2, loss = 0.03455499\n",
      "Iteration 3, loss = 0.01819922\n",
      "Iteration 4, loss = 0.01614441\n",
      "Iteration 5, loss = 0.02033780\n",
      "Iteration 6, loss = 0.02229989\n",
      "Iteration 7, loss = 0.01971629\n",
      "Iteration 8, loss = 0.01441227\n",
      "Iteration 9, loss = 0.00894631\n",
      "Iteration 10, loss = 0.00498134\n",
      "Iteration 11, loss = 0.00317617\n",
      "Iteration 12, loss = 0.00321075\n",
      "Iteration 13, loss = 0.00418281\n",
      "Iteration 14, loss = 0.00514365\n",
      "Iteration 15, loss = 0.00545452\n",
      "Iteration 16, loss = 0.00499342\n",
      "Iteration 17, loss = 0.00409308\n",
      "Iteration 18, loss = 0.00324588\n",
      "Iteration 19, loss = 0.00287371\n",
      "Iteration 20, loss = 0.00305610\n",
      "Iteration 21, loss = 0.00354446\n",
      "Iteration 22, loss = 0.00391929\n",
      "Iteration 23, loss = 0.00390300\n",
      "Iteration 24, loss = 0.00347392\n",
      "Iteration 25, loss = 0.00281780\n",
      "Iteration 26, loss = 0.00218139\n",
      "Iteration 27, loss = 0.00174396\n",
      "Iteration 28, loss = 0.00156121\n",
      "Iteration 29, loss = 0.00157640\n",
      "Iteration 30, loss = 0.00167011\n",
      "Iteration 31, loss = 0.00173256\n",
      "Iteration 32, loss = 0.00170548\n",
      "Iteration 33, loss = 0.00158883\n",
      "Iteration 34, loss = 0.00142888\n",
      "Iteration 35, loss = 0.00128892\n",
      "Iteration 36, loss = 0.00121394\n",
      "Iteration 37, loss = 0.00121580\n",
      "Iteration 38, loss = 0.00127210\n",
      "Iteration 39, loss = 0.00133996\n",
      "Iteration 40, loss = 0.00137663\n",
      "Iteration 41, loss = 0.00135802\n",
      "Iteration 42, loss = 0.00128660\n",
      "Iteration 43, loss = 0.00118630\n",
      "Iteration 44, loss = 0.00108898\n",
      "Iteration 45, loss = 0.00101898\n",
      "Iteration 46, loss = 0.00098324\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06798906\n",
      "Iteration 2, loss = 0.03455249\n",
      "Iteration 3, loss = 0.01819833\n",
      "Iteration 4, loss = 0.01615618\n",
      "Iteration 5, loss = 0.02034984\n",
      "Iteration 6, loss = 0.02229528\n",
      "Iteration 7, loss = 0.01969568\n",
      "Iteration 8, loss = 0.01438368\n",
      "Iteration 9, loss = 0.00891667\n",
      "Iteration 10, loss = 0.00495520\n",
      "Iteration 11, loss = 0.00315314\n",
      "Iteration 12, loss = 0.00319017\n",
      "Iteration 13, loss = 0.00416303\n",
      "Iteration 14, loss = 0.00512426\n",
      "Iteration 15, loss = 0.00543901\n",
      "Iteration 16, loss = 0.00498113\n",
      "Iteration 17, loss = 0.00408668\n",
      "Iteration 18, loss = 0.00325019\n",
      "Iteration 19, loss = 0.00288818\n",
      "Iteration 20, loss = 0.00307825\n",
      "Iteration 21, loss = 0.00356718\n",
      "Iteration 22, loss = 0.00393545\n",
      "Iteration 23, loss = 0.00390916\n",
      "Iteration 24, loss = 0.00347218\n",
      "Iteration 25, loss = 0.00281427\n",
      "Iteration 26, loss = 0.00218109\n",
      "Iteration 27, loss = 0.00175082\n",
      "Iteration 28, loss = 0.00157256\n",
      "Iteration 29, loss = 0.00158884\n",
      "Iteration 30, loss = 0.00168180\n",
      "Iteration 31, loss = 0.00174050\n",
      "Iteration 32, loss = 0.00170758\n",
      "Iteration 33, loss = 0.00158782\n",
      "Iteration 34, loss = 0.00142852\n",
      "Iteration 35, loss = 0.00129034\n",
      "Iteration 36, loss = 0.00121778\n",
      "Iteration 37, loss = 0.00122215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 0.00128005\n",
      "Iteration 39, loss = 0.00134808\n",
      "Iteration 40, loss = 0.00138330\n",
      "Iteration 41, loss = 0.00136262\n",
      "Iteration 42, loss = 0.00128934\n",
      "Iteration 43, loss = 0.00118782\n",
      "Iteration 44, loss = 0.00109017\n",
      "Iteration 45, loss = 0.00102033\n",
      "Iteration 46, loss = 0.00098498\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06799131\n",
      "Iteration 2, loss = 0.03455081\n",
      "Iteration 3, loss = 0.01819390\n",
      "Iteration 4, loss = 0.01615015\n",
      "Iteration 5, loss = 0.02034168\n",
      "Iteration 6, loss = 0.02228923\n",
      "Iteration 7, loss = 0.01969157\n",
      "Iteration 8, loss = 0.01438059\n",
      "Iteration 9, loss = 0.00891356\n",
      "Iteration 10, loss = 0.00495138\n",
      "Iteration 11, loss = 0.00314819\n",
      "Iteration 12, loss = 0.00318441\n",
      "Iteration 13, loss = 0.00415724\n",
      "Iteration 14, loss = 0.00511927\n",
      "Iteration 15, loss = 0.00543552\n",
      "Iteration 16, loss = 0.00497880\n",
      "Iteration 17, loss = 0.00408540\n",
      "Iteration 18, loss = 0.00324949\n",
      "Iteration 19, loss = 0.00288710\n",
      "Iteration 20, loss = 0.00307647\n",
      "Iteration 21, loss = 0.00356479\n",
      "Iteration 22, loss = 0.00393310\n",
      "Iteration 23, loss = 0.00390758\n",
      "Iteration 24, loss = 0.00347163\n",
      "Iteration 25, loss = 0.00281461\n",
      "Iteration 26, loss = 0.00218180\n",
      "Iteration 27, loss = 0.00175153\n",
      "Iteration 28, loss = 0.00157296\n",
      "Iteration 29, loss = 0.00158895\n",
      "Iteration 30, loss = 0.00168183\n",
      "Iteration 31, loss = 0.00174059\n",
      "Iteration 32, loss = 0.00170771\n",
      "Iteration 33, loss = 0.00158805\n",
      "Iteration 34, loss = 0.00142890\n",
      "Iteration 35, loss = 0.00129081\n",
      "Iteration 36, loss = 0.00121824\n",
      "Iteration 37, loss = 0.00122250\n",
      "Iteration 38, loss = 0.00128026\n",
      "Iteration 39, loss = 0.00134821\n",
      "Iteration 40, loss = 0.00138347\n",
      "Iteration 41, loss = 0.00136289\n",
      "Iteration 42, loss = 0.00128977\n",
      "Iteration 43, loss = 0.00118847\n",
      "Iteration 44, loss = 0.00109096\n",
      "Iteration 45, loss = 0.00102115\n",
      "Iteration 46, loss = 0.00098575\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06798888\n",
      "Iteration 2, loss = 0.03455278\n",
      "Iteration 3, loss = 0.01819974\n",
      "Iteration 4, loss = 0.01615844\n",
      "Iteration 5, loss = 0.02035038\n",
      "Iteration 6, loss = 0.02229486\n",
      "Iteration 7, loss = 0.01969446\n",
      "Iteration 8, loss = 0.01438214\n",
      "Iteration 9, loss = 0.00891527\n",
      "Iteration 10, loss = 0.00495430\n",
      "Iteration 11, loss = 0.00315291\n",
      "Iteration 12, loss = 0.00319052\n",
      "Iteration 13, loss = 0.00416368\n",
      "Iteration 14, loss = 0.00512498\n",
      "Iteration 15, loss = 0.00543952\n",
      "Iteration 16, loss = 0.00498097\n",
      "Iteration 17, loss = 0.00408611\n",
      "Iteration 18, loss = 0.00324962\n",
      "Iteration 19, loss = 0.00288799\n",
      "Iteration 20, loss = 0.00307856\n",
      "Iteration 21, loss = 0.00356770\n",
      "Iteration 22, loss = 0.00393577\n",
      "Iteration 23, loss = 0.00390912\n",
      "Iteration 24, loss = 0.00347184\n",
      "Iteration 25, loss = 0.00281381\n",
      "Iteration 26, loss = 0.00218072\n",
      "Iteration 27, loss = 0.00175061\n",
      "Iteration 28, loss = 0.00157251\n",
      "Iteration 29, loss = 0.00158888\n",
      "Iteration 30, loss = 0.00168186\n",
      "Iteration 31, loss = 0.00174040\n",
      "Iteration 32, loss = 0.00170728\n",
      "Iteration 33, loss = 0.00158739\n",
      "Iteration 34, loss = 0.00142803\n",
      "Iteration 35, loss = 0.00128987\n",
      "Iteration 36, loss = 0.00121739\n",
      "Iteration 37, loss = 0.00122185\n",
      "Iteration 38, loss = 0.00127981\n",
      "Iteration 39, loss = 0.00134785\n",
      "Iteration 40, loss = 0.00138301\n",
      "Iteration 41, loss = 0.00136223\n",
      "Iteration 42, loss = 0.00128891\n",
      "Iteration 43, loss = 0.00118740\n",
      "Iteration 44, loss = 0.00108979\n",
      "Iteration 45, loss = 0.00102002\n",
      "Iteration 46, loss = 0.00098472\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06798721\n",
      "Iteration 2, loss = 0.03455331\n",
      "Iteration 3, loss = 0.01820012\n",
      "Iteration 4, loss = 0.01615737\n",
      "Iteration 5, loss = 0.02035425\n",
      "Iteration 6, loss = 0.02229987\n",
      "Iteration 7, loss = 0.01969930\n",
      "Iteration 8, loss = 0.01438685\n",
      "Iteration 9, loss = 0.00892004\n",
      "Iteration 10, loss = 0.00495899\n",
      "Iteration 11, loss = 0.00315754\n",
      "Iteration 12, loss = 0.00319468\n",
      "Iteration 13, loss = 0.00416715\n",
      "Iteration 14, loss = 0.00512753\n",
      "Iteration 15, loss = 0.00544087\n",
      "Iteration 16, loss = 0.00498242\n",
      "Iteration 17, loss = 0.00408709\n",
      "Iteration 18, loss = 0.00324942\n",
      "Iteration 19, loss = 0.00288678\n",
      "Iteration 20, loss = 0.00307555\n",
      "Iteration 21, loss = 0.00356359\n",
      "Iteration 22, loss = 0.00393148\n",
      "Iteration 23, loss = 0.00390509\n",
      "Iteration 24, loss = 0.00346870\n",
      "Iteration 25, loss = 0.00281143\n",
      "Iteration 26, loss = 0.00217890\n",
      "Iteration 27, loss = 0.00174851\n",
      "Iteration 28, loss = 0.00157018\n",
      "Iteration 29, loss = 0.00158640\n",
      "Iteration 30, loss = 0.00167908\n",
      "Iteration 31, loss = 0.00173824\n",
      "Iteration 32, loss = 0.00170601\n",
      "Iteration 33, loss = 0.00158715\n",
      "Iteration 34, loss = 0.00142839\n",
      "Iteration 35, loss = 0.00129047\n",
      "Iteration 36, loss = 0.00121782\n",
      "Iteration 37, loss = 0.00122196\n",
      "Iteration 38, loss = 0.00127956\n",
      "Iteration 39, loss = 0.00134732\n",
      "Iteration 40, loss = 0.00138243\n",
      "Iteration 41, loss = 0.00136175\n",
      "Iteration 42, loss = 0.00128851\n",
      "Iteration 43, loss = 0.00118705\n",
      "Iteration 44, loss = 0.00108942\n",
      "Iteration 45, loss = 0.00101958\n",
      "Iteration 46, loss = 0.00098419\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06797247\n",
      "Iteration 2, loss = 0.03455671\n",
      "Iteration 3, loss = 0.01821611\n",
      "Iteration 4, loss = 0.01617506\n",
      "Iteration 5, loss = 0.02036718\n",
      "Iteration 6, loss = 0.02231250\n",
      "Iteration 7, loss = 0.01971154\n",
      "Iteration 8, loss = 0.01439813\n",
      "Iteration 9, loss = 0.00893161\n",
      "Iteration 10, loss = 0.00497306\n",
      "Iteration 11, loss = 0.00317683\n",
      "Iteration 12, loss = 0.00321922\n",
      "Iteration 13, loss = 0.00419463\n",
      "Iteration 14, loss = 0.00515378\n",
      "Iteration 15, loss = 0.00546014\n",
      "Iteration 16, loss = 0.00499343\n",
      "Iteration 17, loss = 0.00409007\n",
      "Iteration 18, loss = 0.00324450\n",
      "Iteration 19, loss = 0.00287719\n",
      "Iteration 20, loss = 0.00306458\n",
      "Iteration 21, loss = 0.00355400\n",
      "Iteration 22, loss = 0.00392595\n",
      "Iteration 23, loss = 0.00390444\n",
      "Iteration 24, loss = 0.00347030\n",
      "Iteration 25, loss = 0.00281160\n",
      "Iteration 26, loss = 0.00217589\n",
      "Iteration 27, loss = 0.00174109\n",
      "Iteration 28, loss = 0.00156132\n",
      "Iteration 29, loss = 0.00157833\n",
      "Iteration 30, loss = 0.00167179\n",
      "Iteration 31, loss = 0.00173268\n",
      "Iteration 32, loss = 0.00170354\n",
      "Iteration 33, loss = 0.00158578\n",
      "Iteration 34, loss = 0.00142598\n",
      "Iteration 35, loss = 0.00128737\n",
      "Iteration 36, loss = 0.00121407\n",
      "Iteration 37, loss = 0.00121744\n",
      "Iteration 38, loss = 0.00127439\n",
      "Iteration 39, loss = 0.00134194\n",
      "Iteration 40, loss = 0.00137753\n",
      "Iteration 41, loss = 0.00135784\n",
      "Iteration 42, loss = 0.00128556\n",
      "Iteration 43, loss = 0.00118478\n",
      "Iteration 44, loss = 0.00108747\n",
      "Iteration 45, loss = 0.00101773\n",
      "Iteration 46, loss = 0.00098234\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06793671\n",
      "Iteration 2, loss = 0.03455296\n",
      "Iteration 3, loss = 0.01823044\n",
      "Iteration 4, loss = 0.01618637\n",
      "Iteration 5, loss = 0.02036653\n",
      "Iteration 6, loss = 0.02231750\n",
      "Iteration 7, loss = 0.01972661\n",
      "Iteration 8, loss = 0.01441642\n",
      "Iteration 9, loss = 0.00894703\n",
      "Iteration 10, loss = 0.00498254\n",
      "Iteration 11, loss = 0.00318140\n",
      "Iteration 12, loss = 0.00322226\n",
      "Iteration 13, loss = 0.00419791\n",
      "Iteration 14, loss = 0.00515696\n",
      "Iteration 15, loss = 0.00546180\n",
      "Iteration 16, loss = 0.00499229\n",
      "Iteration 17, loss = 0.00408574\n",
      "Iteration 18, loss = 0.00323822\n",
      "Iteration 19, loss = 0.00287141\n",
      "Iteration 20, loss = 0.00306138\n",
      "Iteration 21, loss = 0.00355299\n",
      "Iteration 22, loss = 0.00392524\n",
      "Iteration 23, loss = 0.00390266\n",
      "Iteration 24, loss = 0.00346717\n",
      "Iteration 25, loss = 0.00280747\n",
      "Iteration 26, loss = 0.00217173\n",
      "Iteration 27, loss = 0.00173820\n",
      "Iteration 28, loss = 0.00156022\n",
      "Iteration 29, loss = 0.00157887\n",
      "Iteration 30, loss = 0.00167273\n",
      "Iteration 31, loss = 0.00173259\n",
      "Iteration 32, loss = 0.00170109\n",
      "Iteration 33, loss = 0.00158108\n",
      "Iteration 34, loss = 0.00141995\n",
      "Iteration 35, loss = 0.00128089\n",
      "Iteration 36, loss = 0.00120831\n",
      "Iteration 37, loss = 0.00121306\n",
      "Iteration 38, loss = 0.00127130\n",
      "Iteration 39, loss = 0.00133967\n",
      "Iteration 40, loss = 0.00137543\n",
      "Iteration 41, loss = 0.00135535\n",
      "Iteration 42, loss = 0.00128290\n",
      "Iteration 43, loss = 0.00118247\n",
      "Iteration 44, loss = 0.00108609\n",
      "Iteration 45, loss = 0.00101760\n",
      "Iteration 46, loss = 0.00098334\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06796409\n",
      "Iteration 2, loss = 0.03455672\n",
      "Iteration 3, loss = 0.01820852\n",
      "Iteration 4, loss = 0.01615456\n",
      "Iteration 5, loss = 0.02034465\n",
      "Iteration 6, loss = 0.02230479\n",
      "Iteration 7, loss = 0.01972063\n",
      "Iteration 8, loss = 0.01441625\n",
      "Iteration 9, loss = 0.00894880\n",
      "Iteration 10, loss = 0.00498315\n",
      "Iteration 11, loss = 0.00317857\n",
      "Iteration 12, loss = 0.00321465\n",
      "Iteration 13, loss = 0.00418774\n",
      "Iteration 14, loss = 0.00514779\n",
      "Iteration 15, loss = 0.00545672\n",
      "Iteration 16, loss = 0.00499315\n",
      "Iteration 17, loss = 0.00409143\n",
      "Iteration 18, loss = 0.00324520\n",
      "Iteration 19, loss = 0.00287486\n",
      "Iteration 20, loss = 0.00305824\n",
      "Iteration 21, loss = 0.00354639\n",
      "Iteration 22, loss = 0.00391999\n",
      "Iteration 23, loss = 0.00390218\n",
      "Iteration 24, loss = 0.00347205\n",
      "Iteration 25, loss = 0.00281570\n",
      "Iteration 26, loss = 0.00217974\n",
      "Iteration 27, loss = 0.00174309\n",
      "Iteration 28, loss = 0.00156088\n",
      "Iteration 29, loss = 0.00157641\n",
      "Iteration 30, loss = 0.00166987\n",
      "Iteration 31, loss = 0.00173161\n",
      "Iteration 32, loss = 0.00170340\n",
      "Iteration 33, loss = 0.00158585\n",
      "Iteration 34, loss = 0.00142555\n",
      "Iteration 35, loss = 0.00128581\n",
      "Iteration 36, loss = 0.00121143\n",
      "Iteration 37, loss = 0.00121404\n",
      "Iteration 38, loss = 0.00127086\n",
      "Iteration 39, loss = 0.00133886\n",
      "Iteration 40, loss = 0.00137530\n",
      "Iteration 41, loss = 0.00135648\n",
      "Iteration 42, loss = 0.00128505\n",
      "Iteration 43, loss = 0.00118486\n",
      "Iteration 44, loss = 0.00108788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45, loss = 0.00101834\n",
      "Iteration 46, loss = 0.00098302\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06797748\n",
      "Iteration 2, loss = 0.03455597\n",
      "Iteration 3, loss = 0.01820868\n",
      "Iteration 4, loss = 0.01616502\n",
      "Iteration 5, loss = 0.02035872\n",
      "Iteration 6, loss = 0.02230754\n",
      "Iteration 7, loss = 0.01970982\n",
      "Iteration 8, loss = 0.01439810\n",
      "Iteration 9, loss = 0.00893174\n",
      "Iteration 10, loss = 0.00497169\n",
      "Iteration 11, loss = 0.00317352\n",
      "Iteration 12, loss = 0.00321400\n",
      "Iteration 13, loss = 0.00418800\n",
      "Iteration 14, loss = 0.00514675\n",
      "Iteration 15, loss = 0.00545436\n",
      "Iteration 16, loss = 0.00499015\n",
      "Iteration 17, loss = 0.00408931\n",
      "Iteration 18, loss = 0.00324557\n",
      "Iteration 19, loss = 0.00287903\n",
      "Iteration 20, loss = 0.00306580\n",
      "Iteration 21, loss = 0.00355481\n",
      "Iteration 22, loss = 0.00392589\n",
      "Iteration 23, loss = 0.00390422\n",
      "Iteration 24, loss = 0.00347077\n",
      "Iteration 25, loss = 0.00281314\n",
      "Iteration 26, loss = 0.00217814\n",
      "Iteration 27, loss = 0.00174376\n",
      "Iteration 28, loss = 0.00156366\n",
      "Iteration 29, loss = 0.00158036\n",
      "Iteration 30, loss = 0.00167405\n",
      "Iteration 31, loss = 0.00173518\n",
      "Iteration 32, loss = 0.00170612\n",
      "Iteration 33, loss = 0.00158838\n",
      "Iteration 34, loss = 0.00142875\n",
      "Iteration 35, loss = 0.00128974\n",
      "Iteration 36, loss = 0.00121623\n",
      "Iteration 37, loss = 0.00121944\n",
      "Iteration 38, loss = 0.00127636\n",
      "Iteration 39, loss = 0.00134399\n",
      "Iteration 40, loss = 0.00137966\n",
      "Iteration 41, loss = 0.00135995\n",
      "Iteration 42, loss = 0.00128775\n",
      "Iteration 43, loss = 0.00118703\n",
      "Iteration 44, loss = 0.00108963\n",
      "Iteration 45, loss = 0.00101964\n",
      "Iteration 46, loss = 0.00098388\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06796606\n",
      "Iteration 2, loss = 0.03455678\n",
      "Iteration 3, loss = 0.01821767\n",
      "Iteration 4, loss = 0.01617475\n",
      "Iteration 5, loss = 0.02036514\n",
      "Iteration 6, loss = 0.02231068\n",
      "Iteration 7, loss = 0.01971093\n",
      "Iteration 8, loss = 0.01439855\n",
      "Iteration 9, loss = 0.00893252\n",
      "Iteration 10, loss = 0.00497393\n",
      "Iteration 11, loss = 0.00317747\n",
      "Iteration 12, loss = 0.00321962\n",
      "Iteration 13, loss = 0.00419472\n",
      "Iteration 14, loss = 0.00515343\n",
      "Iteration 15, loss = 0.00545961\n",
      "Iteration 16, loss = 0.00499354\n",
      "Iteration 17, loss = 0.00409094\n",
      "Iteration 18, loss = 0.00324569\n",
      "Iteration 19, loss = 0.00287817\n",
      "Iteration 20, loss = 0.00306496\n",
      "Iteration 21, loss = 0.00355369\n",
      "Iteration 22, loss = 0.00392532\n",
      "Iteration 23, loss = 0.00390388\n",
      "Iteration 24, loss = 0.00347003\n",
      "Iteration 25, loss = 0.00281160\n",
      "Iteration 26, loss = 0.00217601\n",
      "Iteration 27, loss = 0.00174111\n",
      "Iteration 28, loss = 0.00156124\n",
      "Iteration 29, loss = 0.00157809\n",
      "Iteration 30, loss = 0.00167137\n",
      "Iteration 31, loss = 0.00173250\n",
      "Iteration 32, loss = 0.00170365\n",
      "Iteration 33, loss = 0.00158594\n",
      "Iteration 34, loss = 0.00142613\n",
      "Iteration 35, loss = 0.00128734\n",
      "Iteration 36, loss = 0.00121375\n",
      "Iteration 37, loss = 0.00121693\n",
      "Iteration 38, loss = 0.00127383\n",
      "Iteration 39, loss = 0.00134155\n",
      "Iteration 40, loss = 0.00137752\n",
      "Iteration 41, loss = 0.00135814\n",
      "Iteration 42, loss = 0.00128599\n",
      "Iteration 43, loss = 0.00118518\n",
      "Iteration 44, loss = 0.00108777\n",
      "Iteration 45, loss = 0.00101793\n",
      "Iteration 46, loss = 0.00098253\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06794593\n",
      "Iteration 2, loss = 0.03455480\n",
      "Iteration 3, loss = 0.01822652\n",
      "Iteration 4, loss = 0.01618193\n",
      "Iteration 5, loss = 0.02036441\n",
      "Iteration 6, loss = 0.02230852\n",
      "Iteration 7, loss = 0.01971345\n",
      "Iteration 8, loss = 0.01440553\n",
      "Iteration 9, loss = 0.00894063\n",
      "Iteration 10, loss = 0.00497936\n",
      "Iteration 11, loss = 0.00317987\n",
      "Iteration 12, loss = 0.00322023\n",
      "Iteration 13, loss = 0.00419477\n",
      "Iteration 14, loss = 0.00515345\n",
      "Iteration 15, loss = 0.00545881\n",
      "Iteration 16, loss = 0.00499056\n",
      "Iteration 17, loss = 0.00408594\n",
      "Iteration 18, loss = 0.00324017\n",
      "Iteration 19, loss = 0.00287503\n",
      "Iteration 20, loss = 0.00306382\n",
      "Iteration 21, loss = 0.00355344\n",
      "Iteration 22, loss = 0.00392459\n",
      "Iteration 23, loss = 0.00390189\n",
      "Iteration 24, loss = 0.00346686\n",
      "Iteration 25, loss = 0.00280793\n",
      "Iteration 26, loss = 0.00217266\n",
      "Iteration 27, loss = 0.00173876\n",
      "Iteration 28, loss = 0.00156031\n",
      "Iteration 29, loss = 0.00157845\n",
      "Iteration 30, loss = 0.00167214\n",
      "Iteration 31, loss = 0.00173219\n",
      "Iteration 32, loss = 0.00170107\n",
      "Iteration 33, loss = 0.00158144\n",
      "Iteration 34, loss = 0.00142058\n",
      "Iteration 35, loss = 0.00128174\n",
      "Iteration 36, loss = 0.00120913\n",
      "Iteration 37, loss = 0.00121369\n",
      "Iteration 38, loss = 0.00127167\n",
      "Iteration 39, loss = 0.00133981\n",
      "Iteration 40, loss = 0.00137543\n",
      "Iteration 41, loss = 0.00135536\n",
      "Iteration 42, loss = 0.00128289\n",
      "Iteration 43, loss = 0.00118241\n",
      "Iteration 44, loss = 0.00108596\n",
      "Iteration 45, loss = 0.00101738\n",
      "Iteration 46, loss = 0.00098302\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06793027\n",
      "Iteration 2, loss = 0.03455238\n",
      "Iteration 3, loss = 0.01822807\n",
      "Iteration 4, loss = 0.01617064\n",
      "Iteration 5, loss = 0.02035731\n",
      "Iteration 6, loss = 0.02231932\n",
      "Iteration 7, loss = 0.01973832\n",
      "Iteration 8, loss = 0.01443063\n",
      "Iteration 9, loss = 0.00895571\n",
      "Iteration 10, loss = 0.00498680\n",
      "Iteration 11, loss = 0.00318347\n",
      "Iteration 12, loss = 0.00322326\n",
      "Iteration 13, loss = 0.00419951\n",
      "Iteration 14, loss = 0.00516014\n",
      "Iteration 15, loss = 0.00546588\n",
      "Iteration 16, loss = 0.00499633\n",
      "Iteration 17, loss = 0.00408948\n",
      "Iteration 18, loss = 0.00324089\n",
      "Iteration 19, loss = 0.00287276\n",
      "Iteration 20, loss = 0.00306131\n",
      "Iteration 21, loss = 0.00355360\n",
      "Iteration 22, loss = 0.00392690\n",
      "Iteration 23, loss = 0.00390549\n",
      "Iteration 24, loss = 0.00347074\n",
      "Iteration 25, loss = 0.00281099\n",
      "Iteration 26, loss = 0.00217462\n",
      "Iteration 27, loss = 0.00174042\n",
      "Iteration 28, loss = 0.00156150\n",
      "Iteration 29, loss = 0.00157939\n",
      "Iteration 30, loss = 0.00167295\n",
      "Iteration 31, loss = 0.00173294\n",
      "Iteration 32, loss = 0.00170179\n",
      "Iteration 33, loss = 0.00158200\n",
      "Iteration 34, loss = 0.00142092\n",
      "Iteration 35, loss = 0.00128163\n",
      "Iteration 36, loss = 0.00120874\n",
      "Iteration 37, loss = 0.00121314\n",
      "Iteration 38, loss = 0.00127120\n",
      "Iteration 39, loss = 0.00133956\n",
      "Iteration 40, loss = 0.00137547\n",
      "Iteration 41, loss = 0.00135574\n",
      "Iteration 42, loss = 0.00128353\n",
      "Iteration 43, loss = 0.00118321\n",
      "Iteration 44, loss = 0.00108678\n",
      "Iteration 45, loss = 0.00101811\n",
      "Iteration 46, loss = 0.00098366\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06787459\n",
      "Iteration 2, loss = 0.03454993\n",
      "Iteration 3, loss = 0.01821627\n",
      "Iteration 4, loss = 0.01613809\n",
      "Iteration 5, loss = 0.02031592\n",
      "Iteration 6, loss = 0.02228822\n",
      "Iteration 7, loss = 0.01972780\n",
      "Iteration 8, loss = 0.01443304\n",
      "Iteration 9, loss = 0.00896605\n",
      "Iteration 10, loss = 0.00499708\n",
      "Iteration 11, loss = 0.00318134\n",
      "Iteration 12, loss = 0.00320722\n",
      "Iteration 13, loss = 0.00417568\n",
      "Iteration 14, loss = 0.00513226\n",
      "Iteration 15, loss = 0.00543788\n",
      "Iteration 16, loss = 0.00497257\n",
      "Iteration 17, loss = 0.00406527\n",
      "Iteration 18, loss = 0.00321542\n",
      "Iteration 19, loss = 0.00284684\n",
      "Iteration 20, loss = 0.00304252\n",
      "Iteration 21, loss = 0.00354330\n",
      "Iteration 22, loss = 0.00392711\n",
      "Iteration 23, loss = 0.00390586\n",
      "Iteration 24, loss = 0.00346214\n",
      "Iteration 25, loss = 0.00279279\n",
      "Iteration 26, loss = 0.00215511\n",
      "Iteration 27, loss = 0.00172820\n",
      "Iteration 28, loss = 0.00156189\n",
      "Iteration 29, loss = 0.00158935\n",
      "Iteration 30, loss = 0.00168550\n",
      "Iteration 31, loss = 0.00173946\n",
      "Iteration 32, loss = 0.00169628\n",
      "Iteration 33, loss = 0.00156301\n",
      "Iteration 34, loss = 0.00139314\n",
      "Iteration 35, loss = 0.00125287\n",
      "Iteration 36, loss = 0.00118725\n",
      "Iteration 37, loss = 0.00120343\n",
      "Iteration 38, loss = 0.00127259\n",
      "Iteration 39, loss = 0.00134568\n",
      "Iteration 40, loss = 0.00137848\n",
      "Iteration 41, loss = 0.00135042\n",
      "Iteration 42, loss = 0.00127030\n",
      "Iteration 43, loss = 0.00116778\n",
      "Iteration 44, loss = 0.00107625\n",
      "Iteration 45, loss = 0.00101685\n",
      "Iteration 46, loss = 0.00099193\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06785850\n",
      "Iteration 2, loss = 0.03455638\n",
      "Iteration 3, loss = 0.01814865\n",
      "Iteration 4, loss = 0.01597411\n",
      "Iteration 5, loss = 0.02014008\n",
      "Iteration 6, loss = 0.02218724\n",
      "Iteration 7, loss = 0.01970251\n",
      "Iteration 8, loss = 0.01446521\n",
      "Iteration 9, loss = 0.00901991\n",
      "Iteration 10, loss = 0.00504016\n",
      "Iteration 11, loss = 0.00319403\n",
      "Iteration 12, loss = 0.00318087\n",
      "Iteration 13, loss = 0.00411580\n",
      "Iteration 14, loss = 0.00506024\n",
      "Iteration 15, loss = 0.00538454\n",
      "Iteration 16, loss = 0.00495459\n",
      "Iteration 17, loss = 0.00407211\n",
      "Iteration 18, loss = 0.00322044\n",
      "Iteration 19, loss = 0.00282065\n",
      "Iteration 20, loss = 0.00298433\n",
      "Iteration 21, loss = 0.00347202\n",
      "Iteration 22, loss = 0.00387597\n",
      "Iteration 23, loss = 0.00389259\n",
      "Iteration 24, loss = 0.00348288\n",
      "Iteration 25, loss = 0.00283097\n",
      "Iteration 26, loss = 0.00218969\n",
      "Iteration 27, loss = 0.00174593\n",
      "Iteration 28, loss = 0.00156034\n",
      "Iteration 29, loss = 0.00157472\n",
      "Iteration 30, loss = 0.00166699\n",
      "Iteration 31, loss = 0.00172497\n",
      "Iteration 32, loss = 0.00168900\n",
      "Iteration 33, loss = 0.00156283\n",
      "Iteration 34, loss = 0.00139467\n",
      "Iteration 35, loss = 0.00124997\n",
      "Iteration 36, loss = 0.00117670\n",
      "Iteration 37, loss = 0.00118575\n",
      "Iteration 38, loss = 0.00125142\n",
      "Iteration 39, loss = 0.00132568\n",
      "Iteration 40, loss = 0.00136363\n",
      "Iteration 41, loss = 0.00134194\n",
      "Iteration 42, loss = 0.00126679\n",
      "Iteration 43, loss = 0.00116651\n",
      "Iteration 44, loss = 0.00107483\n",
      "Iteration 45, loss = 0.00101396\n",
      "Iteration 46, loss = 0.00098745\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.06787284\n",
      "Iteration 2, loss = 0.03455670\n",
      "Iteration 3, loss = 0.01811689\n",
      "Iteration 4, loss = 0.01591444\n",
      "Iteration 5, loss = 0.02011426\n",
      "Iteration 6, loss = 0.02220865\n",
      "Iteration 7, loss = 0.01976279\n",
      "Iteration 8, loss = 0.01454032\n",
      "Iteration 9, loss = 0.00908896\n",
      "Iteration 10, loss = 0.00509130\n",
      "Iteration 11, loss = 0.00322804\n",
      "Iteration 12, loss = 0.00319969\n",
      "Iteration 13, loss = 0.00412698\n",
      "Iteration 14, loss = 0.00507226\n",
      "Iteration 15, loss = 0.00540812\n",
      "Iteration 16, loss = 0.00498610\n",
      "Iteration 17, loss = 0.00410488\n",
      "Iteration 18, loss = 0.00324214\n",
      "Iteration 19, loss = 0.00282235\n",
      "Iteration 20, loss = 0.00296556\n",
      "Iteration 21, loss = 0.00344628\n",
      "Iteration 22, loss = 0.00385976\n",
      "Iteration 23, loss = 0.00390014\n",
      "Iteration 24, loss = 0.00351816\n",
      "Iteration 25, loss = 0.00288305\n",
      "Iteration 26, loss = 0.00224023\n",
      "Iteration 27, loss = 0.00177857\n",
      "Iteration 28, loss = 0.00156755\n",
      "Iteration 29, loss = 0.00156328\n",
      "Iteration 30, loss = 0.00165115\n",
      "Iteration 31, loss = 0.00171855\n",
      "Iteration 32, loss = 0.00170089\n",
      "Iteration 33, loss = 0.00159388\n",
      "Iteration 34, loss = 0.00143730\n",
      "Iteration 35, loss = 0.00129196\n",
      "Iteration 36, loss = 0.00120570\n",
      "Iteration 37, loss = 0.00119629\n",
      "Iteration 38, loss = 0.00124619\n",
      "Iteration 39, loss = 0.00131507\n",
      "Iteration 40, loss = 0.00135912\n",
      "Iteration 41, loss = 0.00135089\n",
      "Iteration 42, loss = 0.00128829\n",
      "Iteration 43, loss = 0.00119234\n",
      "Iteration 44, loss = 0.00109380\n",
      "Iteration 45, loss = 0.00101906\n",
      "Iteration 46, loss = 0.00097853\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06786529\n",
      "Iteration 2, loss = 0.03455513\n",
      "Iteration 3, loss = 0.01817253\n",
      "Iteration 4, loss = 0.01602081\n",
      "Iteration 5, loss = 0.02020240\n",
      "Iteration 6, loss = 0.02223888\n",
      "Iteration 7, loss = 0.01973917\n",
      "Iteration 8, loss = 0.01449056\n",
      "Iteration 9, loss = 0.00903190\n",
      "Iteration 10, loss = 0.00504017\n",
      "Iteration 11, loss = 0.00319400\n",
      "Iteration 12, loss = 0.00319461\n",
      "Iteration 13, loss = 0.00415700\n",
      "Iteration 14, loss = 0.00513041\n",
      "Iteration 15, loss = 0.00545503\n",
      "Iteration 16, loss = 0.00499356\n",
      "Iteration 17, loss = 0.00407863\n",
      "Iteration 18, loss = 0.00320964\n",
      "Iteration 19, loss = 0.00281318\n",
      "Iteration 20, loss = 0.00298450\n",
      "Iteration 21, loss = 0.00347420\n",
      "Iteration 22, loss = 0.00386724\n",
      "Iteration 23, loss = 0.00387299\n",
      "Iteration 24, loss = 0.00346149\n",
      "Iteration 25, loss = 0.00281516\n",
      "Iteration 26, loss = 0.00218062\n",
      "Iteration 27, loss = 0.00174117\n",
      "Iteration 28, loss = 0.00155590\n",
      "Iteration 29, loss = 0.00156994\n",
      "Iteration 30, loss = 0.00166477\n",
      "Iteration 31, loss = 0.00172862\n",
      "Iteration 32, loss = 0.00170199\n",
      "Iteration 33, loss = 0.00158601\n",
      "Iteration 34, loss = 0.00142500\n",
      "Iteration 35, loss = 0.00128160\n",
      "Iteration 36, loss = 0.00120242\n",
      "Iteration 37, loss = 0.00120116\n",
      "Iteration 38, loss = 0.00125655\n",
      "Iteration 39, loss = 0.00132587\n",
      "Iteration 40, loss = 0.00136550\n",
      "Iteration 41, loss = 0.00135045\n",
      "Iteration 42, loss = 0.00128201\n",
      "Iteration 43, loss = 0.00118325\n",
      "Iteration 44, loss = 0.00108623\n",
      "Iteration 45, loss = 0.00101570\n",
      "Iteration 46, loss = 0.00097981\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06786680\n",
      "Iteration 2, loss = 0.03455575\n",
      "Iteration 3, loss = 0.01816401\n",
      "Iteration 4, loss = 0.01600390\n",
      "Iteration 5, loss = 0.02018856\n",
      "Iteration 6, loss = 0.02223414\n",
      "Iteration 7, loss = 0.01974314\n",
      "Iteration 8, loss = 0.01449479\n",
      "Iteration 9, loss = 0.00903679\n",
      "Iteration 10, loss = 0.00504390\n",
      "Iteration 11, loss = 0.00319723\n",
      "Iteration 12, loss = 0.00319543\n",
      "Iteration 13, loss = 0.00415644\n",
      "Iteration 14, loss = 0.00512991\n",
      "Iteration 15, loss = 0.00545742\n",
      "Iteration 16, loss = 0.00499751\n",
      "Iteration 17, loss = 0.00408282\n",
      "Iteration 18, loss = 0.00321213\n",
      "Iteration 19, loss = 0.00281087\n",
      "Iteration 20, loss = 0.00297787\n",
      "Iteration 21, loss = 0.00346594\n",
      "Iteration 22, loss = 0.00386282\n",
      "Iteration 23, loss = 0.00387413\n",
      "Iteration 24, loss = 0.00346777\n",
      "Iteration 25, loss = 0.00282336\n",
      "Iteration 26, loss = 0.00218826\n",
      "Iteration 27, loss = 0.00174458\n",
      "Iteration 28, loss = 0.00155439\n",
      "Iteration 29, loss = 0.00156501\n",
      "Iteration 30, loss = 0.00165897\n",
      "Iteration 31, loss = 0.00172454\n",
      "Iteration 32, loss = 0.00170136\n",
      "Iteration 33, loss = 0.00158878\n",
      "Iteration 34, loss = 0.00142971\n",
      "Iteration 35, loss = 0.00128610\n",
      "Iteration 36, loss = 0.00120464\n",
      "Iteration 37, loss = 0.00120043\n",
      "Iteration 38, loss = 0.00125355\n",
      "Iteration 39, loss = 0.00132225\n",
      "Iteration 40, loss = 0.00136288\n",
      "Iteration 41, loss = 0.00134950\n",
      "Iteration 42, loss = 0.00128242\n",
      "Iteration 43, loss = 0.00118391\n",
      "Iteration 44, loss = 0.00108607\n",
      "Iteration 45, loss = 0.00101412\n",
      "Iteration 46, loss = 0.00097679\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06786961\n",
      "Iteration 2, loss = 0.03455580\n",
      "Iteration 3, loss = 0.01816419\n",
      "Iteration 4, loss = 0.01600590\n",
      "Iteration 5, loss = 0.02019296\n",
      "Iteration 6, loss = 0.02223749\n",
      "Iteration 7, loss = 0.01974479\n",
      "Iteration 8, loss = 0.01449527\n",
      "Iteration 9, loss = 0.00903696\n",
      "Iteration 10, loss = 0.00504461\n",
      "Iteration 11, loss = 0.00319814\n",
      "Iteration 12, loss = 0.00319620\n",
      "Iteration 13, loss = 0.00415630\n",
      "Iteration 14, loss = 0.00512876\n",
      "Iteration 15, loss = 0.00545690\n",
      "Iteration 16, loss = 0.00499814\n",
      "Iteration 17, loss = 0.00408437\n",
      "Iteration 18, loss = 0.00321402\n",
      "Iteration 19, loss = 0.00281281\n",
      "Iteration 20, loss = 0.00297969\n",
      "Iteration 21, loss = 0.00346763\n",
      "Iteration 22, loss = 0.00386477\n",
      "Iteration 23, loss = 0.00387617\n",
      "Iteration 24, loss = 0.00346961\n",
      "Iteration 25, loss = 0.00282489\n",
      "Iteration 26, loss = 0.00218945\n",
      "Iteration 27, loss = 0.00174555\n",
      "Iteration 28, loss = 0.00155534\n",
      "Iteration 29, loss = 0.00156598\n",
      "Iteration 30, loss = 0.00165995\n",
      "Iteration 31, loss = 0.00172544\n",
      "Iteration 32, loss = 0.00170187\n",
      "Iteration 33, loss = 0.00158891\n",
      "Iteration 34, loss = 0.00142962\n",
      "Iteration 35, loss = 0.00128598\n",
      "Iteration 36, loss = 0.00120462\n",
      "Iteration 37, loss = 0.00120060\n",
      "Iteration 38, loss = 0.00125393\n",
      "Iteration 39, loss = 0.00132277\n",
      "Iteration 40, loss = 0.00136339\n",
      "Iteration 41, loss = 0.00135000\n",
      "Iteration 42, loss = 0.00128279\n",
      "Iteration 43, loss = 0.00118419\n",
      "Iteration 44, loss = 0.00108633\n",
      "Iteration 45, loss = 0.00101454\n",
      "Iteration 46, loss = 0.00097743\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06786514\n",
      "Iteration 2, loss = 0.03455449\n",
      "Iteration 3, loss = 0.01817640\n",
      "Iteration 4, loss = 0.01603201\n",
      "Iteration 5, loss = 0.02021593\n",
      "Iteration 6, loss = 0.02224582\n",
      "Iteration 7, loss = 0.01974076\n",
      "Iteration 8, loss = 0.01448613\n",
      "Iteration 9, loss = 0.00902785\n",
      "Iteration 10, loss = 0.00503842\n",
      "Iteration 11, loss = 0.00319725\n",
      "Iteration 12, loss = 0.00319979\n",
      "Iteration 13, loss = 0.00416078\n",
      "Iteration 14, loss = 0.00513176\n",
      "Iteration 15, loss = 0.00545674\n",
      "Iteration 16, loss = 0.00499484\n",
      "Iteration 17, loss = 0.00408055\n",
      "Iteration 18, loss = 0.00321345\n",
      "Iteration 19, loss = 0.00281807\n",
      "Iteration 20, loss = 0.00298947\n",
      "Iteration 21, loss = 0.00347877\n",
      "Iteration 22, loss = 0.00387190\n",
      "Iteration 23, loss = 0.00387783\n",
      "Iteration 24, loss = 0.00346676\n",
      "Iteration 25, loss = 0.00282008\n",
      "Iteration 26, loss = 0.00218556\n",
      "Iteration 27, loss = 0.00174465\n",
      "Iteration 28, loss = 0.00155767\n",
      "Iteration 29, loss = 0.00157026\n",
      "Iteration 30, loss = 0.00166478\n",
      "Iteration 31, loss = 0.00172887\n",
      "Iteration 32, loss = 0.00170329\n",
      "Iteration 33, loss = 0.00158854\n",
      "Iteration 34, loss = 0.00142856\n",
      "Iteration 35, loss = 0.00128531\n",
      "Iteration 36, loss = 0.00120518\n",
      "Iteration 37, loss = 0.00120260\n",
      "Iteration 38, loss = 0.00125704\n",
      "Iteration 39, loss = 0.00132607\n",
      "Iteration 40, loss = 0.00136610\n",
      "Iteration 41, loss = 0.00135155\n",
      "Iteration 42, loss = 0.00128331\n",
      "Iteration 43, loss = 0.00118431\n",
      "Iteration 44, loss = 0.00108667\n",
      "Iteration 45, loss = 0.00101540\n",
      "Iteration 46, loss = 0.00097881\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06786069\n",
      "Iteration 2, loss = 0.03455419\n",
      "Iteration 3, loss = 0.01818016\n",
      "Iteration 4, loss = 0.01603497\n",
      "Iteration 5, loss = 0.02021141\n",
      "Iteration 6, loss = 0.02223713\n",
      "Iteration 7, loss = 0.01973663\n",
      "Iteration 8, loss = 0.01448758\n",
      "Iteration 9, loss = 0.00902904\n",
      "Iteration 10, loss = 0.00503557\n",
      "Iteration 11, loss = 0.00318841\n",
      "Iteration 12, loss = 0.00319116\n",
      "Iteration 13, loss = 0.00415521\n",
      "Iteration 14, loss = 0.00512750\n",
      "Iteration 15, loss = 0.00544756\n",
      "Iteration 16, loss = 0.00498263\n",
      "Iteration 17, loss = 0.00406647\n",
      "Iteration 18, loss = 0.00320108\n",
      "Iteration 19, loss = 0.00281135\n",
      "Iteration 20, loss = 0.00299225\n",
      "Iteration 21, loss = 0.00348537\n",
      "Iteration 22, loss = 0.00387755\n",
      "Iteration 23, loss = 0.00387591\n",
      "Iteration 24, loss = 0.00345344\n",
      "Iteration 25, loss = 0.00279927\n",
      "Iteration 26, loss = 0.00216573\n",
      "Iteration 27, loss = 0.00173351\n",
      "Iteration 28, loss = 0.00155837\n",
      "Iteration 29, loss = 0.00158171\n",
      "Iteration 30, loss = 0.00167856\n",
      "Iteration 31, loss = 0.00173746\n",
      "Iteration 32, loss = 0.00170178\n",
      "Iteration 33, loss = 0.00157556\n",
      "Iteration 34, loss = 0.00140754\n",
      "Iteration 35, loss = 0.00126322\n",
      "Iteration 36, loss = 0.00118892\n",
      "Iteration 37, loss = 0.00119554\n",
      "Iteration 38, loss = 0.00125774\n",
      "Iteration 39, loss = 0.00133010\n",
      "Iteration 40, loss = 0.00136755\n",
      "Iteration 41, loss = 0.00134704\n",
      "Iteration 42, loss = 0.00127350\n",
      "Iteration 43, loss = 0.00117340\n",
      "Iteration 44, loss = 0.00107996\n",
      "Iteration 45, loss = 0.00101641\n",
      "Iteration 46, loss = 0.00098748\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06785481\n",
      "Iteration 2, loss = 0.03455615\n",
      "Iteration 3, loss = 0.01814793\n",
      "Iteration 4, loss = 0.01597297\n",
      "Iteration 5, loss = 0.02014369\n",
      "Iteration 6, loss = 0.02219151\n",
      "Iteration 7, loss = 0.01972180\n",
      "Iteration 8, loss = 0.01449600\n",
      "Iteration 9, loss = 0.00904631\n",
      "Iteration 10, loss = 0.00505585\n",
      "Iteration 11, loss = 0.00320062\n",
      "Iteration 12, loss = 0.00318448\n",
      "Iteration 13, loss = 0.00412139\n",
      "Iteration 14, loss = 0.00506800\n",
      "Iteration 15, loss = 0.00539180\n",
      "Iteration 16, loss = 0.00495844\n",
      "Iteration 17, loss = 0.00407251\n",
      "Iteration 18, loss = 0.00321804\n",
      "Iteration 19, loss = 0.00281759\n",
      "Iteration 20, loss = 0.00298298\n",
      "Iteration 21, loss = 0.00347414\n",
      "Iteration 22, loss = 0.00388079\n",
      "Iteration 23, loss = 0.00389718\n",
      "Iteration 24, loss = 0.00348522\n",
      "Iteration 25, loss = 0.00283078\n",
      "Iteration 26, loss = 0.00218838\n",
      "Iteration 27, loss = 0.00174571\n",
      "Iteration 28, loss = 0.00156233\n",
      "Iteration 29, loss = 0.00157988\n",
      "Iteration 30, loss = 0.00167300\n",
      "Iteration 31, loss = 0.00172984\n",
      "Iteration 32, loss = 0.00169188\n",
      "Iteration 33, loss = 0.00156311\n",
      "Iteration 34, loss = 0.00139249\n",
      "Iteration 35, loss = 0.00124757\n",
      "Iteration 36, loss = 0.00117577\n",
      "Iteration 37, loss = 0.00118700\n",
      "Iteration 38, loss = 0.00125437\n",
      "Iteration 39, loss = 0.00132949\n",
      "Iteration 40, loss = 0.00136649\n",
      "Iteration 41, loss = 0.00134300\n",
      "Iteration 42, loss = 0.00126637\n",
      "Iteration 43, loss = 0.00116571\n",
      "Iteration 44, loss = 0.00107470\n",
      "Iteration 45, loss = 0.00101547\n",
      "Iteration 46, loss = 0.00099046\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06787936\n",
      "Iteration 2, loss = 0.03455680\n",
      "Iteration 3, loss = 0.01813815\n",
      "Iteration 4, loss = 0.01596572\n",
      "Iteration 5, loss = 0.02016128\n",
      "Iteration 6, loss = 0.02222767\n",
      "Iteration 7, loss = 0.01975355\n",
      "Iteration 8, loss = 0.01451367\n",
      "Iteration 9, loss = 0.00905655\n",
      "Iteration 10, loss = 0.00505800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.00320266\n",
      "Iteration 12, loss = 0.00319287\n",
      "Iteration 13, loss = 0.00414733\n",
      "Iteration 14, loss = 0.00511723\n",
      "Iteration 15, loss = 0.00544988\n",
      "Iteration 16, loss = 0.00499772\n",
      "Iteration 17, loss = 0.00408830\n",
      "Iteration 18, loss = 0.00321706\n",
      "Iteration 19, loss = 0.00281035\n",
      "Iteration 20, loss = 0.00297176\n",
      "Iteration 21, loss = 0.00345738\n",
      "Iteration 22, loss = 0.00385780\n",
      "Iteration 23, loss = 0.00387579\n",
      "Iteration 24, loss = 0.00347538\n",
      "Iteration 25, loss = 0.00283385\n",
      "Iteration 26, loss = 0.00219801\n",
      "Iteration 27, loss = 0.00175102\n",
      "Iteration 28, loss = 0.00155683\n",
      "Iteration 29, loss = 0.00156490\n",
      "Iteration 30, loss = 0.00165811\n",
      "Iteration 31, loss = 0.00172451\n",
      "Iteration 32, loss = 0.00170301\n",
      "Iteration 33, loss = 0.00159185\n",
      "Iteration 34, loss = 0.00143320\n",
      "Iteration 35, loss = 0.00128890\n",
      "Iteration 36, loss = 0.00120615\n",
      "Iteration 37, loss = 0.00120062\n",
      "Iteration 38, loss = 0.00125293\n",
      "Iteration 39, loss = 0.00132168\n",
      "Iteration 40, loss = 0.00136323\n",
      "Iteration 41, loss = 0.00135108\n",
      "Iteration 42, loss = 0.00128504\n",
      "Iteration 43, loss = 0.00118712\n",
      "Iteration 44, loss = 0.00108911\n",
      "Iteration 45, loss = 0.00101649\n",
      "Iteration 46, loss = 0.00097838\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06787599\n",
      "Iteration 2, loss = 0.03455179\n",
      "Iteration 3, loss = 0.01819813\n",
      "Iteration 4, loss = 0.01608896\n",
      "Iteration 5, loss = 0.02027814\n",
      "Iteration 6, loss = 0.02228211\n",
      "Iteration 7, loss = 0.01974667\n",
      "Iteration 8, loss = 0.01447259\n",
      "Iteration 9, loss = 0.00901213\n",
      "Iteration 10, loss = 0.00503365\n",
      "Iteration 11, loss = 0.00320290\n",
      "Iteration 12, loss = 0.00321067\n",
      "Iteration 13, loss = 0.00416597\n",
      "Iteration 14, loss = 0.00512413\n",
      "Iteration 15, loss = 0.00544358\n",
      "Iteration 16, loss = 0.00498879\n",
      "Iteration 17, loss = 0.00408597\n",
      "Iteration 18, loss = 0.00322673\n",
      "Iteration 19, loss = 0.00283826\n",
      "Iteration 20, loss = 0.00301272\n",
      "Iteration 21, loss = 0.00350290\n",
      "Iteration 22, loss = 0.00389241\n",
      "Iteration 23, loss = 0.00389339\n",
      "Iteration 24, loss = 0.00347680\n",
      "Iteration 25, loss = 0.00282609\n",
      "Iteration 26, loss = 0.00218931\n",
      "Iteration 27, loss = 0.00174845\n",
      "Iteration 28, loss = 0.00156160\n",
      "Iteration 29, loss = 0.00157390\n",
      "Iteration 30, loss = 0.00166710\n",
      "Iteration 31, loss = 0.00172938\n",
      "Iteration 32, loss = 0.00170137\n",
      "Iteration 33, loss = 0.00158479\n",
      "Iteration 34, loss = 0.00142428\n",
      "Iteration 35, loss = 0.00128204\n",
      "Iteration 36, loss = 0.00120412\n",
      "Iteration 37, loss = 0.00120410\n",
      "Iteration 38, loss = 0.00126021\n",
      "Iteration 39, loss = 0.00132962\n",
      "Iteration 40, loss = 0.00136876\n",
      "Iteration 41, loss = 0.00135278\n",
      "Iteration 42, loss = 0.00128341\n",
      "Iteration 43, loss = 0.00118401\n",
      "Iteration 44, loss = 0.00108671\n",
      "Iteration 45, loss = 0.00101618\n",
      "Iteration 46, loss = 0.00098017\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06786080\n",
      "Iteration 2, loss = 0.03454701\n",
      "Iteration 3, loss = 0.01821459\n",
      "Iteration 4, loss = 0.01612420\n",
      "Iteration 5, loss = 0.02030581\n",
      "Iteration 6, loss = 0.02229216\n",
      "Iteration 7, loss = 0.01974034\n",
      "Iteration 8, loss = 0.01445916\n",
      "Iteration 9, loss = 0.00899735\n",
      "Iteration 10, loss = 0.00502157\n",
      "Iteration 11, loss = 0.00319656\n",
      "Iteration 12, loss = 0.00321010\n",
      "Iteration 13, loss = 0.00416820\n",
      "Iteration 14, loss = 0.00512505\n",
      "Iteration 15, loss = 0.00543864\n",
      "Iteration 16, loss = 0.00497807\n",
      "Iteration 17, loss = 0.00407317\n",
      "Iteration 18, loss = 0.00321917\n",
      "Iteration 19, loss = 0.00283965\n",
      "Iteration 20, loss = 0.00302154\n",
      "Iteration 21, loss = 0.00351343\n",
      "Iteration 22, loss = 0.00389854\n",
      "Iteration 23, loss = 0.00389173\n",
      "Iteration 24, loss = 0.00346829\n",
      "Iteration 25, loss = 0.00281496\n",
      "Iteration 26, loss = 0.00217954\n",
      "Iteration 27, loss = 0.00174292\n",
      "Iteration 28, loss = 0.00156068\n",
      "Iteration 29, loss = 0.00157595\n",
      "Iteration 30, loss = 0.00166953\n",
      "Iteration 31, loss = 0.00173046\n",
      "Iteration 32, loss = 0.00169964\n",
      "Iteration 33, loss = 0.00158051\n",
      "Iteration 34, loss = 0.00141903\n",
      "Iteration 35, loss = 0.00127786\n",
      "Iteration 36, loss = 0.00120246\n",
      "Iteration 37, loss = 0.00120513\n",
      "Iteration 38, loss = 0.00126298\n",
      "Iteration 39, loss = 0.00133251\n",
      "Iteration 40, loss = 0.00137030\n",
      "Iteration 41, loss = 0.00135234\n",
      "Iteration 42, loss = 0.00128129\n",
      "Iteration 43, loss = 0.00118132\n",
      "Iteration 44, loss = 0.00108460\n",
      "Iteration 45, loss = 0.00101540\n",
      "Iteration 46, loss = 0.00098070\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06785500\n",
      "Iteration 2, loss = 0.03454458\n",
      "Iteration 3, loss = 0.01821755\n",
      "Iteration 4, loss = 0.01612610\n",
      "Iteration 5, loss = 0.02030466\n",
      "Iteration 6, loss = 0.02228865\n",
      "Iteration 7, loss = 0.01973336\n",
      "Iteration 8, loss = 0.01444946\n",
      "Iteration 9, loss = 0.00898853\n",
      "Iteration 10, loss = 0.00501626\n",
      "Iteration 11, loss = 0.00319443\n",
      "Iteration 12, loss = 0.00321050\n",
      "Iteration 13, loss = 0.00417057\n",
      "Iteration 14, loss = 0.00512771\n",
      "Iteration 15, loss = 0.00544104\n",
      "Iteration 16, loss = 0.00498058\n",
      "Iteration 17, loss = 0.00407756\n",
      "Iteration 18, loss = 0.00322401\n",
      "Iteration 19, loss = 0.00284397\n",
      "Iteration 20, loss = 0.00302528\n",
      "Iteration 21, loss = 0.00351577\n",
      "Iteration 22, loss = 0.00389909\n",
      "Iteration 23, loss = 0.00389157\n",
      "Iteration 24, loss = 0.00346862\n",
      "Iteration 25, loss = 0.00281597\n",
      "Iteration 26, loss = 0.00218059\n",
      "Iteration 27, loss = 0.00174328\n",
      "Iteration 28, loss = 0.00156018\n",
      "Iteration 29, loss = 0.00157473\n",
      "Iteration 30, loss = 0.00166825\n",
      "Iteration 31, loss = 0.00172947\n",
      "Iteration 32, loss = 0.00169997\n",
      "Iteration 33, loss = 0.00158235\n",
      "Iteration 34, loss = 0.00142205\n",
      "Iteration 35, loss = 0.00128134\n",
      "Iteration 36, loss = 0.00120574\n",
      "Iteration 37, loss = 0.00120779\n",
      "Iteration 38, loss = 0.00126506\n",
      "Iteration 39, loss = 0.00133428\n",
      "Iteration 40, loss = 0.00137199\n",
      "Iteration 41, loss = 0.00135416\n",
      "Iteration 42, loss = 0.00128323\n",
      "Iteration 43, loss = 0.00118312\n",
      "Iteration 44, loss = 0.00108593\n",
      "Iteration 45, loss = 0.00101609\n",
      "Iteration 46, loss = 0.00098067\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06785652\n",
      "Iteration 2, loss = 0.03454651\n",
      "Iteration 3, loss = 0.01821121\n",
      "Iteration 4, loss = 0.01611231\n",
      "Iteration 5, loss = 0.02029298\n",
      "Iteration 6, loss = 0.02228195\n",
      "Iteration 7, loss = 0.01973345\n",
      "Iteration 8, loss = 0.01445538\n",
      "Iteration 9, loss = 0.00899562\n",
      "Iteration 10, loss = 0.00502134\n",
      "Iteration 11, loss = 0.00319632\n",
      "Iteration 12, loss = 0.00320911\n",
      "Iteration 13, loss = 0.00416655\n",
      "Iteration 14, loss = 0.00512453\n",
      "Iteration 15, loss = 0.00544081\n",
      "Iteration 16, loss = 0.00498306\n",
      "Iteration 17, loss = 0.00408026\n",
      "Iteration 18, loss = 0.00322479\n",
      "Iteration 19, loss = 0.00284235\n",
      "Iteration 20, loss = 0.00302139\n",
      "Iteration 21, loss = 0.00351174\n",
      "Iteration 22, loss = 0.00389759\n",
      "Iteration 23, loss = 0.00389287\n",
      "Iteration 24, loss = 0.00347137\n",
      "Iteration 25, loss = 0.00281900\n",
      "Iteration 26, loss = 0.00218329\n",
      "Iteration 27, loss = 0.00174543\n",
      "Iteration 28, loss = 0.00156160\n",
      "Iteration 29, loss = 0.00157565\n",
      "Iteration 30, loss = 0.00166889\n",
      "Iteration 31, loss = 0.00173008\n",
      "Iteration 32, loss = 0.00170031\n",
      "Iteration 33, loss = 0.00158223\n",
      "Iteration 34, loss = 0.00142145\n",
      "Iteration 35, loss = 0.00128020\n",
      "Iteration 36, loss = 0.00120419\n",
      "Iteration 37, loss = 0.00120597\n",
      "Iteration 38, loss = 0.00126313\n",
      "Iteration 39, loss = 0.00133247\n",
      "Iteration 40, loss = 0.00137053\n",
      "Iteration 41, loss = 0.00135316\n",
      "Iteration 42, loss = 0.00128269\n",
      "Iteration 43, loss = 0.00118293\n",
      "Iteration 44, loss = 0.00108596\n",
      "Iteration 45, loss = 0.00101623\n",
      "Iteration 46, loss = 0.00098086\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06784905\n",
      "Iteration 2, loss = 0.03454576\n",
      "Iteration 3, loss = 0.01821314\n",
      "Iteration 4, loss = 0.01610941\n",
      "Iteration 5, loss = 0.02028573\n",
      "Iteration 6, loss = 0.02227295\n",
      "Iteration 7, loss = 0.01972706\n",
      "Iteration 8, loss = 0.01445340\n",
      "Iteration 9, loss = 0.00899536\n",
      "Iteration 10, loss = 0.00502119\n",
      "Iteration 11, loss = 0.00319478\n",
      "Iteration 12, loss = 0.00320738\n",
      "Iteration 13, loss = 0.00416548\n",
      "Iteration 14, loss = 0.00512301\n",
      "Iteration 15, loss = 0.00543739\n",
      "Iteration 16, loss = 0.00497888\n",
      "Iteration 17, loss = 0.00407625\n",
      "Iteration 18, loss = 0.00322187\n",
      "Iteration 19, loss = 0.00284091\n",
      "Iteration 20, loss = 0.00302217\n",
      "Iteration 21, loss = 0.00351299\n",
      "Iteration 22, loss = 0.00389761\n",
      "Iteration 23, loss = 0.00389114\n",
      "Iteration 24, loss = 0.00346816\n",
      "Iteration 25, loss = 0.00281511\n",
      "Iteration 26, loss = 0.00217964\n",
      "Iteration 27, loss = 0.00174297\n",
      "Iteration 28, loss = 0.00156102\n",
      "Iteration 29, loss = 0.00157743\n",
      "Iteration 30, loss = 0.00167112\n",
      "Iteration 31, loss = 0.00173133\n",
      "Iteration 32, loss = 0.00170008\n",
      "Iteration 33, loss = 0.00158023\n",
      "Iteration 34, loss = 0.00141815\n",
      "Iteration 35, loss = 0.00127673\n",
      "Iteration 36, loss = 0.00120171\n",
      "Iteration 37, loss = 0.00120476\n",
      "Iteration 38, loss = 0.00126278\n",
      "Iteration 39, loss = 0.00133239\n",
      "Iteration 40, loss = 0.00137010\n",
      "Iteration 41, loss = 0.00135202\n",
      "Iteration 42, loss = 0.00128092\n",
      "Iteration 43, loss = 0.00118098\n",
      "Iteration 44, loss = 0.00108440\n",
      "Iteration 45, loss = 0.00101555\n",
      "Iteration 46, loss = 0.00098127\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06783627\n",
      "Iteration 2, loss = 0.03455123\n",
      "Iteration 3, loss = 0.01818812\n",
      "Iteration 4, loss = 0.01605165\n",
      "Iteration 5, loss = 0.02020859\n",
      "Iteration 6, loss = 0.02221041\n",
      "Iteration 7, loss = 0.01970175\n",
      "Iteration 8, loss = 0.01445097\n",
      "Iteration 9, loss = 0.00899325\n",
      "Iteration 10, loss = 0.00500803\n",
      "Iteration 11, loss = 0.00317099\n",
      "Iteration 12, loss = 0.00318207\n",
      "Iteration 13, loss = 0.00415097\n",
      "Iteration 14, loss = 0.00512224\n",
      "Iteration 15, loss = 0.00543638\n",
      "Iteration 16, loss = 0.00496850\n",
      "Iteration 17, loss = 0.00405331\n",
      "Iteration 18, loss = 0.00319464\n",
      "Iteration 19, loss = 0.00281173\n",
      "Iteration 20, loss = 0.00299759\n",
      "Iteration 21, loss = 0.00349225\n",
      "Iteration 22, loss = 0.00388115\n",
      "Iteration 23, loss = 0.00387023\n",
      "Iteration 24, loss = 0.00343695\n",
      "Iteration 25, loss = 0.00277661\n",
      "Iteration 26, loss = 0.00214442\n",
      "Iteration 27, loss = 0.00172052\n",
      "Iteration 28, loss = 0.00155662\n",
      "Iteration 29, loss = 0.00158641\n",
      "Iteration 30, loss = 0.00168326\n",
      "Iteration 31, loss = 0.00173816\n",
      "Iteration 32, loss = 0.00169553\n",
      "Iteration 33, loss = 0.00156207\n",
      "Iteration 34, loss = 0.00139099\n",
      "Iteration 35, loss = 0.00124877\n",
      "Iteration 36, loss = 0.00118149\n",
      "Iteration 37, loss = 0.00119665\n",
      "Iteration 38, loss = 0.00126550\n",
      "Iteration 39, loss = 0.00133910\n",
      "Iteration 40, loss = 0.00137251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 41, loss = 0.00134466\n",
      "Iteration 42, loss = 0.00126453\n",
      "Iteration 43, loss = 0.00116249\n",
      "Iteration 44, loss = 0.00107222\n",
      "Iteration 45, loss = 0.00101472\n",
      "Iteration 46, loss = 0.00099152\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06786679\n",
      "Iteration 2, loss = 0.03455535\n",
      "Iteration 3, loss = 0.01817109\n",
      "Iteration 4, loss = 0.01602535\n",
      "Iteration 5, loss = 0.02020828\n",
      "Iteration 6, loss = 0.02224009\n",
      "Iteration 7, loss = 0.01973747\n",
      "Iteration 8, loss = 0.01448470\n",
      "Iteration 9, loss = 0.00902656\n",
      "Iteration 10, loss = 0.00503640\n",
      "Iteration 11, loss = 0.00319498\n",
      "Iteration 12, loss = 0.00319739\n",
      "Iteration 13, loss = 0.00415841\n",
      "Iteration 14, loss = 0.00512974\n",
      "Iteration 15, loss = 0.00545700\n",
      "Iteration 16, loss = 0.00499820\n",
      "Iteration 17, loss = 0.00408561\n",
      "Iteration 18, loss = 0.00321878\n",
      "Iteration 19, loss = 0.00282065\n",
      "Iteration 20, loss = 0.00298857\n",
      "Iteration 21, loss = 0.00347482\n",
      "Iteration 22, loss = 0.00386794\n",
      "Iteration 23, loss = 0.00387584\n",
      "Iteration 24, loss = 0.00346711\n",
      "Iteration 25, loss = 0.00282191\n",
      "Iteration 26, loss = 0.00218739\n",
      "Iteration 27, loss = 0.00174510\n",
      "Iteration 28, loss = 0.00155612\n",
      "Iteration 29, loss = 0.00156766\n",
      "Iteration 30, loss = 0.00166204\n",
      "Iteration 31, loss = 0.00172726\n",
      "Iteration 32, loss = 0.00170362\n",
      "Iteration 33, loss = 0.00159068\n",
      "Iteration 34, loss = 0.00143157\n",
      "Iteration 35, loss = 0.00128845\n",
      "Iteration 36, loss = 0.00120779\n",
      "Iteration 37, loss = 0.00120436\n",
      "Iteration 38, loss = 0.00125798\n",
      "Iteration 39, loss = 0.00132661\n",
      "Iteration 40, loss = 0.00136672\n",
      "Iteration 41, loss = 0.00135246\n",
      "Iteration 42, loss = 0.00128441\n",
      "Iteration 43, loss = 0.00118538\n",
      "Iteration 44, loss = 0.00108738\n",
      "Iteration 45, loss = 0.00101559\n",
      "Iteration 46, loss = 0.00097853\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06786639\n",
      "Iteration 2, loss = 0.03454793\n",
      "Iteration 3, loss = 0.01821459\n",
      "Iteration 4, loss = 0.01612526\n",
      "Iteration 5, loss = 0.02030651\n",
      "Iteration 6, loss = 0.02229298\n",
      "Iteration 7, loss = 0.01974413\n",
      "Iteration 8, loss = 0.01446138\n",
      "Iteration 9, loss = 0.00899831\n",
      "Iteration 10, loss = 0.00502083\n",
      "Iteration 11, loss = 0.00319528\n",
      "Iteration 12, loss = 0.00320939\n",
      "Iteration 13, loss = 0.00416884\n",
      "Iteration 14, loss = 0.00512721\n",
      "Iteration 15, loss = 0.00544156\n",
      "Iteration 16, loss = 0.00498208\n",
      "Iteration 17, loss = 0.00407796\n",
      "Iteration 18, loss = 0.00322234\n",
      "Iteration 19, loss = 0.00284277\n",
      "Iteration 20, loss = 0.00302511\n",
      "Iteration 21, loss = 0.00351752\n",
      "Iteration 22, loss = 0.00390252\n",
      "Iteration 23, loss = 0.00389462\n",
      "Iteration 24, loss = 0.00346952\n",
      "Iteration 25, loss = 0.00281477\n",
      "Iteration 26, loss = 0.00217863\n",
      "Iteration 27, loss = 0.00174183\n",
      "Iteration 28, loss = 0.00156012\n",
      "Iteration 29, loss = 0.00157603\n",
      "Iteration 30, loss = 0.00167013\n",
      "Iteration 31, loss = 0.00173135\n",
      "Iteration 32, loss = 0.00170101\n",
      "Iteration 33, loss = 0.00158203\n",
      "Iteration 34, loss = 0.00142030\n",
      "Iteration 35, loss = 0.00127883\n",
      "Iteration 36, loss = 0.00120348\n",
      "Iteration 37, loss = 0.00120631\n",
      "Iteration 38, loss = 0.00126435\n",
      "Iteration 39, loss = 0.00133408\n",
      "Iteration 40, loss = 0.00137189\n",
      "Iteration 41, loss = 0.00135375\n",
      "Iteration 42, loss = 0.00128243\n",
      "Iteration 43, loss = 0.00118217\n",
      "Iteration 44, loss = 0.00108505\n",
      "Iteration 45, loss = 0.00101554\n",
      "Iteration 46, loss = 0.00098073\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06784934\n",
      "Iteration 2, loss = 0.03454222\n",
      "Iteration 3, loss = 0.01822646\n",
      "Iteration 4, loss = 0.01615222\n",
      "Iteration 5, loss = 0.02032344\n",
      "Iteration 6, loss = 0.02229120\n",
      "Iteration 7, loss = 0.01973025\n",
      "Iteration 8, loss = 0.01444172\n",
      "Iteration 9, loss = 0.00898007\n",
      "Iteration 10, loss = 0.00500813\n",
      "Iteration 11, loss = 0.00318848\n",
      "Iteration 12, loss = 0.00320822\n",
      "Iteration 13, loss = 0.00417103\n",
      "Iteration 14, loss = 0.00512870\n",
      "Iteration 15, loss = 0.00544154\n",
      "Iteration 16, loss = 0.00497942\n",
      "Iteration 17, loss = 0.00407462\n",
      "Iteration 18, loss = 0.00322192\n",
      "Iteration 19, loss = 0.00284774\n",
      "Iteration 20, loss = 0.00303443\n",
      "Iteration 21, loss = 0.00352767\n",
      "Iteration 22, loss = 0.00390837\n",
      "Iteration 23, loss = 0.00389403\n",
      "Iteration 24, loss = 0.00346344\n",
      "Iteration 25, loss = 0.00280597\n",
      "Iteration 26, loss = 0.00217053\n",
      "Iteration 27, loss = 0.00173694\n",
      "Iteration 28, loss = 0.00155873\n",
      "Iteration 29, loss = 0.00157734\n",
      "Iteration 30, loss = 0.00167200\n",
      "Iteration 31, loss = 0.00173214\n",
      "Iteration 32, loss = 0.00170031\n",
      "Iteration 33, loss = 0.00157939\n",
      "Iteration 34, loss = 0.00141698\n",
      "Iteration 35, loss = 0.00127632\n",
      "Iteration 36, loss = 0.00120299\n",
      "Iteration 37, loss = 0.00120796\n",
      "Iteration 38, loss = 0.00126712\n",
      "Iteration 39, loss = 0.00133675\n",
      "Iteration 40, loss = 0.00137348\n",
      "Iteration 41, loss = 0.00135355\n",
      "Iteration 42, loss = 0.00128071\n",
      "Iteration 43, loss = 0.00117991\n",
      "Iteration 44, loss = 0.00108319\n",
      "Iteration 45, loss = 0.00101473\n",
      "Iteration 46, loss = 0.00098092\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06783354\n",
      "Iteration 2, loss = 0.03453662\n",
      "Iteration 3, loss = 0.01823221\n",
      "Iteration 4, loss = 0.01616518\n",
      "Iteration 5, loss = 0.02033108\n",
      "Iteration 6, loss = 0.02228721\n",
      "Iteration 7, loss = 0.01971263\n",
      "Iteration 8, loss = 0.01441940\n",
      "Iteration 9, loss = 0.00895868\n",
      "Iteration 10, loss = 0.00499243\n",
      "Iteration 11, loss = 0.00318081\n",
      "Iteration 12, loss = 0.00320767\n",
      "Iteration 13, loss = 0.00417576\n",
      "Iteration 14, loss = 0.00513565\n",
      "Iteration 15, loss = 0.00544541\n",
      "Iteration 16, loss = 0.00498007\n",
      "Iteration 17, loss = 0.00407443\n",
      "Iteration 18, loss = 0.00322414\n",
      "Iteration 19, loss = 0.00285412\n",
      "Iteration 20, loss = 0.00304327\n",
      "Iteration 21, loss = 0.00353727\n",
      "Iteration 22, loss = 0.00391535\n",
      "Iteration 23, loss = 0.00389634\n",
      "Iteration 24, loss = 0.00346169\n",
      "Iteration 25, loss = 0.00280170\n",
      "Iteration 26, loss = 0.00216609\n",
      "Iteration 27, loss = 0.00173363\n",
      "Iteration 28, loss = 0.00155706\n",
      "Iteration 29, loss = 0.00157721\n",
      "Iteration 30, loss = 0.00167211\n",
      "Iteration 31, loss = 0.00173156\n",
      "Iteration 32, loss = 0.00169893\n",
      "Iteration 33, loss = 0.00157773\n",
      "Iteration 34, loss = 0.00141574\n",
      "Iteration 35, loss = 0.00127603\n",
      "Iteration 36, loss = 0.00120398\n",
      "Iteration 37, loss = 0.00120967\n",
      "Iteration 38, loss = 0.00126888\n",
      "Iteration 39, loss = 0.00133794\n",
      "Iteration 40, loss = 0.00137399\n",
      "Iteration 41, loss = 0.00135374\n",
      "Iteration 42, loss = 0.00128085\n",
      "Iteration 43, loss = 0.00118016\n",
      "Iteration 44, loss = 0.00108381\n",
      "Iteration 45, loss = 0.00101550\n",
      "Iteration 46, loss = 0.00098151\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06781390\n",
      "Iteration 2, loss = 0.03453694\n",
      "Iteration 3, loss = 0.01822537\n",
      "Iteration 4, loss = 0.01613251\n",
      "Iteration 5, loss = 0.02027954\n",
      "Iteration 6, loss = 0.02223764\n",
      "Iteration 7, loss = 0.01968572\n",
      "Iteration 8, loss = 0.01440878\n",
      "Iteration 9, loss = 0.00895314\n",
      "Iteration 10, loss = 0.00498920\n",
      "Iteration 11, loss = 0.00317375\n",
      "Iteration 12, loss = 0.00319553\n",
      "Iteration 13, loss = 0.00415759\n",
      "Iteration 14, loss = 0.00511297\n",
      "Iteration 15, loss = 0.00542221\n",
      "Iteration 16, loss = 0.00496293\n",
      "Iteration 17, loss = 0.00406167\n",
      "Iteration 18, loss = 0.00321521\n",
      "Iteration 19, loss = 0.00284398\n",
      "Iteration 20, loss = 0.00303568\n",
      "Iteration 21, loss = 0.00352984\n",
      "Iteration 22, loss = 0.00391263\n",
      "Iteration 23, loss = 0.00389389\n",
      "Iteration 24, loss = 0.00345412\n",
      "Iteration 25, loss = 0.00278937\n",
      "Iteration 26, loss = 0.00215455\n",
      "Iteration 27, loss = 0.00172859\n",
      "Iteration 28, loss = 0.00156197\n",
      "Iteration 29, loss = 0.00158849\n",
      "Iteration 30, loss = 0.00168400\n",
      "Iteration 31, loss = 0.00173782\n",
      "Iteration 32, loss = 0.00169490\n",
      "Iteration 33, loss = 0.00156286\n",
      "Iteration 34, loss = 0.00139377\n",
      "Iteration 35, loss = 0.00125351\n",
      "Iteration 36, loss = 0.00118728\n",
      "Iteration 37, loss = 0.00120269\n",
      "Iteration 38, loss = 0.00127143\n",
      "Iteration 39, loss = 0.00134436\n",
      "Iteration 40, loss = 0.00137716\n",
      "Iteration 41, loss = 0.00134908\n",
      "Iteration 42, loss = 0.00126891\n",
      "Iteration 43, loss = 0.00116636\n",
      "Iteration 44, loss = 0.00107491\n",
      "Iteration 45, loss = 0.00101577\n",
      "Iteration 46, loss = 0.00099121\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06783367\n",
      "Iteration 2, loss = 0.03454904\n",
      "Iteration 3, loss = 0.01819670\n",
      "Iteration 4, loss = 0.01606879\n",
      "Iteration 5, loss = 0.02023818\n",
      "Iteration 6, loss = 0.02224225\n",
      "Iteration 7, loss = 0.01971921\n",
      "Iteration 8, loss = 0.01445845\n",
      "Iteration 9, loss = 0.00900500\n",
      "Iteration 10, loss = 0.00502855\n",
      "Iteration 11, loss = 0.00319730\n",
      "Iteration 12, loss = 0.00320389\n",
      "Iteration 13, loss = 0.00415776\n",
      "Iteration 14, loss = 0.00511539\n",
      "Iteration 15, loss = 0.00543859\n",
      "Iteration 16, loss = 0.00498796\n",
      "Iteration 17, loss = 0.00408778\n",
      "Iteration 18, loss = 0.00322935\n",
      "Iteration 19, loss = 0.00283731\n",
      "Iteration 20, loss = 0.00300732\n",
      "Iteration 21, loss = 0.00349308\n",
      "Iteration 22, loss = 0.00388310\n",
      "Iteration 23, loss = 0.00388719\n",
      "Iteration 24, loss = 0.00347494\n",
      "Iteration 25, loss = 0.00282721\n",
      "Iteration 26, loss = 0.00219127\n",
      "Iteration 27, loss = 0.00174880\n",
      "Iteration 28, loss = 0.00155986\n",
      "Iteration 29, loss = 0.00157051\n",
      "Iteration 30, loss = 0.00166340\n",
      "Iteration 31, loss = 0.00172629\n",
      "Iteration 32, loss = 0.00170057\n",
      "Iteration 33, loss = 0.00158642\n",
      "Iteration 34, loss = 0.00142754\n",
      "Iteration 35, loss = 0.00128558\n",
      "Iteration 36, loss = 0.00120658\n",
      "Iteration 37, loss = 0.00120484\n",
      "Iteration 38, loss = 0.00125950\n",
      "Iteration 39, loss = 0.00132821\n",
      "Iteration 40, loss = 0.00136759\n",
      "Iteration 41, loss = 0.00135240\n",
      "Iteration 42, loss = 0.00128378\n",
      "Iteration 43, loss = 0.00118474\n",
      "Iteration 44, loss = 0.00108731\n",
      "Iteration 45, loss = 0.00101625\n",
      "Iteration 46, loss = 0.00097962\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06783973\n",
      "Iteration 2, loss = 0.03454885\n",
      "Iteration 3, loss = 0.01820122\n",
      "Iteration 4, loss = 0.01608224\n",
      "Iteration 5, loss = 0.02024215\n",
      "Iteration 6, loss = 0.02223271\n",
      "Iteration 7, loss = 0.01971000\n",
      "Iteration 8, loss = 0.01444969\n",
      "Iteration 9, loss = 0.00899360\n",
      "Iteration 10, loss = 0.00501479\n",
      "Iteration 11, loss = 0.00318187\n",
      "Iteration 12, loss = 0.00319347\n",
      "Iteration 13, loss = 0.00415338\n",
      "Iteration 14, loss = 0.00511235\n",
      "Iteration 15, loss = 0.00542760\n",
      "Iteration 16, loss = 0.00497074\n",
      "Iteration 17, loss = 0.00406691\n",
      "Iteration 18, loss = 0.00321322\n",
      "Iteration 19, loss = 0.00283157\n",
      "Iteration 20, loss = 0.00301590\n",
      "Iteration 21, loss = 0.00351003\n",
      "Iteration 22, loss = 0.00389969\n",
      "Iteration 23, loss = 0.00389000\n",
      "Iteration 24, loss = 0.00345709\n",
      "Iteration 25, loss = 0.00279484\n",
      "Iteration 26, loss = 0.00215857\n",
      "Iteration 27, loss = 0.00172982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28, loss = 0.00156138\n",
      "Iteration 29, loss = 0.00158809\n",
      "Iteration 30, loss = 0.00168417\n",
      "Iteration 31, loss = 0.00173931\n",
      "Iteration 32, loss = 0.00169737\n",
      "Iteration 33, loss = 0.00156483\n",
      "Iteration 34, loss = 0.00139437\n",
      "Iteration 35, loss = 0.00125210\n",
      "Iteration 36, loss = 0.00118433\n",
      "Iteration 37, loss = 0.00119900\n",
      "Iteration 38, loss = 0.00126743\n",
      "Iteration 39, loss = 0.00134112\n",
      "Iteration 40, loss = 0.00137525\n",
      "Iteration 41, loss = 0.00134834\n",
      "Iteration 42, loss = 0.00126896\n",
      "Iteration 43, loss = 0.00116681\n",
      "Iteration 44, loss = 0.00107570\n",
      "Iteration 45, loss = 0.00101700\n",
      "Iteration 46, loss = 0.00099247\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06784925\n",
      "Iteration 2, loss = 0.03455064\n",
      "Iteration 3, loss = 0.01819826\n",
      "Iteration 4, loss = 0.01607737\n",
      "Iteration 5, loss = 0.02025047\n",
      "Iteration 6, loss = 0.02225751\n",
      "Iteration 7, loss = 0.01973294\n",
      "Iteration 8, loss = 0.01446842\n",
      "Iteration 9, loss = 0.00901119\n",
      "Iteration 10, loss = 0.00503190\n",
      "Iteration 11, loss = 0.00319919\n",
      "Iteration 12, loss = 0.00320558\n",
      "Iteration 13, loss = 0.00416042\n",
      "Iteration 14, loss = 0.00511862\n",
      "Iteration 15, loss = 0.00544083\n",
      "Iteration 16, loss = 0.00498905\n",
      "Iteration 17, loss = 0.00408796\n",
      "Iteration 18, loss = 0.00322889\n",
      "Iteration 19, loss = 0.00283815\n",
      "Iteration 20, loss = 0.00301003\n",
      "Iteration 21, loss = 0.00349822\n",
      "Iteration 22, loss = 0.00388830\n",
      "Iteration 23, loss = 0.00389092\n",
      "Iteration 24, loss = 0.00347617\n",
      "Iteration 25, loss = 0.00282649\n",
      "Iteration 26, loss = 0.00218967\n",
      "Iteration 27, loss = 0.00174743\n",
      "Iteration 28, loss = 0.00155916\n",
      "Iteration 29, loss = 0.00157056\n",
      "Iteration 30, loss = 0.00166406\n",
      "Iteration 31, loss = 0.00172724\n",
      "Iteration 32, loss = 0.00170086\n",
      "Iteration 33, loss = 0.00158548\n",
      "Iteration 34, loss = 0.00142563\n",
      "Iteration 35, loss = 0.00128332\n",
      "Iteration 36, loss = 0.00120475\n",
      "Iteration 37, loss = 0.00120378\n",
      "Iteration 38, loss = 0.00125913\n",
      "Iteration 39, loss = 0.00132821\n",
      "Iteration 40, loss = 0.00136752\n",
      "Iteration 41, loss = 0.00135191\n",
      "Iteration 42, loss = 0.00128281\n",
      "Iteration 43, loss = 0.00118352\n",
      "Iteration 44, loss = 0.00108610\n",
      "Iteration 45, loss = 0.00101530\n",
      "Iteration 46, loss = 0.00097901\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06785602\n",
      "Iteration 2, loss = 0.03454443\n",
      "Iteration 3, loss = 0.01822069\n",
      "Iteration 4, loss = 0.01613707\n",
      "Iteration 5, loss = 0.02031269\n",
      "Iteration 6, loss = 0.02229010\n",
      "Iteration 7, loss = 0.01973561\n",
      "Iteration 8, loss = 0.01444971\n",
      "Iteration 9, loss = 0.00898709\n",
      "Iteration 10, loss = 0.00501298\n",
      "Iteration 11, loss = 0.00319107\n",
      "Iteration 12, loss = 0.00320842\n",
      "Iteration 13, loss = 0.00416985\n",
      "Iteration 14, loss = 0.00512829\n",
      "Iteration 15, loss = 0.00544260\n",
      "Iteration 16, loss = 0.00498205\n",
      "Iteration 17, loss = 0.00407802\n",
      "Iteration 18, loss = 0.00322390\n",
      "Iteration 19, loss = 0.00284678\n",
      "Iteration 20, loss = 0.00303099\n",
      "Iteration 21, loss = 0.00352337\n",
      "Iteration 22, loss = 0.00390551\n",
      "Iteration 23, loss = 0.00389374\n",
      "Iteration 24, loss = 0.00346598\n",
      "Iteration 25, loss = 0.00281016\n",
      "Iteration 26, loss = 0.00217490\n",
      "Iteration 27, loss = 0.00173966\n",
      "Iteration 28, loss = 0.00155932\n",
      "Iteration 29, loss = 0.00157582\n",
      "Iteration 30, loss = 0.00166977\n",
      "Iteration 31, loss = 0.00173050\n",
      "Iteration 32, loss = 0.00169983\n",
      "Iteration 33, loss = 0.00158094\n",
      "Iteration 34, loss = 0.00141967\n",
      "Iteration 35, loss = 0.00127909\n",
      "Iteration 36, loss = 0.00120479\n",
      "Iteration 37, loss = 0.00120835\n",
      "Iteration 38, loss = 0.00126652\n",
      "Iteration 39, loss = 0.00133584\n",
      "Iteration 40, loss = 0.00137299\n",
      "Iteration 41, loss = 0.00135417\n",
      "Iteration 42, loss = 0.00128241\n",
      "Iteration 43, loss = 0.00118203\n",
      "Iteration 44, loss = 0.00108503\n",
      "Iteration 45, loss = 0.00101567\n",
      "Iteration 46, loss = 0.00098092\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06784110\n",
      "Iteration 2, loss = 0.03453960\n",
      "Iteration 3, loss = 0.01822684\n",
      "Iteration 4, loss = 0.01614891\n",
      "Iteration 5, loss = 0.02031892\n",
      "Iteration 6, loss = 0.02228729\n",
      "Iteration 7, loss = 0.01972728\n",
      "Iteration 8, loss = 0.01443933\n",
      "Iteration 9, loss = 0.00897868\n",
      "Iteration 10, loss = 0.00500703\n",
      "Iteration 11, loss = 0.00318734\n",
      "Iteration 12, loss = 0.00320689\n",
      "Iteration 13, loss = 0.00416941\n",
      "Iteration 14, loss = 0.00512731\n",
      "Iteration 15, loss = 0.00544075\n",
      "Iteration 16, loss = 0.00497972\n",
      "Iteration 17, loss = 0.00407503\n",
      "Iteration 18, loss = 0.00322167\n",
      "Iteration 19, loss = 0.00284685\n",
      "Iteration 20, loss = 0.00303323\n",
      "Iteration 21, loss = 0.00352624\n",
      "Iteration 22, loss = 0.00390743\n",
      "Iteration 23, loss = 0.00389350\n",
      "Iteration 24, loss = 0.00346356\n",
      "Iteration 25, loss = 0.00280658\n",
      "Iteration 26, loss = 0.00217144\n",
      "Iteration 27, loss = 0.00173745\n",
      "Iteration 28, loss = 0.00155853\n",
      "Iteration 29, loss = 0.00157617\n",
      "Iteration 30, loss = 0.00167045\n",
      "Iteration 31, loss = 0.00173096\n",
      "Iteration 32, loss = 0.00169965\n",
      "Iteration 33, loss = 0.00158003\n",
      "Iteration 34, loss = 0.00141843\n",
      "Iteration 35, loss = 0.00127812\n",
      "Iteration 36, loss = 0.00120450\n",
      "Iteration 37, loss = 0.00120880\n",
      "Iteration 38, loss = 0.00126745\n",
      "Iteration 39, loss = 0.00133677\n",
      "Iteration 40, loss = 0.00137362\n",
      "Iteration 41, loss = 0.00135414\n",
      "Iteration 42, loss = 0.00128178\n",
      "Iteration 43, loss = 0.00118119\n",
      "Iteration 44, loss = 0.00108435\n",
      "Iteration 45, loss = 0.00101535\n",
      "Iteration 46, loss = 0.00098097\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06781206\n",
      "Iteration 2, loss = 0.03452876\n",
      "Iteration 3, loss = 0.01823736\n",
      "Iteration 4, loss = 0.01617696\n",
      "Iteration 5, loss = 0.02033689\n",
      "Iteration 6, loss = 0.02228022\n",
      "Iteration 7, loss = 0.01969639\n",
      "Iteration 8, loss = 0.01440062\n",
      "Iteration 9, loss = 0.00894205\n",
      "Iteration 10, loss = 0.00498062\n",
      "Iteration 11, loss = 0.00317453\n",
      "Iteration 12, loss = 0.00320543\n",
      "Iteration 13, loss = 0.00417626\n",
      "Iteration 14, loss = 0.00513564\n",
      "Iteration 15, loss = 0.00544352\n",
      "Iteration 16, loss = 0.00497727\n",
      "Iteration 17, loss = 0.00407213\n",
      "Iteration 18, loss = 0.00322341\n",
      "Iteration 19, loss = 0.00285522\n",
      "Iteration 20, loss = 0.00304559\n",
      "Iteration 21, loss = 0.00353876\n",
      "Iteration 22, loss = 0.00391350\n",
      "Iteration 23, loss = 0.00389089\n",
      "Iteration 24, loss = 0.00345383\n",
      "Iteration 25, loss = 0.00279375\n",
      "Iteration 26, loss = 0.00215925\n",
      "Iteration 27, loss = 0.00172828\n",
      "Iteration 28, loss = 0.00155308\n",
      "Iteration 29, loss = 0.00157398\n",
      "Iteration 30, loss = 0.00166858\n",
      "Iteration 31, loss = 0.00172770\n",
      "Iteration 32, loss = 0.00169528\n",
      "Iteration 33, loss = 0.00157469\n",
      "Iteration 34, loss = 0.00141365\n",
      "Iteration 35, loss = 0.00127492\n",
      "Iteration 36, loss = 0.00120353\n",
      "Iteration 37, loss = 0.00120958\n",
      "Iteration 38, loss = 0.00126869\n",
      "Iteration 39, loss = 0.00133729\n",
      "Iteration 40, loss = 0.00137282\n",
      "Iteration 41, loss = 0.00135206\n",
      "Iteration 42, loss = 0.00127861\n",
      "Iteration 43, loss = 0.00117778\n",
      "Iteration 44, loss = 0.00108171\n",
      "Iteration 45, loss = 0.00101394\n",
      "Iteration 46, loss = 0.00098054\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06783829\n",
      "Iteration 2, loss = 0.03453714\n",
      "Iteration 3, loss = 0.01823418\n",
      "Iteration 4, loss = 0.01617360\n",
      "Iteration 5, loss = 0.02033854\n",
      "Iteration 6, loss = 0.02228731\n",
      "Iteration 7, loss = 0.01970790\n",
      "Iteration 8, loss = 0.01441170\n",
      "Iteration 9, loss = 0.00894755\n",
      "Iteration 10, loss = 0.00498154\n",
      "Iteration 11, loss = 0.00317559\n",
      "Iteration 12, loss = 0.00321366\n",
      "Iteration 13, loss = 0.00419160\n",
      "Iteration 14, loss = 0.00515356\n",
      "Iteration 15, loss = 0.00546114\n",
      "Iteration 16, loss = 0.00499247\n",
      "Iteration 17, loss = 0.00408530\n",
      "Iteration 18, loss = 0.00323568\n",
      "Iteration 19, loss = 0.00286829\n",
      "Iteration 20, loss = 0.00306024\n",
      "Iteration 21, loss = 0.00355309\n",
      "Iteration 22, loss = 0.00392578\n",
      "Iteration 23, loss = 0.00390254\n",
      "Iteration 24, loss = 0.00346598\n",
      "Iteration 25, loss = 0.00280474\n",
      "Iteration 26, loss = 0.00216713\n",
      "Iteration 27, loss = 0.00173296\n",
      "Iteration 28, loss = 0.00155483\n",
      "Iteration 29, loss = 0.00157384\n",
      "Iteration 30, loss = 0.00166840\n",
      "Iteration 31, loss = 0.00172878\n",
      "Iteration 32, loss = 0.00169810\n",
      "Iteration 33, loss = 0.00157878\n",
      "Iteration 34, loss = 0.00141838\n",
      "Iteration 35, loss = 0.00127999\n",
      "Iteration 36, loss = 0.00120819\n",
      "Iteration 37, loss = 0.00121327\n",
      "Iteration 38, loss = 0.00127148\n",
      "Iteration 39, loss = 0.00133958\n",
      "Iteration 40, loss = 0.00137501\n",
      "Iteration 41, loss = 0.00135470\n",
      "Iteration 42, loss = 0.00128217\n",
      "Iteration 43, loss = 0.00118187\n",
      "Iteration 44, loss = 0.00108550\n",
      "Iteration 45, loss = 0.00101673\n",
      "Iteration 46, loss = 0.00098200\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06779325\n",
      "Iteration 2, loss = 0.03452029\n",
      "Iteration 3, loss = 0.01823940\n",
      "Iteration 4, loss = 0.01617987\n",
      "Iteration 5, loss = 0.02033707\n",
      "Iteration 6, loss = 0.02227348\n",
      "Iteration 7, loss = 0.01968737\n",
      "Iteration 8, loss = 0.01439011\n",
      "Iteration 9, loss = 0.00893123\n",
      "Iteration 10, loss = 0.00497129\n",
      "Iteration 11, loss = 0.00316784\n",
      "Iteration 12, loss = 0.00320310\n",
      "Iteration 13, loss = 0.00417716\n",
      "Iteration 14, loss = 0.00513803\n",
      "Iteration 15, loss = 0.00544391\n",
      "Iteration 16, loss = 0.00497341\n",
      "Iteration 17, loss = 0.00406589\n",
      "Iteration 18, loss = 0.00321805\n",
      "Iteration 19, loss = 0.00285956\n",
      "Iteration 20, loss = 0.00305857\n",
      "Iteration 21, loss = 0.00355518\n",
      "Iteration 22, loss = 0.00392745\n",
      "Iteration 23, loss = 0.00389828\n",
      "Iteration 24, loss = 0.00345522\n",
      "Iteration 25, loss = 0.00279139\n",
      "Iteration 26, loss = 0.00215694\n",
      "Iteration 27, loss = 0.00172778\n",
      "Iteration 28, loss = 0.00155620\n",
      "Iteration 29, loss = 0.00157880\n",
      "Iteration 30, loss = 0.00167329\n",
      "Iteration 31, loss = 0.00173047\n",
      "Iteration 32, loss = 0.00169479\n",
      "Iteration 33, loss = 0.00157083\n",
      "Iteration 34, loss = 0.00140795\n",
      "Iteration 35, loss = 0.00127001\n",
      "Iteration 36, loss = 0.00120100\n",
      "Iteration 37, loss = 0.00120955\n",
      "Iteration 38, loss = 0.00127064\n",
      "Iteration 39, loss = 0.00133997\n",
      "Iteration 40, loss = 0.00137470\n",
      "Iteration 41, loss = 0.00135220\n",
      "Iteration 42, loss = 0.00127750\n",
      "Iteration 43, loss = 0.00117654\n",
      "Iteration 44, loss = 0.00108156\n",
      "Iteration 45, loss = 0.00101555\n",
      "Iteration 46, loss = 0.00098372\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06777850\n",
      "Iteration 2, loss = 0.03453102\n",
      "Iteration 3, loss = 0.01822701\n",
      "Iteration 4, loss = 0.01613082\n",
      "Iteration 5, loss = 0.02026440\n",
      "Iteration 6, loss = 0.02221766\n",
      "Iteration 7, loss = 0.01966814\n",
      "Iteration 8, loss = 0.01439801\n",
      "Iteration 9, loss = 0.00894556\n",
      "Iteration 10, loss = 0.00498414\n",
      "Iteration 11, loss = 0.00316936\n",
      "Iteration 12, loss = 0.00319219\n",
      "Iteration 13, loss = 0.00415492\n",
      "Iteration 14, loss = 0.00511098\n",
      "Iteration 15, loss = 0.00542149\n",
      "Iteration 16, loss = 0.00496354\n",
      "Iteration 17, loss = 0.00406397\n",
      "Iteration 18, loss = 0.00321940\n",
      "Iteration 19, loss = 0.00284825\n",
      "Iteration 20, loss = 0.00303861\n",
      "Iteration 21, loss = 0.00352992\n",
      "Iteration 22, loss = 0.00390989\n",
      "Iteration 23, loss = 0.00388983\n",
      "Iteration 24, loss = 0.00345029\n",
      "Iteration 25, loss = 0.00278672\n",
      "Iteration 26, loss = 0.00215324\n",
      "Iteration 27, loss = 0.00172805\n",
      "Iteration 28, loss = 0.00156161\n",
      "Iteration 29, loss = 0.00158784\n",
      "Iteration 30, loss = 0.00168306\n",
      "Iteration 31, loss = 0.00173692\n",
      "Iteration 32, loss = 0.00169427\n",
      "Iteration 33, loss = 0.00156237\n",
      "Iteration 34, loss = 0.00139411\n",
      "Iteration 35, loss = 0.00125437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36, loss = 0.00118821\n",
      "Iteration 37, loss = 0.00120337\n",
      "Iteration 38, loss = 0.00127160\n",
      "Iteration 39, loss = 0.00134422\n",
      "Iteration 40, loss = 0.00137701\n",
      "Iteration 41, loss = 0.00134912\n",
      "Iteration 42, loss = 0.00126923\n",
      "Iteration 43, loss = 0.00116686\n",
      "Iteration 44, loss = 0.00107560\n",
      "Iteration 45, loss = 0.00101652\n",
      "Iteration 46, loss = 0.00099171\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06783153\n",
      "Iteration 2, loss = 0.03454766\n",
      "Iteration 3, loss = 0.01820261\n",
      "Iteration 4, loss = 0.01608228\n",
      "Iteration 5, loss = 0.02024706\n",
      "Iteration 6, loss = 0.02224201\n",
      "Iteration 7, loss = 0.01971374\n",
      "Iteration 8, loss = 0.01444970\n",
      "Iteration 9, loss = 0.00899634\n",
      "Iteration 10, loss = 0.00502044\n",
      "Iteration 11, loss = 0.00319148\n",
      "Iteration 12, loss = 0.00320177\n",
      "Iteration 13, loss = 0.00415926\n",
      "Iteration 14, loss = 0.00511905\n",
      "Iteration 15, loss = 0.00544189\n",
      "Iteration 16, loss = 0.00499047\n",
      "Iteration 17, loss = 0.00409070\n",
      "Iteration 18, loss = 0.00323400\n",
      "Iteration 19, loss = 0.00284557\n",
      "Iteration 20, loss = 0.00301891\n",
      "Iteration 21, loss = 0.00350580\n",
      "Iteration 22, loss = 0.00389264\n",
      "Iteration 23, loss = 0.00389127\n",
      "Iteration 24, loss = 0.00347391\n",
      "Iteration 25, loss = 0.00282335\n",
      "Iteration 26, loss = 0.00218735\n",
      "Iteration 27, loss = 0.00174623\n",
      "Iteration 28, loss = 0.00155910\n",
      "Iteration 29, loss = 0.00157089\n",
      "Iteration 30, loss = 0.00166409\n",
      "Iteration 31, loss = 0.00172703\n",
      "Iteration 32, loss = 0.00170094\n",
      "Iteration 33, loss = 0.00158624\n",
      "Iteration 34, loss = 0.00142719\n",
      "Iteration 35, loss = 0.00128586\n",
      "Iteration 36, loss = 0.00120808\n",
      "Iteration 37, loss = 0.00120744\n",
      "Iteration 38, loss = 0.00126254\n",
      "Iteration 39, loss = 0.00133113\n",
      "Iteration 40, loss = 0.00136978\n",
      "Iteration 41, loss = 0.00135361\n",
      "Iteration 42, loss = 0.00128406\n",
      "Iteration 43, loss = 0.00118446\n",
      "Iteration 44, loss = 0.00108676\n",
      "Iteration 45, loss = 0.00101559\n",
      "Iteration 46, loss = 0.00097914\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06781833\n",
      "Iteration 2, loss = 0.03453818\n",
      "Iteration 3, loss = 0.01822332\n",
      "Iteration 4, loss = 0.01612857\n",
      "Iteration 5, loss = 0.02027877\n",
      "Iteration 6, loss = 0.02224420\n",
      "Iteration 7, loss = 0.01969613\n",
      "Iteration 8, loss = 0.01442176\n",
      "Iteration 9, loss = 0.00896606\n",
      "Iteration 10, loss = 0.00499640\n",
      "Iteration 11, loss = 0.00317584\n",
      "Iteration 12, loss = 0.00319614\n",
      "Iteration 13, loss = 0.00415937\n",
      "Iteration 14, loss = 0.00511602\n",
      "Iteration 15, loss = 0.00542444\n",
      "Iteration 16, loss = 0.00496223\n",
      "Iteration 17, loss = 0.00405917\n",
      "Iteration 18, loss = 0.00321243\n",
      "Iteration 19, loss = 0.00284092\n",
      "Iteration 20, loss = 0.00303252\n",
      "Iteration 21, loss = 0.00352726\n",
      "Iteration 22, loss = 0.00391072\n",
      "Iteration 23, loss = 0.00389250\n",
      "Iteration 24, loss = 0.00345308\n",
      "Iteration 25, loss = 0.00278857\n",
      "Iteration 26, loss = 0.00215409\n",
      "Iteration 27, loss = 0.00172896\n",
      "Iteration 28, loss = 0.00156343\n",
      "Iteration 29, loss = 0.00159084\n",
      "Iteration 30, loss = 0.00168633\n",
      "Iteration 31, loss = 0.00173952\n",
      "Iteration 32, loss = 0.00169569\n",
      "Iteration 33, loss = 0.00156278\n",
      "Iteration 34, loss = 0.00139291\n",
      "Iteration 35, loss = 0.00125224\n",
      "Iteration 36, loss = 0.00118647\n",
      "Iteration 37, loss = 0.00120281\n",
      "Iteration 38, loss = 0.00127205\n",
      "Iteration 39, loss = 0.00134518\n",
      "Iteration 40, loss = 0.00137786\n",
      "Iteration 41, loss = 0.00134932\n",
      "Iteration 42, loss = 0.00126870\n",
      "Iteration 43, loss = 0.00116611\n",
      "Iteration 44, loss = 0.00107509\n",
      "Iteration 45, loss = 0.00101669\n",
      "Iteration 46, loss = 0.00099256\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06782758\n",
      "Iteration 2, loss = 0.03454020\n",
      "Iteration 3, loss = 0.01822188\n",
      "Iteration 4, loss = 0.01612609\n",
      "Iteration 5, loss = 0.02029026\n",
      "Iteration 6, loss = 0.02226392\n",
      "Iteration 7, loss = 0.01971205\n",
      "Iteration 8, loss = 0.01443468\n",
      "Iteration 9, loss = 0.00897891\n",
      "Iteration 10, loss = 0.00500879\n",
      "Iteration 11, loss = 0.00318821\n",
      "Iteration 12, loss = 0.00320443\n",
      "Iteration 13, loss = 0.00416398\n",
      "Iteration 14, loss = 0.00512143\n",
      "Iteration 15, loss = 0.00543617\n",
      "Iteration 16, loss = 0.00497839\n",
      "Iteration 17, loss = 0.00407736\n",
      "Iteration 18, loss = 0.00322540\n",
      "Iteration 19, loss = 0.00284710\n",
      "Iteration 20, loss = 0.00302770\n",
      "Iteration 21, loss = 0.00351597\n",
      "Iteration 22, loss = 0.00389759\n",
      "Iteration 23, loss = 0.00388872\n",
      "Iteration 24, loss = 0.00346497\n",
      "Iteration 25, loss = 0.00281233\n",
      "Iteration 26, loss = 0.00217765\n",
      "Iteration 27, loss = 0.00174110\n",
      "Iteration 28, loss = 0.00155863\n",
      "Iteration 29, loss = 0.00157351\n",
      "Iteration 30, loss = 0.00166670\n",
      "Iteration 31, loss = 0.00172789\n",
      "Iteration 32, loss = 0.00169847\n",
      "Iteration 33, loss = 0.00158077\n",
      "Iteration 34, loss = 0.00142050\n",
      "Iteration 35, loss = 0.00128018\n",
      "Iteration 36, loss = 0.00120526\n",
      "Iteration 37, loss = 0.00120778\n",
      "Iteration 38, loss = 0.00126497\n",
      "Iteration 39, loss = 0.00133373\n",
      "Iteration 40, loss = 0.00137087\n",
      "Iteration 41, loss = 0.00135254\n",
      "Iteration 42, loss = 0.00128133\n",
      "Iteration 43, loss = 0.00118124\n",
      "Iteration 44, loss = 0.00108432\n",
      "Iteration 45, loss = 0.00101468\n",
      "Iteration 46, loss = 0.00097953\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06781799\n",
      "Iteration 2, loss = 0.03452985\n",
      "Iteration 3, loss = 0.01823735\n",
      "Iteration 4, loss = 0.01617651\n",
      "Iteration 5, loss = 0.02033641\n",
      "Iteration 6, loss = 0.02227832\n",
      "Iteration 7, loss = 0.01969452\n",
      "Iteration 8, loss = 0.01439834\n",
      "Iteration 9, loss = 0.00893632\n",
      "Iteration 10, loss = 0.00497463\n",
      "Iteration 11, loss = 0.00317122\n",
      "Iteration 12, loss = 0.00320836\n",
      "Iteration 13, loss = 0.00418556\n",
      "Iteration 14, loss = 0.00514707\n",
      "Iteration 15, loss = 0.00545396\n",
      "Iteration 16, loss = 0.00498519\n",
      "Iteration 17, loss = 0.00407808\n",
      "Iteration 18, loss = 0.00322882\n",
      "Iteration 19, loss = 0.00286284\n",
      "Iteration 20, loss = 0.00305593\n",
      "Iteration 21, loss = 0.00355123\n",
      "Iteration 22, loss = 0.00392341\n",
      "Iteration 23, loss = 0.00389778\n",
      "Iteration 24, loss = 0.00345922\n",
      "Iteration 25, loss = 0.00279737\n",
      "Iteration 26, loss = 0.00216152\n",
      "Iteration 27, loss = 0.00172938\n",
      "Iteration 28, loss = 0.00155322\n",
      "Iteration 29, loss = 0.00157314\n",
      "Iteration 30, loss = 0.00166768\n",
      "Iteration 31, loss = 0.00172727\n",
      "Iteration 32, loss = 0.00169580\n",
      "Iteration 33, loss = 0.00157586\n",
      "Iteration 34, loss = 0.00141509\n",
      "Iteration 35, loss = 0.00127700\n",
      "Iteration 36, loss = 0.00120612\n",
      "Iteration 37, loss = 0.00121217\n",
      "Iteration 38, loss = 0.00127107\n",
      "Iteration 39, loss = 0.00133929\n",
      "Iteration 40, loss = 0.00137438\n",
      "Iteration 41, loss = 0.00135340\n",
      "Iteration 42, loss = 0.00128018\n",
      "Iteration 43, loss = 0.00117971\n",
      "Iteration 44, loss = 0.00108368\n",
      "Iteration 45, loss = 0.00101551\n",
      "Iteration 46, loss = 0.00098137\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06779137\n",
      "Iteration 2, loss = 0.03451884\n",
      "Iteration 3, loss = 0.01824154\n",
      "Iteration 4, loss = 0.01618753\n",
      "Iteration 5, loss = 0.02034083\n",
      "Iteration 6, loss = 0.02226894\n",
      "Iteration 7, loss = 0.01967583\n",
      "Iteration 8, loss = 0.01437734\n",
      "Iteration 9, loss = 0.00891770\n",
      "Iteration 10, loss = 0.00496120\n",
      "Iteration 11, loss = 0.00316360\n",
      "Iteration 12, loss = 0.00320630\n",
      "Iteration 13, loss = 0.00418591\n",
      "Iteration 14, loss = 0.00514645\n",
      "Iteration 15, loss = 0.00545047\n",
      "Iteration 16, loss = 0.00497917\n",
      "Iteration 17, loss = 0.00407196\n",
      "Iteration 18, loss = 0.00322512\n",
      "Iteration 19, loss = 0.00286376\n",
      "Iteration 20, loss = 0.00306101\n",
      "Iteration 21, loss = 0.00355514\n",
      "Iteration 22, loss = 0.00392403\n",
      "Iteration 23, loss = 0.00389443\n",
      "Iteration 24, loss = 0.00345271\n",
      "Iteration 25, loss = 0.00278960\n",
      "Iteration 26, loss = 0.00215365\n",
      "Iteration 27, loss = 0.00172308\n",
      "Iteration 28, loss = 0.00154930\n",
      "Iteration 29, loss = 0.00157118\n",
      "Iteration 30, loss = 0.00166600\n",
      "Iteration 31, loss = 0.00172519\n",
      "Iteration 32, loss = 0.00169312\n",
      "Iteration 33, loss = 0.00157274\n",
      "Iteration 34, loss = 0.00141201\n",
      "Iteration 35, loss = 0.00127469\n",
      "Iteration 36, loss = 0.00120458\n",
      "Iteration 37, loss = 0.00121144\n",
      "Iteration 38, loss = 0.00127075\n",
      "Iteration 39, loss = 0.00133892\n",
      "Iteration 40, loss = 0.00137365\n",
      "Iteration 41, loss = 0.00135193\n",
      "Iteration 42, loss = 0.00127800\n",
      "Iteration 43, loss = 0.00117710\n",
      "Iteration 44, loss = 0.00108116\n",
      "Iteration 45, loss = 0.00101356\n",
      "Iteration 46, loss = 0.00098016\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06777557\n",
      "Iteration 2, loss = 0.03451455\n",
      "Iteration 3, loss = 0.01824082\n",
      "Iteration 4, loss = 0.01618476\n",
      "Iteration 5, loss = 0.02033881\n",
      "Iteration 6, loss = 0.02226504\n",
      "Iteration 7, loss = 0.01967473\n",
      "Iteration 8, loss = 0.01437679\n",
      "Iteration 9, loss = 0.00891977\n",
      "Iteration 10, loss = 0.00496171\n",
      "Iteration 11, loss = 0.00316134\n",
      "Iteration 12, loss = 0.00320051\n",
      "Iteration 13, loss = 0.00417825\n",
      "Iteration 14, loss = 0.00514003\n",
      "Iteration 15, loss = 0.00544436\n",
      "Iteration 16, loss = 0.00497368\n",
      "Iteration 17, loss = 0.00406593\n",
      "Iteration 18, loss = 0.00322436\n",
      "Iteration 19, loss = 0.00286762\n",
      "Iteration 20, loss = 0.00307181\n",
      "Iteration 21, loss = 0.00356867\n",
      "Iteration 22, loss = 0.00393848\n",
      "Iteration 23, loss = 0.00390474\n",
      "Iteration 24, loss = 0.00345547\n",
      "Iteration 25, loss = 0.00278610\n",
      "Iteration 26, loss = 0.00214999\n",
      "Iteration 27, loss = 0.00172375\n",
      "Iteration 28, loss = 0.00155581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29, loss = 0.00158284\n",
      "Iteration 30, loss = 0.00167899\n",
      "Iteration 31, loss = 0.00173444\n",
      "Iteration 32, loss = 0.00169516\n",
      "Iteration 33, loss = 0.00156659\n",
      "Iteration 34, loss = 0.00140101\n",
      "Iteration 35, loss = 0.00126302\n",
      "Iteration 36, loss = 0.00119678\n",
      "Iteration 37, loss = 0.00121004\n",
      "Iteration 38, loss = 0.00127542\n",
      "Iteration 39, loss = 0.00134676\n",
      "Iteration 40, loss = 0.00138008\n",
      "Iteration 41, loss = 0.00135393\n",
      "Iteration 42, loss = 0.00127547\n",
      "Iteration 43, loss = 0.00117294\n",
      "Iteration 44, loss = 0.00107939\n",
      "Iteration 45, loss = 0.00101688\n",
      "Iteration 46, loss = 0.00098875\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06768307\n",
      "Iteration 2, loss = 0.03449014\n",
      "Iteration 3, loss = 0.01824319\n",
      "Iteration 4, loss = 0.01618090\n",
      "Iteration 5, loss = 0.02030008\n",
      "Iteration 6, loss = 0.02220896\n",
      "Iteration 7, loss = 0.01962466\n",
      "Iteration 8, loss = 0.01434289\n",
      "Iteration 9, loss = 0.00890010\n",
      "Iteration 10, loss = 0.00495378\n",
      "Iteration 11, loss = 0.00315610\n",
      "Iteration 12, loss = 0.00319190\n",
      "Iteration 13, loss = 0.00415954\n",
      "Iteration 14, loss = 0.00511243\n",
      "Iteration 15, loss = 0.00541392\n",
      "Iteration 16, loss = 0.00494871\n",
      "Iteration 17, loss = 0.00404894\n",
      "Iteration 18, loss = 0.00321270\n",
      "Iteration 19, loss = 0.00285574\n",
      "Iteration 20, loss = 0.00305576\n",
      "Iteration 21, loss = 0.00354857\n",
      "Iteration 22, loss = 0.00392000\n",
      "Iteration 23, loss = 0.00388860\n",
      "Iteration 24, loss = 0.00344087\n",
      "Iteration 25, loss = 0.00277476\n",
      "Iteration 26, loss = 0.00214382\n",
      "Iteration 27, loss = 0.00172325\n",
      "Iteration 28, loss = 0.00156031\n",
      "Iteration 29, loss = 0.00158839\n",
      "Iteration 30, loss = 0.00168384\n",
      "Iteration 31, loss = 0.00173642\n",
      "Iteration 32, loss = 0.00169193\n",
      "Iteration 33, loss = 0.00155932\n",
      "Iteration 34, loss = 0.00139139\n",
      "Iteration 35, loss = 0.00125335\n",
      "Iteration 36, loss = 0.00118887\n",
      "Iteration 37, loss = 0.00120531\n",
      "Iteration 38, loss = 0.00127399\n",
      "Iteration 39, loss = 0.00134619\n",
      "Iteration 40, loss = 0.00137775\n",
      "Iteration 41, loss = 0.00134852\n",
      "Iteration 42, loss = 0.00126773\n",
      "Iteration 43, loss = 0.00116505\n",
      "Iteration 44, loss = 0.00107394\n",
      "Iteration 45, loss = 0.00101525\n",
      "Iteration 46, loss = 0.00099093\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06778625\n",
      "Iteration 2, loss = 0.03453375\n",
      "Iteration 3, loss = 0.01822240\n",
      "Iteration 4, loss = 0.01611870\n",
      "Iteration 5, loss = 0.02026442\n",
      "Iteration 6, loss = 0.02222928\n",
      "Iteration 7, loss = 0.01967846\n",
      "Iteration 8, loss = 0.01440589\n",
      "Iteration 9, loss = 0.00895639\n",
      "Iteration 10, loss = 0.00499260\n",
      "Iteration 11, loss = 0.00317775\n",
      "Iteration 12, loss = 0.00319883\n",
      "Iteration 13, loss = 0.00416104\n",
      "Iteration 14, loss = 0.00512066\n",
      "Iteration 15, loss = 0.00543941\n",
      "Iteration 16, loss = 0.00498560\n",
      "Iteration 17, loss = 0.00408782\n",
      "Iteration 18, loss = 0.00323818\n",
      "Iteration 19, loss = 0.00285850\n",
      "Iteration 20, loss = 0.00303592\n",
      "Iteration 21, loss = 0.00351920\n",
      "Iteration 22, loss = 0.00389772\n",
      "Iteration 23, loss = 0.00388757\n",
      "Iteration 24, loss = 0.00346457\n",
      "Iteration 25, loss = 0.00281273\n",
      "Iteration 26, loss = 0.00217855\n",
      "Iteration 27, loss = 0.00174065\n",
      "Iteration 28, loss = 0.00155624\n",
      "Iteration 29, loss = 0.00156897\n",
      "Iteration 30, loss = 0.00166140\n",
      "Iteration 31, loss = 0.00172322\n",
      "Iteration 32, loss = 0.00169644\n",
      "Iteration 33, loss = 0.00158211\n",
      "Iteration 34, loss = 0.00142466\n",
      "Iteration 35, loss = 0.00128599\n",
      "Iteration 36, loss = 0.00121090\n",
      "Iteration 37, loss = 0.00121195\n",
      "Iteration 38, loss = 0.00126714\n",
      "Iteration 39, loss = 0.00133435\n",
      "Iteration 40, loss = 0.00137113\n",
      "Iteration 41, loss = 0.00135351\n",
      "Iteration 42, loss = 0.00128339\n",
      "Iteration 43, loss = 0.00118413\n",
      "Iteration 44, loss = 0.00108717\n",
      "Iteration 45, loss = 0.00101668\n",
      "Iteration 46, loss = 0.00098017\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06775014\n",
      "Iteration 2, loss = 0.03450127\n",
      "Iteration 3, loss = 0.01824277\n",
      "Iteration 4, loss = 0.01619489\n",
      "Iteration 5, loss = 0.02034306\n",
      "Iteration 6, loss = 0.02226233\n",
      "Iteration 7, loss = 0.01966212\n",
      "Iteration 8, loss = 0.01435866\n",
      "Iteration 9, loss = 0.00890220\n",
      "Iteration 10, loss = 0.00495116\n",
      "Iteration 11, loss = 0.00315843\n",
      "Iteration 12, loss = 0.00320235\n",
      "Iteration 13, loss = 0.00418133\n",
      "Iteration 14, loss = 0.00514127\n",
      "Iteration 15, loss = 0.00544392\n",
      "Iteration 16, loss = 0.00497189\n",
      "Iteration 17, loss = 0.00406559\n",
      "Iteration 18, loss = 0.00322239\n",
      "Iteration 19, loss = 0.00286673\n",
      "Iteration 20, loss = 0.00306710\n",
      "Iteration 21, loss = 0.00355989\n",
      "Iteration 22, loss = 0.00392576\n",
      "Iteration 23, loss = 0.00389356\n",
      "Iteration 24, loss = 0.00345032\n",
      "Iteration 25, loss = 0.00278732\n",
      "Iteration 26, loss = 0.00215224\n",
      "Iteration 27, loss = 0.00172134\n",
      "Iteration 28, loss = 0.00154828\n",
      "Iteration 29, loss = 0.00157074\n",
      "Iteration 30, loss = 0.00166577\n",
      "Iteration 31, loss = 0.00172457\n",
      "Iteration 32, loss = 0.00169167\n",
      "Iteration 33, loss = 0.00157096\n",
      "Iteration 34, loss = 0.00141050\n",
      "Iteration 35, loss = 0.00127374\n",
      "Iteration 36, loss = 0.00120430\n",
      "Iteration 37, loss = 0.00121162\n",
      "Iteration 38, loss = 0.00127110\n",
      "Iteration 39, loss = 0.00133929\n",
      "Iteration 40, loss = 0.00137361\n",
      "Iteration 41, loss = 0.00135137\n",
      "Iteration 42, loss = 0.00127705\n",
      "Iteration 43, loss = 0.00117608\n",
      "Iteration 44, loss = 0.00108043\n",
      "Iteration 45, loss = 0.00101330\n",
      "Iteration 46, loss = 0.00098034\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06772258\n",
      "Iteration 2, loss = 0.03448686\n",
      "Iteration 3, loss = 0.01824242\n",
      "Iteration 4, loss = 0.01620233\n",
      "Iteration 5, loss = 0.02035147\n",
      "Iteration 6, loss = 0.02226539\n",
      "Iteration 7, loss = 0.01965907\n",
      "Iteration 8, loss = 0.01435236\n",
      "Iteration 9, loss = 0.00889409\n",
      "Iteration 10, loss = 0.00494310\n",
      "Iteration 11, loss = 0.00315500\n",
      "Iteration 12, loss = 0.00320666\n",
      "Iteration 13, loss = 0.00419032\n",
      "Iteration 14, loss = 0.00515081\n",
      "Iteration 15, loss = 0.00545207\n",
      "Iteration 16, loss = 0.00497675\n",
      "Iteration 17, loss = 0.00406965\n",
      "Iteration 18, loss = 0.00323117\n",
      "Iteration 19, loss = 0.00287777\n",
      "Iteration 20, loss = 0.00307609\n",
      "Iteration 21, loss = 0.00356625\n",
      "Iteration 22, loss = 0.00393007\n",
      "Iteration 23, loss = 0.00389649\n",
      "Iteration 24, loss = 0.00345275\n",
      "Iteration 25, loss = 0.00278934\n",
      "Iteration 26, loss = 0.00215404\n",
      "Iteration 27, loss = 0.00172263\n",
      "Iteration 28, loss = 0.00154752\n",
      "Iteration 29, loss = 0.00156775\n",
      "Iteration 30, loss = 0.00166349\n",
      "Iteration 31, loss = 0.00172391\n",
      "Iteration 32, loss = 0.00169329\n",
      "Iteration 33, loss = 0.00157362\n",
      "Iteration 34, loss = 0.00141409\n",
      "Iteration 35, loss = 0.00127757\n",
      "Iteration 36, loss = 0.00120778\n",
      "Iteration 37, loss = 0.00121443\n",
      "Iteration 38, loss = 0.00127362\n",
      "Iteration 39, loss = 0.00134182\n",
      "Iteration 40, loss = 0.00137641\n",
      "Iteration 41, loss = 0.00135434\n",
      "Iteration 42, loss = 0.00127964\n",
      "Iteration 43, loss = 0.00117792\n",
      "Iteration 44, loss = 0.00108140\n",
      "Iteration 45, loss = 0.00101353\n",
      "Iteration 46, loss = 0.00098009\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06770962\n",
      "Iteration 2, loss = 0.03447105\n",
      "Iteration 3, loss = 0.01824092\n",
      "Iteration 4, loss = 0.01621064\n",
      "Iteration 5, loss = 0.02035737\n",
      "Iteration 6, loss = 0.02226381\n",
      "Iteration 7, loss = 0.01965215\n",
      "Iteration 8, loss = 0.01434347\n",
      "Iteration 9, loss = 0.00888074\n",
      "Iteration 10, loss = 0.00492957\n",
      "Iteration 11, loss = 0.00314824\n",
      "Iteration 12, loss = 0.00320777\n",
      "Iteration 13, loss = 0.00419573\n",
      "Iteration 14, loss = 0.00515550\n",
      "Iteration 15, loss = 0.00545233\n",
      "Iteration 16, loss = 0.00497742\n",
      "Iteration 17, loss = 0.00407231\n",
      "Iteration 18, loss = 0.00323455\n",
      "Iteration 19, loss = 0.00288352\n",
      "Iteration 20, loss = 0.00308366\n",
      "Iteration 21, loss = 0.00357428\n",
      "Iteration 22, loss = 0.00393693\n",
      "Iteration 23, loss = 0.00390015\n",
      "Iteration 24, loss = 0.00345277\n",
      "Iteration 25, loss = 0.00278678\n",
      "Iteration 26, loss = 0.00215116\n",
      "Iteration 27, loss = 0.00172083\n",
      "Iteration 28, loss = 0.00154834\n",
      "Iteration 29, loss = 0.00157048\n",
      "Iteration 30, loss = 0.00166535\n",
      "Iteration 31, loss = 0.00172477\n",
      "Iteration 32, loss = 0.00169319\n",
      "Iteration 33, loss = 0.00157301\n",
      "Iteration 34, loss = 0.00141333\n",
      "Iteration 35, loss = 0.00127683\n",
      "Iteration 36, loss = 0.00120721\n",
      "Iteration 37, loss = 0.00121431\n",
      "Iteration 38, loss = 0.00127409\n",
      "Iteration 39, loss = 0.00134272\n",
      "Iteration 40, loss = 0.00137720\n",
      "Iteration 41, loss = 0.00135457\n",
      "Iteration 42, loss = 0.00127920\n",
      "Iteration 43, loss = 0.00117695\n",
      "Iteration 44, loss = 0.00108021\n",
      "Iteration 45, loss = 0.00101237\n",
      "Iteration 46, loss = 0.00097909\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06758968\n",
      "Iteration 2, loss = 0.03442501\n",
      "Iteration 3, loss = 0.01823547\n",
      "Iteration 4, loss = 0.01621978\n",
      "Iteration 5, loss = 0.02033604\n",
      "Iteration 6, loss = 0.02220843\n",
      "Iteration 7, loss = 0.01958452\n",
      "Iteration 8, loss = 0.01428202\n",
      "Iteration 9, loss = 0.00884205\n",
      "Iteration 10, loss = 0.00491104\n",
      "Iteration 11, loss = 0.00313702\n",
      "Iteration 12, loss = 0.00319248\n",
      "Iteration 13, loss = 0.00417239\n",
      "Iteration 14, loss = 0.00512592\n",
      "Iteration 15, loss = 0.00542254\n",
      "Iteration 16, loss = 0.00495278\n",
      "Iteration 17, loss = 0.00405490\n",
      "Iteration 18, loss = 0.00323117\n",
      "Iteration 19, loss = 0.00288970\n",
      "Iteration 20, loss = 0.00309584\n",
      "Iteration 21, loss = 0.00358399\n",
      "Iteration 22, loss = 0.00394168\n",
      "Iteration 23, loss = 0.00389682\n",
      "Iteration 24, loss = 0.00344254\n",
      "Iteration 25, loss = 0.00277386\n",
      "Iteration 26, loss = 0.00214199\n",
      "Iteration 27, loss = 0.00171922\n",
      "Iteration 28, loss = 0.00155220\n",
      "Iteration 29, loss = 0.00157737\n",
      "Iteration 30, loss = 0.00167168\n",
      "Iteration 31, loss = 0.00172467\n",
      "Iteration 32, loss = 0.00168435\n",
      "Iteration 33, loss = 0.00155552\n",
      "Iteration 34, loss = 0.00139210\n",
      "Iteration 35, loss = 0.00125831\n",
      "Iteration 36, loss = 0.00119663\n",
      "Iteration 37, loss = 0.00121333\n",
      "Iteration 38, loss = 0.00128017\n",
      "Iteration 39, loss = 0.00135032\n",
      "Iteration 40, loss = 0.00138049\n",
      "Iteration 41, loss = 0.00135098\n",
      "Iteration 42, loss = 0.00127048\n",
      "Iteration 43, loss = 0.00116763\n",
      "Iteration 44, loss = 0.00107545\n",
      "Iteration 45, loss = 0.00101498\n",
      "Iteration 46, loss = 0.00098845\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06766976\n",
      "Iteration 2, loss = 0.03448250\n",
      "Iteration 3, loss = 0.01824321\n",
      "Iteration 4, loss = 0.01618085\n",
      "Iteration 5, loss = 0.02030309\n",
      "Iteration 6, loss = 0.02221044\n",
      "Iteration 7, loss = 0.01961433\n",
      "Iteration 8, loss = 0.01432599\n",
      "Iteration 9, loss = 0.00888651\n",
      "Iteration 10, loss = 0.00494611\n",
      "Iteration 11, loss = 0.00315783\n",
      "Iteration 12, loss = 0.00319966\n",
      "Iteration 13, loss = 0.00417035\n",
      "Iteration 14, loss = 0.00512427\n",
      "Iteration 15, loss = 0.00542948\n",
      "Iteration 16, loss = 0.00496614\n",
      "Iteration 17, loss = 0.00406910\n",
      "Iteration 18, loss = 0.00323271\n",
      "Iteration 19, loss = 0.00287632\n",
      "Iteration 20, loss = 0.00306562\n",
      "Iteration 21, loss = 0.00354534\n",
      "Iteration 22, loss = 0.00390776\n",
      "Iteration 23, loss = 0.00388031\n",
      "Iteration 24, loss = 0.00344590\n",
      "Iteration 25, loss = 0.00279090\n",
      "Iteration 26, loss = 0.00215943\n",
      "Iteration 27, loss = 0.00172698\n",
      "Iteration 28, loss = 0.00154734\n",
      "Iteration 29, loss = 0.00156451\n",
      "Iteration 30, loss = 0.00165808\n",
      "Iteration 31, loss = 0.00171948\n",
      "Iteration 32, loss = 0.00169176\n",
      "Iteration 33, loss = 0.00157616\n",
      "Iteration 34, loss = 0.00141892\n",
      "Iteration 35, loss = 0.00128285\n",
      "Iteration 36, loss = 0.00121155\n",
      "Iteration 37, loss = 0.00121589\n",
      "Iteration 38, loss = 0.00127273\n",
      "Iteration 39, loss = 0.00133946\n",
      "Iteration 40, loss = 0.00137396\n",
      "Iteration 41, loss = 0.00135298\n",
      "Iteration 42, loss = 0.00127981\n",
      "Iteration 43, loss = 0.00117893\n",
      "Iteration 44, loss = 0.00108193\n",
      "Iteration 45, loss = 0.00101255\n",
      "Iteration 46, loss = 0.00097731\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06769714\n",
      "Iteration 2, loss = 0.03448879\n",
      "Iteration 3, loss = 0.01824421\n",
      "Iteration 4, loss = 0.01618887\n",
      "Iteration 5, loss = 0.02030820\n",
      "Iteration 6, loss = 0.02221233\n",
      "Iteration 7, loss = 0.01962222\n",
      "Iteration 8, loss = 0.01433471\n",
      "Iteration 9, loss = 0.00889051\n",
      "Iteration 10, loss = 0.00494499\n",
      "Iteration 11, loss = 0.00315056\n",
      "Iteration 12, loss = 0.00319164\n",
      "Iteration 13, loss = 0.00416621\n",
      "Iteration 14, loss = 0.00512316\n",
      "Iteration 15, loss = 0.00542710\n",
      "Iteration 16, loss = 0.00496310\n",
      "Iteration 17, loss = 0.00406408\n",
      "Iteration 18, loss = 0.00322832\n",
      "Iteration 19, loss = 0.00287401\n",
      "Iteration 20, loss = 0.00307709\n",
      "Iteration 21, loss = 0.00356927\n",
      "Iteration 22, loss = 0.00393557\n",
      "Iteration 23, loss = 0.00389902\n",
      "Iteration 24, loss = 0.00344782\n",
      "Iteration 25, loss = 0.00277878\n",
      "Iteration 26, loss = 0.00214488\n",
      "Iteration 27, loss = 0.00172182\n",
      "Iteration 28, loss = 0.00155769\n",
      "Iteration 29, loss = 0.00158561\n",
      "Iteration 30, loss = 0.00168102\n",
      "Iteration 31, loss = 0.00173454\n",
      "Iteration 32, loss = 0.00169151\n",
      "Iteration 33, loss = 0.00155939\n",
      "Iteration 34, loss = 0.00139313\n",
      "Iteration 35, loss = 0.00125699\n",
      "Iteration 36, loss = 0.00119402\n",
      "Iteration 37, loss = 0.00121103\n",
      "Iteration 38, loss = 0.00127925\n",
      "Iteration 39, loss = 0.00135048\n",
      "Iteration 40, loss = 0.00138103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 41, loss = 0.00135095\n",
      "Iteration 42, loss = 0.00126951\n",
      "Iteration 43, loss = 0.00116642\n",
      "Iteration 44, loss = 0.00107482\n",
      "Iteration 45, loss = 0.00101564\n",
      "Iteration 46, loss = 0.00099075\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06774635\n",
      "Iteration 2, loss = 0.03451030\n",
      "Iteration 3, loss = 0.01823945\n",
      "Iteration 4, loss = 0.01616523\n",
      "Iteration 5, loss = 0.02030679\n",
      "Iteration 6, loss = 0.02223899\n",
      "Iteration 7, loss = 0.01965756\n",
      "Iteration 8, loss = 0.01437079\n",
      "Iteration 9, loss = 0.00892340\n",
      "Iteration 10, loss = 0.00497009\n",
      "Iteration 11, loss = 0.00316865\n",
      "Iteration 12, loss = 0.00320101\n",
      "Iteration 13, loss = 0.00416866\n",
      "Iteration 14, loss = 0.00512457\n",
      "Iteration 15, loss = 0.00543369\n",
      "Iteration 16, loss = 0.00497324\n",
      "Iteration 17, loss = 0.00407516\n",
      "Iteration 18, loss = 0.00323098\n",
      "Iteration 19, loss = 0.00286337\n",
      "Iteration 20, loss = 0.00305240\n",
      "Iteration 21, loss = 0.00353972\n",
      "Iteration 22, loss = 0.00391052\n",
      "Iteration 23, loss = 0.00388813\n",
      "Iteration 24, loss = 0.00345449\n",
      "Iteration 25, loss = 0.00279725\n",
      "Iteration 26, loss = 0.00216303\n",
      "Iteration 27, loss = 0.00173061\n",
      "Iteration 28, loss = 0.00155271\n",
      "Iteration 29, loss = 0.00157073\n",
      "Iteration 30, loss = 0.00166438\n",
      "Iteration 31, loss = 0.00172439\n",
      "Iteration 32, loss = 0.00169430\n",
      "Iteration 33, loss = 0.00157639\n",
      "Iteration 34, loss = 0.00141725\n",
      "Iteration 35, loss = 0.00127969\n",
      "Iteration 36, loss = 0.00120797\n",
      "Iteration 37, loss = 0.00121265\n",
      "Iteration 38, loss = 0.00127026\n",
      "Iteration 39, loss = 0.00133785\n",
      "Iteration 40, loss = 0.00137295\n",
      "Iteration 41, loss = 0.00135242\n",
      "Iteration 42, loss = 0.00127960\n",
      "Iteration 43, loss = 0.00117924\n",
      "Iteration 44, loss = 0.00108293\n",
      "Iteration 45, loss = 0.00101425\n",
      "Iteration 46, loss = 0.00097957\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06767396\n",
      "Iteration 2, loss = 0.03445899\n",
      "Iteration 3, loss = 0.01823879\n",
      "Iteration 4, loss = 0.01620943\n",
      "Iteration 5, loss = 0.02035126\n",
      "Iteration 6, loss = 0.02225197\n",
      "Iteration 7, loss = 0.01963582\n",
      "Iteration 8, loss = 0.01432610\n",
      "Iteration 9, loss = 0.00887037\n",
      "Iteration 10, loss = 0.00492533\n",
      "Iteration 11, loss = 0.00314439\n",
      "Iteration 12, loss = 0.00320153\n",
      "Iteration 13, loss = 0.00418692\n",
      "Iteration 14, loss = 0.00514538\n",
      "Iteration 15, loss = 0.00544328\n",
      "Iteration 16, loss = 0.00496691\n",
      "Iteration 17, loss = 0.00406426\n",
      "Iteration 18, loss = 0.00322895\n",
      "Iteration 19, loss = 0.00288002\n",
      "Iteration 20, loss = 0.00308127\n",
      "Iteration 21, loss = 0.00357158\n",
      "Iteration 22, loss = 0.00393311\n",
      "Iteration 23, loss = 0.00389583\n",
      "Iteration 24, loss = 0.00344890\n",
      "Iteration 25, loss = 0.00278374\n",
      "Iteration 26, loss = 0.00214909\n",
      "Iteration 27, loss = 0.00171922\n",
      "Iteration 28, loss = 0.00154649\n",
      "Iteration 29, loss = 0.00156781\n",
      "Iteration 30, loss = 0.00166221\n",
      "Iteration 31, loss = 0.00172208\n",
      "Iteration 32, loss = 0.00169044\n",
      "Iteration 33, loss = 0.00156959\n",
      "Iteration 34, loss = 0.00140975\n",
      "Iteration 35, loss = 0.00127373\n",
      "Iteration 36, loss = 0.00120499\n",
      "Iteration 37, loss = 0.00121312\n",
      "Iteration 38, loss = 0.00127356\n",
      "Iteration 39, loss = 0.00134228\n",
      "Iteration 40, loss = 0.00137626\n",
      "Iteration 41, loss = 0.00135301\n",
      "Iteration 42, loss = 0.00127730\n",
      "Iteration 43, loss = 0.00117514\n",
      "Iteration 44, loss = 0.00107895\n",
      "Iteration 45, loss = 0.00101189\n",
      "Iteration 46, loss = 0.00097928\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06761722\n",
      "Iteration 2, loss = 0.03443893\n",
      "Iteration 3, loss = 0.01823926\n",
      "Iteration 4, loss = 0.01621624\n",
      "Iteration 5, loss = 0.02033598\n",
      "Iteration 6, loss = 0.02221321\n",
      "Iteration 7, loss = 0.01959093\n",
      "Iteration 8, loss = 0.01428738\n",
      "Iteration 9, loss = 0.00884530\n",
      "Iteration 10, loss = 0.00491328\n",
      "Iteration 11, loss = 0.00313712\n",
      "Iteration 12, loss = 0.00319064\n",
      "Iteration 13, loss = 0.00417260\n",
      "Iteration 14, loss = 0.00512934\n",
      "Iteration 15, loss = 0.00542670\n",
      "Iteration 16, loss = 0.00495593\n",
      "Iteration 17, loss = 0.00405756\n",
      "Iteration 18, loss = 0.00323293\n",
      "Iteration 19, loss = 0.00288903\n",
      "Iteration 20, loss = 0.00309334\n",
      "Iteration 21, loss = 0.00358169\n",
      "Iteration 22, loss = 0.00393962\n",
      "Iteration 23, loss = 0.00389467\n",
      "Iteration 24, loss = 0.00343886\n",
      "Iteration 25, loss = 0.00276908\n",
      "Iteration 26, loss = 0.00213693\n",
      "Iteration 27, loss = 0.00171556\n",
      "Iteration 28, loss = 0.00155135\n",
      "Iteration 29, loss = 0.00157756\n",
      "Iteration 30, loss = 0.00167321\n",
      "Iteration 31, loss = 0.00172780\n",
      "Iteration 32, loss = 0.00168687\n",
      "Iteration 33, loss = 0.00155679\n",
      "Iteration 34, loss = 0.00139311\n",
      "Iteration 35, loss = 0.00125904\n",
      "Iteration 36, loss = 0.00119719\n",
      "Iteration 37, loss = 0.00121444\n",
      "Iteration 38, loss = 0.00128218\n",
      "Iteration 39, loss = 0.00135258\n",
      "Iteration 40, loss = 0.00138228\n",
      "Iteration 41, loss = 0.00135158\n",
      "Iteration 42, loss = 0.00126973\n",
      "Iteration 43, loss = 0.00116633\n",
      "Iteration 44, loss = 0.00107442\n",
      "Iteration 45, loss = 0.00101475\n",
      "Iteration 46, loss = 0.00098931\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06746579\n",
      "Iteration 2, loss = 0.03436677\n",
      "Iteration 3, loss = 0.01822147\n",
      "Iteration 4, loss = 0.01622387\n",
      "Iteration 5, loss = 0.02032523\n",
      "Iteration 6, loss = 0.02216761\n",
      "Iteration 7, loss = 0.01951897\n",
      "Iteration 8, loss = 0.01421207\n",
      "Iteration 9, loss = 0.00878357\n",
      "Iteration 10, loss = 0.00487168\n",
      "Iteration 11, loss = 0.00311740\n",
      "Iteration 12, loss = 0.00318472\n",
      "Iteration 13, loss = 0.00416682\n",
      "Iteration 14, loss = 0.00511919\n",
      "Iteration 15, loss = 0.00540781\n",
      "Iteration 16, loss = 0.00493391\n",
      "Iteration 17, loss = 0.00404174\n",
      "Iteration 18, loss = 0.00322407\n",
      "Iteration 19, loss = 0.00288940\n",
      "Iteration 20, loss = 0.00309696\n",
      "Iteration 21, loss = 0.00358152\n",
      "Iteration 22, loss = 0.00393004\n",
      "Iteration 23, loss = 0.00387917\n",
      "Iteration 24, loss = 0.00342282\n",
      "Iteration 25, loss = 0.00275658\n",
      "Iteration 26, loss = 0.00212775\n",
      "Iteration 27, loss = 0.00170595\n",
      "Iteration 28, loss = 0.00153969\n",
      "Iteration 29, loss = 0.00156390\n",
      "Iteration 30, loss = 0.00165643\n",
      "Iteration 31, loss = 0.00171204\n",
      "Iteration 32, loss = 0.00167565\n",
      "Iteration 33, loss = 0.00155268\n",
      "Iteration 34, loss = 0.00139445\n",
      "Iteration 35, loss = 0.00126179\n",
      "Iteration 36, loss = 0.00119768\n",
      "Iteration 37, loss = 0.00121022\n",
      "Iteration 38, loss = 0.00127347\n",
      "Iteration 39, loss = 0.00134189\n",
      "Iteration 40, loss = 0.00137304\n",
      "Iteration 41, loss = 0.00134584\n",
      "Iteration 42, loss = 0.00126690\n",
      "Iteration 43, loss = 0.00116382\n",
      "Iteration 44, loss = 0.00106921\n",
      "Iteration 45, loss = 0.00100496\n",
      "Iteration 46, loss = 0.00097538\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06753275\n",
      "Iteration 2, loss = 0.03437356\n",
      "Iteration 3, loss = 0.01821862\n",
      "Iteration 4, loss = 0.01622793\n",
      "Iteration 5, loss = 0.02035801\n",
      "Iteration 6, loss = 0.02221813\n",
      "Iteration 7, loss = 0.01957181\n",
      "Iteration 8, loss = 0.01425368\n",
      "Iteration 9, loss = 0.00880822\n",
      "Iteration 10, loss = 0.00488424\n",
      "Iteration 11, loss = 0.00313086\n",
      "Iteration 12, loss = 0.00321239\n",
      "Iteration 13, loss = 0.00421098\n",
      "Iteration 14, loss = 0.00517030\n",
      "Iteration 15, loss = 0.00546204\n",
      "Iteration 16, loss = 0.00498059\n",
      "Iteration 17, loss = 0.00407229\n",
      "Iteration 18, loss = 0.00323868\n",
      "Iteration 19, loss = 0.00289472\n",
      "Iteration 20, loss = 0.00310349\n",
      "Iteration 21, loss = 0.00359566\n",
      "Iteration 22, loss = 0.00395216\n",
      "Iteration 23, loss = 0.00390270\n",
      "Iteration 24, loss = 0.00344105\n",
      "Iteration 25, loss = 0.00276724\n",
      "Iteration 26, loss = 0.00213353\n",
      "Iteration 27, loss = 0.00171247\n",
      "Iteration 28, loss = 0.00154996\n",
      "Iteration 29, loss = 0.00157973\n",
      "Iteration 30, loss = 0.00167636\n",
      "Iteration 31, loss = 0.00173213\n",
      "Iteration 32, loss = 0.00169309\n",
      "Iteration 33, loss = 0.00156476\n",
      "Iteration 34, loss = 0.00140179\n",
      "Iteration 35, loss = 0.00126695\n",
      "Iteration 36, loss = 0.00120343\n",
      "Iteration 37, loss = 0.00121804\n",
      "Iteration 38, loss = 0.00128352\n",
      "Iteration 39, loss = 0.00135314\n",
      "Iteration 40, loss = 0.00138382\n",
      "Iteration 41, loss = 0.00135487\n",
      "Iteration 42, loss = 0.00127408\n",
      "Iteration 43, loss = 0.00117016\n",
      "Iteration 44, loss = 0.00107617\n",
      "Iteration 45, loss = 0.00101355\n",
      "Iteration 46, loss = 0.00098547\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06749456\n",
      "Iteration 2, loss = 0.03433855\n",
      "Iteration 3, loss = 0.01820262\n",
      "Iteration 4, loss = 0.01623265\n",
      "Iteration 5, loss = 0.02036242\n",
      "Iteration 6, loss = 0.02221495\n",
      "Iteration 7, loss = 0.01956668\n",
      "Iteration 8, loss = 0.01424904\n",
      "Iteration 9, loss = 0.00880634\n",
      "Iteration 10, loss = 0.00488554\n",
      "Iteration 11, loss = 0.00313044\n",
      "Iteration 12, loss = 0.00320699\n",
      "Iteration 13, loss = 0.00420020\n",
      "Iteration 14, loss = 0.00515711\n",
      "Iteration 15, loss = 0.00544925\n",
      "Iteration 16, loss = 0.00497011\n",
      "Iteration 17, loss = 0.00406428\n",
      "Iteration 18, loss = 0.00323371\n",
      "Iteration 19, loss = 0.00288963\n",
      "Iteration 20, loss = 0.00309725\n",
      "Iteration 21, loss = 0.00358504\n",
      "Iteration 22, loss = 0.00394136\n",
      "Iteration 23, loss = 0.00389334\n",
      "Iteration 24, loss = 0.00343435\n",
      "Iteration 25, loss = 0.00276397\n",
      "Iteration 26, loss = 0.00213311\n",
      "Iteration 27, loss = 0.00171410\n",
      "Iteration 28, loss = 0.00155221\n",
      "Iteration 29, loss = 0.00158179\n",
      "Iteration 30, loss = 0.00167787\n",
      "Iteration 31, loss = 0.00173266\n",
      "Iteration 32, loss = 0.00169301\n",
      "Iteration 33, loss = 0.00156480\n",
      "Iteration 34, loss = 0.00140188\n",
      "Iteration 35, loss = 0.00126725\n",
      "Iteration 36, loss = 0.00120377\n",
      "Iteration 37, loss = 0.00121847\n",
      "Iteration 38, loss = 0.00128377\n",
      "Iteration 39, loss = 0.00135328\n",
      "Iteration 40, loss = 0.00138388\n",
      "Iteration 41, loss = 0.00135488\n",
      "Iteration 42, loss = 0.00127432\n",
      "Iteration 43, loss = 0.00117073\n",
      "Iteration 44, loss = 0.00107716\n",
      "Iteration 45, loss = 0.00101508\n",
      "Iteration 46, loss = 0.00098722\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06741016\n",
      "Iteration 2, loss = 0.03432919\n",
      "Iteration 3, loss = 0.01820700\n",
      "Iteration 4, loss = 0.01622972\n",
      "Iteration 5, loss = 0.02032923\n",
      "Iteration 6, loss = 0.02214918\n",
      "Iteration 7, loss = 0.01949345\n",
      "Iteration 8, loss = 0.01419079\n",
      "Iteration 9, loss = 0.00877060\n",
      "Iteration 10, loss = 0.00486864\n",
      "Iteration 11, loss = 0.00312087\n",
      "Iteration 12, loss = 0.00319068\n",
      "Iteration 13, loss = 0.00417532\n",
      "Iteration 14, loss = 0.00512573\n",
      "Iteration 15, loss = 0.00542024\n",
      "Iteration 16, loss = 0.00495323\n",
      "Iteration 17, loss = 0.00406336\n",
      "Iteration 18, loss = 0.00324495\n",
      "Iteration 19, loss = 0.00290615\n",
      "Iteration 20, loss = 0.00311158\n",
      "Iteration 21, loss = 0.00359329\n",
      "Iteration 22, loss = 0.00394204\n",
      "Iteration 23, loss = 0.00388838\n",
      "Iteration 24, loss = 0.00342855\n",
      "Iteration 25, loss = 0.00275927\n",
      "Iteration 26, loss = 0.00213004\n",
      "Iteration 27, loss = 0.00171126\n",
      "Iteration 28, loss = 0.00154806\n",
      "Iteration 29, loss = 0.00157385\n",
      "Iteration 30, loss = 0.00166667\n",
      "Iteration 31, loss = 0.00171940\n",
      "Iteration 32, loss = 0.00167889\n",
      "Iteration 33, loss = 0.00155140\n",
      "Iteration 34, loss = 0.00139101\n",
      "Iteration 35, loss = 0.00126010\n",
      "Iteration 36, loss = 0.00120014\n",
      "Iteration 37, loss = 0.00121726\n",
      "Iteration 38, loss = 0.00128329\n",
      "Iteration 39, loss = 0.00135197\n",
      "Iteration 40, loss = 0.00138044\n",
      "Iteration 41, loss = 0.00134969\n",
      "Iteration 42, loss = 0.00126852\n",
      "Iteration 43, loss = 0.00116529\n",
      "Iteration 44, loss = 0.00107279\n",
      "Iteration 45, loss = 0.00101209\n",
      "Iteration 46, loss = 0.00098490\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06762958\n",
      "Iteration 2, loss = 0.03446280\n",
      "Iteration 3, loss = 0.01824325\n",
      "Iteration 4, loss = 0.01619615\n",
      "Iteration 5, loss = 0.02031071\n",
      "Iteration 6, loss = 0.02219477\n",
      "Iteration 7, loss = 0.01958426\n",
      "Iteration 8, loss = 0.01429291\n",
      "Iteration 9, loss = 0.00885784\n",
      "Iteration 10, loss = 0.00492510\n",
      "Iteration 11, loss = 0.00314384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.00319408\n",
      "Iteration 13, loss = 0.00417341\n",
      "Iteration 14, loss = 0.00513030\n",
      "Iteration 15, loss = 0.00543141\n",
      "Iteration 16, loss = 0.00497082\n",
      "Iteration 17, loss = 0.00407655\n",
      "Iteration 18, loss = 0.00324445\n",
      "Iteration 19, loss = 0.00288798\n",
      "Iteration 20, loss = 0.00308182\n",
      "Iteration 21, loss = 0.00356238\n",
      "Iteration 22, loss = 0.00392204\n",
      "Iteration 23, loss = 0.00388481\n",
      "Iteration 24, loss = 0.00343638\n",
      "Iteration 25, loss = 0.00277066\n",
      "Iteration 26, loss = 0.00213816\n",
      "Iteration 27, loss = 0.00171339\n",
      "Iteration 28, loss = 0.00154556\n",
      "Iteration 29, loss = 0.00157083\n",
      "Iteration 30, loss = 0.00166653\n",
      "Iteration 31, loss = 0.00172458\n",
      "Iteration 32, loss = 0.00168859\n",
      "Iteration 33, loss = 0.00156259\n",
      "Iteration 34, loss = 0.00140092\n",
      "Iteration 35, loss = 0.00126597\n",
      "Iteration 36, loss = 0.00120087\n",
      "Iteration 37, loss = 0.00121403\n",
      "Iteration 38, loss = 0.00127855\n",
      "Iteration 39, loss = 0.00134840\n",
      "Iteration 40, loss = 0.00137984\n",
      "Iteration 41, loss = 0.00135175\n",
      "Iteration 42, loss = 0.00127175\n",
      "Iteration 43, loss = 0.00116851\n",
      "Iteration 44, loss = 0.00107516\n",
      "Iteration 45, loss = 0.00101336\n",
      "Iteration 46, loss = 0.00098574\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06768935\n",
      "Iteration 2, loss = 0.03447402\n",
      "Iteration 3, loss = 0.01824270\n",
      "Iteration 4, loss = 0.01619709\n",
      "Iteration 5, loss = 0.02033772\n",
      "Iteration 6, loss = 0.02224661\n",
      "Iteration 7, loss = 0.01964281\n",
      "Iteration 8, loss = 0.01434064\n",
      "Iteration 9, loss = 0.00888648\n",
      "Iteration 10, loss = 0.00493784\n",
      "Iteration 11, loss = 0.00314962\n",
      "Iteration 12, loss = 0.00320068\n",
      "Iteration 13, loss = 0.00418501\n",
      "Iteration 14, loss = 0.00514565\n",
      "Iteration 15, loss = 0.00544675\n",
      "Iteration 16, loss = 0.00497422\n",
      "Iteration 17, loss = 0.00407427\n",
      "Iteration 18, loss = 0.00323953\n",
      "Iteration 19, loss = 0.00288643\n",
      "Iteration 20, loss = 0.00308569\n",
      "Iteration 21, loss = 0.00357506\n",
      "Iteration 22, loss = 0.00393872\n",
      "Iteration 23, loss = 0.00390207\n",
      "Iteration 24, loss = 0.00345282\n",
      "Iteration 25, loss = 0.00278478\n",
      "Iteration 26, loss = 0.00214952\n",
      "Iteration 27, loss = 0.00172193\n",
      "Iteration 28, loss = 0.00155141\n",
      "Iteration 29, loss = 0.00157597\n",
      "Iteration 30, loss = 0.00167108\n",
      "Iteration 31, loss = 0.00172919\n",
      "Iteration 32, loss = 0.00169383\n",
      "Iteration 33, loss = 0.00156828\n",
      "Iteration 34, loss = 0.00140533\n",
      "Iteration 35, loss = 0.00126839\n",
      "Iteration 36, loss = 0.00120157\n",
      "Iteration 37, loss = 0.00121346\n",
      "Iteration 38, loss = 0.00127742\n",
      "Iteration 39, loss = 0.00134784\n",
      "Iteration 40, loss = 0.00138085\n",
      "Iteration 41, loss = 0.00135479\n",
      "Iteration 42, loss = 0.00127629\n",
      "Iteration 43, loss = 0.00117334\n",
      "Iteration 44, loss = 0.00107900\n",
      "Iteration 45, loss = 0.00101548\n",
      "Iteration 46, loss = 0.00098641\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06754223\n",
      "Iteration 2, loss = 0.03438288\n",
      "Iteration 3, loss = 0.01822403\n",
      "Iteration 4, loss = 0.01622229\n",
      "Iteration 5, loss = 0.02034271\n",
      "Iteration 6, loss = 0.02220437\n",
      "Iteration 7, loss = 0.01956164\n",
      "Iteration 8, loss = 0.01424729\n",
      "Iteration 9, loss = 0.00880455\n",
      "Iteration 10, loss = 0.00487964\n",
      "Iteration 11, loss = 0.00311787\n",
      "Iteration 12, loss = 0.00318728\n",
      "Iteration 13, loss = 0.00417781\n",
      "Iteration 14, loss = 0.00513405\n",
      "Iteration 15, loss = 0.00542313\n",
      "Iteration 16, loss = 0.00494736\n",
      "Iteration 17, loss = 0.00404917\n",
      "Iteration 18, loss = 0.00322595\n",
      "Iteration 19, loss = 0.00288824\n",
      "Iteration 20, loss = 0.00309771\n",
      "Iteration 21, loss = 0.00358718\n",
      "Iteration 22, loss = 0.00394082\n",
      "Iteration 23, loss = 0.00389125\n",
      "Iteration 24, loss = 0.00343346\n",
      "Iteration 25, loss = 0.00276382\n",
      "Iteration 26, loss = 0.00213152\n",
      "Iteration 27, loss = 0.00170839\n",
      "Iteration 28, loss = 0.00154162\n",
      "Iteration 29, loss = 0.00156650\n",
      "Iteration 30, loss = 0.00165952\n",
      "Iteration 31, loss = 0.00171436\n",
      "Iteration 32, loss = 0.00167690\n",
      "Iteration 33, loss = 0.00155219\n",
      "Iteration 34, loss = 0.00139208\n",
      "Iteration 35, loss = 0.00125885\n",
      "Iteration 36, loss = 0.00119526\n",
      "Iteration 37, loss = 0.00120907\n",
      "Iteration 38, loss = 0.00127355\n",
      "Iteration 39, loss = 0.00134277\n",
      "Iteration 40, loss = 0.00137412\n",
      "Iteration 41, loss = 0.00134658\n",
      "Iteration 42, loss = 0.00126731\n",
      "Iteration 43, loss = 0.00116432\n",
      "Iteration 44, loss = 0.00107030\n",
      "Iteration 45, loss = 0.00100690\n",
      "Iteration 46, loss = 0.00097808\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06741894\n",
      "Iteration 2, loss = 0.03431806\n",
      "Iteration 3, loss = 0.01819762\n",
      "Iteration 4, loss = 0.01621991\n",
      "Iteration 5, loss = 0.02033630\n",
      "Iteration 6, loss = 0.02216841\n",
      "Iteration 7, loss = 0.01951056\n",
      "Iteration 8, loss = 0.01419798\n",
      "Iteration 9, loss = 0.00876753\n",
      "Iteration 10, loss = 0.00485821\n",
      "Iteration 11, loss = 0.00311162\n",
      "Iteration 12, loss = 0.00319241\n",
      "Iteration 13, loss = 0.00418942\n",
      "Iteration 14, loss = 0.00514864\n",
      "Iteration 15, loss = 0.00544127\n",
      "Iteration 16, loss = 0.00496435\n",
      "Iteration 17, loss = 0.00406332\n",
      "Iteration 18, loss = 0.00323681\n",
      "Iteration 19, loss = 0.00289806\n",
      "Iteration 20, loss = 0.00310755\n",
      "Iteration 21, loss = 0.00359640\n",
      "Iteration 22, loss = 0.00394774\n",
      "Iteration 23, loss = 0.00389382\n",
      "Iteration 24, loss = 0.00343050\n",
      "Iteration 25, loss = 0.00275759\n",
      "Iteration 26, loss = 0.00212583\n",
      "Iteration 27, loss = 0.00170696\n",
      "Iteration 28, loss = 0.00154531\n",
      "Iteration 29, loss = 0.00157437\n",
      "Iteration 30, loss = 0.00166967\n",
      "Iteration 31, loss = 0.00172483\n",
      "Iteration 32, loss = 0.00168520\n",
      "Iteration 33, loss = 0.00155795\n",
      "Iteration 34, loss = 0.00139712\n",
      "Iteration 35, loss = 0.00126422\n",
      "Iteration 36, loss = 0.00120211\n",
      "Iteration 37, loss = 0.00121728\n",
      "Iteration 38, loss = 0.00128264\n",
      "Iteration 39, loss = 0.00135170\n",
      "Iteration 40, loss = 0.00138160\n",
      "Iteration 41, loss = 0.00135202\n",
      "Iteration 42, loss = 0.00127110\n",
      "Iteration 43, loss = 0.00116743\n",
      "Iteration 44, loss = 0.00107377\n",
      "Iteration 45, loss = 0.00101149\n",
      "Iteration 46, loss = 0.00098367\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06728201\n",
      "Iteration 2, loss = 0.03422582\n",
      "Iteration 3, loss = 0.01814911\n",
      "Iteration 4, loss = 0.01622405\n",
      "Iteration 5, loss = 0.02034731\n",
      "Iteration 6, loss = 0.02215774\n",
      "Iteration 7, loss = 0.01947371\n",
      "Iteration 8, loss = 0.01414860\n",
      "Iteration 9, loss = 0.00872331\n",
      "Iteration 10, loss = 0.00483354\n",
      "Iteration 11, loss = 0.00311049\n",
      "Iteration 12, loss = 0.00320933\n",
      "Iteration 13, loss = 0.00421058\n",
      "Iteration 14, loss = 0.00516029\n",
      "Iteration 15, loss = 0.00544086\n",
      "Iteration 16, loss = 0.00495513\n",
      "Iteration 17, loss = 0.00405170\n",
      "Iteration 18, loss = 0.00323155\n",
      "Iteration 19, loss = 0.00290178\n",
      "Iteration 20, loss = 0.00311766\n",
      "Iteration 21, loss = 0.00360706\n",
      "Iteration 22, loss = 0.00395279\n",
      "Iteration 23, loss = 0.00389097\n",
      "Iteration 24, loss = 0.00342208\n",
      "Iteration 25, loss = 0.00274800\n",
      "Iteration 26, loss = 0.00211969\n",
      "Iteration 27, loss = 0.00170653\n",
      "Iteration 28, loss = 0.00155024\n",
      "Iteration 29, loss = 0.00158195\n",
      "Iteration 30, loss = 0.00167723\n",
      "Iteration 31, loss = 0.00172951\n",
      "Iteration 32, loss = 0.00168643\n",
      "Iteration 33, loss = 0.00155536\n",
      "Iteration 34, loss = 0.00139283\n",
      "Iteration 35, loss = 0.00126143\n",
      "Iteration 36, loss = 0.00120247\n",
      "Iteration 37, loss = 0.00122106\n",
      "Iteration 38, loss = 0.00128831\n",
      "Iteration 39, loss = 0.00135694\n",
      "Iteration 40, loss = 0.00138444\n",
      "Iteration 41, loss = 0.00135184\n",
      "Iteration 42, loss = 0.00126867\n",
      "Iteration 43, loss = 0.00116466\n",
      "Iteration 44, loss = 0.00107259\n",
      "Iteration 45, loss = 0.00101277\n",
      "Iteration 46, loss = 0.00098676\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06711434\n",
      "Iteration 2, loss = 0.03413371\n",
      "Iteration 3, loss = 0.01811421\n",
      "Iteration 4, loss = 0.01620409\n",
      "Iteration 5, loss = 0.02030845\n",
      "Iteration 6, loss = 0.02209291\n",
      "Iteration 7, loss = 0.01940345\n",
      "Iteration 8, loss = 0.01409439\n",
      "Iteration 9, loss = 0.00869156\n",
      "Iteration 10, loss = 0.00482093\n",
      "Iteration 11, loss = 0.00310644\n",
      "Iteration 12, loss = 0.00320495\n",
      "Iteration 13, loss = 0.00420212\n",
      "Iteration 14, loss = 0.00514435\n",
      "Iteration 15, loss = 0.00541912\n",
      "Iteration 16, loss = 0.00493467\n",
      "Iteration 17, loss = 0.00403784\n",
      "Iteration 18, loss = 0.00322633\n",
      "Iteration 19, loss = 0.00290137\n",
      "Iteration 20, loss = 0.00311878\n",
      "Iteration 21, loss = 0.00360336\n",
      "Iteration 22, loss = 0.00394311\n",
      "Iteration 23, loss = 0.00387583\n",
      "Iteration 24, loss = 0.00340542\n",
      "Iteration 25, loss = 0.00273360\n",
      "Iteration 26, loss = 0.00211012\n",
      "Iteration 27, loss = 0.00170229\n",
      "Iteration 28, loss = 0.00154968\n",
      "Iteration 29, loss = 0.00158153\n",
      "Iteration 30, loss = 0.00167475\n",
      "Iteration 31, loss = 0.00172377\n",
      "Iteration 32, loss = 0.00167764\n",
      "Iteration 33, loss = 0.00154505\n",
      "Iteration 34, loss = 0.00138284\n",
      "Iteration 35, loss = 0.00125356\n",
      "Iteration 36, loss = 0.00119730\n",
      "Iteration 37, loss = 0.00121822\n",
      "Iteration 38, loss = 0.00128639\n",
      "Iteration 39, loss = 0.00135447\n",
      "Iteration 40, loss = 0.00138041\n",
      "Iteration 41, loss = 0.00134646\n",
      "Iteration 42, loss = 0.00126305\n",
      "Iteration 43, loss = 0.00115979\n",
      "Iteration 44, loss = 0.00106892\n",
      "Iteration 45, loss = 0.00101054\n",
      "Iteration 46, loss = 0.00098558\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06710125\n",
      "Iteration 2, loss = 0.03416623\n",
      "Iteration 3, loss = 0.01815267\n",
      "Iteration 4, loss = 0.01621023\n",
      "Iteration 5, loss = 0.02026602\n",
      "Iteration 6, loss = 0.02204115\n",
      "Iteration 7, loss = 0.01937178\n",
      "Iteration 8, loss = 0.01408838\n",
      "Iteration 9, loss = 0.00870039\n",
      "Iteration 10, loss = 0.00482864\n",
      "Iteration 11, loss = 0.00310084\n",
      "Iteration 12, loss = 0.00317901\n",
      "Iteration 13, loss = 0.00416363\n",
      "Iteration 14, loss = 0.00511004\n",
      "Iteration 15, loss = 0.00539745\n",
      "Iteration 16, loss = 0.00493053\n",
      "Iteration 17, loss = 0.00404640\n",
      "Iteration 18, loss = 0.00323943\n",
      "Iteration 19, loss = 0.00291000\n",
      "Iteration 20, loss = 0.00311904\n",
      "Iteration 21, loss = 0.00359278\n",
      "Iteration 22, loss = 0.00392784\n",
      "Iteration 23, loss = 0.00385961\n",
      "Iteration 24, loss = 0.00339165\n",
      "Iteration 25, loss = 0.00272295\n",
      "Iteration 26, loss = 0.00209999\n",
      "Iteration 27, loss = 0.00169120\n",
      "Iteration 28, loss = 0.00153793\n",
      "Iteration 29, loss = 0.00156656\n",
      "Iteration 30, loss = 0.00165877\n",
      "Iteration 31, loss = 0.00170943\n",
      "Iteration 32, loss = 0.00166378\n",
      "Iteration 33, loss = 0.00153437\n",
      "Iteration 34, loss = 0.00137533\n",
      "Iteration 35, loss = 0.00124769\n",
      "Iteration 36, loss = 0.00119237\n",
      "Iteration 37, loss = 0.00121316\n",
      "Iteration 38, loss = 0.00128067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39, loss = 0.00134773\n",
      "Iteration 40, loss = 0.00137294\n",
      "Iteration 41, loss = 0.00133875\n",
      "Iteration 42, loss = 0.00125554\n",
      "Iteration 43, loss = 0.00115314\n",
      "Iteration 44, loss = 0.00106317\n",
      "Iteration 45, loss = 0.00100507\n",
      "Iteration 46, loss = 0.00097978\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06759561\n",
      "Iteration 2, loss = 0.03446095\n",
      "Iteration 3, loss = 0.01824353\n",
      "Iteration 4, loss = 0.01617401\n",
      "Iteration 5, loss = 0.02027212\n",
      "Iteration 6, loss = 0.02215751\n",
      "Iteration 7, loss = 0.01956473\n",
      "Iteration 8, loss = 0.01428457\n",
      "Iteration 9, loss = 0.00885062\n",
      "Iteration 10, loss = 0.00491561\n",
      "Iteration 11, loss = 0.00313007\n",
      "Iteration 12, loss = 0.00317659\n",
      "Iteration 13, loss = 0.00415326\n",
      "Iteration 14, loss = 0.00511328\n",
      "Iteration 15, loss = 0.00542540\n",
      "Iteration 16, loss = 0.00497033\n",
      "Iteration 17, loss = 0.00407648\n",
      "Iteration 18, loss = 0.00324033\n",
      "Iteration 19, loss = 0.00287475\n",
      "Iteration 20, loss = 0.00306142\n",
      "Iteration 21, loss = 0.00353840\n",
      "Iteration 22, loss = 0.00390296\n",
      "Iteration 23, loss = 0.00387064\n",
      "Iteration 24, loss = 0.00342513\n",
      "Iteration 25, loss = 0.00275979\n",
      "Iteration 26, loss = 0.00212565\n",
      "Iteration 27, loss = 0.00170021\n",
      "Iteration 28, loss = 0.00153420\n",
      "Iteration 29, loss = 0.00156003\n",
      "Iteration 30, loss = 0.00165726\n",
      "Iteration 31, loss = 0.00171733\n",
      "Iteration 32, loss = 0.00168255\n",
      "Iteration 33, loss = 0.00155721\n",
      "Iteration 34, loss = 0.00139549\n",
      "Iteration 35, loss = 0.00125991\n",
      "Iteration 36, loss = 0.00119520\n",
      "Iteration 37, loss = 0.00120870\n",
      "Iteration 38, loss = 0.00127412\n",
      "Iteration 39, loss = 0.00134453\n",
      "Iteration 40, loss = 0.00137608\n",
      "Iteration 41, loss = 0.00134812\n",
      "Iteration 42, loss = 0.00126817\n",
      "Iteration 43, loss = 0.00116496\n",
      "Iteration 44, loss = 0.00107174\n",
      "Iteration 45, loss = 0.00101027\n",
      "Iteration 46, loss = 0.00098323\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06734900\n",
      "Iteration 2, loss = 0.03425694\n",
      "Iteration 3, loss = 0.01816214\n",
      "Iteration 4, loss = 0.01621049\n",
      "Iteration 5, loss = 0.02032748\n",
      "Iteration 6, loss = 0.02214612\n",
      "Iteration 7, loss = 0.01947216\n",
      "Iteration 8, loss = 0.01415330\n",
      "Iteration 9, loss = 0.00872395\n",
      "Iteration 10, loss = 0.00482278\n",
      "Iteration 11, loss = 0.00309026\n",
      "Iteration 12, loss = 0.00318312\n",
      "Iteration 13, loss = 0.00418469\n",
      "Iteration 14, loss = 0.00514012\n",
      "Iteration 15, loss = 0.00542413\n",
      "Iteration 16, loss = 0.00494230\n",
      "Iteration 17, loss = 0.00404174\n",
      "Iteration 18, loss = 0.00322133\n",
      "Iteration 19, loss = 0.00288965\n",
      "Iteration 20, loss = 0.00310432\n",
      "Iteration 21, loss = 0.00359344\n",
      "Iteration 22, loss = 0.00394055\n",
      "Iteration 23, loss = 0.00388140\n",
      "Iteration 24, loss = 0.00341434\n",
      "Iteration 25, loss = 0.00274113\n",
      "Iteration 26, loss = 0.00211196\n",
      "Iteration 27, loss = 0.00169700\n",
      "Iteration 28, loss = 0.00153891\n",
      "Iteration 29, loss = 0.00156926\n",
      "Iteration 30, loss = 0.00166431\n",
      "Iteration 31, loss = 0.00171771\n",
      "Iteration 32, loss = 0.00167541\n",
      "Iteration 33, loss = 0.00154676\n",
      "Iteration 34, loss = 0.00138616\n",
      "Iteration 35, loss = 0.00125516\n",
      "Iteration 36, loss = 0.00119566\n",
      "Iteration 37, loss = 0.00121327\n",
      "Iteration 38, loss = 0.00128005\n",
      "Iteration 39, loss = 0.00134899\n",
      "Iteration 40, loss = 0.00137753\n",
      "Iteration 41, loss = 0.00134622\n",
      "Iteration 42, loss = 0.00126418\n",
      "Iteration 43, loss = 0.00116069\n",
      "Iteration 44, loss = 0.00106859\n",
      "Iteration 45, loss = 0.00100829\n",
      "Iteration 46, loss = 0.00098199\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06709216\n",
      "Iteration 2, loss = 0.03408765\n",
      "Iteration 3, loss = 0.01805670\n",
      "Iteration 4, loss = 0.01618728\n",
      "Iteration 5, loss = 0.02032256\n",
      "Iteration 6, loss = 0.02210655\n",
      "Iteration 7, loss = 0.01940021\n",
      "Iteration 8, loss = 0.01408092\n",
      "Iteration 9, loss = 0.00867655\n",
      "Iteration 10, loss = 0.00481356\n",
      "Iteration 11, loss = 0.00310919\n",
      "Iteration 12, loss = 0.00321083\n",
      "Iteration 13, loss = 0.00420261\n",
      "Iteration 14, loss = 0.00513969\n",
      "Iteration 15, loss = 0.00541083\n",
      "Iteration 16, loss = 0.00492181\n",
      "Iteration 17, loss = 0.00402207\n",
      "Iteration 18, loss = 0.00321253\n",
      "Iteration 19, loss = 0.00289029\n",
      "Iteration 20, loss = 0.00311127\n",
      "Iteration 21, loss = 0.00359681\n",
      "Iteration 22, loss = 0.00393824\n",
      "Iteration 23, loss = 0.00387259\n",
      "Iteration 24, loss = 0.00340279\n",
      "Iteration 25, loss = 0.00273214\n",
      "Iteration 26, loss = 0.00211007\n",
      "Iteration 27, loss = 0.00170372\n",
      "Iteration 28, loss = 0.00155267\n",
      "Iteration 29, loss = 0.00158659\n",
      "Iteration 30, loss = 0.00168083\n",
      "Iteration 31, loss = 0.00172841\n",
      "Iteration 32, loss = 0.00168098\n",
      "Iteration 33, loss = 0.00154828\n",
      "Iteration 34, loss = 0.00138492\n",
      "Iteration 35, loss = 0.00125445\n",
      "Iteration 36, loss = 0.00119714\n",
      "Iteration 37, loss = 0.00121740\n",
      "Iteration 38, loss = 0.00128542\n",
      "Iteration 39, loss = 0.00135376\n",
      "Iteration 40, loss = 0.00138016\n",
      "Iteration 41, loss = 0.00134671\n",
      "Iteration 42, loss = 0.00126365\n",
      "Iteration 43, loss = 0.00116070\n",
      "Iteration 44, loss = 0.00107019\n",
      "Iteration 45, loss = 0.00101214\n",
      "Iteration 46, loss = 0.00098744\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06687830\n",
      "Iteration 2, loss = 0.03393704\n",
      "Iteration 3, loss = 0.01796836\n",
      "Iteration 4, loss = 0.01615018\n",
      "Iteration 5, loss = 0.02030157\n",
      "Iteration 6, loss = 0.02206958\n",
      "Iteration 7, loss = 0.01934940\n",
      "Iteration 8, loss = 0.01403064\n",
      "Iteration 9, loss = 0.00864196\n",
      "Iteration 10, loss = 0.00480144\n",
      "Iteration 11, loss = 0.00311753\n",
      "Iteration 12, loss = 0.00323261\n",
      "Iteration 13, loss = 0.00422521\n",
      "Iteration 14, loss = 0.00515171\n",
      "Iteration 15, loss = 0.00540899\n",
      "Iteration 16, loss = 0.00490699\n",
      "Iteration 17, loss = 0.00400146\n",
      "Iteration 18, loss = 0.00319690\n",
      "Iteration 19, loss = 0.00288503\n",
      "Iteration 20, loss = 0.00311634\n",
      "Iteration 21, loss = 0.00360603\n",
      "Iteration 22, loss = 0.00394493\n",
      "Iteration 23, loss = 0.00387270\n",
      "Iteration 24, loss = 0.00339735\n",
      "Iteration 25, loss = 0.00272540\n",
      "Iteration 26, loss = 0.00210650\n",
      "Iteration 27, loss = 0.00170549\n",
      "Iteration 28, loss = 0.00155944\n",
      "Iteration 29, loss = 0.00159635\n",
      "Iteration 30, loss = 0.00169010\n",
      "Iteration 31, loss = 0.00173518\n",
      "Iteration 32, loss = 0.00168427\n",
      "Iteration 33, loss = 0.00154976\n",
      "Iteration 34, loss = 0.00138516\n",
      "Iteration 35, loss = 0.00125442\n",
      "Iteration 36, loss = 0.00119818\n",
      "Iteration 37, loss = 0.00121917\n",
      "Iteration 38, loss = 0.00128742\n",
      "Iteration 39, loss = 0.00135499\n",
      "Iteration 40, loss = 0.00138038\n",
      "Iteration 41, loss = 0.00134620\n",
      "Iteration 42, loss = 0.00126299\n",
      "Iteration 43, loss = 0.00116041\n",
      "Iteration 44, loss = 0.00107045\n",
      "Iteration 45, loss = 0.00101293\n",
      "Iteration 46, loss = 0.00098856\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06699379\n",
      "Iteration 2, loss = 0.03402584\n",
      "Iteration 3, loss = 0.01803020\n",
      "Iteration 4, loss = 0.01618665\n",
      "Iteration 5, loss = 0.02032591\n",
      "Iteration 6, loss = 0.02210451\n",
      "Iteration 7, loss = 0.01939862\n",
      "Iteration 8, loss = 0.01408167\n",
      "Iteration 9, loss = 0.00868530\n",
      "Iteration 10, loss = 0.00482954\n",
      "Iteration 11, loss = 0.00312825\n",
      "Iteration 12, loss = 0.00322994\n",
      "Iteration 13, loss = 0.00421506\n",
      "Iteration 14, loss = 0.00514010\n",
      "Iteration 15, loss = 0.00540080\n",
      "Iteration 16, loss = 0.00490202\n",
      "Iteration 17, loss = 0.00399844\n",
      "Iteration 18, loss = 0.00319391\n",
      "Iteration 19, loss = 0.00288323\n",
      "Iteration 20, loss = 0.00311754\n",
      "Iteration 21, loss = 0.00361096\n",
      "Iteration 22, loss = 0.00395251\n",
      "Iteration 23, loss = 0.00387916\n",
      "Iteration 24, loss = 0.00340003\n",
      "Iteration 25, loss = 0.00272455\n",
      "Iteration 26, loss = 0.00210469\n",
      "Iteration 27, loss = 0.00170604\n",
      "Iteration 28, loss = 0.00156256\n",
      "Iteration 29, loss = 0.00160038\n",
      "Iteration 30, loss = 0.00169387\n",
      "Iteration 31, loss = 0.00173663\n",
      "Iteration 32, loss = 0.00168123\n",
      "Iteration 33, loss = 0.00154252\n",
      "Iteration 34, loss = 0.00137621\n",
      "Iteration 35, loss = 0.00124623\n",
      "Iteration 36, loss = 0.00119245\n",
      "Iteration 37, loss = 0.00121751\n",
      "Iteration 38, loss = 0.00128896\n",
      "Iteration 39, loss = 0.00135768\n",
      "Iteration 40, loss = 0.00138160\n",
      "Iteration 41, loss = 0.00134444\n",
      "Iteration 42, loss = 0.00125861\n",
      "Iteration 43, loss = 0.00115496\n",
      "Iteration 44, loss = 0.00106615\n",
      "Iteration 45, loss = 0.00101124\n",
      "Iteration 46, loss = 0.00098951\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06670475\n",
      "Iteration 2, loss = 0.03383002\n",
      "Iteration 3, loss = 0.01792554\n",
      "Iteration 4, loss = 0.01612483\n",
      "Iteration 5, loss = 0.02025363\n",
      "Iteration 6, loss = 0.02200244\n",
      "Iteration 7, loss = 0.01927817\n",
      "Iteration 8, loss = 0.01397355\n",
      "Iteration 9, loss = 0.00860309\n",
      "Iteration 10, loss = 0.00477799\n",
      "Iteration 11, loss = 0.00310502\n",
      "Iteration 12, loss = 0.00322256\n",
      "Iteration 13, loss = 0.00421496\n",
      "Iteration 14, loss = 0.00514131\n",
      "Iteration 15, loss = 0.00539758\n",
      "Iteration 16, loss = 0.00489705\n",
      "Iteration 17, loss = 0.00399603\n",
      "Iteration 18, loss = 0.00319581\n",
      "Iteration 19, loss = 0.00288654\n",
      "Iteration 20, loss = 0.00311588\n",
      "Iteration 21, loss = 0.00360076\n",
      "Iteration 22, loss = 0.00393325\n",
      "Iteration 23, loss = 0.00385785\n",
      "Iteration 24, loss = 0.00338294\n",
      "Iteration 25, loss = 0.00271393\n",
      "Iteration 26, loss = 0.00209771\n",
      "Iteration 27, loss = 0.00169795\n",
      "Iteration 28, loss = 0.00155264\n",
      "Iteration 29, loss = 0.00158879\n",
      "Iteration 30, loss = 0.00168201\n",
      "Iteration 31, loss = 0.00172813\n",
      "Iteration 32, loss = 0.00167921\n",
      "Iteration 33, loss = 0.00154658\n",
      "Iteration 34, loss = 0.00138404\n",
      "Iteration 35, loss = 0.00125453\n",
      "Iteration 36, loss = 0.00119793\n",
      "Iteration 37, loss = 0.00121774\n",
      "Iteration 38, loss = 0.00128445\n",
      "Iteration 39, loss = 0.00135141\n",
      "Iteration 40, loss = 0.00137729\n",
      "Iteration 41, loss = 0.00134399\n",
      "Iteration 42, loss = 0.00126141\n",
      "Iteration 43, loss = 0.00115877\n",
      "Iteration 44, loss = 0.00106794\n",
      "Iteration 45, loss = 0.00100916\n",
      "Iteration 46, loss = 0.00098376\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06690116\n",
      "Iteration 2, loss = 0.03407357\n",
      "Iteration 3, loss = 0.01812354\n",
      "Iteration 4, loss = 0.01620191\n",
      "Iteration 5, loss = 0.02024748\n",
      "Iteration 6, loss = 0.02199949\n",
      "Iteration 7, loss = 0.01931254\n",
      "Iteration 8, loss = 0.01402874\n",
      "Iteration 9, loss = 0.00865377\n",
      "Iteration 10, loss = 0.00480290\n",
      "Iteration 11, loss = 0.00309837\n",
      "Iteration 12, loss = 0.00319324\n",
      "Iteration 13, loss = 0.00418477\n",
      "Iteration 14, loss = 0.00512551\n",
      "Iteration 15, loss = 0.00540479\n",
      "Iteration 16, loss = 0.00492864\n",
      "Iteration 17, loss = 0.00404093\n",
      "Iteration 18, loss = 0.00323685\n",
      "Iteration 19, loss = 0.00291376\n",
      "Iteration 20, loss = 0.00312818\n",
      "Iteration 21, loss = 0.00360260\n",
      "Iteration 22, loss = 0.00393347\n",
      "Iteration 23, loss = 0.00385869\n",
      "Iteration 24, loss = 0.00338566\n",
      "Iteration 25, loss = 0.00271580\n",
      "Iteration 26, loss = 0.00209506\n",
      "Iteration 27, loss = 0.00168965\n",
      "Iteration 28, loss = 0.00153890\n",
      "Iteration 29, loss = 0.00157017\n",
      "Iteration 30, loss = 0.00166309\n",
      "Iteration 31, loss = 0.00171299\n",
      "Iteration 32, loss = 0.00166693\n",
      "Iteration 33, loss = 0.00153661\n",
      "Iteration 34, loss = 0.00137699\n",
      "Iteration 35, loss = 0.00124984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36, loss = 0.00119477\n",
      "Iteration 37, loss = 0.00121565\n",
      "Iteration 38, loss = 0.00128296\n",
      "Iteration 39, loss = 0.00134958\n",
      "Iteration 40, loss = 0.00137410\n",
      "Iteration 41, loss = 0.00133916\n",
      "Iteration 42, loss = 0.00125541\n",
      "Iteration 43, loss = 0.00115236\n",
      "Iteration 44, loss = 0.00106220\n",
      "Iteration 45, loss = 0.00100458\n",
      "Iteration 46, loss = 0.00097984\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06717157\n",
      "Iteration 2, loss = 0.03423050\n",
      "Iteration 3, loss = 0.01818637\n",
      "Iteration 4, loss = 0.01621800\n",
      "Iteration 5, loss = 0.02028007\n",
      "Iteration 6, loss = 0.02206047\n",
      "Iteration 7, loss = 0.01938881\n",
      "Iteration 8, loss = 0.01409779\n",
      "Iteration 9, loss = 0.00870209\n",
      "Iteration 10, loss = 0.00482630\n",
      "Iteration 11, loss = 0.00310067\n",
      "Iteration 12, loss = 0.00318739\n",
      "Iteration 13, loss = 0.00418127\n",
      "Iteration 14, loss = 0.00513310\n",
      "Iteration 15, loss = 0.00541888\n",
      "Iteration 16, loss = 0.00494533\n",
      "Iteration 17, loss = 0.00405232\n",
      "Iteration 18, loss = 0.00323560\n",
      "Iteration 19, loss = 0.00290105\n",
      "Iteration 20, loss = 0.00310796\n",
      "Iteration 21, loss = 0.00358551\n",
      "Iteration 22, loss = 0.00392365\n",
      "Iteration 23, loss = 0.00385983\n",
      "Iteration 24, loss = 0.00339328\n",
      "Iteration 25, loss = 0.00272452\n",
      "Iteration 26, loss = 0.00210024\n",
      "Iteration 27, loss = 0.00168964\n",
      "Iteration 28, loss = 0.00153553\n",
      "Iteration 29, loss = 0.00156607\n",
      "Iteration 30, loss = 0.00166112\n",
      "Iteration 31, loss = 0.00171500\n",
      "Iteration 32, loss = 0.00167170\n",
      "Iteration 33, loss = 0.00154364\n",
      "Iteration 34, loss = 0.00138411\n",
      "Iteration 35, loss = 0.00125428\n",
      "Iteration 36, loss = 0.00119571\n",
      "Iteration 37, loss = 0.00121450\n",
      "Iteration 38, loss = 0.00128156\n",
      "Iteration 39, loss = 0.00134933\n",
      "Iteration 40, loss = 0.00137548\n",
      "Iteration 41, loss = 0.00134165\n",
      "Iteration 42, loss = 0.00125785\n",
      "Iteration 43, loss = 0.00115388\n",
      "Iteration 44, loss = 0.00106253\n",
      "Iteration 45, loss = 0.00100374\n",
      "Iteration 46, loss = 0.00097875\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06740792\n",
      "Iteration 2, loss = 0.03431990\n",
      "Iteration 3, loss = 0.01819912\n",
      "Iteration 4, loss = 0.01623533\n",
      "Iteration 5, loss = 0.02035169\n",
      "Iteration 6, loss = 0.02218362\n",
      "Iteration 7, loss = 0.01952153\n",
      "Iteration 8, loss = 0.01420171\n",
      "Iteration 9, loss = 0.00876888\n",
      "Iteration 10, loss = 0.00486288\n",
      "Iteration 11, loss = 0.00312174\n",
      "Iteration 12, loss = 0.00320543\n",
      "Iteration 13, loss = 0.00419842\n",
      "Iteration 14, loss = 0.00514767\n",
      "Iteration 15, loss = 0.00543234\n",
      "Iteration 16, loss = 0.00495059\n",
      "Iteration 17, loss = 0.00404758\n",
      "Iteration 18, loss = 0.00322646\n",
      "Iteration 19, loss = 0.00289537\n",
      "Iteration 20, loss = 0.00311230\n",
      "Iteration 21, loss = 0.00360484\n",
      "Iteration 22, loss = 0.00395355\n",
      "Iteration 23, loss = 0.00389063\n",
      "Iteration 24, loss = 0.00341841\n",
      "Iteration 25, loss = 0.00274117\n",
      "Iteration 26, loss = 0.00211227\n",
      "Iteration 27, loss = 0.00170246\n",
      "Iteration 28, loss = 0.00155075\n",
      "Iteration 29, loss = 0.00158458\n",
      "Iteration 30, loss = 0.00167947\n",
      "Iteration 31, loss = 0.00172772\n",
      "Iteration 32, loss = 0.00167960\n",
      "Iteration 33, loss = 0.00154606\n",
      "Iteration 34, loss = 0.00138154\n",
      "Iteration 35, loss = 0.00125022\n",
      "Iteration 36, loss = 0.00119420\n",
      "Iteration 37, loss = 0.00121684\n",
      "Iteration 38, loss = 0.00128690\n",
      "Iteration 39, loss = 0.00135586\n",
      "Iteration 40, loss = 0.00138152\n",
      "Iteration 41, loss = 0.00134661\n",
      "Iteration 42, loss = 0.00126240\n",
      "Iteration 43, loss = 0.00115901\n",
      "Iteration 44, loss = 0.00106917\n",
      "Iteration 45, loss = 0.00101234\n",
      "Iteration 46, loss = 0.00098862\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06717969\n",
      "Iteration 2, loss = 0.03413797\n",
      "Iteration 3, loss = 0.01808108\n",
      "Iteration 4, loss = 0.01619689\n",
      "Iteration 5, loss = 0.02032840\n",
      "Iteration 6, loss = 0.02211849\n",
      "Iteration 7, loss = 0.01941631\n",
      "Iteration 8, loss = 0.01409454\n",
      "Iteration 9, loss = 0.00868390\n",
      "Iteration 10, loss = 0.00481445\n",
      "Iteration 11, loss = 0.00310500\n",
      "Iteration 12, loss = 0.00320514\n",
      "Iteration 13, loss = 0.00420030\n",
      "Iteration 14, loss = 0.00514385\n",
      "Iteration 15, loss = 0.00541819\n",
      "Iteration 16, loss = 0.00492860\n",
      "Iteration 17, loss = 0.00402697\n",
      "Iteration 18, loss = 0.00321326\n",
      "Iteration 19, loss = 0.00288879\n",
      "Iteration 20, loss = 0.00310974\n",
      "Iteration 21, loss = 0.00359710\n",
      "Iteration 22, loss = 0.00393987\n",
      "Iteration 23, loss = 0.00387461\n",
      "Iteration 24, loss = 0.00340462\n",
      "Iteration 25, loss = 0.00273366\n",
      "Iteration 26, loss = 0.00211063\n",
      "Iteration 27, loss = 0.00170227\n",
      "Iteration 28, loss = 0.00154984\n",
      "Iteration 29, loss = 0.00158351\n",
      "Iteration 30, loss = 0.00167766\n",
      "Iteration 31, loss = 0.00172665\n",
      "Iteration 32, loss = 0.00168124\n",
      "Iteration 33, loss = 0.00155070\n",
      "Iteration 34, loss = 0.00138870\n",
      "Iteration 35, loss = 0.00125792\n",
      "Iteration 36, loss = 0.00119919\n",
      "Iteration 37, loss = 0.00121762\n",
      "Iteration 38, loss = 0.00128403\n",
      "Iteration 39, loss = 0.00135216\n",
      "Iteration 40, loss = 0.00137950\n",
      "Iteration 41, loss = 0.00134761\n",
      "Iteration 42, loss = 0.00126577\n",
      "Iteration 43, loss = 0.00116288\n",
      "Iteration 44, loss = 0.00107137\n",
      "Iteration 45, loss = 0.00101164\n",
      "Iteration 46, loss = 0.00098549\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06687090\n",
      "Iteration 2, loss = 0.03391357\n",
      "Iteration 3, loss = 0.01794662\n",
      "Iteration 4, loss = 0.01612925\n",
      "Iteration 5, loss = 0.02029174\n",
      "Iteration 6, loss = 0.02206049\n",
      "Iteration 7, loss = 0.01933566\n",
      "Iteration 8, loss = 0.01401374\n",
      "Iteration 9, loss = 0.00862724\n",
      "Iteration 10, loss = 0.00479237\n",
      "Iteration 11, loss = 0.00311513\n",
      "Iteration 12, loss = 0.00323391\n",
      "Iteration 13, loss = 0.00422725\n",
      "Iteration 14, loss = 0.00515173\n",
      "Iteration 15, loss = 0.00540517\n",
      "Iteration 16, loss = 0.00490040\n",
      "Iteration 17, loss = 0.00399425\n",
      "Iteration 18, loss = 0.00319021\n",
      "Iteration 19, loss = 0.00287971\n",
      "Iteration 20, loss = 0.00311252\n",
      "Iteration 21, loss = 0.00360286\n",
      "Iteration 22, loss = 0.00394076\n",
      "Iteration 23, loss = 0.00386758\n",
      "Iteration 24, loss = 0.00339187\n",
      "Iteration 25, loss = 0.00272016\n",
      "Iteration 26, loss = 0.00210191\n",
      "Iteration 27, loss = 0.00170134\n",
      "Iteration 28, loss = 0.00155608\n",
      "Iteration 29, loss = 0.00159323\n",
      "Iteration 30, loss = 0.00168698\n",
      "Iteration 31, loss = 0.00173279\n",
      "Iteration 32, loss = 0.00168266\n",
      "Iteration 33, loss = 0.00154812\n",
      "Iteration 34, loss = 0.00138395\n",
      "Iteration 35, loss = 0.00125359\n",
      "Iteration 36, loss = 0.00119744\n",
      "Iteration 37, loss = 0.00121828\n",
      "Iteration 38, loss = 0.00128640\n",
      "Iteration 39, loss = 0.00135418\n",
      "Iteration 40, loss = 0.00138002\n",
      "Iteration 41, loss = 0.00134611\n",
      "Iteration 42, loss = 0.00126290\n",
      "Iteration 43, loss = 0.00116003\n",
      "Iteration 44, loss = 0.00106965\n",
      "Iteration 45, loss = 0.00101169\n",
      "Iteration 46, loss = 0.00098704\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06646367\n",
      "Iteration 2, loss = 0.03362536\n",
      "Iteration 3, loss = 0.01777331\n",
      "Iteration 4, loss = 0.01603964\n",
      "Iteration 5, loss = 0.02023370\n",
      "Iteration 6, loss = 0.02199203\n",
      "Iteration 7, loss = 0.01924548\n",
      "Iteration 8, loss = 0.01392067\n",
      "Iteration 9, loss = 0.00855297\n",
      "Iteration 10, loss = 0.00475314\n",
      "Iteration 11, loss = 0.00311389\n",
      "Iteration 12, loss = 0.00325917\n",
      "Iteration 13, loss = 0.00425747\n",
      "Iteration 14, loss = 0.00516766\n",
      "Iteration 15, loss = 0.00539777\n",
      "Iteration 16, loss = 0.00487200\n",
      "Iteration 17, loss = 0.00395619\n",
      "Iteration 18, loss = 0.00315712\n",
      "Iteration 19, loss = 0.00286170\n",
      "Iteration 20, loss = 0.00311109\n",
      "Iteration 21, loss = 0.00360903\n",
      "Iteration 22, loss = 0.00394273\n",
      "Iteration 23, loss = 0.00385791\n",
      "Iteration 24, loss = 0.00337186\n",
      "Iteration 25, loss = 0.00269682\n",
      "Iteration 26, loss = 0.00208374\n",
      "Iteration 27, loss = 0.00169304\n",
      "Iteration 28, loss = 0.00155734\n",
      "Iteration 29, loss = 0.00159984\n",
      "Iteration 30, loss = 0.00169290\n",
      "Iteration 31, loss = 0.00173426\n",
      "Iteration 32, loss = 0.00167792\n",
      "Iteration 33, loss = 0.00153843\n",
      "Iteration 34, loss = 0.00137242\n",
      "Iteration 35, loss = 0.00124366\n",
      "Iteration 36, loss = 0.00119129\n",
      "Iteration 37, loss = 0.00121595\n",
      "Iteration 38, loss = 0.00128608\n",
      "Iteration 39, loss = 0.00135317\n",
      "Iteration 40, loss = 0.00137643\n",
      "Iteration 41, loss = 0.00133935\n",
      "Iteration 42, loss = 0.00125413\n",
      "Iteration 43, loss = 0.00115121\n",
      "Iteration 44, loss = 0.00106253\n",
      "Iteration 45, loss = 0.00100715\n",
      "Iteration 46, loss = 0.00098459\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06623580\n",
      "Iteration 2, loss = 0.03349633\n",
      "Iteration 3, loss = 0.01772361\n",
      "Iteration 4, loss = 0.01602799\n",
      "Iteration 5, loss = 0.02022562\n",
      "Iteration 6, loss = 0.02197315\n",
      "Iteration 7, loss = 0.01921819\n",
      "Iteration 8, loss = 0.01389564\n",
      "Iteration 9, loss = 0.00854045\n",
      "Iteration 10, loss = 0.00475714\n",
      "Iteration 11, loss = 0.00313062\n",
      "Iteration 12, loss = 0.00327704\n",
      "Iteration 13, loss = 0.00426481\n",
      "Iteration 14, loss = 0.00516136\n",
      "Iteration 15, loss = 0.00538160\n",
      "Iteration 16, loss = 0.00484894\n",
      "Iteration 17, loss = 0.00393612\n",
      "Iteration 18, loss = 0.00314707\n",
      "Iteration 19, loss = 0.00286541\n",
      "Iteration 20, loss = 0.00312848\n",
      "Iteration 21, loss = 0.00362951\n",
      "Iteration 22, loss = 0.00395583\n",
      "Iteration 23, loss = 0.00385928\n",
      "Iteration 24, loss = 0.00336536\n",
      "Iteration 25, loss = 0.00268985\n",
      "Iteration 26, loss = 0.00208122\n",
      "Iteration 27, loss = 0.00169622\n",
      "Iteration 28, loss = 0.00156476\n",
      "Iteration 29, loss = 0.00160670\n",
      "Iteration 30, loss = 0.00169678\n",
      "Iteration 31, loss = 0.00173312\n",
      "Iteration 32, loss = 0.00167191\n",
      "Iteration 33, loss = 0.00153126\n",
      "Iteration 34, loss = 0.00136679\n",
      "Iteration 35, loss = 0.00124041\n",
      "Iteration 36, loss = 0.00118993\n",
      "Iteration 37, loss = 0.00121586\n",
      "Iteration 38, loss = 0.00128587\n",
      "Iteration 39, loss = 0.00135164\n",
      "Iteration 40, loss = 0.00137308\n",
      "Iteration 41, loss = 0.00133477\n",
      "Iteration 42, loss = 0.00124904\n",
      "Iteration 43, loss = 0.00114626\n",
      "Iteration 44, loss = 0.00105784\n",
      "Iteration 45, loss = 0.00100249\n",
      "Iteration 46, loss = 0.00097994\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06529779\n",
      "Iteration 2, loss = 0.03285941\n",
      "Iteration 3, loss = 0.01735026\n",
      "Iteration 4, loss = 0.01579542\n",
      "Iteration 5, loss = 0.01993863\n",
      "Iteration 6, loss = 0.02158455\n",
      "Iteration 7, loss = 0.01878450\n",
      "Iteration 8, loss = 0.01351460\n",
      "Iteration 9, loss = 0.00825462\n",
      "Iteration 10, loss = 0.00456960\n",
      "Iteration 11, loss = 0.00302434\n",
      "Iteration 12, loss = 0.00322093\n",
      "Iteration 13, loss = 0.00423306\n",
      "Iteration 14, loss = 0.00512217\n",
      "Iteration 15, loss = 0.00532285\n",
      "Iteration 16, loss = 0.00478613\n",
      "Iteration 17, loss = 0.00388424\n",
      "Iteration 18, loss = 0.00311129\n",
      "Iteration 19, loss = 0.00284573\n",
      "Iteration 20, loss = 0.00310455\n",
      "Iteration 21, loss = 0.00358919\n",
      "Iteration 22, loss = 0.00389000\n",
      "Iteration 23, loss = 0.00377705\n",
      "Iteration 24, loss = 0.00327932\n",
      "Iteration 25, loss = 0.00261129\n",
      "Iteration 26, loss = 0.00201456\n",
      "Iteration 27, loss = 0.00164171\n",
      "Iteration 28, loss = 0.00151960\n",
      "Iteration 29, loss = 0.00156492\n",
      "Iteration 30, loss = 0.00165632\n",
      "Iteration 31, loss = 0.00169573\n",
      "Iteration 32, loss = 0.00163716\n",
      "Iteration 33, loss = 0.00149939\n",
      "Iteration 34, loss = 0.00133983\n",
      "Iteration 35, loss = 0.00121818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36, loss = 0.00117171\n",
      "Iteration 37, loss = 0.00119888\n",
      "Iteration 38, loss = 0.00126748\n",
      "Iteration 39, loss = 0.00133060\n",
      "Iteration 40, loss = 0.00134960\n",
      "Iteration 41, loss = 0.00130955\n",
      "Iteration 42, loss = 0.00122337\n",
      "Iteration 43, loss = 0.00112144\n",
      "Iteration 44, loss = 0.00103497\n",
      "Iteration 45, loss = 0.00098175\n",
      "Iteration 46, loss = 0.00096025\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06715224\n",
      "Iteration 2, loss = 0.03419957\n",
      "Iteration 3, loss = 0.01817355\n",
      "Iteration 4, loss = 0.01622811\n",
      "Iteration 5, loss = 0.02029375\n",
      "Iteration 6, loss = 0.02207637\n",
      "Iteration 7, loss = 0.01940012\n",
      "Iteration 8, loss = 0.01409108\n",
      "Iteration 9, loss = 0.00866468\n",
      "Iteration 10, loss = 0.00476009\n",
      "Iteration 11, loss = 0.00300767\n",
      "Iteration 12, loss = 0.00307231\n",
      "Iteration 13, loss = 0.00404278\n",
      "Iteration 14, loss = 0.00497143\n",
      "Iteration 15, loss = 0.00524713\n",
      "Iteration 16, loss = 0.00476320\n",
      "Iteration 17, loss = 0.00387001\n",
      "Iteration 18, loss = 0.00307252\n",
      "Iteration 19, loss = 0.00276875\n",
      "Iteration 20, loss = 0.00301308\n",
      "Iteration 21, loss = 0.00351595\n",
      "Iteration 22, loss = 0.00386112\n",
      "Iteration 23, loss = 0.00378356\n",
      "Iteration 24, loss = 0.00329840\n",
      "Iteration 25, loss = 0.00262009\n",
      "Iteration 26, loss = 0.00200213\n",
      "Iteration 27, loss = 0.00161139\n",
      "Iteration 28, loss = 0.00147417\n",
      "Iteration 29, loss = 0.00151459\n",
      "Iteration 30, loss = 0.00160759\n",
      "Iteration 31, loss = 0.00164615\n",
      "Iteration 32, loss = 0.00158465\n",
      "Iteration 33, loss = 0.00144257\n",
      "Iteration 34, loss = 0.00127836\n",
      "Iteration 35, loss = 0.00115378\n",
      "Iteration 36, loss = 0.00110789\n",
      "Iteration 37, loss = 0.00114112\n",
      "Iteration 38, loss = 0.00121862\n",
      "Iteration 39, loss = 0.00129022\n",
      "Iteration 40, loss = 0.00131478\n",
      "Iteration 41, loss = 0.00127726\n",
      "Iteration 42, loss = 0.00119216\n",
      "Iteration 43, loss = 0.00109183\n",
      "Iteration 44, loss = 0.00100844\n",
      "Iteration 45, loss = 0.00095942\n",
      "Iteration 46, loss = 0.00094248\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06744003\n",
      "Iteration 2, loss = 0.03431084\n",
      "Iteration 3, loss = 0.01818573\n",
      "Iteration 4, loss = 0.01623431\n",
      "Iteration 5, loss = 0.02036767\n",
      "Iteration 6, loss = 0.02220398\n",
      "Iteration 7, loss = 0.01953949\n",
      "Iteration 8, loss = 0.01422044\n",
      "Iteration 9, loss = 0.00878748\n",
      "Iteration 10, loss = 0.00487828\n",
      "Iteration 11, loss = 0.00312741\n",
      "Iteration 12, loss = 0.00320074\n",
      "Iteration 13, loss = 0.00418442\n",
      "Iteration 14, loss = 0.00512905\n",
      "Iteration 15, loss = 0.00541541\n",
      "Iteration 16, loss = 0.00493474\n",
      "Iteration 17, loss = 0.00403453\n",
      "Iteration 18, loss = 0.00321626\n",
      "Iteration 19, loss = 0.00288426\n",
      "Iteration 20, loss = 0.00310232\n",
      "Iteration 21, loss = 0.00359327\n",
      "Iteration 22, loss = 0.00394506\n",
      "Iteration 23, loss = 0.00388676\n",
      "Iteration 24, loss = 0.00341806\n",
      "Iteration 25, loss = 0.00274383\n",
      "Iteration 26, loss = 0.00211692\n",
      "Iteration 27, loss = 0.00170827\n",
      "Iteration 28, loss = 0.00155560\n",
      "Iteration 29, loss = 0.00158929\n",
      "Iteration 30, loss = 0.00168396\n",
      "Iteration 31, loss = 0.00173147\n",
      "Iteration 32, loss = 0.00168135\n",
      "Iteration 33, loss = 0.00154642\n",
      "Iteration 34, loss = 0.00138081\n",
      "Iteration 35, loss = 0.00124812\n",
      "Iteration 36, loss = 0.00119009\n",
      "Iteration 37, loss = 0.00121147\n",
      "Iteration 38, loss = 0.00128142\n",
      "Iteration 39, loss = 0.00135159\n",
      "Iteration 40, loss = 0.00137879\n",
      "Iteration 41, loss = 0.00134488\n",
      "Iteration 42, loss = 0.00126084\n",
      "Iteration 43, loss = 0.00115705\n",
      "Iteration 44, loss = 0.00106666\n",
      "Iteration 45, loss = 0.00100967\n",
      "Iteration 46, loss = 0.00098620\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06628598\n",
      "Iteration 2, loss = 0.03342601\n",
      "Iteration 3, loss = 0.01756434\n",
      "Iteration 4, loss = 0.01588279\n",
      "Iteration 5, loss = 0.02008161\n",
      "Iteration 6, loss = 0.02181580\n",
      "Iteration 7, loss = 0.01905330\n",
      "Iteration 8, loss = 0.01373018\n",
      "Iteration 9, loss = 0.00837579\n",
      "Iteration 10, loss = 0.00459988\n",
      "Iteration 11, loss = 0.00298435\n",
      "Iteration 12, loss = 0.00314701\n",
      "Iteration 13, loss = 0.00415717\n",
      "Iteration 14, loss = 0.00507364\n",
      "Iteration 15, loss = 0.00530688\n",
      "Iteration 16, loss = 0.00478969\n",
      "Iteration 17, loss = 0.00388311\n",
      "Iteration 18, loss = 0.00308882\n",
      "Iteration 19, loss = 0.00279259\n",
      "Iteration 20, loss = 0.00303923\n",
      "Iteration 21, loss = 0.00353269\n",
      "Iteration 22, loss = 0.00386665\n",
      "Iteration 23, loss = 0.00378323\n",
      "Iteration 24, loss = 0.00329905\n",
      "Iteration 25, loss = 0.00262655\n",
      "Iteration 26, loss = 0.00201482\n",
      "Iteration 27, loss = 0.00162702\n",
      "Iteration 28, loss = 0.00149545\n",
      "Iteration 29, loss = 0.00153871\n",
      "Iteration 30, loss = 0.00163486\n",
      "Iteration 31, loss = 0.00168034\n",
      "Iteration 32, loss = 0.00162555\n",
      "Iteration 33, loss = 0.00148734\n",
      "Iteration 34, loss = 0.00132420\n",
      "Iteration 35, loss = 0.00119817\n",
      "Iteration 36, loss = 0.00114878\n",
      "Iteration 37, loss = 0.00117735\n",
      "Iteration 38, loss = 0.00125084\n",
      "Iteration 39, loss = 0.00131995\n",
      "Iteration 40, loss = 0.00134349\n",
      "Iteration 41, loss = 0.00130594\n",
      "Iteration 42, loss = 0.00122054\n",
      "Iteration 43, loss = 0.00111888\n",
      "Iteration 44, loss = 0.00103318\n",
      "Iteration 45, loss = 0.00098124\n",
      "Iteration 46, loss = 0.00096167\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06626848\n",
      "Iteration 2, loss = 0.03351794\n",
      "Iteration 3, loss = 0.01772851\n",
      "Iteration 4, loss = 0.01603542\n",
      "Iteration 5, loss = 0.02024442\n",
      "Iteration 6, loss = 0.02200146\n",
      "Iteration 7, loss = 0.01924874\n",
      "Iteration 8, loss = 0.01392344\n",
      "Iteration 9, loss = 0.00856332\n",
      "Iteration 10, loss = 0.00477435\n",
      "Iteration 11, loss = 0.00314183\n",
      "Iteration 12, loss = 0.00328246\n",
      "Iteration 13, loss = 0.00426590\n",
      "Iteration 14, loss = 0.00515844\n",
      "Iteration 15, loss = 0.00537463\n",
      "Iteration 16, loss = 0.00483817\n",
      "Iteration 17, loss = 0.00392146\n",
      "Iteration 18, loss = 0.00313079\n",
      "Iteration 19, loss = 0.00285011\n",
      "Iteration 20, loss = 0.00311729\n",
      "Iteration 21, loss = 0.00362299\n",
      "Iteration 22, loss = 0.00395242\n",
      "Iteration 23, loss = 0.00385644\n",
      "Iteration 24, loss = 0.00336141\n",
      "Iteration 25, loss = 0.00268528\n",
      "Iteration 26, loss = 0.00207740\n",
      "Iteration 27, loss = 0.00169421\n",
      "Iteration 28, loss = 0.00156420\n",
      "Iteration 29, loss = 0.00160707\n",
      "Iteration 30, loss = 0.00169663\n",
      "Iteration 31, loss = 0.00173095\n",
      "Iteration 32, loss = 0.00166703\n",
      "Iteration 33, loss = 0.00152392\n",
      "Iteration 34, loss = 0.00135805\n",
      "Iteration 35, loss = 0.00123166\n",
      "Iteration 36, loss = 0.00118207\n",
      "Iteration 37, loss = 0.00120947\n",
      "Iteration 38, loss = 0.00128071\n",
      "Iteration 39, loss = 0.00134701\n",
      "Iteration 40, loss = 0.00136825\n",
      "Iteration 41, loss = 0.00132932\n",
      "Iteration 42, loss = 0.00124317\n",
      "Iteration 43, loss = 0.00114063\n",
      "Iteration 44, loss = 0.00105317\n",
      "Iteration 45, loss = 0.00099925\n",
      "Iteration 46, loss = 0.00097789\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06606784\n",
      "Iteration 2, loss = 0.03336242\n",
      "Iteration 3, loss = 0.01761970\n",
      "Iteration 4, loss = 0.01596447\n",
      "Iteration 5, loss = 0.02019107\n",
      "Iteration 6, loss = 0.02194308\n",
      "Iteration 7, loss = 0.01918325\n",
      "Iteration 8, loss = 0.01386109\n",
      "Iteration 9, loss = 0.00851601\n",
      "Iteration 10, loss = 0.00474766\n",
      "Iteration 11, loss = 0.00313568\n",
      "Iteration 12, loss = 0.00329079\n",
      "Iteration 13, loss = 0.00427864\n",
      "Iteration 14, loss = 0.00516655\n",
      "Iteration 15, loss = 0.00537427\n",
      "Iteration 16, loss = 0.00483024\n",
      "Iteration 17, loss = 0.00390954\n",
      "Iteration 18, loss = 0.00312048\n",
      "Iteration 19, loss = 0.00284413\n",
      "Iteration 20, loss = 0.00311597\n",
      "Iteration 21, loss = 0.00362315\n",
      "Iteration 22, loss = 0.00395051\n",
      "Iteration 23, loss = 0.00384993\n",
      "Iteration 24, loss = 0.00335104\n",
      "Iteration 25, loss = 0.00267387\n",
      "Iteration 26, loss = 0.00206897\n",
      "Iteration 27, loss = 0.00169173\n",
      "Iteration 28, loss = 0.00156684\n",
      "Iteration 29, loss = 0.00161384\n",
      "Iteration 30, loss = 0.00170388\n",
      "Iteration 31, loss = 0.00173635\n",
      "Iteration 32, loss = 0.00166933\n",
      "Iteration 33, loss = 0.00152299\n",
      "Iteration 34, loss = 0.00135522\n",
      "Iteration 35, loss = 0.00122891\n",
      "Iteration 36, loss = 0.00118167\n",
      "Iteration 37, loss = 0.00121187\n",
      "Iteration 38, loss = 0.00128475\n",
      "Iteration 39, loss = 0.00135108\n",
      "Iteration 40, loss = 0.00137041\n",
      "Iteration 41, loss = 0.00132878\n",
      "Iteration 42, loss = 0.00124076\n",
      "Iteration 43, loss = 0.00113798\n",
      "Iteration 44, loss = 0.00105189\n",
      "Iteration 45, loss = 0.00100040\n",
      "Iteration 46, loss = 0.00098117\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06599981\n",
      "Iteration 2, loss = 0.03334016\n",
      "Iteration 3, loss = 0.01761925\n",
      "Iteration 4, loss = 0.01597359\n",
      "Iteration 5, loss = 0.02019718\n",
      "Iteration 6, loss = 0.02194403\n",
      "Iteration 7, loss = 0.01918086\n",
      "Iteration 8, loss = 0.01385915\n",
      "Iteration 9, loss = 0.00851684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.00475146\n",
      "Iteration 11, loss = 0.00314069\n",
      "Iteration 12, loss = 0.00329469\n",
      "Iteration 13, loss = 0.00427952\n",
      "Iteration 14, loss = 0.00516339\n",
      "Iteration 15, loss = 0.00536760\n",
      "Iteration 16, loss = 0.00482146\n",
      "Iteration 17, loss = 0.00390048\n",
      "Iteration 18, loss = 0.00311347\n",
      "Iteration 19, loss = 0.00284002\n",
      "Iteration 20, loss = 0.00311486\n",
      "Iteration 21, loss = 0.00362280\n",
      "Iteration 22, loss = 0.00394868\n",
      "Iteration 23, loss = 0.00384572\n",
      "Iteration 24, loss = 0.00334497\n",
      "Iteration 25, loss = 0.00266726\n",
      "Iteration 26, loss = 0.00206371\n",
      "Iteration 27, loss = 0.00168932\n",
      "Iteration 28, loss = 0.00156683\n",
      "Iteration 29, loss = 0.00161448\n",
      "Iteration 30, loss = 0.00170376\n",
      "Iteration 31, loss = 0.00173445\n",
      "Iteration 32, loss = 0.00166556\n",
      "Iteration 33, loss = 0.00151784\n",
      "Iteration 34, loss = 0.00134973\n",
      "Iteration 35, loss = 0.00122396\n",
      "Iteration 36, loss = 0.00117806\n",
      "Iteration 37, loss = 0.00120949\n",
      "Iteration 38, loss = 0.00128290\n",
      "Iteration 39, loss = 0.00134886\n",
      "Iteration 40, loss = 0.00136734\n",
      "Iteration 41, loss = 0.00132481\n",
      "Iteration 42, loss = 0.00123630\n",
      "Iteration 43, loss = 0.00113379\n",
      "Iteration 44, loss = 0.00104864\n",
      "Iteration 45, loss = 0.00099825\n",
      "Iteration 46, loss = 0.00097978\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06594397\n",
      "Iteration 2, loss = 0.03334971\n",
      "Iteration 3, loss = 0.01769528\n",
      "Iteration 4, loss = 0.01604402\n",
      "Iteration 5, loss = 0.02018327\n",
      "Iteration 6, loss = 0.02188277\n",
      "Iteration 7, loss = 0.01911924\n",
      "Iteration 8, loss = 0.01382360\n",
      "Iteration 9, loss = 0.00850068\n",
      "Iteration 10, loss = 0.00473959\n",
      "Iteration 11, loss = 0.00312008\n",
      "Iteration 12, loss = 0.00325975\n",
      "Iteration 13, loss = 0.00423924\n",
      "Iteration 14, loss = 0.00512934\n",
      "Iteration 15, loss = 0.00534893\n",
      "Iteration 16, loss = 0.00482304\n",
      "Iteration 17, loss = 0.00392063\n",
      "Iteration 18, loss = 0.00314442\n",
      "Iteration 19, loss = 0.00287254\n",
      "Iteration 20, loss = 0.00313593\n",
      "Iteration 21, loss = 0.00362812\n",
      "Iteration 22, loss = 0.00394180\n",
      "Iteration 23, loss = 0.00383365\n",
      "Iteration 24, loss = 0.00333345\n",
      "Iteration 25, loss = 0.00265850\n",
      "Iteration 26, loss = 0.00205720\n",
      "Iteration 27, loss = 0.00168284\n",
      "Iteration 28, loss = 0.00155892\n",
      "Iteration 29, loss = 0.00160529\n",
      "Iteration 30, loss = 0.00169442\n",
      "Iteration 31, loss = 0.00172539\n",
      "Iteration 32, loss = 0.00165913\n",
      "Iteration 33, loss = 0.00151565\n",
      "Iteration 34, loss = 0.00135170\n",
      "Iteration 35, loss = 0.00122982\n",
      "Iteration 36, loss = 0.00118607\n",
      "Iteration 37, loss = 0.00121770\n",
      "Iteration 38, loss = 0.00129011\n",
      "Iteration 39, loss = 0.00135420\n",
      "Iteration 40, loss = 0.00137088\n",
      "Iteration 41, loss = 0.00132745\n",
      "Iteration 42, loss = 0.00123907\n",
      "Iteration 43, loss = 0.00113719\n",
      "Iteration 44, loss = 0.00105257\n",
      "Iteration 45, loss = 0.00100205\n",
      "Iteration 46, loss = 0.00098273\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06704261\n",
      "Iteration 2, loss = 0.03409971\n",
      "Iteration 3, loss = 0.01810833\n",
      "Iteration 4, loss = 0.01622232\n",
      "Iteration 5, loss = 0.02032949\n",
      "Iteration 6, loss = 0.02210702\n",
      "Iteration 7, loss = 0.01941113\n",
      "Iteration 8, loss = 0.01409679\n",
      "Iteration 9, loss = 0.00868622\n",
      "Iteration 10, loss = 0.00481029\n",
      "Iteration 11, loss = 0.00309034\n",
      "Iteration 12, loss = 0.00318130\n",
      "Iteration 13, loss = 0.00416608\n",
      "Iteration 14, loss = 0.00509316\n",
      "Iteration 15, loss = 0.00535779\n",
      "Iteration 16, loss = 0.00486166\n",
      "Iteration 17, loss = 0.00395880\n",
      "Iteration 18, loss = 0.00315400\n",
      "Iteration 19, loss = 0.00284396\n",
      "Iteration 20, loss = 0.00308321\n",
      "Iteration 21, loss = 0.00358472\n",
      "Iteration 22, loss = 0.00392761\n",
      "Iteration 23, loss = 0.00384711\n",
      "Iteration 24, loss = 0.00336074\n",
      "Iteration 25, loss = 0.00268121\n",
      "Iteration 26, loss = 0.00206228\n",
      "Iteration 27, loss = 0.00166930\n",
      "Iteration 28, loss = 0.00153191\n",
      "Iteration 29, loss = 0.00157192\n",
      "Iteration 30, loss = 0.00166359\n",
      "Iteration 31, loss = 0.00170232\n",
      "Iteration 32, loss = 0.00164234\n",
      "Iteration 33, loss = 0.00150057\n",
      "Iteration 34, loss = 0.00133387\n",
      "Iteration 35, loss = 0.00120652\n",
      "Iteration 36, loss = 0.00115820\n",
      "Iteration 37, loss = 0.00118919\n",
      "Iteration 38, loss = 0.00126459\n",
      "Iteration 39, loss = 0.00133429\n",
      "Iteration 40, loss = 0.00135695\n",
      "Iteration 41, loss = 0.00131744\n",
      "Iteration 42, loss = 0.00123020\n",
      "Iteration 43, loss = 0.00112747\n",
      "Iteration 44, loss = 0.00104180\n",
      "Iteration 45, loss = 0.00099085\n",
      "Iteration 46, loss = 0.00097201\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06724946\n",
      "Iteration 2, loss = 0.03416547\n",
      "Iteration 3, loss = 0.01811116\n",
      "Iteration 4, loss = 0.01621292\n",
      "Iteration 5, loss = 0.02035980\n",
      "Iteration 6, loss = 0.02217832\n",
      "Iteration 7, loss = 0.01949566\n",
      "Iteration 8, loss = 0.01417368\n",
      "Iteration 9, loss = 0.00875044\n",
      "Iteration 10, loss = 0.00486261\n",
      "Iteration 11, loss = 0.00313263\n",
      "Iteration 12, loss = 0.00321614\n",
      "Iteration 13, loss = 0.00419785\n",
      "Iteration 14, loss = 0.00513032\n",
      "Iteration 15, loss = 0.00540039\n",
      "Iteration 16, loss = 0.00490832\n",
      "Iteration 17, loss = 0.00400464\n",
      "Iteration 18, loss = 0.00319288\n",
      "Iteration 19, loss = 0.00287341\n",
      "Iteration 20, loss = 0.00310313\n",
      "Iteration 21, loss = 0.00359876\n",
      "Iteration 22, loss = 0.00394732\n",
      "Iteration 23, loss = 0.00388191\n",
      "Iteration 24, loss = 0.00340823\n",
      "Iteration 25, loss = 0.00273386\n",
      "Iteration 26, loss = 0.00211097\n",
      "Iteration 27, loss = 0.00170727\n",
      "Iteration 28, loss = 0.00155841\n",
      "Iteration 29, loss = 0.00159361\n",
      "Iteration 30, loss = 0.00168688\n",
      "Iteration 31, loss = 0.00173075\n",
      "Iteration 32, loss = 0.00167728\n",
      "Iteration 33, loss = 0.00154021\n",
      "Iteration 34, loss = 0.00137427\n",
      "Iteration 35, loss = 0.00124320\n",
      "Iteration 36, loss = 0.00118761\n",
      "Iteration 37, loss = 0.00121096\n",
      "Iteration 38, loss = 0.00128155\n",
      "Iteration 39, loss = 0.00135098\n",
      "Iteration 40, loss = 0.00137676\n",
      "Iteration 41, loss = 0.00134164\n",
      "Iteration 42, loss = 0.00125702\n",
      "Iteration 43, loss = 0.00115360\n",
      "Iteration 44, loss = 0.00106431\n",
      "Iteration 45, loss = 0.00100841\n",
      "Iteration 46, loss = 0.00098568\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06594365\n",
      "Iteration 2, loss = 0.03321207\n",
      "Iteration 3, loss = 0.01743105\n",
      "Iteration 4, loss = 0.01577227\n",
      "Iteration 5, loss = 0.02000849\n",
      "Iteration 6, loss = 0.02175575\n",
      "Iteration 7, loss = 0.01898847\n",
      "Iteration 8, loss = 0.01366932\n",
      "Iteration 9, loss = 0.00833413\n",
      "Iteration 10, loss = 0.00458314\n",
      "Iteration 11, loss = 0.00299652\n",
      "Iteration 12, loss = 0.00318099\n",
      "Iteration 13, loss = 0.00419639\n",
      "Iteration 14, loss = 0.00510398\n",
      "Iteration 15, loss = 0.00532157\n",
      "Iteration 16, loss = 0.00478447\n",
      "Iteration 17, loss = 0.00386583\n",
      "Iteration 18, loss = 0.00307348\n",
      "Iteration 19, loss = 0.00278843\n",
      "Iteration 20, loss = 0.00304656\n",
      "Iteration 21, loss = 0.00354541\n",
      "Iteration 22, loss = 0.00387347\n",
      "Iteration 23, loss = 0.00378090\n",
      "Iteration 24, loss = 0.00329033\n",
      "Iteration 25, loss = 0.00261716\n",
      "Iteration 26, loss = 0.00201094\n",
      "Iteration 27, loss = 0.00163007\n",
      "Iteration 28, loss = 0.00150383\n",
      "Iteration 29, loss = 0.00155208\n",
      "Iteration 30, loss = 0.00164886\n",
      "Iteration 31, loss = 0.00169206\n",
      "Iteration 32, loss = 0.00163640\n",
      "Iteration 33, loss = 0.00149663\n",
      "Iteration 34, loss = 0.00133218\n",
      "Iteration 35, loss = 0.00120638\n",
      "Iteration 36, loss = 0.00115740\n",
      "Iteration 37, loss = 0.00118571\n",
      "Iteration 38, loss = 0.00125813\n",
      "Iteration 39, loss = 0.00132622\n",
      "Iteration 40, loss = 0.00134889\n",
      "Iteration 41, loss = 0.00131056\n",
      "Iteration 42, loss = 0.00122448\n",
      "Iteration 43, loss = 0.00112230\n",
      "Iteration 44, loss = 0.00103619\n",
      "Iteration 45, loss = 0.00098403\n",
      "Iteration 46, loss = 0.00096438\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06563642\n",
      "Iteration 2, loss = 0.03309361\n",
      "Iteration 3, loss = 0.01747161\n",
      "Iteration 4, loss = 0.01589051\n",
      "Iteration 5, loss = 0.02012422\n",
      "Iteration 6, loss = 0.02184760\n",
      "Iteration 7, loss = 0.01906596\n",
      "Iteration 8, loss = 0.01375704\n",
      "Iteration 9, loss = 0.00844230\n",
      "Iteration 10, loss = 0.00471246\n",
      "Iteration 11, loss = 0.00313280\n",
      "Iteration 12, loss = 0.00330240\n",
      "Iteration 13, loss = 0.00428685\n",
      "Iteration 14, loss = 0.00516050\n",
      "Iteration 15, loss = 0.00535274\n",
      "Iteration 16, loss = 0.00479890\n",
      "Iteration 17, loss = 0.00387938\n",
      "Iteration 18, loss = 0.00310142\n",
      "Iteration 19, loss = 0.00284035\n",
      "Iteration 20, loss = 0.00312420\n",
      "Iteration 21, loss = 0.00363231\n",
      "Iteration 22, loss = 0.00394957\n",
      "Iteration 23, loss = 0.00383330\n",
      "Iteration 24, loss = 0.00332198\n",
      "Iteration 25, loss = 0.00264224\n",
      "Iteration 26, loss = 0.00204525\n",
      "Iteration 27, loss = 0.00168177\n",
      "Iteration 28, loss = 0.00156848\n",
      "Iteration 29, loss = 0.00162016\n",
      "Iteration 30, loss = 0.00170870\n",
      "Iteration 31, loss = 0.00173423\n",
      "Iteration 32, loss = 0.00165884\n",
      "Iteration 33, loss = 0.00150671\n",
      "Iteration 34, loss = 0.00133862\n",
      "Iteration 35, loss = 0.00121702\n",
      "Iteration 36, loss = 0.00117718\n",
      "Iteration 37, loss = 0.00121419\n",
      "Iteration 38, loss = 0.00129031\n",
      "Iteration 39, loss = 0.00135455\n",
      "Iteration 40, loss = 0.00136812\n",
      "Iteration 41, loss = 0.00132003\n",
      "Iteration 42, loss = 0.00122807\n",
      "Iteration 43, loss = 0.00112571\n",
      "Iteration 44, loss = 0.00104389\n",
      "Iteration 45, loss = 0.00099790\n",
      "Iteration 46, loss = 0.00098253\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06623721\n",
      "Iteration 2, loss = 0.03353020\n",
      "Iteration 3, loss = 0.01775978\n",
      "Iteration 4, loss = 0.01606533\n",
      "Iteration 5, loss = 0.02026290\n",
      "Iteration 6, loss = 0.02201477\n",
      "Iteration 7, loss = 0.01926299\n",
      "Iteration 8, loss = 0.01393999\n",
      "Iteration 9, loss = 0.00857880\n",
      "Iteration 10, loss = 0.00478447\n",
      "Iteration 11, loss = 0.00314361\n",
      "Iteration 12, loss = 0.00327672\n",
      "Iteration 13, loss = 0.00425460\n",
      "Iteration 14, loss = 0.00514175\n",
      "Iteration 15, loss = 0.00535260\n",
      "Iteration 16, loss = 0.00481220\n",
      "Iteration 17, loss = 0.00389200\n",
      "Iteration 18, loss = 0.00310113\n",
      "Iteration 19, loss = 0.00282322\n",
      "Iteration 20, loss = 0.00309878\n",
      "Iteration 21, loss = 0.00361341\n",
      "Iteration 22, loss = 0.00394710\n",
      "Iteration 23, loss = 0.00384937\n",
      "Iteration 24, loss = 0.00334933\n",
      "Iteration 25, loss = 0.00267001\n",
      "Iteration 26, loss = 0.00206370\n",
      "Iteration 27, loss = 0.00168710\n",
      "Iteration 28, loss = 0.00156368\n",
      "Iteration 29, loss = 0.00161183\n",
      "Iteration 30, loss = 0.00170198\n",
      "Iteration 31, loss = 0.00173254\n",
      "Iteration 32, loss = 0.00166204\n",
      "Iteration 33, loss = 0.00151232\n",
      "Iteration 34, loss = 0.00134202\n",
      "Iteration 35, loss = 0.00121488\n",
      "Iteration 36, loss = 0.00116901\n",
      "Iteration 37, loss = 0.00120231\n",
      "Iteration 38, loss = 0.00127806\n",
      "Iteration 39, loss = 0.00134569\n",
      "Iteration 40, loss = 0.00136470\n",
      "Iteration 41, loss = 0.00132194\n",
      "Iteration 42, loss = 0.00123312\n",
      "Iteration 43, loss = 0.00113090\n",
      "Iteration 44, loss = 0.00104683\n",
      "Iteration 45, loss = 0.00099805\n",
      "Iteration 46, loss = 0.00098118\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06642658\n",
      "Iteration 2, loss = 0.03364759\n",
      "Iteration 3, loss = 0.01782191\n",
      "Iteration 4, loss = 0.01609206\n",
      "Iteration 5, loss = 0.02028367\n",
      "Iteration 6, loss = 0.02204814\n",
      "Iteration 7, loss = 0.01930356\n",
      "Iteration 8, loss = 0.01397785\n",
      "Iteration 9, loss = 0.00860636\n",
      "Iteration 10, loss = 0.00479807\n",
      "Iteration 11, loss = 0.00314330\n",
      "Iteration 12, loss = 0.00326550\n",
      "Iteration 13, loss = 0.00423956\n",
      "Iteration 14, loss = 0.00513180\n",
      "Iteration 15, loss = 0.00535314\n",
      "Iteration 16, loss = 0.00482388\n",
      "Iteration 17, loss = 0.00391280\n",
      "Iteration 18, loss = 0.00312671\n",
      "Iteration 19, loss = 0.00284896\n",
      "Iteration 20, loss = 0.00311839\n",
      "Iteration 21, loss = 0.00362528\n",
      "Iteration 22, loss = 0.00395378\n",
      "Iteration 23, loss = 0.00385400\n",
      "Iteration 24, loss = 0.00335468\n",
      "Iteration 25, loss = 0.00267650\n",
      "Iteration 26, loss = 0.00206930\n",
      "Iteration 27, loss = 0.00168893\n",
      "Iteration 28, loss = 0.00156055\n",
      "Iteration 29, loss = 0.00160452\n",
      "Iteration 30, loss = 0.00169268\n",
      "Iteration 31, loss = 0.00172395\n",
      "Iteration 32, loss = 0.00165686\n",
      "Iteration 33, loss = 0.00151184\n",
      "Iteration 34, loss = 0.00134595\n",
      "Iteration 35, loss = 0.00122143\n",
      "Iteration 36, loss = 0.00117460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37, loss = 0.00120483\n",
      "Iteration 38, loss = 0.00127755\n",
      "Iteration 39, loss = 0.00134412\n",
      "Iteration 40, loss = 0.00136423\n",
      "Iteration 41, loss = 0.00132362\n",
      "Iteration 42, loss = 0.00123645\n",
      "Iteration 43, loss = 0.00113402\n",
      "Iteration 44, loss = 0.00104789\n",
      "Iteration 45, loss = 0.00099597\n",
      "Iteration 46, loss = 0.00097620\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06645779\n",
      "Iteration 2, loss = 0.03371394\n",
      "Iteration 3, loss = 0.01788762\n",
      "Iteration 4, loss = 0.01612478\n",
      "Iteration 5, loss = 0.02023412\n",
      "Iteration 6, loss = 0.02195577\n",
      "Iteration 7, loss = 0.01922014\n",
      "Iteration 8, loss = 0.01392754\n",
      "Iteration 9, loss = 0.00858042\n",
      "Iteration 10, loss = 0.00477782\n",
      "Iteration 11, loss = 0.00311751\n",
      "Iteration 12, loss = 0.00323279\n",
      "Iteration 13, loss = 0.00421256\n",
      "Iteration 14, loss = 0.00512475\n",
      "Iteration 15, loss = 0.00537145\n",
      "Iteration 16, loss = 0.00486787\n",
      "Iteration 17, loss = 0.00397397\n",
      "Iteration 18, loss = 0.00318570\n",
      "Iteration 19, loss = 0.00289084\n",
      "Iteration 20, loss = 0.00313004\n",
      "Iteration 21, loss = 0.00361449\n",
      "Iteration 22, loss = 0.00393712\n",
      "Iteration 23, loss = 0.00384879\n",
      "Iteration 24, loss = 0.00336631\n",
      "Iteration 25, loss = 0.00269790\n",
      "Iteration 26, loss = 0.00208798\n",
      "Iteration 27, loss = 0.00169534\n",
      "Iteration 28, loss = 0.00155523\n",
      "Iteration 29, loss = 0.00159162\n",
      "Iteration 30, loss = 0.00168206\n",
      "Iteration 31, loss = 0.00172300\n",
      "Iteration 32, loss = 0.00166971\n",
      "Iteration 33, loss = 0.00153692\n",
      "Iteration 34, loss = 0.00137681\n",
      "Iteration 35, loss = 0.00125063\n",
      "Iteration 36, loss = 0.00119687\n",
      "Iteration 37, loss = 0.00121836\n",
      "Iteration 38, loss = 0.00128494\n",
      "Iteration 39, loss = 0.00135007\n",
      "Iteration 40, loss = 0.00137343\n",
      "Iteration 41, loss = 0.00133818\n",
      "Iteration 42, loss = 0.00125464\n",
      "Iteration 43, loss = 0.00115181\n",
      "Iteration 44, loss = 0.00106136\n",
      "Iteration 45, loss = 0.00100316\n",
      "Iteration 46, loss = 0.00097818\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06714243\n",
      "Iteration 2, loss = 0.03417322\n",
      "Iteration 3, loss = 0.01813781\n",
      "Iteration 4, loss = 0.01622302\n",
      "Iteration 5, loss = 0.02032705\n",
      "Iteration 6, loss = 0.02211310\n",
      "Iteration 7, loss = 0.01942254\n",
      "Iteration 8, loss = 0.01410847\n",
      "Iteration 9, loss = 0.00870104\n",
      "Iteration 10, loss = 0.00482355\n",
      "Iteration 11, loss = 0.00310445\n",
      "Iteration 12, loss = 0.00319928\n",
      "Iteration 13, loss = 0.00419324\n",
      "Iteration 14, loss = 0.00513499\n",
      "Iteration 15, loss = 0.00541077\n",
      "Iteration 16, loss = 0.00492157\n",
      "Iteration 17, loss = 0.00401950\n",
      "Iteration 18, loss = 0.00320735\n",
      "Iteration 19, loss = 0.00288652\n",
      "Iteration 20, loss = 0.00311114\n",
      "Iteration 21, loss = 0.00360064\n",
      "Iteration 22, loss = 0.00394104\n",
      "Iteration 23, loss = 0.00386672\n",
      "Iteration 24, loss = 0.00338736\n",
      "Iteration 25, loss = 0.00270989\n",
      "Iteration 26, loss = 0.00208703\n",
      "Iteration 27, loss = 0.00168627\n",
      "Iteration 28, loss = 0.00154182\n",
      "Iteration 29, loss = 0.00157847\n",
      "Iteration 30, loss = 0.00167203\n",
      "Iteration 31, loss = 0.00171663\n",
      "Iteration 32, loss = 0.00166410\n",
      "Iteration 33, loss = 0.00152811\n",
      "Iteration 34, loss = 0.00136410\n",
      "Iteration 35, loss = 0.00123545\n",
      "Iteration 36, loss = 0.00118345\n",
      "Iteration 37, loss = 0.00120937\n",
      "Iteration 38, loss = 0.00128114\n",
      "Iteration 39, loss = 0.00134957\n",
      "Iteration 40, loss = 0.00137293\n",
      "Iteration 41, loss = 0.00133510\n",
      "Iteration 42, loss = 0.00124876\n",
      "Iteration 43, loss = 0.00114536\n",
      "Iteration 44, loss = 0.00105725\n",
      "Iteration 45, loss = 0.00100301\n",
      "Iteration 46, loss = 0.00098179\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06691191\n",
      "Iteration 2, loss = 0.03392148\n",
      "Iteration 3, loss = 0.01795272\n",
      "Iteration 4, loss = 0.01613875\n",
      "Iteration 5, loss = 0.02030668\n",
      "Iteration 6, loss = 0.02209194\n",
      "Iteration 7, loss = 0.01937519\n",
      "Iteration 8, loss = 0.01404927\n",
      "Iteration 9, loss = 0.00865345\n",
      "Iteration 10, loss = 0.00481028\n",
      "Iteration 11, loss = 0.00312658\n",
      "Iteration 12, loss = 0.00324135\n",
      "Iteration 13, loss = 0.00423158\n",
      "Iteration 14, loss = 0.00515449\n",
      "Iteration 15, loss = 0.00540714\n",
      "Iteration 16, loss = 0.00490127\n",
      "Iteration 17, loss = 0.00399340\n",
      "Iteration 18, loss = 0.00318794\n",
      "Iteration 19, loss = 0.00287696\n",
      "Iteration 20, loss = 0.00311159\n",
      "Iteration 21, loss = 0.00360537\n",
      "Iteration 22, loss = 0.00394718\n",
      "Iteration 23, loss = 0.00387562\n",
      "Iteration 24, loss = 0.00339943\n",
      "Iteration 25, loss = 0.00272634\n",
      "Iteration 26, loss = 0.00210686\n",
      "Iteration 27, loss = 0.00170593\n",
      "Iteration 28, loss = 0.00156050\n",
      "Iteration 29, loss = 0.00159792\n",
      "Iteration 30, loss = 0.00169164\n",
      "Iteration 31, loss = 0.00173656\n",
      "Iteration 32, loss = 0.00168492\n",
      "Iteration 33, loss = 0.00154912\n",
      "Iteration 34, loss = 0.00138341\n",
      "Iteration 35, loss = 0.00125243\n",
      "Iteration 36, loss = 0.00119647\n",
      "Iteration 37, loss = 0.00121814\n",
      "Iteration 38, loss = 0.00128720\n",
      "Iteration 39, loss = 0.00135551\n",
      "Iteration 40, loss = 0.00138138\n",
      "Iteration 41, loss = 0.00134722\n",
      "Iteration 42, loss = 0.00126367\n",
      "Iteration 43, loss = 0.00116067\n",
      "Iteration 44, loss = 0.00107040\n",
      "Iteration 45, loss = 0.00101284\n",
      "Iteration 46, loss = 0.00098860\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06578212\n",
      "Iteration 2, loss = 0.03311889\n",
      "Iteration 3, loss = 0.01738906\n",
      "Iteration 4, loss = 0.01575719\n",
      "Iteration 5, loss = 0.02001081\n",
      "Iteration 6, loss = 0.02176697\n",
      "Iteration 7, loss = 0.01900393\n",
      "Iteration 8, loss = 0.01368706\n",
      "Iteration 9, loss = 0.00835942\n",
      "Iteration 10, loss = 0.00462152\n",
      "Iteration 11, loss = 0.00304696\n",
      "Iteration 12, loss = 0.00323725\n",
      "Iteration 13, loss = 0.00424911\n",
      "Iteration 14, loss = 0.00514799\n",
      "Iteration 15, loss = 0.00535592\n",
      "Iteration 16, loss = 0.00480800\n",
      "Iteration 17, loss = 0.00388095\n",
      "Iteration 18, loss = 0.00308490\n",
      "Iteration 19, loss = 0.00280166\n",
      "Iteration 20, loss = 0.00306600\n",
      "Iteration 21, loss = 0.00357056\n",
      "Iteration 22, loss = 0.00390020\n",
      "Iteration 23, loss = 0.00380564\n",
      "Iteration 24, loss = 0.00331205\n",
      "Iteration 25, loss = 0.00263767\n",
      "Iteration 26, loss = 0.00203228\n",
      "Iteration 27, loss = 0.00165348\n",
      "Iteration 28, loss = 0.00152947\n",
      "Iteration 29, loss = 0.00157886\n",
      "Iteration 30, loss = 0.00167485\n",
      "Iteration 31, loss = 0.00171491\n",
      "Iteration 32, loss = 0.00165540\n",
      "Iteration 33, loss = 0.00151360\n",
      "Iteration 34, loss = 0.00134658\n",
      "Iteration 35, loss = 0.00121883\n",
      "Iteration 36, loss = 0.00116892\n",
      "Iteration 37, loss = 0.00119647\n",
      "Iteration 38, loss = 0.00126823\n",
      "Iteration 39, loss = 0.00133544\n",
      "Iteration 40, loss = 0.00135719\n",
      "Iteration 41, loss = 0.00131807\n",
      "Iteration 42, loss = 0.00123167\n",
      "Iteration 43, loss = 0.00112936\n",
      "Iteration 44, loss = 0.00104300\n",
      "Iteration 45, loss = 0.00099071\n",
      "Iteration 46, loss = 0.00097097\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06598716\n",
      "Iteration 2, loss = 0.03334129\n",
      "Iteration 3, loss = 0.01762270\n",
      "Iteration 4, loss = 0.01598070\n",
      "Iteration 5, loss = 0.02021346\n",
      "Iteration 6, loss = 0.02196915\n",
      "Iteration 7, loss = 0.01920754\n",
      "Iteration 8, loss = 0.01388764\n",
      "Iteration 9, loss = 0.00854297\n",
      "Iteration 10, loss = 0.00477337\n",
      "Iteration 11, loss = 0.00315537\n",
      "Iteration 12, loss = 0.00329744\n",
      "Iteration 13, loss = 0.00426891\n",
      "Iteration 14, loss = 0.00514110\n",
      "Iteration 15, loss = 0.00533633\n",
      "Iteration 16, loss = 0.00478575\n",
      "Iteration 17, loss = 0.00386631\n",
      "Iteration 18, loss = 0.00308755\n",
      "Iteration 19, loss = 0.00282705\n",
      "Iteration 20, loss = 0.00311441\n",
      "Iteration 21, loss = 0.00362899\n",
      "Iteration 22, loss = 0.00395241\n",
      "Iteration 23, loss = 0.00384125\n",
      "Iteration 24, loss = 0.00333356\n",
      "Iteration 25, loss = 0.00265481\n",
      "Iteration 26, loss = 0.00205563\n",
      "Iteration 27, loss = 0.00168796\n",
      "Iteration 28, loss = 0.00157067\n",
      "Iteration 29, loss = 0.00162001\n",
      "Iteration 30, loss = 0.00170683\n",
      "Iteration 31, loss = 0.00173190\n",
      "Iteration 32, loss = 0.00165667\n",
      "Iteration 33, loss = 0.00150474\n",
      "Iteration 34, loss = 0.00133538\n",
      "Iteration 35, loss = 0.00121143\n",
      "Iteration 36, loss = 0.00116930\n",
      "Iteration 37, loss = 0.00120498\n",
      "Iteration 38, loss = 0.00128089\n",
      "Iteration 39, loss = 0.00134661\n",
      "Iteration 40, loss = 0.00136290\n",
      "Iteration 41, loss = 0.00131784\n",
      "Iteration 42, loss = 0.00122809\n",
      "Iteration 43, loss = 0.00112638\n",
      "Iteration 44, loss = 0.00104394\n",
      "Iteration 45, loss = 0.00099679\n",
      "Iteration 46, loss = 0.00098076\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06689903\n",
      "Iteration 2, loss = 0.03397535\n",
      "Iteration 3, loss = 0.01802044\n",
      "Iteration 4, loss = 0.01619206\n",
      "Iteration 5, loss = 0.02035866\n",
      "Iteration 6, loss = 0.02214845\n",
      "Iteration 7, loss = 0.01942969\n",
      "Iteration 8, loss = 0.01409412\n",
      "Iteration 9, loss = 0.00868339\n",
      "Iteration 10, loss = 0.00481927\n",
      "Iteration 11, loss = 0.00310871\n",
      "Iteration 12, loss = 0.00319405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.00415715\n",
      "Iteration 14, loss = 0.00505940\n",
      "Iteration 15, loss = 0.00529886\n",
      "Iteration 16, loss = 0.00478497\n",
      "Iteration 17, loss = 0.00387856\n",
      "Iteration 18, loss = 0.00308613\n",
      "Iteration 19, loss = 0.00279966\n",
      "Iteration 20, loss = 0.00306448\n",
      "Iteration 21, loss = 0.00357767\n",
      "Iteration 22, loss = 0.00392060\n",
      "Iteration 23, loss = 0.00383520\n",
      "Iteration 24, loss = 0.00334365\n",
      "Iteration 25, loss = 0.00266562\n",
      "Iteration 26, loss = 0.00205263\n",
      "Iteration 27, loss = 0.00166516\n",
      "Iteration 28, loss = 0.00153107\n",
      "Iteration 29, loss = 0.00157290\n",
      "Iteration 30, loss = 0.00166239\n",
      "Iteration 31, loss = 0.00169674\n",
      "Iteration 32, loss = 0.00163222\n",
      "Iteration 33, loss = 0.00148825\n",
      "Iteration 34, loss = 0.00132139\n",
      "Iteration 35, loss = 0.00119404\n",
      "Iteration 36, loss = 0.00114538\n",
      "Iteration 37, loss = 0.00117534\n",
      "Iteration 38, loss = 0.00124964\n",
      "Iteration 39, loss = 0.00131911\n",
      "Iteration 40, loss = 0.00134297\n",
      "Iteration 41, loss = 0.00130575\n",
      "Iteration 42, loss = 0.00122066\n",
      "Iteration 43, loss = 0.00111904\n",
      "Iteration 44, loss = 0.00103290\n",
      "Iteration 45, loss = 0.00098050\n",
      "Iteration 46, loss = 0.00096045\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06323934\n",
      "Iteration 2, loss = 0.03105196\n",
      "Iteration 3, loss = 0.01576895\n",
      "Iteration 4, loss = 0.01452095\n",
      "Iteration 5, loss = 0.01895859\n",
      "Iteration 6, loss = 0.02072283\n",
      "Iteration 7, loss = 0.01794614\n",
      "Iteration 8, loss = 0.01269069\n",
      "Iteration 9, loss = 0.00750569\n",
      "Iteration 10, loss = 0.00396310\n",
      "Iteration 11, loss = 0.00258132\n",
      "Iteration 12, loss = 0.00290635\n",
      "Iteration 13, loss = 0.00396698\n",
      "Iteration 14, loss = 0.00483789\n",
      "Iteration 15, loss = 0.00498290\n",
      "Iteration 16, loss = 0.00437223\n",
      "Iteration 17, loss = 0.00341377\n",
      "Iteration 18, loss = 0.00262301\n",
      "Iteration 19, loss = 0.00236393\n",
      "Iteration 20, loss = 0.00265248\n",
      "Iteration 21, loss = 0.00316448\n",
      "Iteration 22, loss = 0.00348736\n",
      "Iteration 23, loss = 0.00338003\n",
      "Iteration 24, loss = 0.00288182\n",
      "Iteration 25, loss = 0.00221877\n",
      "Iteration 26, loss = 0.00164094\n",
      "Iteration 27, loss = 0.00129769\n",
      "Iteration 28, loss = 0.00120495\n",
      "Iteration 29, loss = 0.00127486\n",
      "Iteration 30, loss = 0.00138041\n",
      "Iteration 31, loss = 0.00142210\n",
      "Iteration 32, loss = 0.00136008\n",
      "Iteration 33, loss = 0.00121769\n",
      "Iteration 34, loss = 0.00105454\n",
      "Iteration 35, loss = 0.00093357\n",
      "Iteration 36, loss = 0.00089268\n",
      "Iteration 37, loss = 0.00093042\n",
      "Iteration 38, loss = 0.00100987\n",
      "Iteration 39, loss = 0.00108054\n",
      "Iteration 40, loss = 0.00110226\n",
      "Iteration 41, loss = 0.00106184\n",
      "Iteration 42, loss = 0.00097603\n",
      "Iteration 43, loss = 0.00087881\n",
      "Iteration 44, loss = 0.00080197\n",
      "Iteration 45, loss = 0.00076109\n",
      "Iteration 46, loss = 0.00075233\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06543072\n",
      "Iteration 2, loss = 0.03308628\n",
      "Iteration 3, loss = 0.01757813\n",
      "Iteration 4, loss = 0.01602525\n",
      "Iteration 5, loss = 0.02020717\n",
      "Iteration 6, loss = 0.02189587\n",
      "Iteration 7, loss = 0.01910830\n",
      "Iteration 8, loss = 0.01380117\n",
      "Iteration 9, loss = 0.00848405\n",
      "Iteration 10, loss = 0.00473397\n",
      "Iteration 11, loss = 0.00311120\n",
      "Iteration 12, loss = 0.00322548\n",
      "Iteration 13, loss = 0.00415409\n",
      "Iteration 14, loss = 0.00498184\n",
      "Iteration 15, loss = 0.00514317\n",
      "Iteration 16, loss = 0.00457858\n",
      "Iteration 17, loss = 0.00366498\n",
      "Iteration 18, loss = 0.00291643\n",
      "Iteration 19, loss = 0.00270035\n",
      "Iteration 20, loss = 0.00303225\n",
      "Iteration 21, loss = 0.00356734\n",
      "Iteration 22, loss = 0.00388218\n",
      "Iteration 23, loss = 0.00374601\n",
      "Iteration 24, loss = 0.00321897\n",
      "Iteration 25, loss = 0.00254073\n",
      "Iteration 26, loss = 0.00196085\n",
      "Iteration 27, loss = 0.00162157\n",
      "Iteration 28, loss = 0.00152787\n",
      "Iteration 29, loss = 0.00158633\n",
      "Iteration 30, loss = 0.00166582\n",
      "Iteration 31, loss = 0.00167141\n",
      "Iteration 32, loss = 0.00157464\n",
      "Iteration 33, loss = 0.00141007\n",
      "Iteration 34, loss = 0.00124165\n",
      "Iteration 35, loss = 0.00113015\n",
      "Iteration 36, loss = 0.00110610\n",
      "Iteration 37, loss = 0.00115836\n",
      "Iteration 38, loss = 0.00124273\n",
      "Iteration 39, loss = 0.00130748\n",
      "Iteration 40, loss = 0.00131646\n",
      "Iteration 41, loss = 0.00126399\n",
      "Iteration 42, loss = 0.00117222\n",
      "Iteration 43, loss = 0.00107666\n",
      "Iteration 44, loss = 0.00100694\n",
      "Iteration 45, loss = 0.00097347\n",
      "Iteration 46, loss = 0.00096670\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Config of the regressors and cross val cross val leave one out\n",
    "####\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes = (50,), alpha = 0.001,\n",
    "    learning_rate_init = 0.01, max_iter = 1000,\n",
    "    random_state = 9, tol = 0.0001, verbose = True)\n",
    "svr = SVR(kernel = 'linear', C = 0.25, epsilon = 0.01, verbose = True, max_iter = 1000)\n",
    "\n",
    "full_predict_mlp = cross_val_predict(mlp, X, target, cv = loo)\n",
    "full_predict_svr = cross_val_predict(svr, X, target, cv = loo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error in MLP: 0.002362236727882373\n",
      "Mean Squared Error in SVR: 0.00019982562195105142\n",
      "R² score in MLP: 0.9444229173631248\n",
      "R² score in SVR: 0.9952986400672492\n",
      "adjusted R² score in MLP: 0.9424201396104446\n",
      "adjusted R² score in SVR: 0.9951292216912943\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Printing some metrics of the regressors\n",
    "####\n",
    "\n",
    "print('Mean Squared Error in MLP: %s' %(metrics.mean_squared_error(target, full_predict_mlp)))\n",
    "print('Mean Squared Error in SVR: %s' %(metrics.mean_squared_error(target, full_predict_svr)))\n",
    "\n",
    "r_squared_mlp = metrics.r2_score(target, full_predict_mlp)\n",
    "r_squared_svr = metrics.r2_score(target, full_predict_svr)\n",
    "\n",
    "print('R² score in MLP: %s' %(r_squared_mlp))\n",
    "print('R² score in SVR: %s' %(r_squared_svr))\n",
    "\n",
    "adjusted_r_squared_mlp = 1 - (1 - r_squared_mlp) * (len(target) - 1) / (len(target) - X.shape[1] - 1)\n",
    "adjusted_r_squared_svr = 1 - (1 - r_squared_svr) * (len(target) - 1) / (len(target) - X.shape[1] - 1)\n",
    "\n",
    "print('adjusted R² score in MLP: %s' %(adjusted_r_squared_mlp))\n",
    "print('adjusted R² score in SVR: %s' %(adjusted_r_squared_svr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117, 1)\n",
      "(117, 1)\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Filling lists with NaN so the len is the same across all lists \n",
    "## so that a graph can be generated\n",
    "####\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "values_to_add = list()\n",
    "for i in range(0, window_size):\n",
    "    values_to_add.append(float('NaN'))\n",
    "    \n",
    "full_predict_svr = np.insert(full_predict_svr, 0, values_to_add)\n",
    "full_predict_svr.shape = (len(full_predict_svr), 1)\n",
    "    \n",
    "full_predict_mlp = np.insert(full_predict_mlp, 0, values_to_add)\n",
    "full_predict_mlp.shape = (len(full_predict_mlp), 1)\n",
    "\n",
    "print(full_predict_svr.shape)\n",
    "print(full_predict_mlp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Casos</th>\n",
       "      <th>Taxa</th>\n",
       "      <th>CasosNormalizados</th>\n",
       "      <th>TaxaNormalizadas</th>\n",
       "      <th>Predict_mlp</th>\n",
       "      <th>Predict_svr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26/2/20</th>\n",
       "      <td>1</td>\n",
       "      <td>24.7</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>27.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074667</td>\n",
       "      <td>0.026539</td>\n",
       "      <td>0.009819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050667</td>\n",
       "      <td>0.025051</td>\n",
       "      <td>0.008534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29/2/20</th>\n",
       "      <td>0</td>\n",
       "      <td>31.4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178667</td>\n",
       "      <td>0.012591</td>\n",
       "      <td>0.010370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/3/20</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.461333</td>\n",
       "      <td>-0.000368</td>\n",
       "      <td>0.011176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17/6/20</th>\n",
       "      <td>34918</td>\n",
       "      <td>37.3</td>\n",
       "      <td>0.637527</td>\n",
       "      <td>0.336000</td>\n",
       "      <td>0.522379</td>\n",
       "      <td>0.597666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18/6/20</th>\n",
       "      <td>32188</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0.587683</td>\n",
       "      <td>0.368000</td>\n",
       "      <td>0.585322</td>\n",
       "      <td>0.584533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19/6/20</th>\n",
       "      <td>22765</td>\n",
       "      <td>34.7</td>\n",
       "      <td>0.415640</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.492881</td>\n",
       "      <td>0.429907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20/6/20</th>\n",
       "      <td>54771</td>\n",
       "      <td>39.1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>0.660831</td>\n",
       "      <td>0.906418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21/6/20</th>\n",
       "      <td>34666</td>\n",
       "      <td>47.3</td>\n",
       "      <td>0.632926</td>\n",
       "      <td>0.602667</td>\n",
       "      <td>0.730449</td>\n",
       "      <td>0.672471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Casos  Taxa  CasosNormalizados  TaxaNormalizadas  Predict_mlp  \\\n",
       "Data                                                                     \n",
       "26/2/20      1  24.7           0.000018          0.000000          NaN   \n",
       "27/2/20      0  27.5           0.000000          0.074667     0.026539   \n",
       "28/2/20      0  26.6           0.000000          0.050667     0.025051   \n",
       "29/2/20      0  31.4           0.000000          0.178667     0.012591   \n",
       "1/3/20       1    42           0.000018          0.461333    -0.000368   \n",
       "...        ...   ...                ...               ...          ...   \n",
       "17/6/20  34918  37.3           0.637527          0.336000     0.522379   \n",
       "18/6/20  32188  38.5           0.587683          0.368000     0.585322   \n",
       "19/6/20  22765  34.7           0.415640          0.266667     0.492881   \n",
       "20/6/20  54771  39.1           1.000000          0.384000     0.660831   \n",
       "21/6/20  34666  47.3           0.632926          0.602667     0.730449   \n",
       "\n",
       "         Predict_svr  \n",
       "Data                  \n",
       "26/2/20          NaN  \n",
       "27/2/20     0.009819  \n",
       "28/2/20     0.008534  \n",
       "29/2/20     0.010370  \n",
       "1/3/20      0.011176  \n",
       "...              ...  \n",
       "17/6/20     0.597666  \n",
       "18/6/20     0.584533  \n",
       "19/6/20     0.429907  \n",
       "20/6/20     0.906418  \n",
       "21/6/20     0.672471  \n",
       "\n",
       "[117 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####\n",
    "## Adding the data to plot \n",
    "####\n",
    "\n",
    "data['Predict_mlp'] = full_predict_mlp\n",
    "data['Predict_svr'] = full_predict_svr\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "####\n",
    "## Ploting the graph\n",
    "####\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "data.plot(y=['CasosNormalizados', 'Predict_mlp', 'Predict_svr', 'TaxaNormalizadas'], style=['-s', '--o'])\n",
    "#data.plot(y=['CasosNormalizados', 'Predict_mlp', 'Predict_svr',], style=['-s', '--o'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.03892879028152567, 0.6768859337467233)\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "## Calculating Pearson correlation \n",
    "####\n",
    "\n",
    "import numpy\n",
    "from scipy.stats import pearsonr\n",
    "arrTaxa = data['TaxaNormalizadas'].values.copy()\n",
    "arrCasos = data['CasosNormalizados'].values.copy()\n",
    "print(pearsonr(arrTaxa, arrCasos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
